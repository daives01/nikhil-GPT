{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 225,094 tokens\n",
      "val has 25,247 tokens\n"
     ]
    }
   ],
   "source": [
    "! python3 ./data/prepare.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we had to tokenize our data. We used the GPT2 tokenizer rather than a simple one to hopefully get a better result. Let's start by training a small model, these parameters are taken from the nanoGPT repo for building a small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_config.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "eval_interval = 100 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False \n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'nikhil-gpt'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'medium'\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 20\n",
      "Overriding: log_interval = 1\n",
      "Overriding: block_size = 64\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 2000\n",
      "Overriding: lr_decay_iters = 2000\n",
      "Overriding: dropout = 0.0\n",
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "using fused AdamW: False\n",
      "step 0: train loss 10.8502, val loss 10.8463\n",
      "iter 0: loss 10.8636, time 1017.91ms, mfu -100.00%\n",
      "iter 1: loss 10.8582, time 376.08ms, mfu -100.00%\n",
      "iter 2: loss 10.8367, time 376.80ms, mfu -100.00%\n",
      "iter 3: loss 10.8351, time 372.70ms, mfu -100.00%\n",
      "iter 4: loss 10.8162, time 376.87ms, mfu -100.00%\n",
      "iter 5: loss 10.7756, time 366.81ms, mfu 1.17%\n",
      "iter 6: loss 10.7320, time 369.31ms, mfu 1.17%\n",
      "iter 7: loss 10.7089, time 368.61ms, mfu 1.17%\n",
      "iter 8: loss 10.6398, time 367.96ms, mfu 1.17%\n",
      "iter 9: loss 10.6214, time 365.65ms, mfu 1.17%\n",
      "iter 10: loss 10.5922, time 365.90ms, mfu 1.17%\n",
      "iter 11: loss 10.5196, time 367.01ms, mfu 1.17%\n",
      "iter 12: loss 10.5111, time 368.11ms, mfu 1.17%\n",
      "iter 13: loss 10.4722, time 369.43ms, mfu 1.17%\n",
      "iter 14: loss 10.4326, time 368.00ms, mfu 1.17%\n",
      "iter 15: loss 10.4437, time 375.37ms, mfu 1.17%\n",
      "iter 16: loss 10.3831, time 367.76ms, mfu 1.17%\n",
      "iter 17: loss 10.3241, time 379.13ms, mfu 1.17%\n",
      "iter 18: loss 10.3032, time 369.68ms, mfu 1.17%\n",
      "iter 19: loss 10.2599, time 367.68ms, mfu 1.17%\n",
      "iter 20: loss 10.2380, time 366.15ms, mfu 1.17%\n",
      "iter 21: loss 10.2154, time 369.72ms, mfu 1.17%\n",
      "iter 22: loss 10.1318, time 367.53ms, mfu 1.17%\n",
      "iter 23: loss 10.0988, time 367.13ms, mfu 1.17%\n",
      "iter 24: loss 10.0623, time 368.26ms, mfu 1.17%\n",
      "iter 25: loss 9.9999, time 367.96ms, mfu 1.17%\n",
      "iter 26: loss 9.9436, time 368.21ms, mfu 1.17%\n",
      "iter 27: loss 9.8480, time 367.02ms, mfu 1.17%\n",
      "iter 28: loss 9.8130, time 368.94ms, mfu 1.17%\n",
      "iter 29: loss 9.7481, time 376.06ms, mfu 1.17%\n",
      "iter 30: loss 9.6830, time 372.97ms, mfu 1.17%\n",
      "iter 31: loss 9.6215, time 380.25ms, mfu 1.16%\n",
      "iter 32: loss 9.4881, time 366.46ms, mfu 1.16%\n",
      "iter 33: loss 9.4415, time 368.02ms, mfu 1.16%\n",
      "iter 34: loss 9.3670, time 369.17ms, mfu 1.16%\n",
      "iter 35: loss 9.2785, time 367.58ms, mfu 1.17%\n",
      "iter 36: loss 9.1884, time 368.09ms, mfu 1.17%\n",
      "iter 37: loss 9.1356, time 367.72ms, mfu 1.17%\n",
      "iter 38: loss 9.0438, time 366.58ms, mfu 1.17%\n",
      "iter 39: loss 8.9727, time 368.53ms, mfu 1.17%\n",
      "iter 40: loss 8.8481, time 368.63ms, mfu 1.17%\n",
      "iter 41: loss 8.7469, time 367.04ms, mfu 1.17%\n",
      "iter 42: loss 8.7093, time 379.94ms, mfu 1.16%\n",
      "iter 43: loss 8.6051, time 368.50ms, mfu 1.17%\n",
      "iter 44: loss 8.5688, time 376.65ms, mfu 1.16%\n",
      "iter 45: loss 8.3865, time 366.87ms, mfu 1.16%\n",
      "iter 46: loss 8.3183, time 366.22ms, mfu 1.17%\n",
      "iter 47: loss 8.2152, time 368.31ms, mfu 1.17%\n",
      "iter 48: loss 8.2144, time 372.11ms, mfu 1.17%\n",
      "iter 49: loss 8.0437, time 366.76ms, mfu 1.17%\n",
      "iter 50: loss 8.0484, time 366.16ms, mfu 1.17%\n",
      "iter 51: loss 7.8847, time 368.21ms, mfu 1.17%\n",
      "iter 52: loss 7.7358, time 371.33ms, mfu 1.17%\n",
      "iter 53: loss 7.7818, time 369.95ms, mfu 1.17%\n",
      "iter 54: loss 7.5642, time 367.24ms, mfu 1.17%\n",
      "iter 55: loss 7.4735, time 373.04ms, mfu 1.17%\n",
      "iter 56: loss 7.4249, time 373.22ms, mfu 1.16%\n",
      "iter 57: loss 7.3444, time 368.75ms, mfu 1.17%\n",
      "iter 58: loss 7.2468, time 375.78ms, mfu 1.16%\n",
      "iter 59: loss 7.1916, time 367.08ms, mfu 1.16%\n",
      "iter 60: loss 7.1404, time 368.38ms, mfu 1.16%\n",
      "iter 61: loss 7.0099, time 366.69ms, mfu 1.17%\n",
      "iter 62: loss 6.9705, time 367.90ms, mfu 1.17%\n",
      "iter 63: loss 6.9086, time 365.60ms, mfu 1.17%\n",
      "iter 64: loss 6.8124, time 366.44ms, mfu 1.17%\n",
      "iter 65: loss 6.8218, time 368.16ms, mfu 1.17%\n",
      "iter 66: loss 6.7445, time 374.70ms, mfu 1.17%\n",
      "iter 67: loss 6.5525, time 385.25ms, mfu 1.16%\n",
      "iter 68: loss 6.4289, time 384.70ms, mfu 1.16%\n",
      "iter 69: loss 6.4457, time 392.80ms, mfu 1.15%\n",
      "iter 70: loss 6.4960, time 388.81ms, mfu 1.15%\n",
      "iter 71: loss 6.3459, time 372.59ms, mfu 1.15%\n",
      "iter 72: loss 6.3601, time 370.07ms, mfu 1.15%\n",
      "iter 73: loss 6.2553, time 366.33ms, mfu 1.15%\n",
      "iter 74: loss 6.0537, time 368.08ms, mfu 1.15%\n",
      "iter 75: loss 6.2544, time 367.12ms, mfu 1.16%\n",
      "iter 76: loss 6.0817, time 367.87ms, mfu 1.16%\n",
      "iter 77: loss 6.1840, time 370.82ms, mfu 1.16%\n",
      "iter 78: loss 5.8844, time 367.02ms, mfu 1.16%\n",
      "iter 79: loss 6.0105, time 370.77ms, mfu 1.16%\n",
      "iter 80: loss 5.9473, time 368.84ms, mfu 1.16%\n",
      "iter 81: loss 5.9708, time 366.97ms, mfu 1.16%\n",
      "iter 82: loss 5.7501, time 378.63ms, mfu 1.16%\n",
      "iter 83: loss 5.8767, time 368.97ms, mfu 1.16%\n",
      "iter 84: loss 5.7877, time 373.54ms, mfu 1.16%\n",
      "iter 85: loss 5.5026, time 369.48ms, mfu 1.16%\n",
      "iter 86: loss 5.9141, time 365.22ms, mfu 1.16%\n",
      "iter 87: loss 5.6938, time 366.68ms, mfu 1.16%\n",
      "iter 88: loss 5.5965, time 366.70ms, mfu 1.16%\n",
      "iter 89: loss 5.7085, time 371.66ms, mfu 1.16%\n",
      "iter 90: loss 5.5678, time 366.93ms, mfu 1.17%\n",
      "iter 91: loss 5.7412, time 367.18ms, mfu 1.17%\n",
      "iter 92: loss 5.6959, time 365.87ms, mfu 1.17%\n",
      "iter 93: loss 5.4179, time 372.60ms, mfu 1.17%\n",
      "iter 94: loss 5.3793, time 368.23ms, mfu 1.17%\n",
      "iter 95: loss 5.5860, time 367.59ms, mfu 1.17%\n",
      "iter 96: loss 5.7250, time 374.22ms, mfu 1.17%\n",
      "iter 97: loss 5.5389, time 365.31ms, mfu 1.17%\n",
      "iter 98: loss 5.3763, time 372.55ms, mfu 1.17%\n",
      "iter 99: loss 5.3536, time 368.39ms, mfu 1.17%\n",
      "step 100: train loss 5.4010, val loss 5.4406\n",
      "saving checkpoint to out\n",
      "iter 100: loss 5.3186, time 1785.94ms, mfu 1.07%\n",
      "iter 101: loss 5.4432, time 369.07ms, mfu 1.08%\n",
      "iter 102: loss 5.5121, time 366.43ms, mfu 1.09%\n",
      "iter 103: loss 5.3036, time 378.37ms, mfu 1.10%\n",
      "iter 104: loss 5.1665, time 371.78ms, mfu 1.10%\n",
      "iter 105: loss 5.2166, time 389.10ms, mfu 1.10%\n",
      "iter 106: loss 5.2530, time 379.64ms, mfu 1.11%\n",
      "iter 107: loss 5.2128, time 370.54ms, mfu 1.11%\n",
      "iter 108: loss 5.2659, time 379.91ms, mfu 1.11%\n",
      "iter 109: loss 5.1234, time 366.36ms, mfu 1.12%\n",
      "iter 110: loss 5.2408, time 367.33ms, mfu 1.13%\n",
      "iter 111: loss 5.1091, time 373.89ms, mfu 1.13%\n",
      "iter 112: loss 5.0916, time 367.68ms, mfu 1.13%\n",
      "iter 113: loss 5.3001, time 372.01ms, mfu 1.14%\n",
      "iter 114: loss 5.1823, time 366.51ms, mfu 1.14%\n",
      "iter 115: loss 4.8461, time 366.94ms, mfu 1.14%\n",
      "iter 116: loss 5.2124, time 368.72ms, mfu 1.15%\n",
      "iter 117: loss 4.7976, time 368.45ms, mfu 1.15%\n",
      "iter 118: loss 4.9360, time 381.98ms, mfu 1.15%\n",
      "iter 119: loss 4.8947, time 379.09ms, mfu 1.14%\n",
      "iter 120: loss 4.9953, time 370.14ms, mfu 1.15%\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/s/bach/l/under/daives/cs445/assignments/nikhil-GPT/nanoGPT/train.py\", line 293, in <module>\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py config/train_config.py --out_dir=out-nikhil-gpt-first-try --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained until the loss no longer got better, about 400 iterations. We got to a loss of 3.21, which isn't very good. let's generate 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-nikhil-gpt-first-try\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      " from any 10, but I'm trying to be doing so if I want that gradient descent shape to cancel out as the exact or is basically just looking at this, because I'll do the amount of values. And I'm moving in the output tasks. So now I see 100 minutes, I'll have to take a lot of samples, and then I can do I can just take the sum of zero for that by one. So let's create the weight matrix of X and right so now we have all my velocity is just trying to deal with this validation or 1. So more what I'm not going to do is this is basically going to be some single one and then i would be my hidden layer to find my only. I'm going to see that I'll go. So I see how to see if my X is the outputs are the n by my inputs. So we, this should be doing Q layer, the blue2, my inputs, the input is 0. So there was a single value that I'm going to be written with respect to be. So of this for each time, if I take the predicted function, for each of the normal distribution, I'm going to change the network. Right, and then that kind of a bunch of values of feature, I don't want to have to take like some square it to be k. So now I'll just have two operations like a single value of training training data. So if I'll multiply them of the training data to get to all my weight. So I take the test and the test sets of those values and I'm doing everything in a different classes, I'm just going to take like negative output. I'm just getting no different outcomes. So I'm getting something like this, you know, kind of say, this is what's a couple of different features. So then I'm trying to scale, I'm going to say, I can try as a sort of this to take a bunch of steps to measure of samples in a 10ative people. So now I can see why have a one thing an array of these derivatives and I'm just got in a weight. Okay, and I'm not going to have patches. So I'm looking at this error now I'm going to give me to keep a single column of inputs. And I'm going to be doing our weights. I'm just going to do this into some weights. And I have the value for each column, I\n",
      "---------------\n",
      "\n",
      " in the following values, but we're just try times Q function. Okay, and use the gradient over neural network, then the gradient descent. And so now we'll see if you know, and what the first weight is the same function that the current state. This is the value of the error. Of course, assuming that I have, then I'm actually going to activate on the negative unit network numbers just going to do a specific samples. I'm going to compute my probabilities. So a linear regression problems. So, now I'm going to expect the model, for more with the output output layer. So now I'm doing back to this two classes and this is going to be the values and then I were to do one. So. So I can try and then I can try to be fit to the sample to other step to to the activation function function. And so these are the gradient descent with an argument that is, this is going to be a single feature map. Right. So now so if it? Yeah, we're going to be from the output value for all of a whole lot, so I have to specify some other array of those columns using the value of the number of weights and then I've done linear values that I have to set the normal distribution as a one matrix. So now for now I have to have a fully connected layer. And then do it to be a generative words. So you can see that in a one sample. It's the probability function, I take the weights. Okay one value of the mean layer, this is going to be the derivative of the Q value. So if we do is basically say this three dimensional version of a weight between P of epsilon going to be the input because the input is the error times the input size that function as two plus one by the matrixithmsm of w. So I want to keep there. And how we can have to do that back in the moment to figure out to each dimensions. So what we talked about this thing we are the weights. And then I can see that I'm going to do the error table. I want to take a single hidden layer for some weight to do is just have a return the sum of samples that position, and then the training data is going to be a return. So now I'll get the row points remember that T squared values, is, I can, I take a function this to the same thing that I'm\n",
      "---------------\n",
      "\n",
      " for the second weightance matrix multiplication. So this is not just no. And so if it happens to be the same, I'm trying to be doing, you're going to get the average of the output distribution. So what we'll take in x sub k plus state. And I'm going, I want to make sure that am now. So, we can make this, so let's do is basically the return the Q function as you can do is the e. But if we're going to use this look at the gradient of the model to actually need to be my ground truth term and then I'll take my weight weight TD is what's the delta weights. You can see that's going to be in the sample. So this will do is going to be as the same formula. This is going to be the same otherality of the error, where I'm going to be 100% of the error over the target activation function. So if I need to take each outputs. My nets, update, I can use myh of my input size of my previous value I choose the classes, one. So then let me take got to create a state and that target model. So now you know if I have an current linear action of a linear model of these weights and then'm just going to take the the ground truth probability of the current state or this. So now I'm going to try to be the network. So the same row, I'm going to initialize the action across the sum to my state, my input. So I have the first action function, I'm doing this. So now I'm going to do. So if I'm going to take a high, I have a single single one plus one,, or is three by one thing, a function of this to train function. So I will specify the inputs and take the ground truth output unit these inputs. So I'm just looking at the output, then I can just have w just get going to be the input weights. So, these values, what and I can be just going to take. So, I can just do y, then then be the state and then first value that I need to get a number of distribution. So now I'm just trying to specify this is use the next state my minimum function so I can set up with my time that value of samples. And then I'm just just a mean of the current action to just going to choose the current state\n",
      "---------------\n",
      "\n",
      " for some convolutional layer, they don't know, you know what they're going to work for those things, so this might use all the weights. So where all you would see? What things do we have an error function. So I want this to use the gradient of the network. And so now that I will just do the actual sample and so we can make this covariance for the square and basically have the output. So we would take the output values of the prediction times the inverse values for this input and then number of the same as we'll call this logarithms, and then it's going to happen for every output layer. So what they have to do we'll take. Right. So we can do here. What me do is you'll go. So here is this first for the QDA and then I can do that I have to be the weights, and then can do this from the neural networks in this for this can basically say, it's just, i'm going to be going to take like a bunch of different values. And then I'm going to get an input in this into a bunch of those values. And the patch of weights are equal to the next sample and the weight steps that's going to be some input. It's going to be like what I'm just going to apply that to Q value more than this function to that's going to be important for that's going to be able to get to be the same as a different data. So I need to do to complete the same state or take that right now, I can turn this into the probability that by the testing data, and then I'll need to have a non-validits that would be doing the goal where I'm trying to look at the state of this, right, we can do this function and then I just take for a value, and then go to optimize terms of the number of neural network as to that a function, because what we were to be with those epochs. And then that's the QDA function that I moved for the partial derivative of these times the image right from the standard inputs. So it's going to be a sample. So this is going to be the weights. So there's a matrix of that I'm now, multiply by D two by two things that each one is basically going to create this two dimensional row for the first one set of the error. But I've already know this is basically a\n",
      "---------------\n",
      "\n",
      " in the neural network as for some samples, you can see that are more samples, and these are the first. So you don't really want that's really want the parameters, you can know this error. So for this is like a very similar log likelihood of the optimization is the same class and then do some probability of the bias and we can kind of. So let's take the forward pass and use this basically use this case, actually going to be a bunch of the same as, but you're going to use some neural net. So you can just run a model to actually have to the output of these things through the same image image. So I'm going to do this to give me a return but just's going to be defined. So, it's not the error in this case, if I have this, I'll have two columns of a two hidden layer seven, but I will use a different classes. And then I'm around the input and I can use the error with my indicator variables for a reward of X. So remember i'll see here the following weights. And so now I'm going to be going to be all the error. And then I can I can do Q rate of the sum informationbook. So then I'm trying to basically just going to do this function to read. I have to a neural networks, there at the total gradient. So if I'm going to get a one minus the sample times the value of the convolutional layer step, and now I will train in the next state. So this is, I'm going to be some error. And I have the Q relationship between these hidden layer set time. So I'm going to have just at the softmax function of ones. And then you can get to this. So this allows me to do is I I can print them into the error. So what I will do is I can the actual as a sample and take in the next gradient descent function with error. So then if I find my target samples for a sample. So I have some weights w, I'm trying to specify the likelihood of that classes of sample. And then I will say I're going to see a weight to take the error matrix of the V and then sigma. So now I'll use the expected probability of respect to the first values of my output of outputs. So so, this is basically the same thing, now the output. So, in my log likelihood of W with\n",
      "---------------\n",
      "\n",
      " first one is going to be a little bit. So now, the train return, now we have the targets, what we have to draw the data and its linear function. So we'll take a go into a great so we can use some convolutional network function actually be kind of. So we'll want to measure the test, minus the same as the neural networks. And then, the weights that's a big output. So now I'll have's going to run that's going to be a bit of that so the matrices in the fruit inputs, I'm going to be my prediction. We can also sum for a lot of the patch or so for that I'm going to move this image from the filter. So we're actually change the input to actually just into a single output, what's going to be going to be as the gradient. So 10, I'm trying to turn this. The targets, and then I'm getting the goal is going to create a different values. So yeah, this is, let's read the error so if we already have to do that by the first values and I can use that into aPy version. So I'm doing my weight to create two. And then I can need to be the weight matrix into the input size. There's where you're my output of X, this is that we have this one. But now just going to be a linear function to be how to do is I'll be the output of that output, I can see that I can go from the other time to solve this into the testing data set of a matrix. But now I'm not going to do the same time. So the gradient is we'll just just see we're just trying to get the output size of T at the actual results. And so then, I need to measure this all of w is going to be the weights. And I'm going to give me my patches and an arbitrary image and then I have the individual samples and then I have to predict, will take that, and then a linear function starts to be a single vector. So if I have get this class two days about this is going to be a force of a point and this is going to be one of the same class. So if I have a member of matrix that these hidden layer size that's a single output of that X by n and W is zero. So linear regression, this is going to be being going to be so this is\n",
      "---------------\n",
      "\n",
      " before I want to do my training, and then do is not going to be a model. And so now if I do one the neural networks want to get this data, I will make sure it to do I want to see what I should do is I'm going to say, I'm going to do also going to do something like I'm going to have an instance of my state one class difference between my by my entire weights and then so I did use that. So we're trying to plot the weight matrix for this as a output. So now I'm going to add a prediction of our state and the activation function will see here in cross time distribution of basically the second one thing is going to be some of them. So, if I use the training t, for this case of the goal. So, I need to you can you can see just just say, I'll use to see that my action. I can return the one. So basically I see if I'm now if I think I'm going to take a normal distribution. So on this is our different weights and then I'll just say we're just going to add an input, and then I want to take the final output into 10% of this into neural networks as I'm going to classify the output layer layer. So I'm going to take that row values of the next hidden layer or the sum of the classes. So now this is going to be the output distribution of the state values of each sum of those units and a seven by the units in this error with neural network. So a best sum being a total of those size of all four feature and I'm going to be going to have one by D. But and so let's got a seven by one is going to be the expected value of the number of samples of units in the number of weights, in this prediction plus the value. So that's what Z has to be all of matrix for the same point. So if I mean one input to the output I'm going to look at this, I'll take all that. So now I think about this. if I'm trying to this really very intuitive for other layers of right because we can see that if we're just looking at a test set into a bit of training data, and then you're trying to maximize the return of my two things to update. And then I'm going to add some time well for all of that the same time, basically in terms of all\n",
      "---------------\n",
      "\n",
      " into the training. So this is just going to be the training, the inputs will get the best size of the error in the data. And then these would be the network problem and then be all of the weights. So, I can have a bunch of features to what what we will need to get into the individual two things that I can do that in this to be in the weights. So if I have a bunch of samples. So for each sample a function that are going to be defined another output of by one. So, for that I'm trying to print out of this one and then two samples. And in the state. And so now I can see that my my two works closer for the weights. So this case of a filter so we have a good prediction. And then just do this, if I can see that's going to be a column to do here. So I'm you have in one step to that sort of them. And so what the same problem is is actually going to be evenly between the second two. So if I make that, then I have my convolutional layer, and then I'm interested in this one. So I'm not going to be seven by 10. And then I'm trying to update that to be very small set and then I'm get with this sample to all of the individual values. So, this is going to be the best likelihood of my weights. We can see, my, I appreciate we just take me to take the neural networks. So, right, this is the input size of this is already got things that we had that's not really nice we can see that a minute going to be able to be some of a neural networks. So, if I have a member of class or a bunch of kind of class, we've got in the other values that mean of the matrix of these are all all, are going to be doing in the weights. So now it's going to look at the probability that's the sum of the minimum so now I have my two of classes, each weight in the error direction of five. Let me get some probability of the probability of each value. So some n sub n is just just going to be how many of those values would be a one hot column of all of zero, and then I can take the class. So I'll also kind of the output of of our difference function should be the hidden layer of a function. So, if I need\n",
      "---------------\n",
      "\n",
      " for the gradient, but it's going to do, it's not necessarily I'm not in two. So we've got some class classes. So now don't need to look at this last time I could not necessarily want to actually have a square or a member of that's no problem. So the number of course if I could use the derivative of the distribution of the first one by the input size is. So let's use the indicator variableigma. So now I'm trying to print the same thing to solve the bias. So this is here, a position of doing here. So this would be the probability of weights, it's not not going to say, if I'll notice that I multiply how. So basically the individual distribution for the center of the weight matrix. So we're all the test set of a continuous relationship and then the features in that I have a state that. So you can see the same thing here is just to do maybe a significant by like what I want to do. So this, when I'll imagine, let's I'm trying to do sure that I'm looking at the prediction right so I will see that I can know do the indicator variable is it's going to be the input and then I'll multiply my hidden units in the output layers to do you multiply that has to be defined that into a constant. So we'll define H. And then I'll have an input with this function, and then the matrix for n by its samples. So this is going to basically taking the sum from an sum for my test over the QDA function would be going to be the same thing. And then I have the standardize the same thing for things that is going to do we're going to go. Then we could be like the same time. This is still because it's a number ofize it's going to import it a lot of time I'm going to be pretty well here. And so there's these are all all these samples that that's going to be, I can take some it to reshape it into a single weight two dimensions here. So we can do the input to scale the output of x right which weight. I'll take the end up and then I'll take that has my indicator variable, change with the number of samples. So I should get to the same list of my line. So we're to see that I have a different variables. I'll put. So let's get my samples. So\n",
      "---------------\n",
      "\n",
      " on the same to make the class labels in Python, but but if you do a continuous variables. But this to can fit a sudden we can just want to actually get a whole lot of cases. So now we can see a bit more well this. So the Q function that likelihood is not I'm closer to the inputs that as the expected value for each of a single value of the image. So this is just just a little bit of just these, and I will train, or all of our values, remember, do is trying to classify the convolutional nets. So in this case, this allows me to make this to calculate that I'm with a inputs. So I can then take my data, I can try to take the weight matrix. So now we just make some probability of the max of class distributed, I'm some values to multiply that's now, because I'm trying to do a single value to be a list of those patches. And I'm not necessarily this case to run this. And then the error, I'm going to make a single point, but also just a scale the two different size of our probability of the function as a particular sample. So, for an example, if I should have some function that's the entire sort of that sort of a linear job of the number of weights just a only very nice number of things. So what I've got the value is I'm trying to optimize for a seven by doing a bit for more that, right so there's a particular function that's just like a simple for the first one, I'm going to get a member of that you have a year of a function. That is basically say when your image. So here I'll do my last thing, I'm trying to do the same one and I'm going to take something like the and I would the same value times. And I'm going to do a negative weight. So this I'm not doing is going to take a discriminant function of the end up with respect to the logar action. And so I'll see that of my inputs. So this is of N H function is going to be a high errors. So I'm going to be going to be taking the two components, and then the QDA is going to set of Q function. We have the number of samples in a different derivative of course, because now I want to probably be my inputs, assuming that we'll do that I'll do. So\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "! python3 sample.py --out_dir=out-nikhil-gpt-first-try"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below lets you choose the starting \"prompt\" and the model generates from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-nikhil-gpt-first-try\n",
      "Overriding: start = Another way to look at it is\n",
      "Overriding: num_samples = 1\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "Another way to look at it is what we're just doing that they have we're not so if I want that gradient descent in generally you want to handle a model that data, and we'll also tell me an error. And then we can do that for the probability of the final hidden layers. And so now I will be the gradient of the state to actually print the hidden layer. This is basically the different weights, I actually always a single inputs. So let's create the layer. And so now I'll plot the sum all my velocity is just to say I'm going to take the classes. And then I'm not going to be a single convolutional layer. So I'll do the weights from each of the entire hidden layer, the stride length of the end of the value of all. So I see that I'm actually done. So the in this is a action,, that I'm going to get a column, I can also say the output of all Bayes' rule. So this is just going to be going to be a bunch of values. And so now. So of this was always just just a single error for my kind of my convolutional layer and the second arrays will going to be a sample. So now I'm kind of going to take the same feature map here. So I'll have to take just draw the gradient of weights which output or the right so if I'm doing my feature, this is going to take a random action that I'll take of the end of the same testing way to try. So I take the test and the test set and then I use the test set of a bigger network. So this do this is here. So I have a I'm just getting no different outcomes. So I'm getting something like this, you know, kind of say, this is what I'm on the data in the first value. I'll print,, and, you can say, Adam and we want enough to activate this covariance values on my training set. So then in a set of a two for more or a high sum to my training samples, and then I'm going to get a target in a weight matrix. But and I'm not going to have patches, just can plot the shape for pretty now will take this look at pretty small size. So if I're doing my target right if I'm doing means I can I'm just trying to do this into some weights. Yeah, if I look at this is I'm\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# get input from user\n",
    "prompt = input(\"Enter a prompt: \")\n",
    "\n",
    "! python sample.py --out_dir=out-nikhil-gpt-first-try --start=\"$prompt\" --num_samples=1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to make a bigger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_config.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "eval_interval = 100 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False \n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'nikhil-gpt'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'medium'\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out-nikhil-gpt-med\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 200\n",
      "Overriding: log_interval = 1\n",
      "Overriding: block_size = 128\n",
      "Overriding: batch_size = 32\n",
      "Overriding: n_layer = 8\n",
      "Overriding: n_head = 8\n",
      "Overriding: n_embd = 256\n",
      "Overriding: max_iters = 2000\n",
      "Overriding: lr_decay_iters = 2000\n",
      "Overriding: dropout = 0.0\n",
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 19.17M\n",
      "using fused AdamW: False\n",
      "step 0: train loss 10.8750, val loss 10.8774\n",
      "iter 0: loss 10.8840, time 12013.23ms, mfu -100.00%\n",
      "iter 1: loss 10.8760, time 2291.49ms, mfu -100.00%\n",
      "iter 2: loss 10.8121, time 2288.65ms, mfu -100.00%\n",
      "iter 3: loss 10.6759, time 2290.85ms, mfu -100.00%\n",
      "iter 4: loss 10.5444, time 2298.06ms, mfu -100.00%\n",
      "iter 5: loss 10.4563, time 2294.47ms, mfu 2.70%\n",
      "iter 6: loss 10.4082, time 2293.78ms, mfu 2.71%\n",
      "iter 7: loss 10.3321, time 2291.56ms, mfu 2.71%\n",
      "iter 8: loss 10.2971, time 2286.97ms, mfu 2.71%\n",
      "iter 9: loss 10.2385, time 2285.61ms, mfu 2.71%\n",
      "iter 10: loss 10.2234, time 2287.19ms, mfu 2.71%\n",
      "iter 11: loss 10.1273, time 2290.18ms, mfu 2.71%\n",
      "iter 12: loss 10.1265, time 2293.58ms, mfu 2.71%\n",
      "iter 13: loss 10.0443, time 2295.94ms, mfu 2.71%\n",
      "iter 14: loss 9.9647, time 2299.15ms, mfu 2.71%\n",
      "iter 15: loss 9.8334, time 2290.97ms, mfu 2.71%\n",
      "iter 16: loss 9.8354, time 2295.56ms, mfu 2.71%\n",
      "iter 17: loss 9.7288, time 2291.58ms, mfu 2.71%\n",
      "iter 18: loss 9.6613, time 2297.91ms, mfu 2.71%\n",
      "iter 19: loss 9.5828, time 2292.21ms, mfu 2.71%\n",
      "iter 20: loss 9.5052, time 2293.84ms, mfu 2.71%\n",
      "iter 21: loss 9.4442, time 2286.82ms, mfu 2.71%\n",
      "iter 22: loss 9.2752, time 2295.55ms, mfu 2.71%\n",
      "iter 23: loss 9.1676, time 2292.06ms, mfu 2.71%\n",
      "iter 24: loss 9.1205, time 2293.50ms, mfu 2.71%\n",
      "iter 25: loss 9.0391, time 2289.40ms, mfu 2.71%\n",
      "iter 26: loss 8.8952, time 2292.88ms, mfu 2.71%\n",
      "iter 27: loss 8.7092, time 2289.59ms, mfu 2.71%\n",
      "iter 28: loss 8.6696, time 2304.94ms, mfu 2.71%\n",
      "iter 29: loss 8.5506, time 2294.35ms, mfu 2.71%\n",
      "iter 30: loss 8.4405, time 2296.20ms, mfu 2.71%\n",
      "iter 31: loss 8.3386, time 2292.46ms, mfu 2.71%\n",
      "iter 32: loss 8.1488, time 2295.16ms, mfu 2.71%\n",
      "iter 33: loss 8.0584, time 2294.61ms, mfu 2.71%\n",
      "iter 34: loss 7.9410, time 2297.87ms, mfu 2.71%\n",
      "iter 35: loss 7.8356, time 2294.84ms, mfu 2.70%\n",
      "iter 36: loss 7.6475, time 2301.01ms, mfu 2.70%\n",
      "iter 37: loss 7.5106, time 2291.28ms, mfu 2.70%\n",
      "iter 38: loss 7.4459, time 2291.42ms, mfu 2.71%\n",
      "iter 39: loss 7.2509, time 2291.66ms, mfu 2.71%\n",
      "iter 40: loss 7.1390, time 2298.49ms, mfu 2.70%\n",
      "iter 41: loss 6.9984, time 2294.66ms, mfu 2.70%\n",
      "iter 42: loss 6.8542, time 2292.07ms, mfu 2.71%\n",
      "iter 43: loss 6.7784, time 2296.52ms, mfu 2.70%\n",
      "iter 44: loss 6.6822, time 2293.82ms, mfu 2.70%\n",
      "iter 45: loss 6.5257, time 2304.38ms, mfu 2.70%\n",
      "iter 46: loss 6.4227, time 2299.26ms, mfu 2.70%\n",
      "iter 47: loss 6.3450, time 2296.52ms, mfu 2.70%\n",
      "iter 48: loss 6.2795, time 2296.23ms, mfu 2.70%\n",
      "iter 49: loss 6.2472, time 2297.32ms, mfu 2.70%\n",
      "iter 50: loss 6.1266, time 2290.72ms, mfu 2.70%\n",
      "iter 51: loss 5.9420, time 2297.90ms, mfu 2.70%\n",
      "iter 52: loss 5.8300, time 2291.61ms, mfu 2.70%\n",
      "iter 53: loss 5.8929, time 2296.66ms, mfu 2.70%\n",
      "iter 54: loss 5.7966, time 2282.75ms, mfu 2.71%\n",
      "iter 55: loss 5.7519, time 2299.27ms, mfu 2.70%\n",
      "iter 56: loss 5.5278, time 2293.69ms, mfu 2.70%\n",
      "iter 57: loss 5.5586, time 2294.01ms, mfu 2.70%\n",
      "iter 58: loss 5.5507, time 2293.17ms, mfu 2.70%\n",
      "iter 59: loss 5.5064, time 2293.57ms, mfu 2.71%\n",
      "iter 60: loss 5.3766, time 2296.57ms, mfu 2.70%\n",
      "iter 61: loss 5.3490, time 2283.97ms, mfu 2.71%\n",
      "iter 62: loss 5.1958, time 2292.49ms, mfu 2.71%\n",
      "iter 63: loss 5.1754, time 2297.78ms, mfu 2.71%\n",
      "iter 64: loss 5.2580, time 2296.91ms, mfu 2.71%\n",
      "iter 65: loss 5.1895, time 2297.51ms, mfu 2.70%\n",
      "iter 66: loss 5.0928, time 2300.44ms, mfu 2.70%\n",
      "iter 67: loss 5.0328, time 2294.87ms, mfu 2.70%\n",
      "iter 68: loss 5.0231, time 2297.21ms, mfu 2.70%\n",
      "iter 69: loss 5.0162, time 2298.07ms, mfu 2.70%\n",
      "iter 70: loss 5.0501, time 2294.86ms, mfu 2.70%\n",
      "iter 71: loss 5.1282, time 2292.00ms, mfu 2.70%\n",
      "iter 72: loss 4.8424, time 2296.23ms, mfu 2.70%\n",
      "iter 73: loss 4.8994, time 2298.14ms, mfu 2.70%\n",
      "iter 74: loss 4.8365, time 2290.80ms, mfu 2.70%\n",
      "iter 75: loss 4.8963, time 2299.07ms, mfu 2.70%\n",
      "iter 76: loss 4.8712, time 2296.46ms, mfu 2.70%\n",
      "iter 77: loss 4.7737, time 2296.41ms, mfu 2.70%\n",
      "iter 78: loss 4.7458, time 2297.29ms, mfu 2.70%\n",
      "iter 79: loss 4.7602, time 2294.92ms, mfu 2.70%\n",
      "iter 80: loss 4.6842, time 2289.73ms, mfu 2.70%\n",
      "iter 81: loss 4.4954, time 2298.00ms, mfu 2.70%\n",
      "iter 82: loss 4.6727, time 2296.98ms, mfu 2.70%\n",
      "iter 83: loss 4.6020, time 2286.82ms, mfu 2.70%\n",
      "iter 84: loss 4.5959, time 2293.64ms, mfu 2.70%\n",
      "iter 85: loss 4.4918, time 2295.25ms, mfu 2.70%\n",
      "iter 86: loss 4.6710, time 2299.61ms, mfu 2.70%\n",
      "iter 87: loss 4.5705, time 2295.90ms, mfu 2.70%\n",
      "iter 88: loss 4.4707, time 2294.17ms, mfu 2.70%\n",
      "iter 89: loss 4.4585, time 2291.00ms, mfu 2.70%\n",
      "iter 90: loss 4.5031, time 2292.64ms, mfu 2.70%\n",
      "iter 91: loss 4.4853, time 2301.51ms, mfu 2.70%\n",
      "iter 92: loss 4.2944, time 2293.11ms, mfu 2.70%\n",
      "iter 93: loss 4.3535, time 2295.21ms, mfu 2.70%\n",
      "iter 94: loss 4.5243, time 2291.77ms, mfu 2.70%\n",
      "iter 95: loss 4.3256, time 2289.80ms, mfu 2.71%\n",
      "iter 96: loss 4.2345, time 2301.58ms, mfu 2.70%\n",
      "iter 97: loss 4.3183, time 2296.75ms, mfu 2.70%\n",
      "iter 98: loss 4.3032, time 2288.74ms, mfu 2.70%\n",
      "iter 99: loss 4.2753, time 2303.14ms, mfu 2.70%\n",
      "step 100: train loss 4.2631, val loss 4.6482\n",
      "saving checkpoint to out-nikhil-gpt-med\n",
      "iter 100: loss 4.2092, time 14836.23ms, mfu 2.48%\n",
      "iter 101: loss 4.2283, time 2291.26ms, mfu 2.50%\n",
      "iter 102: loss 4.0637, time 2293.14ms, mfu 2.52%\n",
      "iter 103: loss 4.2247, time 2287.34ms, mfu 2.54%\n",
      "iter 104: loss 4.2164, time 2291.55ms, mfu 2.56%\n",
      "iter 105: loss 4.2139, time 2291.54ms, mfu 2.57%\n",
      "iter 106: loss 4.1446, time 2296.15ms, mfu 2.58%\n",
      "iter 107: loss 4.1892, time 2286.87ms, mfu 2.60%\n",
      "iter 108: loss 4.2404, time 2295.67ms, mfu 2.61%\n",
      "iter 109: loss 4.0832, time 2290.87ms, mfu 2.62%\n",
      "iter 110: loss 4.1531, time 2288.66ms, mfu 2.63%\n",
      "iter 111: loss 4.0817, time 2286.10ms, mfu 2.64%\n",
      "iter 112: loss 4.1162, time 2288.63ms, mfu 2.64%\n",
      "iter 113: loss 3.9665, time 2292.88ms, mfu 2.65%\n",
      "iter 114: loss 4.0176, time 2292.65ms, mfu 2.66%\n",
      "iter 115: loss 4.0018, time 2297.65ms, mfu 2.66%\n",
      "iter 116: loss 3.9194, time 2290.78ms, mfu 2.67%\n",
      "iter 117: loss 4.0140, time 2298.18ms, mfu 2.67%\n",
      "iter 118: loss 3.9505, time 2289.91ms, mfu 2.67%\n",
      "iter 119: loss 3.9261, time 2293.94ms, mfu 2.68%\n",
      "iter 120: loss 3.9947, time 2291.64ms, mfu 2.68%\n",
      "iter 121: loss 4.0747, time 2293.90ms, mfu 2.68%\n",
      "iter 122: loss 3.9642, time 2290.65ms, mfu 2.68%\n",
      "iter 123: loss 3.8194, time 2292.27ms, mfu 2.69%\n",
      "iter 124: loss 3.8109, time 2293.23ms, mfu 2.69%\n",
      "iter 125: loss 3.9371, time 2293.30ms, mfu 2.69%\n",
      "iter 126: loss 3.8802, time 2292.08ms, mfu 2.69%\n",
      "iter 127: loss 3.9526, time 2291.80ms, mfu 2.69%\n",
      "iter 128: loss 3.6833, time 2291.87ms, mfu 2.70%\n",
      "iter 129: loss 3.7994, time 2288.75ms, mfu 2.70%\n",
      "iter 130: loss 3.7581, time 2297.50ms, mfu 2.70%\n",
      "iter 131: loss 3.7393, time 2288.17ms, mfu 2.70%\n",
      "iter 132: loss 3.7270, time 2293.99ms, mfu 2.70%\n",
      "iter 133: loss 3.7584, time 2290.80ms, mfu 2.70%\n",
      "iter 134: loss 3.7817, time 2290.76ms, mfu 2.70%\n",
      "iter 135: loss 3.7056, time 2288.99ms, mfu 2.70%\n",
      "iter 136: loss 3.7529, time 2288.92ms, mfu 2.70%\n",
      "iter 137: loss 3.6971, time 2283.38ms, mfu 2.70%\n",
      "iter 138: loss 3.6800, time 2288.16ms, mfu 2.71%\n",
      "iter 139: loss 3.6884, time 2297.46ms, mfu 2.71%\n",
      "iter 140: loss 3.6206, time 2283.53ms, mfu 2.71%\n",
      "iter 141: loss 3.6566, time 2296.29ms, mfu 2.71%\n",
      "iter 142: loss 3.6598, time 2289.92ms, mfu 2.71%\n",
      "iter 143: loss 3.6108, time 2292.89ms, mfu 2.71%\n",
      "iter 144: loss 3.5883, time 2294.37ms, mfu 2.71%\n",
      "iter 145: loss 3.5249, time 2291.40ms, mfu 2.71%\n",
      "iter 146: loss 3.5783, time 2288.50ms, mfu 2.71%\n",
      "iter 147: loss 3.6208, time 2291.33ms, mfu 2.71%\n",
      "iter 148: loss 3.4530, time 2293.42ms, mfu 2.71%\n",
      "iter 149: loss 3.5305, time 2290.80ms, mfu 2.71%\n",
      "iter 150: loss 3.4621, time 2288.31ms, mfu 2.71%\n",
      "iter 151: loss 3.4202, time 2287.61ms, mfu 2.71%\n",
      "iter 152: loss 3.4686, time 2285.34ms, mfu 2.71%\n",
      "iter 153: loss 3.4006, time 2290.89ms, mfu 2.71%\n",
      "iter 154: loss 3.5082, time 2294.52ms, mfu 2.71%\n",
      "iter 155: loss 3.3442, time 2294.43ms, mfu 2.71%\n",
      "iter 156: loss 3.4588, time 2295.45ms, mfu 2.71%\n",
      "iter 157: loss 3.2896, time 2290.37ms, mfu 2.71%\n",
      "iter 158: loss 3.3885, time 2293.59ms, mfu 2.71%\n",
      "iter 159: loss 3.3994, time 2294.42ms, mfu 2.71%\n",
      "iter 160: loss 3.3466, time 2289.74ms, mfu 2.71%\n",
      "iter 161: loss 3.3242, time 2290.69ms, mfu 2.71%\n",
      "iter 162: loss 3.3017, time 2288.78ms, mfu 2.71%\n",
      "iter 163: loss 3.2164, time 2287.24ms, mfu 2.71%\n",
      "iter 164: loss 3.3225, time 2288.41ms, mfu 2.71%\n",
      "iter 165: loss 3.2895, time 2288.85ms, mfu 2.71%\n",
      "iter 166: loss 3.2898, time 2289.51ms, mfu 2.71%\n",
      "iter 167: loss 3.2143, time 2290.93ms, mfu 2.71%\n",
      "iter 168: loss 3.2103, time 2285.87ms, mfu 2.71%\n",
      "iter 169: loss 3.2000, time 2294.88ms, mfu 2.71%\n",
      "iter 170: loss 3.1348, time 2290.61ms, mfu 2.71%\n",
      "iter 171: loss 3.2118, time 2288.49ms, mfu 2.71%\n",
      "iter 172: loss 3.1792, time 2296.88ms, mfu 2.71%\n",
      "iter 173: loss 3.1287, time 2291.84ms, mfu 2.71%\n",
      "iter 174: loss 3.1765, time 2290.15ms, mfu 2.71%\n",
      "iter 175: loss 3.1136, time 2290.17ms, mfu 2.71%\n",
      "iter 176: loss 3.0057, time 2289.04ms, mfu 2.71%\n",
      "iter 177: loss 3.2074, time 2290.19ms, mfu 2.71%\n",
      "iter 178: loss 3.1231, time 2293.09ms, mfu 2.71%\n",
      "iter 179: loss 3.0751, time 2292.60ms, mfu 2.71%\n",
      "iter 180: loss 3.0653, time 2289.46ms, mfu 2.71%\n",
      "iter 181: loss 3.0358, time 2291.83ms, mfu 2.71%\n",
      "iter 182: loss 3.0424, time 2289.45ms, mfu 2.71%\n",
      "iter 183: loss 2.9324, time 2290.91ms, mfu 2.71%\n",
      "iter 184: loss 2.9907, time 2289.35ms, mfu 2.71%\n",
      "iter 185: loss 3.0159, time 2294.38ms, mfu 2.71%\n",
      "iter 186: loss 2.9677, time 2296.03ms, mfu 2.71%\n",
      "iter 187: loss 3.0635, time 2293.94ms, mfu 2.71%\n",
      "iter 188: loss 3.0119, time 2295.25ms, mfu 2.71%\n",
      "iter 189: loss 2.9208, time 2293.73ms, mfu 2.71%\n",
      "iter 190: loss 2.9735, time 2293.57ms, mfu 2.71%\n",
      "iter 191: loss 2.8876, time 2298.88ms, mfu 2.71%\n",
      "iter 192: loss 2.9043, time 2290.59ms, mfu 2.71%\n",
      "iter 193: loss 2.8216, time 2298.46ms, mfu 2.71%\n",
      "iter 194: loss 2.8675, time 2303.29ms, mfu 2.71%\n",
      "iter 195: loss 2.9029, time 2296.31ms, mfu 2.70%\n",
      "iter 196: loss 2.9974, time 2294.43ms, mfu 2.70%\n",
      "iter 197: loss 2.8698, time 2293.10ms, mfu 2.70%\n",
      "iter 198: loss 2.8352, time 2298.60ms, mfu 2.70%\n",
      "iter 199: loss 2.8148, time 2297.56ms, mfu 2.70%\n",
      "step 200: train loss 2.7991, val loss 4.9054\n",
      "iter 200: loss 2.7866, time 11572.89ms, mfu 2.49%\n",
      "iter 201: loss 2.7550, time 2293.93ms, mfu 2.51%\n",
      "iter 202: loss 2.8513, time 2300.76ms, mfu 2.53%\n",
      "iter 203: loss 2.7915, time 2297.90ms, mfu 2.55%\n",
      "iter 204: loss 2.7708, time 2295.99ms, mfu 2.56%\n",
      "iter 205: loss 2.8065, time 2296.61ms, mfu 2.58%\n",
      "iter 206: loss 2.6703, time 2294.91ms, mfu 2.59%\n",
      "iter 207: loss 2.7447, time 2292.40ms, mfu 2.60%\n",
      "iter 208: loss 2.7581, time 2295.23ms, mfu 2.61%\n",
      "iter 209: loss 2.6495, time 2295.44ms, mfu 2.62%\n",
      "iter 210: loss 2.6320, time 2292.08ms, mfu 2.63%\n",
      "iter 211: loss 2.6199, time 2293.40ms, mfu 2.64%\n",
      "iter 212: loss 2.6815, time 2301.71ms, mfu 2.64%\n",
      "iter 213: loss 2.7221, time 2295.88ms, mfu 2.65%\n",
      "iter 214: loss 2.6158, time 2295.03ms, mfu 2.65%\n",
      "iter 215: loss 2.6066, time 2287.80ms, mfu 2.66%\n",
      "iter 216: loss 2.5824, time 2302.34ms, mfu 2.66%\n",
      "iter 217: loss 2.6094, time 2294.22ms, mfu 2.67%\n",
      "iter 218: loss 2.6262, time 2296.18ms, mfu 2.67%\n",
      "iter 219: loss 2.6617, time 2294.63ms, mfu 2.67%\n",
      "iter 220: loss 2.5565, time 2293.25ms, mfu 2.68%\n",
      "iter 221: loss 2.4563, time 2294.60ms, mfu 2.68%\n",
      "iter 222: loss 2.5682, time 2296.69ms, mfu 2.68%\n",
      "iter 223: loss 2.5284, time 2291.99ms, mfu 2.69%\n",
      "iter 224: loss 2.5055, time 2295.49ms, mfu 2.69%\n",
      "iter 225: loss 2.5715, time 2287.94ms, mfu 2.69%\n",
      "iter 226: loss 2.5053, time 2290.09ms, mfu 2.69%\n",
      "iter 227: loss 2.4330, time 2296.21ms, mfu 2.69%\n",
      "iter 228: loss 2.4791, time 2293.22ms, mfu 2.69%\n",
      "iter 229: loss 2.4609, time 2293.16ms, mfu 2.70%\n",
      "iter 230: loss 2.3850, time 2293.38ms, mfu 2.70%\n",
      "iter 231: loss 2.5181, time 2296.46ms, mfu 2.70%\n",
      "iter 232: loss 2.3711, time 2289.90ms, mfu 2.70%\n",
      "iter 233: loss 2.4952, time 2297.78ms, mfu 2.70%\n",
      "iter 234: loss 2.3347, time 2293.76ms, mfu 2.70%\n",
      "iter 235: loss 2.3696, time 2294.10ms, mfu 2.70%\n",
      "iter 236: loss 2.3469, time 2293.21ms, mfu 2.70%\n",
      "iter 237: loss 2.3970, time 2292.23ms, mfu 2.70%\n",
      "iter 238: loss 2.4085, time 2294.23ms, mfu 2.70%\n",
      "iter 239: loss 2.2964, time 2294.17ms, mfu 2.70%\n",
      "iter 240: loss 2.3175, time 2292.15ms, mfu 2.70%\n",
      "iter 241: loss 2.3447, time 2296.23ms, mfu 2.70%\n",
      "iter 242: loss 2.3921, time 2294.48ms, mfu 2.70%\n",
      "iter 243: loss 2.1967, time 2300.82ms, mfu 2.70%\n",
      "iter 244: loss 2.2636, time 2289.89ms, mfu 2.70%\n",
      "iter 245: loss 2.3643, time 2288.17ms, mfu 2.70%\n",
      "iter 246: loss 2.3202, time 2297.70ms, mfu 2.70%\n",
      "iter 247: loss 2.3147, time 2296.04ms, mfu 2.70%\n",
      "iter 248: loss 2.2295, time 2295.81ms, mfu 2.70%\n",
      "iter 249: loss 2.2762, time 2290.71ms, mfu 2.70%\n",
      "iter 250: loss 2.2131, time 2295.14ms, mfu 2.70%\n",
      "iter 251: loss 2.2105, time 2290.88ms, mfu 2.70%\n",
      "iter 252: loss 2.1726, time 2295.88ms, mfu 2.70%\n",
      "iter 253: loss 2.1811, time 2287.55ms, mfu 2.71%\n",
      "iter 254: loss 2.2389, time 2291.06ms, mfu 2.71%\n",
      "iter 255: loss 2.2034, time 2294.66ms, mfu 2.71%\n",
      "iter 256: loss 2.1884, time 2292.97ms, mfu 2.71%\n",
      "iter 257: loss 2.1058, time 2288.04ms, mfu 2.71%\n",
      "iter 258: loss 2.2038, time 2290.08ms, mfu 2.71%\n",
      "iter 259: loss 2.1492, time 2293.91ms, mfu 2.71%\n",
      "iter 260: loss 2.1617, time 2288.73ms, mfu 2.71%\n",
      "iter 261: loss 2.1491, time 2296.18ms, mfu 2.71%\n",
      "iter 262: loss 2.1027, time 2291.67ms, mfu 2.71%\n",
      "iter 263: loss 2.1250, time 2292.34ms, mfu 2.71%\n",
      "iter 264: loss 2.1671, time 2293.58ms, mfu 2.71%\n",
      "iter 265: loss 2.1539, time 2287.79ms, mfu 2.71%\n",
      "iter 266: loss 2.0720, time 2290.55ms, mfu 2.71%\n",
      "iter 267: loss 2.1601, time 2290.54ms, mfu 2.71%\n",
      "iter 268: loss 2.0325, time 2292.41ms, mfu 2.71%\n",
      "iter 269: loss 2.0636, time 2293.40ms, mfu 2.71%\n",
      "iter 270: loss 2.0418, time 2294.48ms, mfu 2.71%\n",
      "iter 271: loss 2.0742, time 2291.15ms, mfu 2.71%\n",
      "iter 272: loss 2.0192, time 2291.13ms, mfu 2.71%\n",
      "iter 273: loss 1.9253, time 2292.65ms, mfu 2.71%\n",
      "iter 274: loss 1.9685, time 2282.33ms, mfu 2.71%\n",
      "iter 275: loss 1.9973, time 2284.85ms, mfu 2.71%\n",
      "iter 276: loss 1.9475, time 2293.48ms, mfu 2.71%\n",
      "iter 277: loss 1.9665, time 2294.35ms, mfu 2.71%\n",
      "iter 278: loss 2.0132, time 2291.33ms, mfu 2.71%\n",
      "iter 279: loss 1.9437, time 2298.10ms, mfu 2.71%\n",
      "iter 280: loss 1.9712, time 2295.93ms, mfu 2.71%\n",
      "iter 281: loss 1.9974, time 2291.94ms, mfu 2.71%\n",
      "iter 282: loss 1.8794, time 2294.42ms, mfu 2.71%\n",
      "iter 283: loss 1.9279, time 2287.69ms, mfu 2.71%\n",
      "iter 284: loss 1.9342, time 2295.32ms, mfu 2.71%\n",
      "iter 285: loss 1.9356, time 2292.22ms, mfu 2.71%\n",
      "iter 286: loss 1.8558, time 2284.40ms, mfu 2.71%\n",
      "iter 287: loss 1.8284, time 2286.46ms, mfu 2.71%\n",
      "iter 288: loss 1.9751, time 2291.81ms, mfu 2.71%\n",
      "iter 289: loss 1.7516, time 2291.22ms, mfu 2.71%\n",
      "iter 290: loss 1.7656, time 2292.64ms, mfu 2.71%\n",
      "iter 291: loss 1.8255, time 2299.62ms, mfu 2.71%\n",
      "iter 292: loss 1.7523, time 2296.28ms, mfu 2.71%\n",
      "iter 293: loss 1.8499, time 2293.22ms, mfu 2.71%\n",
      "iter 294: loss 1.7849, time 2290.96ms, mfu 2.71%\n",
      "iter 295: loss 1.7647, time 2289.27ms, mfu 2.71%\n",
      "iter 296: loss 1.7821, time 2293.12ms, mfu 2.71%\n",
      "iter 297: loss 1.8289, time 2291.92ms, mfu 2.71%\n",
      "iter 298: loss 1.8088, time 2296.02ms, mfu 2.71%\n",
      "iter 299: loss 1.8336, time 2292.24ms, mfu 2.71%\n",
      "step 300: train loss 1.7613, val loss 5.8768\n",
      "iter 300: loss 1.7854, time 11531.98ms, mfu 2.49%\n",
      "iter 301: loss 1.7369, time 2286.09ms, mfu 2.51%\n",
      "iter 302: loss 1.7257, time 2290.88ms, mfu 2.53%\n",
      "iter 303: loss 1.7437, time 2286.21ms, mfu 2.55%\n",
      "iter 304: loss 1.6517, time 2290.67ms, mfu 2.57%\n",
      "iter 305: loss 1.7729, time 2293.38ms, mfu 2.58%\n",
      "iter 306: loss 1.6826, time 2289.17ms, mfu 2.59%\n",
      "iter 307: loss 1.7213, time 2291.17ms, mfu 2.61%\n",
      "iter 308: loss 1.6995, time 2290.51ms, mfu 2.62%\n",
      "iter 309: loss 1.7257, time 2298.50ms, mfu 2.62%\n",
      "iter 310: loss 1.6993, time 2290.89ms, mfu 2.63%\n",
      "iter 311: loss 1.7241, time 2286.82ms, mfu 2.64%\n",
      "iter 312: loss 1.5542, time 2291.90ms, mfu 2.65%\n",
      "iter 313: loss 1.6616, time 2288.52ms, mfu 2.65%\n",
      "iter 314: loss 1.5910, time 2293.54ms, mfu 2.66%\n",
      "iter 315: loss 1.6409, time 2295.57ms, mfu 2.66%\n",
      "iter 316: loss 1.6585, time 2299.84ms, mfu 2.67%\n",
      "iter 317: loss 1.6181, time 2291.01ms, mfu 2.67%\n",
      "iter 318: loss 1.5301, time 2291.90ms, mfu 2.67%\n",
      "iter 319: loss 1.5456, time 2295.60ms, mfu 2.68%\n",
      "iter 320: loss 1.5267, time 2298.73ms, mfu 2.68%\n",
      "iter 321: loss 1.5760, time 2296.92ms, mfu 2.68%\n",
      "iter 322: loss 1.5407, time 2295.40ms, mfu 2.68%\n",
      "iter 323: loss 1.5784, time 2293.71ms, mfu 2.69%\n",
      "iter 324: loss 1.4791, time 2297.01ms, mfu 2.69%\n",
      "iter 325: loss 1.4901, time 2291.35ms, mfu 2.69%\n",
      "iter 326: loss 1.6164, time 2285.83ms, mfu 2.69%\n",
      "iter 327: loss 1.4594, time 2291.61ms, mfu 2.69%\n",
      "iter 328: loss 1.5371, time 2291.83ms, mfu 2.70%\n",
      "iter 329: loss 1.4776, time 2292.01ms, mfu 2.70%\n",
      "iter 330: loss 1.5274, time 2295.95ms, mfu 2.70%\n",
      "iter 331: loss 1.3891, time 2292.95ms, mfu 2.70%\n",
      "iter 332: loss 1.4863, time 2294.01ms, mfu 2.70%\n",
      "iter 333: loss 1.4171, time 2293.36ms, mfu 2.70%\n",
      "iter 334: loss 1.4853, time 2294.43ms, mfu 2.70%\n",
      "iter 335: loss 1.4303, time 2297.30ms, mfu 2.70%\n",
      "iter 336: loss 1.5088, time 2293.30ms, mfu 2.70%\n",
      "iter 337: loss 1.4053, time 2291.43ms, mfu 2.70%\n",
      "iter 338: loss 1.3852, time 2293.82ms, mfu 2.70%\n",
      "iter 339: loss 1.3886, time 2289.18ms, mfu 2.70%\n",
      "iter 340: loss 1.3934, time 2296.58ms, mfu 2.70%\n",
      "iter 341: loss 1.3908, time 2291.87ms, mfu 2.70%\n",
      "iter 342: loss 1.3332, time 2295.35ms, mfu 2.70%\n",
      "iter 343: loss 1.3529, time 2295.93ms, mfu 2.70%\n",
      "iter 344: loss 1.3238, time 2281.20ms, mfu 2.71%\n",
      "iter 345: loss 1.3253, time 2293.20ms, mfu 2.71%\n",
      "iter 346: loss 1.4206, time 2285.63ms, mfu 2.71%\n",
      "iter 347: loss 1.3979, time 2290.30ms, mfu 2.71%\n",
      "iter 348: loss 1.3096, time 2292.22ms, mfu 2.71%\n",
      "iter 349: loss 1.2777, time 2287.61ms, mfu 2.71%\n",
      "iter 350: loss 1.3155, time 2289.28ms, mfu 2.71%\n",
      "iter 351: loss 1.3549, time 2294.53ms, mfu 2.71%\n",
      "iter 352: loss 1.2696, time 2291.63ms, mfu 2.71%\n",
      "iter 353: loss 1.2723, time 2287.02ms, mfu 2.71%\n",
      "iter 354: loss 1.3376, time 2295.68ms, mfu 2.71%\n",
      "iter 355: loss 1.2438, time 2290.50ms, mfu 2.71%\n",
      "iter 356: loss 1.3598, time 2290.43ms, mfu 2.71%\n",
      "iter 357: loss 1.2481, time 2288.46ms, mfu 2.71%\n",
      "iter 358: loss 1.2339, time 2293.96ms, mfu 2.71%\n",
      "iter 359: loss 1.3062, time 2290.72ms, mfu 2.71%\n",
      "iter 360: loss 1.2733, time 2297.44ms, mfu 2.71%\n",
      "iter 361: loss 1.1920, time 2292.94ms, mfu 2.71%\n",
      "iter 362: loss 1.2378, time 2288.42ms, mfu 2.71%\n",
      "iter 363: loss 1.1996, time 2296.37ms, mfu 2.71%\n",
      "iter 364: loss 1.1760, time 2289.28ms, mfu 2.71%\n",
      "iter 365: loss 1.1656, time 2289.34ms, mfu 2.71%\n",
      "iter 366: loss 1.2151, time 2290.40ms, mfu 2.71%\n",
      "iter 367: loss 1.1230, time 2291.15ms, mfu 2.71%\n",
      "iter 368: loss 1.1575, time 2292.25ms, mfu 2.71%\n",
      "iter 369: loss 1.1702, time 2288.98ms, mfu 2.71%\n",
      "iter 370: loss 1.1249, time 2292.20ms, mfu 2.71%\n",
      "iter 371: loss 1.1411, time 2292.18ms, mfu 2.71%\n",
      "iter 372: loss 1.1308, time 2288.54ms, mfu 2.71%\n",
      "iter 373: loss 1.0633, time 2292.90ms, mfu 2.71%\n",
      "iter 374: loss 1.0755, time 2292.10ms, mfu 2.71%\n",
      "iter 375: loss 1.1118, time 2296.70ms, mfu 2.71%\n",
      "iter 376: loss 1.0467, time 2292.16ms, mfu 2.71%\n",
      "iter 377: loss 1.0505, time 2293.52ms, mfu 2.71%\n",
      "iter 378: loss 1.1844, time 2290.00ms, mfu 2.71%\n",
      "iter 379: loss 1.0710, time 2292.64ms, mfu 2.71%\n",
      "iter 380: loss 1.0807, time 2295.86ms, mfu 2.71%\n",
      "iter 381: loss 1.0414, time 2289.35ms, mfu 2.71%\n",
      "iter 382: loss 1.0116, time 2291.77ms, mfu 2.71%\n",
      "iter 383: loss 1.0709, time 2296.86ms, mfu 2.71%\n",
      "iter 384: loss 1.0455, time 2292.82ms, mfu 2.71%\n",
      "iter 385: loss 1.0212, time 2288.53ms, mfu 2.71%\n",
      "iter 386: loss 1.0316, time 2291.15ms, mfu 2.71%\n",
      "iter 387: loss 1.0256, time 2286.01ms, mfu 2.71%\n",
      "iter 388: loss 1.0073, time 2291.92ms, mfu 2.71%\n",
      "iter 389: loss 0.9831, time 2293.95ms, mfu 2.71%\n",
      "iter 390: loss 1.0164, time 2289.22ms, mfu 2.71%\n",
      "iter 391: loss 0.9650, time 2287.76ms, mfu 2.71%\n",
      "iter 392: loss 1.0018, time 2294.25ms, mfu 2.71%\n",
      "iter 393: loss 0.9620, time 2287.62ms, mfu 2.71%\n",
      "iter 394: loss 1.0593, time 2291.73ms, mfu 2.71%\n",
      "iter 395: loss 0.9608, time 2291.15ms, mfu 2.71%\n",
      "iter 396: loss 0.9263, time 2301.42ms, mfu 2.71%\n",
      "iter 397: loss 0.9109, time 2297.66ms, mfu 2.71%\n",
      "iter 398: loss 0.9706, time 2291.70ms, mfu 2.71%\n",
      "iter 399: loss 0.9895, time 2291.35ms, mfu 2.71%\n",
      "step 400: train loss 0.9286, val loss 6.7818\n",
      "iter 400: loss 0.9349, time 11515.19ms, mfu 2.49%\n",
      "iter 401: loss 0.9195, time 2286.78ms, mfu 2.51%\n",
      "iter 402: loss 0.8421, time 2295.14ms, mfu 2.53%\n",
      "iter 403: loss 0.8776, time 2293.56ms, mfu 2.55%\n",
      "iter 404: loss 0.9144, time 2290.43ms, mfu 2.57%\n",
      "iter 405: loss 0.8258, time 2289.22ms, mfu 2.58%\n",
      "iter 406: loss 0.8863, time 2298.22ms, mfu 2.59%\n",
      "iter 407: loss 0.8876, time 2294.12ms, mfu 2.60%\n",
      "iter 408: loss 0.8647, time 2293.77ms, mfu 2.61%\n",
      "iter 409: loss 0.8839, time 2291.98ms, mfu 2.62%\n",
      "iter 410: loss 0.8012, time 2297.36ms, mfu 2.63%\n",
      "iter 411: loss 0.8503, time 2294.32ms, mfu 2.64%\n",
      "iter 412: loss 0.8691, time 2289.05ms, mfu 2.65%\n",
      "iter 413: loss 0.7688, time 2292.73ms, mfu 2.65%\n",
      "iter 414: loss 0.8311, time 2297.33ms, mfu 2.66%\n",
      "iter 415: loss 0.8245, time 2294.95ms, mfu 2.66%\n",
      "iter 416: loss 0.7165, time 2290.53ms, mfu 2.67%\n",
      "iter 417: loss 0.8352, time 2289.25ms, mfu 2.67%\n",
      "iter 418: loss 0.7644, time 2287.07ms, mfu 2.68%\n",
      "iter 419: loss 0.8051, time 2289.08ms, mfu 2.68%\n",
      "iter 420: loss 0.8339, time 2287.58ms, mfu 2.68%\n",
      "iter 421: loss 0.8593, time 2295.33ms, mfu 2.68%\n",
      "iter 422: loss 0.7220, time 2287.96ms, mfu 2.69%\n",
      "iter 423: loss 0.7904, time 2287.18ms, mfu 2.69%\n",
      "iter 424: loss 0.7349, time 2292.36ms, mfu 2.69%\n",
      "iter 425: loss 0.7494, time 2295.20ms, mfu 2.69%\n",
      "iter 426: loss 0.7408, time 2289.53ms, mfu 2.69%\n",
      "iter 427: loss 0.7512, time 2291.49ms, mfu 2.70%\n",
      "iter 428: loss 0.7750, time 2288.41ms, mfu 2.70%\n",
      "iter 429: loss 0.7330, time 2294.79ms, mfu 2.70%\n",
      "iter 430: loss 0.8039, time 2295.18ms, mfu 2.70%\n",
      "iter 431: loss 0.7426, time 2295.60ms, mfu 2.70%\n",
      "iter 432: loss 0.6843, time 2293.95ms, mfu 2.70%\n",
      "iter 433: loss 0.7256, time 2296.04ms, mfu 2.70%\n",
      "iter 434: loss 0.7108, time 2292.20ms, mfu 2.70%\n",
      "iter 435: loss 0.7180, time 2291.55ms, mfu 2.70%\n",
      "iter 436: loss 0.7090, time 2294.71ms, mfu 2.70%\n",
      "iter 437: loss 0.6765, time 2287.88ms, mfu 2.70%\n",
      "iter 438: loss 0.6764, time 2288.00ms, mfu 2.70%\n",
      "iter 439: loss 0.6795, time 2291.98ms, mfu 2.70%\n",
      "iter 440: loss 0.6970, time 2293.08ms, mfu 2.70%\n",
      "iter 441: loss 0.6622, time 2293.42ms, mfu 2.70%\n",
      "iter 442: loss 0.6686, time 2288.85ms, mfu 2.71%\n",
      "iter 443: loss 0.6712, time 2288.89ms, mfu 2.71%\n",
      "iter 444: loss 0.6589, time 2291.24ms, mfu 2.71%\n",
      "iter 445: loss 0.5954, time 2296.80ms, mfu 2.71%\n",
      "iter 446: loss 0.6765, time 2289.27ms, mfu 2.71%\n",
      "iter 447: loss 0.6023, time 2289.30ms, mfu 2.71%\n",
      "iter 448: loss 0.6268, time 2292.54ms, mfu 2.71%\n",
      "iter 449: loss 0.6402, time 2295.00ms, mfu 2.71%\n",
      "iter 450: loss 0.5904, time 2288.65ms, mfu 2.71%\n",
      "iter 451: loss 0.6902, time 2294.65ms, mfu 2.71%\n",
      "iter 452: loss 0.6436, time 2286.40ms, mfu 2.71%\n",
      "iter 453: loss 0.5554, time 2295.48ms, mfu 2.71%\n",
      "iter 454: loss 0.6262, time 2292.17ms, mfu 2.71%\n",
      "iter 455: loss 0.6040, time 2292.03ms, mfu 2.71%\n",
      "iter 456: loss 0.5937, time 2292.57ms, mfu 2.71%\n",
      "iter 457: loss 0.5929, time 2291.80ms, mfu 2.71%\n",
      "iter 458: loss 0.5586, time 2291.48ms, mfu 2.71%\n",
      "iter 459: loss 0.6114, time 2295.29ms, mfu 2.71%\n",
      "iter 460: loss 0.5372, time 2292.34ms, mfu 2.71%\n",
      "iter 461: loss 0.5952, time 2295.78ms, mfu 2.71%\n",
      "iter 462: loss 0.5938, time 2289.33ms, mfu 2.71%\n",
      "iter 463: loss 0.6100, time 2288.09ms, mfu 2.71%\n",
      "iter 464: loss 0.5832, time 2289.48ms, mfu 2.71%\n",
      "iter 465: loss 0.6196, time 2289.40ms, mfu 2.71%\n",
      "iter 466: loss 0.5514, time 2291.98ms, mfu 2.71%\n",
      "iter 467: loss 0.6083, time 2295.45ms, mfu 2.71%\n",
      "iter 468: loss 0.5308, time 2291.90ms, mfu 2.71%\n",
      "iter 469: loss 0.6003, time 2293.02ms, mfu 2.71%\n",
      "iter 470: loss 0.5622, time 2278.93ms, mfu 2.71%\n",
      "iter 471: loss 0.5680, time 2291.66ms, mfu 2.71%\n",
      "iter 472: loss 0.5501, time 2290.88ms, mfu 2.71%\n",
      "iter 473: loss 0.5246, time 2296.01ms, mfu 2.71%\n",
      "iter 474: loss 0.5267, time 2284.89ms, mfu 2.71%\n",
      "iter 475: loss 0.5407, time 2286.02ms, mfu 2.71%\n",
      "iter 476: loss 0.5332, time 2295.33ms, mfu 2.71%\n",
      "iter 477: loss 0.5050, time 2296.05ms, mfu 2.71%\n",
      "iter 478: loss 0.5382, time 2297.45ms, mfu 2.71%\n",
      "iter 479: loss 0.5686, time 2293.43ms, mfu 2.71%\n",
      "iter 480: loss 0.5296, time 2291.12ms, mfu 2.71%\n",
      "iter 481: loss 0.4923, time 2288.76ms, mfu 2.71%\n",
      "iter 482: loss 0.5101, time 2292.02ms, mfu 2.71%\n",
      "iter 483: loss 0.5102, time 2289.53ms, mfu 2.71%\n",
      "iter 484: loss 0.5003, time 2295.37ms, mfu 2.71%\n",
      "iter 485: loss 0.4969, time 2293.03ms, mfu 2.71%\n",
      "iter 486: loss 0.4928, time 2293.04ms, mfu 2.71%\n",
      "iter 487: loss 0.5478, time 2295.09ms, mfu 2.71%\n",
      "iter 488: loss 0.5154, time 2295.05ms, mfu 2.71%\n",
      "iter 489: loss 0.4542, time 2293.92ms, mfu 2.71%\n",
      "iter 490: loss 0.5084, time 2290.37ms, mfu 2.71%\n",
      "iter 491: loss 0.4975, time 2288.96ms, mfu 2.71%\n",
      "iter 492: loss 0.4644, time 2289.91ms, mfu 2.71%\n",
      "iter 493: loss 0.4795, time 2291.12ms, mfu 2.71%\n",
      "iter 494: loss 0.4887, time 2293.72ms, mfu 2.71%\n",
      "iter 495: loss 0.4992, time 2294.10ms, mfu 2.71%\n",
      "iter 496: loss 0.4382, time 2290.73ms, mfu 2.71%\n",
      "iter 497: loss 0.5035, time 2294.06ms, mfu 2.71%\n",
      "iter 498: loss 0.4637, time 2296.88ms, mfu 2.71%\n",
      "iter 499: loss 0.4961, time 2291.76ms, mfu 2.71%\n",
      "step 500: train loss 0.4823, val loss 7.3903\n",
      "iter 500: loss 0.5006, time 11534.59ms, mfu 2.49%\n",
      "iter 501: loss 0.4312, time 2286.60ms, mfu 2.51%\n",
      "iter 502: loss 0.4732, time 2295.53ms, mfu 2.53%\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/s/bach/l/under/daives/cs445/assignments/nikhil-GPT/nanoGPT/train.py\", line 293, in <module>\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! python3 train.py config/train_config.py --out_dir=out-nikhil-gpt-med --compile=False --eval_iters=200 --log_interval=1 --block_size=128 --batch_size=32 --n_layer=8 --n_head=8 --n_embd=256 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We overfit after only after a few hundred iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-nikhil-gpt-med\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 19.17M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      ". So you think about that they have we can see so if I want that gradient of the model. Well, and then you can do data, an individual weights. I'll do they're not help. And that. So, but the output is going to get this is going to have a table and then I'm going to be the hidden layer to be the activation function I'll have a different classes that we can create the action. And it with this thing. And so that is the convolutional layer, here and I'm just going to take the next action more test. This is I'm just probably this is basically the training data for all of this. So we're going to get to the second layer. I'm going to show the, and then I have a see how to see if I'm not the one. So I will,, that I'm going to do 1. No, we also say the last class one, the derivative of 0 time, there was going to be one, I'm just be us to be the number of the data this for your time,, so the sum of kind of the convolutional layer and the second set that the number of us to be the class of that kind of this the probability of the feature, here. So I have very n by just draw the gradient of the target output of the right so if I'm doing x by a this is going to compute the difference through a other by the output or the weights to be the way, because it and I take the test data. So for some problem. So I'm doing everything in some error function, this is just basically here. Any negative output is I'm just do that different class. So I will be. So this, you know my kind X. So this is now I'm on the number of interest. This. I'll print,, now, you can say, I can try as we have seven. And so now I need to run this, then in my logative value is for class1, if then I have the product, the first one, and one, this is in a weight. Okay with and remember here by that is basically, I'm can know if I take pretty now I'm going to give me to keep a lot of the reward of my target moment, you know each means. And I'm just change the number of those two weights. Yeah is, convolutional nets, I'm\n",
      "---------------\n",
      "\n",
      " in the following and you see for like the log times. So we can see like this set and it and we're going to be pretty cost of the other things if you're going to data and what you know let me the other. So, but like I have to the same value. So previously. So I can do that it, then I'm actually figure is I'm going to do is going to do this is the last diamond action, we get that and my data. So a linear regression or the number of a test on the state and then now we're basically this is the output. So if I'm going to think is to two code that this is going to scale it's a moment is a single action one. That. So I can try and we can do you do I can do. And so I will have to the activation function that we're going to be the gradient of we know the one that, this because this we're getting the update out over how my end of those if it? I'm going to be a set from the output value and then because I don't, and 10 epoch, one and I have some a patch size, say we can do. So I'm trying to do not the my goal is going to the normal two input size of I'm trying to get now I the sum of the other prediction as a whole car. And then the sample to take some probability of respect to this is in the ground value of the stride of the next data that we're going to your one value or the mean of an output and if I mean the derivative of the Q value not going to do is going to be done. I have some regression representation of P of e to be going to the current input because it is going to be the best input input that in the goal is then I'm going to do is the average. So I want to keep the model is how we can have a two, then I run the other data, I'm going to the QDA and see this thing and then the previous problem is going to it's going to predict the log likelihood. So this is I want to take this is going to be some action to do is going to be the output of the samples that, the first class and the same value, I'm going to go to just equal to get the row operation. And so you at the output, I can, I take the layer this case, in this looks like,\n",
      "---------------\n",
      "\n",
      " for multiple things is not doing this list of the joint hidden error of. And so if it to the other regression to specify the prior value that for every value you're going to be the average of the output of that. And so here and W is not the same state. And so, and then zero and linear value. Okay, let's let's still look at this, so it is really neural network. So now I'm going to a couple of the assignment. But if you can see, then I look at the gradient of the model to actually be a very better than my gradient of the left to be like for the update the prediction what's do this, this output and the same. And then we'll be familiar different weights that will do is sort of this as the neural network action.6 and then I mean the other in the gradient. And so this is going to be 100 first one, we have a good activation function. So if I need to the other outputs. My n. So, so that. And so I'm going to take my previous right I choose this are the one descent entropy is let me the class then the current state and that I will actually try it's pretty this is not an learning to the same, we're not going to make some test data, I can write the ground values there's going to be some value. So now the things, the probability of us to that, I'm going to be the next moment, so now I'm going to update my ground would be in the same Q class of the action with those classes, we have the gradient of error to be just done. So if I'm going to be a high, I have now I have the action or 0, here is going to take just a a nice state. Right. So now we can not the sample, I take this is the output is these this will go to be I'll print the output, then I need to do this just get going to be the input, actually be the delta sample, what and I think the state, 1 and then the end of that we'll do, this class so if I can just a very values. So I can be like this distribution. So now I'm just be the exact next is use the next state and and then I'm going to use the entire time that thisDA and then the end. This is now? I'll do is our the first class to that the right so\n",
      "---------------\n",
      "\n",
      " and the convolutional layer. And so when a bit of the end of each of the same way that I can be this is going to do this is the error all of, for the testing things that it's not going to see that I want this is you get to be the second column of the output that that we'll do the actual. You will do, but it's probably kind of X and basically the second nine right here. And then the same action things like this function. So I'm actually try and I'm trying to be pretty linear model. So let's make a particular state so it's like that is I'm going to be able to be the zero and we just take the patch. So we can do is going to be the the notebook right so that I'll so this one for the Q to be with the input this that is basically at my other weights, and we can do of W is have a close. So this is basically say, it's just, i'm going to each of the world is going to apply the one is going to create this thing I'll write my sample, this row, is the test data, the patch of the training data as a next sample and the sample is that you have the weights for all the gradient of the error be like what I'm going to the convolutional Q value more weights, and not Q function. So I can have some of different, right? We can say the same thing that right and multiply that it's have a one for every function is that convolutional net to it's going to be using the probability of the probability is going to look at the state so now I can just like input right now I will have the goal where these are I can look use the linear representation and then And then we can do is, and my gradient of the class of n, I'll see class that we have a way to do this I to be a a log of what x function, some of those, if I want to call the weights here. So we have this is what's probably just pretty close to do this from the standard inputs and them for sum of m? So yeah. So this is I'm going to take one and there and if I can be an epsilon, you do is going to be a column. It is going to create this, for this is the first one set of the gradient this one I've already. So now I'll\n",
      "---------------\n",
      "\n",
      " in, if that you for of you seen three, right, are more assignments, and you imagine if I'm going to be a cost of that's really close to use this is we know this is a couple of course is like a couple of the state and I want to the test on the return in some probability of that, and we can kind of. So, it's going to be going to be basically this so we can actually plot the data. So you can basically going to be a neural case, you can tell you know that you be really be a two thing actually have a end of those these things that you just take some those different action plus the first fullyically to give me a return but we mean of these two things that I'm going to see the error in this class. And I have this, I'll have the output of a I'm probably like this is one for the same. So now you can have some neural network would have the class of the error with my indicator variables you know what we can see in the first class or the test data. And then z sub those, so then we'll find the weights, or the return is just basically I can do is going to be the information, so that data. Let's take the testing data. If I'm going to initialize them have like a neural networks, there is the total gradient of them is going to the number of a one is we can have the best thing around in them. So what I'm now I will train in the next is our hidden layers is, so it's going to be the probability that with the right, if I don't know that is we'll make a function, in this at your softmax function into one and then then you can get function this. So this is now this will get the I'm going to find the error. So I have to do, that neural network. And so you want to be familiar next gradient. So if I will go then I'm going to fall across the same sample. So I'm going to make the sum of the bottom value. So you know, I have a high inputs. But if I're going to see a week, right now I might be the error and then I can do is going to look at your expected data, this is like the test zero. So I have a, so, this is some sum of x, now the output. So, in my log point times W with\n",
      "---------------\n",
      "\n",
      ". So you know that you need these two values for anything, the train questions, a whole hidden layer. Let me do it's like the data and its linear function. So we do. You can do you have some point. And some of the sum of those sort of the world. So we got before. So is, you should plot an model is the state. And then, the assignment one, 1. So, you look at the only neural network. So, if you will go into the same way of the data, we can use a nice basically call your state of my prediction, then if you can actually I'm just have the only so for that I'm going to move this is I'm trying to use that you have four by that is actually just just remember this, take a single reason I'm going to be a single function. So 10, I'm going to turn this. The targets, so you can see if the goal is I guess I'm going to update is using a large array of S,? All right so if we can get small class thing that is going to be them, so I can I will notice that data for the Q in weight one of all my goal is going to need to be the next sample. But I need to use these where you're my number of X, this is that we have this is going to do this will allow like my position now I don't want to do is not be the training net that is going to find again, so for the same value is represented to solve this basically say I'll be able to do is going to take the same. So I will get the left, the gradient is going to be your n output is just be the different example, let's the nonlinear actions that right, I'm, I need to be go all of class is not linear data and then start your output of the edge and then. So A. This is a one of this, I'm I have here. So I did will be the, and then I have this have to be the world and I also be some of get this class two days and the current time. So a couple of the point and this is the bias. So what I'm going to make some neural network and here to have well, except in other that is I'm going to minimize X by all the output is I need to be the number of the X right so it, the only so this is\n",
      "---------------\n",
      "\n",
      " before I'm going to be getting this, and basically, but not we have a model is the learningpose if I'm trying to neural networks out of the negative other values. So, so I'll do I want to see what I should do my state, and I'm going to be a little bit also get we have a the output from the input that I'm going to with the class, I go to just probably back in your error. So this is. So we have classification function that I can be able to do like the next to the goal, I have some sample. So I can be the activation problem, I can choose a time distribution of basically have the one by two class of those bias, T descent. And so this, if I'm going to be the output. I have an neural network so now you can you know all just just actually plot the stride. This is a linear linear data is just the sum one. So basically like what we're not now if I think when it, and that I don't need to be a decent sample one of a diamond, those the end of you that would add a lot of the only input, 1, this function to read the input, where it's say, the class information for the Q value. And so we'll see if you can see that row of the action, So I'm going to do this. And so all one of the gradient with the output one, it will be pretty probability of that this is going to actually a sample. So this class and then the shape of a way with being a constant of those two, this update the training value of the red class that. So it's going to look at this case it, which is if you've got a number of things. Right. So in the learning with your goal. So if you'm going to draw the information. So that. You may be the sum of my last dimension problem that point of where you don't be the column of the gradient represents the number of an minimum, I'll have will be kind of the step, and then I can use the same right this really mean that we can see that right because we'm going to be the learning rate in x or they're just a as this is I'm going to make do you to probably to be my best here. So here. And then I'm think you can do is well for all of that the quote validation and basically in the class I\n",
      "---------------\n",
      "\n",
      " into a single. So this is just a lot of the action, what I will get the best left, it should just now I'm going to be a moment equals going to do is basically have a line in the first column. I'm going to be a to be some sample. So if I'll just in two things that I take this is going to be the output. So, because I'll really want to be the validation, there's basically a function that are going to be defined by my yance matrix. So, for well I'm going to print out. So I'm quite two, I'm trying to be some sample and then I'm not going to be the second day for the error one learning problem. And then I can also have a good data. And then I will pick the bias. And so I'm going to be a column in all of the output, and you have n by this is that gives me the previous action is what the same problem because I will be the neural networks I'm going to be a little bit of that, then I will be. So what I get and then I'm going to train what it's do is the sort of the Q here is going to change this. So these are familiar to be the row is and then I'm get with this is I'm going to apply the first value. And then I'm getting the best time in the number of the output is, you can be the same and then it's take the neural network. So, right. And this the input should do this sum of I'm looking through all of the is I want we can see that is now going to be the return right, which is I gave all of the input. If I'll do is basically want to be kind'm not, we've have in the state, what I could be doing the other target that all, we can use this data in this is the next. Right. And then I'm just going to optimize the sum of the minimum, but I have my the state and I have I'm going to the answer five by the thing in the end of the best model of what do this is going to be the same. So if I'll be an individual and I'm going to only on the time, you need to take a different data. So I'm going to have kind of the output point of our difference function should, so I can do is function. So, if I need\n",
      "---------------\n",
      "\n",
      " like the gradient of how many samples, but the, it's not necessarily be each of my two. So we have a value, it. So now I'm just a standard code. And that means I will be my return. So I need to do is if I'm trying to compute the first? So, if I'm not want to have a column of it's say if I'm going to. I'm going to do is the second model, right, you have the row is I'm going to the bias to have this is here, a position,. So now I'll make this is going to be an dimension of x, I'm say, if I'll notice that two class one. So basically the same distribution. So we have some features that we have a probability of the test things, I want to be the goal, one? I have this state, there there are all sample minus what I can do is to do maybe a single function, what I want the outputs. These are the number of the convolutional model I'm trying to have a one time to make the train, right should be this, I'm not very certain action and all the it's going to predict the input and I can specify the training data can make 0. And so, you multiply that has well. So that I can do is the is I'm going to be some time and then we can just going to be able to the end for n by its samples. So right then I'm basically my standard deviations that filter, for my test same class so let's run the output, I can see that it you're going to be quite a bunch of the number or I'm going to do that's now if I said in this. And it seems to be a. So, this really think where right. I it's going to import one for example. And I'm going to be pretty a one vector, all the model that is and I need to reach that I'll use the end of a single value it should be the, let's just a softmax here about the action. So now I'll see the moment here, right, even now I'll then I'm going to be basically the same that. So now I'm trying to do one in one, because I will be a two of basically plot my class. So and the to use that I have try all we saw everything does this is just sort of the training to the sample\n",
      "---------------\n",
      "\n",
      " on the same to the the class so basically what I'm trying to be the value of example about this means to be like a sudden test, and then I actually like, then I need to get then I have a neural network. So I'll just be the Q thing that I do my I'm closer to the inputs that as the expected wrong, it's going to do is the image. So this is going to be the data to try these, and I will train, or all of the target values in the second number of course, I'll make a linear number of the output is t. So, you have d. I'm going to with a good next layer. So I'm going to the bottom up with a nonlinear path that I think of neural network number of the action is the next class is each sum of some individual world of that's now, because I'm trying to do is actually be the output of all of z, the training data, all the this case, and then I will want to take the model is going to make so therefore the state here is just a long matrix two different size of our probability you can do this is this sample. So I will do is we have the second thing that we can calculate the entire sort of that sort of this. So here is a line, just just kind of one. So you can use of the next state the value update the next layer times the first a seven by 2, and for the return, now I'm not much also going to the entire test, right, they can actually get the right here, the epsilon. So I would be out the tan neural network and class. So, you have an gallon. So this function so you, I'm going to be the weights. So this is, I will be a bunch of the value of y, it's a nice into them in least time. So what look at I'm going to be the update the number of the time, then have some output of two, I'm'm going to take a linear x and then, so I'm going to be the input as a gradient now right, which is I'll think of this. And we can do I'm not where only of the number of two is going to set of in all. Right. And so I'm going to look at this,, because what I want to probably be my. So I will have to be the next classes is still can\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "! python3 sample.py --out_dir=out-nikhil-gpt-med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-nikhil-gpt-med\n",
      "Overriding: start = another way to look at it is\n",
      "Overriding: num_samples = 1\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 19.17M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "another way to look at it is what we're just doing that they have we can see so if I want that and then you take a list of the exact velocity is basically just do an individual weights. I'll do is what I have two. So what I have to the output. So now I see 100, but I'll do is going to the value of those, if I have multiple other, and I'll have a different classes and then I'm just have a lot of the mathematics up in these right so now we have rid of 10 values. So I'm just have two by 1 with this more by kind of X is just just probably this is basically this, I'm going to this. So we're going to get to the second layer. I'm going to show the, and then I have a different same distribution of all my X, the one. So I will,, that I'm going to do 1. No, we also say the last class one, the derivative of 0 time, there was going to be one, I'm just be us to be the number. So of this for your time,, so the sum of kind of the convolutional N by the second set that the number of us to be the class of that kind of this the probability of the feature, here. So I have very n by just draw the gradient of the target output of the right so if I'm doing x by a this is going to compute the difference through a other by the output or the weights to be the way, because it and I take the test data. So for some problem. So I'm doing everything in some error function, this is just basically here. Any negative output is I'm just do that different class. So I will be. So this, you know my kind X of the prediction of orange, I want to be the probability of that. I'll print,, one, you can say, I can try as one of the current covariance of the activation function set of two variables in my logative value is for the error, the sum of my next now an the first one, and one, this is in a weight. Okay with and then I can see this. And then make this that weight shape for the softmax over this look at the input size of the have the reward. And I'm going to be each means. And I'm just change the number of those two weights. Yeah is, convolutional layer, I'm\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# get input from user\n",
    "prompt = input(\"Enter a prompt: \")\n",
    "\n",
    "! python sample.py --out_dir=out-nikhil-gpt-med --start=\"$prompt\" --num_samples=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d54705a7c51399f88ff26e5af010bf5b52b49f4051d63947538750a079d2bb3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
