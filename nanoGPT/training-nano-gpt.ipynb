{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 225,094 tokens\n",
      "val has 25,247 tokens\n"
     ]
    }
   ],
   "source": [
    "! python3 ./data/prepare.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we had to tokenize our data. We used the GPT2 tokenizer rather than a simple one to hopefully get a better result. Let's start by training a small model, these parameters are taken from the nanoGPT repo for building a small model with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_config.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-nikhil-gpt'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'nikhil-gpt'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'medium'\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cpu\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 20\n",
      "Overriding: log_interval = 1\n",
      "Overriding: block_size = 64\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 2000\n",
      "Overriding: lr_decay_iters = 2000\n",
      "Overriding: dropout = 0.0\n",
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "using fused AdamW: False\n",
      "step 0: train loss 10.8502, val loss 10.8464\n",
      "Traceback (most recent call last):\n",
      "  File \"/s/bach/l/under/daives/cs445/assignments/nikhil-GPT/nanoGPT/train.py\", line 293, in <module>\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py\", line 161, in scale\n",
      "    assert outputs.is_cuda or outputs.device.type == 'xla'\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py config/train_config.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d54705a7c51399f88ff26e5af010bf5b52b49f4051d63947538750a079d2bb3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
