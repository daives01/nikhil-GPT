{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 225,094 tokens\n",
      "val has 25,247 tokens\n"
     ]
    }
   ],
   "source": [
    "! python3 ./data/prepare.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we had to tokenize our data. We used the GPT2 tokenizer rather than a simple one to hopefully get a better result. Let's start by training a small model, these parameters are taken from the nanoGPT repo for building a small model with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_config.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-nikhil-gpt'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'nikhil-gpt'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'medium'\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 20\n",
      "Overriding: log_interval = 1\n",
      "Overriding: block_size = 64\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 2000\n",
      "Overriding: lr_decay_iters = 2000\n",
      "Overriding: dropout = 0.0\n",
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "using fused AdamW: False\n",
      "step 0: train loss 10.8502, val loss 10.8463\n",
      "iter 0: loss 10.8636, time 3186.11ms, mfu -100.00%\n",
      "iter 1: loss 10.8582, time 395.05ms, mfu -100.00%\n",
      "iter 2: loss 10.8367, time 394.45ms, mfu -100.00%\n",
      "iter 3: loss 10.8351, time 398.42ms, mfu -100.00%\n",
      "iter 4: loss 10.8162, time 399.03ms, mfu -100.00%\n",
      "iter 5: loss 10.7756, time 392.38ms, mfu 1.10%\n",
      "iter 6: loss 10.7320, time 395.34ms, mfu 1.10%\n",
      "iter 7: loss 10.7089, time 400.78ms, mfu 1.09%\n",
      "iter 8: loss 10.6398, time 399.70ms, mfu 1.09%\n",
      "iter 9: loss 10.6214, time 402.09ms, mfu 1.09%\n",
      "iter 10: loss 10.5922, time 402.17ms, mfu 1.09%\n",
      "iter 11: loss 10.5196, time 393.26ms, mfu 1.09%\n",
      "iter 12: loss 10.5111, time 399.66ms, mfu 1.09%\n",
      "iter 13: loss 10.4722, time 395.44ms, mfu 1.09%\n",
      "iter 14: loss 10.4326, time 395.85ms, mfu 1.09%\n",
      "iter 15: loss 10.4437, time 399.91ms, mfu 1.09%\n",
      "iter 16: loss 10.3831, time 396.04ms, mfu 1.09%\n",
      "iter 17: loss 10.3241, time 394.44ms, mfu 1.09%\n",
      "iter 18: loss 10.3032, time 400.80ms, mfu 1.09%\n",
      "iter 19: loss 10.2599, time 400.77ms, mfu 1.09%\n",
      "iter 20: loss 10.2380, time 398.80ms, mfu 1.08%\n",
      "iter 21: loss 10.2154, time 392.69ms, mfu 1.09%\n",
      "iter 22: loss 10.1318, time 396.53ms, mfu 1.09%\n",
      "iter 23: loss 10.0988, time 400.32ms, mfu 1.09%\n",
      "iter 24: loss 10.0623, time 400.05ms, mfu 1.08%\n",
      "iter 25: loss 9.9999, time 392.22ms, mfu 1.09%\n",
      "iter 26: loss 9.9436, time 400.11ms, mfu 1.08%\n",
      "iter 27: loss 9.8480, time 399.25ms, mfu 1.08%\n",
      "iter 28: loss 9.8130, time 400.05ms, mfu 1.08%\n",
      "iter 29: loss 9.7481, time 399.94ms, mfu 1.08%\n",
      "iter 30: loss 9.6830, time 398.33ms, mfu 1.08%\n",
      "iter 31: loss 9.6215, time 399.19ms, mfu 1.08%\n",
      "iter 32: loss 9.4881, time 399.50ms, mfu 1.08%\n",
      "iter 33: loss 9.4415, time 399.56ms, mfu 1.08%\n",
      "iter 34: loss 9.3670, time 400.18ms, mfu 1.08%\n",
      "iter 35: loss 9.2785, time 399.92ms, mfu 1.08%\n",
      "iter 36: loss 9.1884, time 399.12ms, mfu 1.08%\n",
      "iter 37: loss 9.1356, time 399.77ms, mfu 1.08%\n",
      "iter 38: loss 9.0438, time 399.09ms, mfu 1.08%\n",
      "iter 39: loss 8.9727, time 400.16ms, mfu 1.08%\n",
      "iter 40: loss 8.8481, time 397.02ms, mfu 1.08%\n",
      "iter 41: loss 8.7469, time 398.54ms, mfu 1.08%\n",
      "iter 42: loss 8.7093, time 398.76ms, mfu 1.08%\n",
      "iter 43: loss 8.6051, time 399.86ms, mfu 1.08%\n",
      "iter 44: loss 8.5688, time 399.87ms, mfu 1.08%\n",
      "iter 45: loss 8.3865, time 397.82ms, mfu 1.08%\n",
      "iter 46: loss 8.3183, time 400.73ms, mfu 1.08%\n",
      "iter 47: loss 8.2152, time 399.77ms, mfu 1.08%\n",
      "iter 48: loss 8.2144, time 399.91ms, mfu 1.08%\n",
      "iter 49: loss 8.0437, time 400.15ms, mfu 1.08%\n",
      "iter 50: loss 8.0484, time 395.44ms, mfu 1.08%\n",
      "iter 51: loss 7.8847, time 396.80ms, mfu 1.08%\n",
      "iter 52: loss 7.7358, time 400.36ms, mfu 1.08%\n",
      "iter 53: loss 7.7818, time 397.76ms, mfu 1.08%\n",
      "iter 54: loss 7.5642, time 400.87ms, mfu 1.08%\n",
      "iter 55: loss 7.4735, time 400.48ms, mfu 1.08%\n",
      "iter 56: loss 7.4249, time 400.28ms, mfu 1.08%\n",
      "iter 57: loss 7.3444, time 401.13ms, mfu 1.08%\n",
      "iter 58: loss 7.2468, time 399.27ms, mfu 1.08%\n",
      "iter 59: loss 7.1916, time 399.61ms, mfu 1.08%\n",
      "iter 60: loss 7.1404, time 400.41ms, mfu 1.08%\n",
      "iter 61: loss 7.0099, time 403.08ms, mfu 1.08%\n",
      "iter 62: loss 6.9705, time 398.04ms, mfu 1.08%\n",
      "iter 63: loss 6.9086, time 398.91ms, mfu 1.08%\n",
      "iter 64: loss 6.8124, time 395.23ms, mfu 1.08%\n",
      "iter 65: loss 6.8218, time 399.88ms, mfu 1.08%\n",
      "iter 66: loss 6.7445, time 399.45ms, mfu 1.08%\n",
      "iter 67: loss 6.5525, time 398.66ms, mfu 1.08%\n",
      "iter 68: loss 6.4289, time 397.75ms, mfu 1.08%\n",
      "iter 69: loss 6.4457, time 401.70ms, mfu 1.08%\n",
      "iter 70: loss 6.4960, time 399.84ms, mfu 1.08%\n",
      "iter 71: loss 6.3459, time 399.27ms, mfu 1.08%\n",
      "iter 72: loss 6.3601, time 400.50ms, mfu 1.08%\n",
      "iter 73: loss 6.2553, time 399.61ms, mfu 1.08%\n",
      "iter 74: loss 6.0537, time 390.14ms, mfu 1.08%\n",
      "iter 75: loss 6.2544, time 388.33ms, mfu 1.08%\n",
      "iter 76: loss 6.0817, time 395.92ms, mfu 1.08%\n",
      "iter 77: loss 6.1840, time 396.13ms, mfu 1.08%\n",
      "iter 78: loss 5.8844, time 398.68ms, mfu 1.08%\n",
      "iter 79: loss 6.0105, time 400.18ms, mfu 1.08%\n",
      "iter 80: loss 5.9473, time 398.38ms, mfu 1.08%\n",
      "iter 81: loss 5.9708, time 398.87ms, mfu 1.08%\n",
      "iter 82: loss 5.7501, time 399.09ms, mfu 1.08%\n",
      "iter 83: loss 5.8767, time 398.76ms, mfu 1.08%\n",
      "iter 84: loss 5.7877, time 400.91ms, mfu 1.08%\n",
      "iter 85: loss 5.5026, time 397.74ms, mfu 1.08%\n",
      "iter 86: loss 5.9141, time 399.16ms, mfu 1.08%\n",
      "iter 87: loss 5.6938, time 400.28ms, mfu 1.08%\n",
      "iter 88: loss 5.5965, time 399.02ms, mfu 1.08%\n",
      "iter 89: loss 5.7085, time 397.16ms, mfu 1.08%\n",
      "iter 90: loss 5.5678, time 397.19ms, mfu 1.08%\n",
      "iter 91: loss 5.7412, time 399.94ms, mfu 1.08%\n",
      "iter 92: loss 5.6959, time 398.74ms, mfu 1.08%\n",
      "iter 93: loss 5.4179, time 399.64ms, mfu 1.08%\n",
      "iter 94: loss 5.3793, time 398.47ms, mfu 1.08%\n",
      "iter 95: loss 5.5860, time 399.65ms, mfu 1.08%\n",
      "iter 96: loss 5.7250, time 399.34ms, mfu 1.08%\n",
      "iter 97: loss 5.5389, time 399.63ms, mfu 1.08%\n",
      "iter 98: loss 5.3763, time 398.90ms, mfu 1.08%\n",
      "iter 99: loss 5.3536, time 400.96ms, mfu 1.08%\n",
      "iter 100: loss 5.5388, time 387.58ms, mfu 1.08%\n",
      "iter 101: loss 5.2831, time 400.30ms, mfu 1.08%\n",
      "iter 102: loss 5.4256, time 399.99ms, mfu 1.08%\n",
      "iter 103: loss 5.4781, time 400.79ms, mfu 1.08%\n",
      "iter 104: loss 5.2827, time 398.34ms, mfu 1.08%\n",
      "iter 105: loss 5.1317, time 397.49ms, mfu 1.08%\n",
      "iter 106: loss 5.2016, time 399.97ms, mfu 1.08%\n",
      "iter 107: loss 5.2240, time 399.86ms, mfu 1.08%\n",
      "iter 108: loss 5.1898, time 398.11ms, mfu 1.08%\n",
      "iter 109: loss 5.2370, time 399.84ms, mfu 1.08%\n",
      "iter 110: loss 5.0930, time 399.33ms, mfu 1.08%\n",
      "iter 111: loss 5.2151, time 397.73ms, mfu 1.08%\n",
      "iter 112: loss 5.0794, time 400.69ms, mfu 1.08%\n",
      "iter 113: loss 5.0685, time 399.67ms, mfu 1.08%\n",
      "iter 114: loss 5.2882, time 399.90ms, mfu 1.08%\n",
      "iter 115: loss 5.1539, time 399.29ms, mfu 1.08%\n",
      "iter 116: loss 4.8254, time 399.98ms, mfu 1.08%\n",
      "iter 117: loss 5.1936, time 399.91ms, mfu 1.08%\n",
      "iter 118: loss 4.7870, time 397.03ms, mfu 1.08%\n",
      "iter 119: loss 4.9156, time 395.98ms, mfu 1.08%\n",
      "iter 120: loss 4.8908, time 399.59ms, mfu 1.08%\n",
      "iter 121: loss 4.9908, time 399.70ms, mfu 1.08%\n",
      "iter 122: loss 5.0571, time 399.62ms, mfu 1.08%\n",
      "iter 123: loss 5.1087, time 400.33ms, mfu 1.08%\n",
      "iter 124: loss 4.8343, time 400.59ms, mfu 1.08%\n",
      "iter 125: loss 4.5610, time 387.95ms, mfu 1.08%\n",
      "iter 126: loss 4.6912, time 400.76ms, mfu 1.08%\n",
      "iter 127: loss 4.6056, time 399.60ms, mfu 1.08%\n",
      "iter 128: loss 4.7756, time 398.96ms, mfu 1.08%\n",
      "iter 129: loss 4.8427, time 399.94ms, mfu 1.08%\n",
      "iter 130: loss 4.6901, time 398.27ms, mfu 1.08%\n",
      "iter 131: loss 4.7720, time 400.64ms, mfu 1.08%\n",
      "iter 132: loss 4.8653, time 398.48ms, mfu 1.08%\n",
      "iter 133: loss 4.5943, time 399.07ms, mfu 1.08%\n",
      "iter 134: loss 4.9779, time 399.73ms, mfu 1.08%\n",
      "iter 135: loss 4.5816, time 398.53ms, mfu 1.08%\n",
      "iter 136: loss 4.6233, time 398.94ms, mfu 1.08%\n",
      "iter 137: loss 4.4638, time 396.30ms, mfu 1.08%\n",
      "iter 138: loss 4.7539, time 399.50ms, mfu 1.08%\n",
      "iter 139: loss 4.7844, time 399.60ms, mfu 1.08%\n",
      "iter 140: loss 4.6210, time 398.87ms, mfu 1.08%\n",
      "iter 141: loss 4.4620, time 399.87ms, mfu 1.08%\n",
      "iter 142: loss 4.7866, time 398.73ms, mfu 1.08%\n",
      "iter 143: loss 4.6248, time 396.99ms, mfu 1.08%\n",
      "iter 144: loss 4.6544, time 398.99ms, mfu 1.08%\n",
      "iter 145: loss 4.5347, time 400.53ms, mfu 1.08%\n",
      "iter 146: loss 4.5130, time 397.26ms, mfu 1.08%\n",
      "iter 147: loss 4.5531, time 394.07ms, mfu 1.08%\n",
      "iter 148: loss 4.5985, time 399.19ms, mfu 1.08%\n",
      "iter 149: loss 4.5449, time 399.40ms, mfu 1.08%\n",
      "iter 150: loss 4.5377, time 388.11ms, mfu 1.08%\n",
      "iter 151: loss 4.5278, time 398.01ms, mfu 1.08%\n",
      "iter 152: loss 4.4374, time 396.41ms, mfu 1.08%\n",
      "iter 153: loss 4.4543, time 399.93ms, mfu 1.08%\n",
      "iter 154: loss 4.5225, time 399.30ms, mfu 1.08%\n",
      "iter 155: loss 4.6007, time 398.56ms, mfu 1.08%\n",
      "iter 156: loss 4.3380, time 401.23ms, mfu 1.08%\n",
      "iter 157: loss 4.2750, time 400.57ms, mfu 1.08%\n",
      "iter 158: loss 4.6158, time 398.70ms, mfu 1.08%\n",
      "iter 159: loss 4.4834, time 398.40ms, mfu 1.08%\n",
      "iter 160: loss 4.2432, time 395.68ms, mfu 1.08%\n",
      "iter 161: loss 4.1946, time 399.56ms, mfu 1.08%\n",
      "iter 162: loss 4.2116, time 397.77ms, mfu 1.08%\n",
      "iter 163: loss 4.5470, time 400.14ms, mfu 1.08%\n",
      "iter 164: loss 4.1217, time 399.64ms, mfu 1.08%\n",
      "iter 165: loss 4.3427, time 395.93ms, mfu 1.08%\n",
      "iter 166: loss 4.1737, time 398.30ms, mfu 1.08%\n",
      "iter 167: loss 4.3870, time 395.30ms, mfu 1.08%\n",
      "iter 168: loss 4.2625, time 396.44ms, mfu 1.08%\n",
      "iter 169: loss 4.3341, time 394.76ms, mfu 1.08%\n",
      "iter 170: loss 4.3191, time 397.53ms, mfu 1.08%\n",
      "iter 171: loss 4.4389, time 401.52ms, mfu 1.08%\n",
      "iter 172: loss 4.3915, time 399.78ms, mfu 1.08%\n",
      "iter 173: loss 4.3965, time 400.63ms, mfu 1.08%\n",
      "iter 174: loss 4.2769, time 402.09ms, mfu 1.08%\n",
      "iter 175: loss 4.2413, time 389.86ms, mfu 1.08%\n",
      "iter 176: loss 4.1725, time 397.33ms, mfu 1.08%\n",
      "iter 177: loss 3.9571, time 396.19ms, mfu 1.08%\n",
      "iter 178: loss 4.2376, time 388.69ms, mfu 1.09%\n",
      "iter 179: loss 4.3445, time 394.16ms, mfu 1.09%\n",
      "iter 180: loss 4.1489, time 393.16ms, mfu 1.09%\n",
      "iter 181: loss 4.1976, time 400.61ms, mfu 1.09%\n",
      "iter 182: loss 4.2295, time 397.55ms, mfu 1.09%\n",
      "iter 183: loss 4.1945, time 399.37ms, mfu 1.09%\n",
      "iter 184: loss 4.0878, time 398.91ms, mfu 1.08%\n",
      "iter 185: loss 4.1028, time 399.10ms, mfu 1.08%\n",
      "iter 186: loss 4.0543, time 399.65ms, mfu 1.08%\n",
      "iter 187: loss 4.2750, time 400.39ms, mfu 1.08%\n",
      "iter 188: loss 4.2404, time 398.07ms, mfu 1.08%\n",
      "iter 189: loss 4.0453, time 400.42ms, mfu 1.08%\n",
      "iter 190: loss 4.2756, time 399.31ms, mfu 1.08%\n",
      "iter 191: loss 4.0846, time 399.35ms, mfu 1.08%\n",
      "iter 192: loss 4.1595, time 399.71ms, mfu 1.08%\n",
      "iter 193: loss 4.0997, time 399.96ms, mfu 1.08%\n",
      "iter 194: loss 4.1586, time 396.61ms, mfu 1.08%\n",
      "iter 195: loss 4.1059, time 400.45ms, mfu 1.08%\n",
      "iter 196: loss 4.3180, time 399.03ms, mfu 1.08%\n",
      "iter 197: loss 4.2657, time 399.15ms, mfu 1.08%\n",
      "iter 198: loss 4.0632, time 399.34ms, mfu 1.08%\n",
      "iter 199: loss 4.1221, time 398.51ms, mfu 1.08%\n",
      "iter 200: loss 4.0862, time 393.30ms, mfu 1.08%\n",
      "iter 201: loss 3.7953, time 394.99ms, mfu 1.08%\n",
      "iter 202: loss 4.0666, time 398.46ms, mfu 1.08%\n",
      "iter 203: loss 4.0304, time 399.10ms, mfu 1.08%\n",
      "iter 204: loss 4.1692, time 400.14ms, mfu 1.08%\n",
      "iter 205: loss 4.1471, time 397.71ms, mfu 1.08%\n",
      "iter 206: loss 4.3072, time 401.34ms, mfu 1.08%\n",
      "iter 207: loss 4.0697, time 401.72ms, mfu 1.08%\n",
      "iter 208: loss 4.0121, time 400.74ms, mfu 1.08%\n",
      "iter 209: loss 3.7922, time 398.22ms, mfu 1.08%\n",
      "iter 210: loss 3.8638, time 399.90ms, mfu 1.08%\n",
      "iter 211: loss 4.3114, time 400.84ms, mfu 1.08%\n",
      "iter 212: loss 4.0522, time 400.37ms, mfu 1.08%\n",
      "iter 213: loss 3.9659, time 399.14ms, mfu 1.08%\n",
      "iter 214: loss 4.0591, time 394.32ms, mfu 1.08%\n",
      "iter 215: loss 3.9510, time 387.88ms, mfu 1.08%\n",
      "iter 216: loss 3.8369, time 397.37ms, mfu 1.08%\n",
      "iter 217: loss 4.0013, time 400.47ms, mfu 1.08%\n",
      "iter 218: loss 3.9696, time 400.67ms, mfu 1.08%\n",
      "iter 219: loss 3.9791, time 407.35ms, mfu 1.08%\n",
      "iter 220: loss 3.8667, time 402.02ms, mfu 1.08%\n",
      "iter 221: loss 3.8900, time 389.17ms, mfu 1.08%\n",
      "iter 222: loss 3.9924, time 380.46ms, mfu 1.09%\n",
      "iter 223: loss 3.8716, time 380.11ms, mfu 1.09%\n",
      "iter 224: loss 3.9642, time 388.70ms, mfu 1.09%\n",
      "iter 225: loss 3.9845, time 395.00ms, mfu 1.09%\n",
      "iter 226: loss 4.0020, time 392.65ms, mfu 1.09%\n",
      "iter 227: loss 3.9349, time 397.81ms, mfu 1.09%\n",
      "iter 228: loss 4.0829, time 399.93ms, mfu 1.09%\n",
      "iter 229: loss 3.6692, time 398.08ms, mfu 1.09%\n",
      "iter 230: loss 3.8632, time 399.14ms, mfu 1.09%\n",
      "iter 231: loss 3.8965, time 400.28ms, mfu 1.09%\n",
      "iter 232: loss 3.9942, time 396.05ms, mfu 1.09%\n",
      "iter 233: loss 3.8323, time 400.48ms, mfu 1.09%\n",
      "iter 234: loss 3.8030, time 399.35ms, mfu 1.09%\n",
      "iter 235: loss 3.9968, time 396.48ms, mfu 1.09%\n",
      "iter 236: loss 4.0580, time 399.03ms, mfu 1.09%\n",
      "iter 237: loss 3.8091, time 399.68ms, mfu 1.08%\n",
      "iter 238: loss 3.8550, time 384.12ms, mfu 1.09%\n",
      "iter 239: loss 3.8462, time 390.39ms, mfu 1.09%\n",
      "iter 240: loss 3.7857, time 396.83ms, mfu 1.09%\n",
      "iter 241: loss 3.7814, time 398.71ms, mfu 1.09%\n",
      "iter 242: loss 3.8806, time 394.95ms, mfu 1.09%\n",
      "iter 243: loss 3.7690, time 400.46ms, mfu 1.09%\n",
      "iter 244: loss 3.7594, time 397.57ms, mfu 1.09%\n",
      "iter 245: loss 4.0022, time 390.58ms, mfu 1.09%\n",
      "iter 246: loss 3.6631, time 396.07ms, mfu 1.09%\n",
      "iter 247: loss 3.8334, time 397.80ms, mfu 1.09%\n",
      "iter 248: loss 3.9445, time 393.30ms, mfu 1.09%\n",
      "iter 249: loss 4.0049, time 390.29ms, mfu 1.09%\n",
      "step 250: train loss 3.8875, val loss 4.4846\n",
      "saving checkpoint to out-nikhil-gpt\n",
      "iter 250: loss 3.7831, time 1617.15ms, mfu 1.01%\n",
      "iter 251: loss 3.7086, time 399.71ms, mfu 1.01%\n",
      "iter 252: loss 3.8499, time 393.48ms, mfu 1.02%\n",
      "iter 253: loss 3.7153, time 373.29ms, mfu 1.04%\n",
      "iter 254: loss 3.5539, time 384.78ms, mfu 1.04%\n",
      "iter 255: loss 3.7761, time 395.71ms, mfu 1.05%\n",
      "iter 256: loss 3.6394, time 399.22ms, mfu 1.05%\n",
      "iter 257: loss 3.9465, time 400.73ms, mfu 1.05%\n",
      "iter 258: loss 3.6137, time 399.55ms, mfu 1.06%\n",
      "iter 259: loss 3.7014, time 394.25ms, mfu 1.06%\n",
      "iter 260: loss 3.7513, time 399.89ms, mfu 1.06%\n",
      "iter 261: loss 3.6432, time 396.94ms, mfu 1.06%\n",
      "iter 262: loss 3.6597, time 400.21ms, mfu 1.07%\n",
      "iter 263: loss 3.8756, time 383.51ms, mfu 1.07%\n",
      "iter 264: loss 3.8750, time 408.59ms, mfu 1.07%\n",
      "iter 265: loss 3.6819, time 392.04ms, mfu 1.07%\n",
      "iter 266: loss 3.7339, time 388.09ms, mfu 1.08%\n",
      "iter 267: loss 3.7200, time 398.68ms, mfu 1.08%\n",
      "iter 268: loss 3.8101, time 400.99ms, mfu 1.08%\n",
      "iter 269: loss 3.8494, time 399.14ms, mfu 1.08%\n",
      "iter 270: loss 3.7210, time 400.06ms, mfu 1.08%\n",
      "iter 271: loss 3.7527, time 400.41ms, mfu 1.08%\n",
      "iter 272: loss 3.6323, time 400.34ms, mfu 1.08%\n",
      "iter 273: loss 3.6761, time 386.74ms, mfu 1.08%\n",
      "iter 274: loss 3.8244, time 399.78ms, mfu 1.08%\n",
      "iter 275: loss 3.7330, time 397.98ms, mfu 1.08%\n",
      "iter 276: loss 3.8063, time 399.85ms, mfu 1.08%\n",
      "iter 277: loss 3.6635, time 398.87ms, mfu 1.08%\n",
      "iter 278: loss 3.6379, time 398.85ms, mfu 1.08%\n",
      "iter 279: loss 3.6340, time 400.37ms, mfu 1.08%\n",
      "iter 280: loss 3.9156, time 390.74ms, mfu 1.08%\n",
      "iter 281: loss 3.5819, time 391.42ms, mfu 1.08%\n",
      "iter 282: loss 3.6582, time 385.75ms, mfu 1.09%\n",
      "iter 283: loss 3.7540, time 389.66ms, mfu 1.09%\n",
      "iter 284: loss 3.6425, time 394.34ms, mfu 1.09%\n",
      "iter 285: loss 3.6655, time 397.77ms, mfu 1.09%\n",
      "iter 286: loss 3.5267, time 390.47ms, mfu 1.09%\n",
      "iter 287: loss 3.5674, time 398.78ms, mfu 1.09%\n",
      "iter 288: loss 3.8615, time 389.26ms, mfu 1.09%\n",
      "iter 289: loss 3.5897, time 399.04ms, mfu 1.09%\n",
      "iter 290: loss 3.6534, time 398.26ms, mfu 1.09%\n",
      "iter 291: loss 3.5121, time 399.71ms, mfu 1.09%\n",
      "iter 292: loss 3.6212, time 398.09ms, mfu 1.09%\n",
      "iter 293: loss 3.5224, time 401.66ms, mfu 1.09%\n",
      "iter 294: loss 3.5156, time 399.26ms, mfu 1.09%\n",
      "iter 295: loss 3.5596, time 398.79ms, mfu 1.08%\n",
      "iter 296: loss 3.5552, time 400.07ms, mfu 1.08%\n",
      "iter 297: loss 3.6933, time 400.74ms, mfu 1.08%\n",
      "iter 298: loss 3.5960, time 393.53ms, mfu 1.08%\n",
      "iter 299: loss 3.6271, time 405.88ms, mfu 1.08%\n",
      "iter 300: loss 3.6819, time 398.96ms, mfu 1.08%\n",
      "iter 301: loss 3.4585, time 387.28ms, mfu 1.08%\n",
      "iter 302: loss 3.6915, time 390.84ms, mfu 1.09%\n",
      "iter 303: loss 3.4744, time 401.11ms, mfu 1.09%\n",
      "iter 304: loss 3.5395, time 403.78ms, mfu 1.08%\n",
      "iter 305: loss 3.5844, time 398.36ms, mfu 1.08%\n",
      "iter 306: loss 3.4846, time 394.32ms, mfu 1.08%\n",
      "iter 307: loss 3.4934, time 399.68ms, mfu 1.08%\n",
      "iter 308: loss 3.6253, time 390.59ms, mfu 1.09%\n",
      "iter 309: loss 3.6324, time 398.83ms, mfu 1.08%\n",
      "iter 310: loss 3.5844, time 399.48ms, mfu 1.08%\n",
      "iter 311: loss 3.6449, time 398.89ms, mfu 1.08%\n",
      "iter 312: loss 3.7248, time 400.30ms, mfu 1.08%\n",
      "iter 313: loss 3.4438, time 400.72ms, mfu 1.08%\n",
      "iter 314: loss 3.5800, time 399.71ms, mfu 1.08%\n",
      "iter 315: loss 3.4518, time 400.94ms, mfu 1.08%\n",
      "iter 316: loss 3.3502, time 400.01ms, mfu 1.08%\n",
      "iter 317: loss 3.7155, time 397.06ms, mfu 1.08%\n",
      "iter 318: loss 3.5149, time 400.99ms, mfu 1.08%\n",
      "iter 319: loss 3.4880, time 400.34ms, mfu 1.08%\n",
      "iter 320: loss 3.6304, time 398.47ms, mfu 1.08%\n",
      "iter 321: loss 3.4488, time 400.28ms, mfu 1.08%\n",
      "iter 322: loss 3.5235, time 400.35ms, mfu 1.08%\n",
      "iter 323: loss 3.5197, time 393.23ms, mfu 1.08%\n",
      "iter 324: loss 3.3801, time 395.68ms, mfu 1.08%\n",
      "iter 325: loss 3.4274, time 399.61ms, mfu 1.08%\n",
      "iter 326: loss 3.3167, time 399.06ms, mfu 1.08%\n",
      "iter 327: loss 3.3934, time 393.87ms, mfu 1.08%\n",
      "iter 328: loss 3.5220, time 399.30ms, mfu 1.08%\n",
      "iter 329: loss 3.6297, time 399.47ms, mfu 1.08%\n",
      "iter 330: loss 3.3331, time 391.87ms, mfu 1.08%\n",
      "iter 331: loss 3.3705, time 397.66ms, mfu 1.08%\n",
      "iter 332: loss 3.5047, time 395.19ms, mfu 1.08%\n",
      "iter 333: loss 3.5555, time 394.75ms, mfu 1.08%\n",
      "iter 334: loss 3.5205, time 389.58ms, mfu 1.09%\n",
      "iter 335: loss 3.4179, time 389.56ms, mfu 1.09%\n",
      "iter 336: loss 3.5556, time 397.86ms, mfu 1.09%\n",
      "iter 337: loss 3.7266, time 396.85ms, mfu 1.09%\n",
      "iter 338: loss 3.5260, time 396.12ms, mfu 1.09%\n",
      "iter 339: loss 3.2613, time 397.93ms, mfu 1.09%\n",
      "iter 340: loss 3.3164, time 400.32ms, mfu 1.09%\n",
      "iter 341: loss 3.4295, time 403.07ms, mfu 1.08%\n",
      "iter 342: loss 3.3434, time 403.70ms, mfu 1.08%\n",
      "iter 343: loss 3.3040, time 396.40ms, mfu 1.08%\n",
      "iter 344: loss 3.4411, time 400.08ms, mfu 1.08%\n",
      "iter 345: loss 3.4641, time 392.55ms, mfu 1.08%\n",
      "iter 346: loss 3.5310, time 398.35ms, mfu 1.08%\n",
      "iter 347: loss 3.3622, time 400.32ms, mfu 1.08%\n",
      "iter 348: loss 3.4439, time 392.72ms, mfu 1.08%\n",
      "iter 349: loss 3.4593, time 393.41ms, mfu 1.09%\n",
      "iter 350: loss 3.5031, time 399.30ms, mfu 1.08%\n",
      "iter 351: loss 3.3590, time 399.43ms, mfu 1.08%\n",
      "iter 352: loss 3.4191, time 398.47ms, mfu 1.08%\n",
      "iter 353: loss 3.4440, time 399.71ms, mfu 1.08%\n",
      "iter 354: loss 3.3469, time 402.93ms, mfu 1.08%\n",
      "iter 355: loss 3.3944, time 399.80ms, mfu 1.08%\n",
      "iter 356: loss 3.3332, time 400.48ms, mfu 1.08%\n",
      "iter 357: loss 3.2106, time 399.88ms, mfu 1.08%\n",
      "iter 358: loss 3.4721, time 397.35ms, mfu 1.08%\n",
      "iter 359: loss 3.3012, time 399.34ms, mfu 1.08%\n",
      "iter 360: loss 3.2978, time 401.42ms, mfu 1.08%\n",
      "iter 361: loss 3.1938, time 400.44ms, mfu 1.08%\n",
      "iter 362: loss 3.2968, time 400.06ms, mfu 1.08%\n",
      "iter 363: loss 3.3302, time 395.29ms, mfu 1.08%\n",
      "iter 364: loss 3.3965, time 398.95ms, mfu 1.08%\n",
      "iter 365: loss 3.2527, time 398.57ms, mfu 1.08%\n",
      "iter 366: loss 3.3689, time 401.94ms, mfu 1.08%\n",
      "iter 367: loss 3.2907, time 400.48ms, mfu 1.08%\n",
      "iter 368: loss 3.3494, time 401.12ms, mfu 1.08%\n",
      "iter 369: loss 3.2621, time 401.04ms, mfu 1.08%\n",
      "iter 370: loss 3.1988, time 400.29ms, mfu 1.08%\n",
      "iter 371: loss 3.3559, time 398.53ms, mfu 1.08%\n",
      "iter 372: loss 3.1649, time 400.50ms, mfu 1.08%\n",
      "iter 373: loss 3.3287, time 397.22ms, mfu 1.08%\n",
      "iter 374: loss 3.3217, time 391.51ms, mfu 1.08%\n",
      "iter 375: loss 3.3333, time 401.50ms, mfu 1.08%\n",
      "iter 376: loss 3.1953, time 391.92ms, mfu 1.08%\n",
      "iter 377: loss 3.4391, time 400.47ms, mfu 1.08%\n",
      "iter 378: loss 3.4596, time 401.14ms, mfu 1.08%\n",
      "iter 379: loss 3.1601, time 402.84ms, mfu 1.08%\n",
      "iter 380: loss 3.3024, time 396.27ms, mfu 1.08%\n",
      "iter 381: loss 3.2849, time 394.78ms, mfu 1.08%\n",
      "iter 382: loss 3.3455, time 399.84ms, mfu 1.08%\n",
      "iter 383: loss 3.4110, time 397.66ms, mfu 1.08%\n",
      "iter 384: loss 3.1359, time 399.53ms, mfu 1.08%\n",
      "iter 385: loss 3.3519, time 402.50ms, mfu 1.08%\n",
      "iter 386: loss 3.3490, time 400.94ms, mfu 1.08%\n",
      "iter 387: loss 3.2655, time 400.69ms, mfu 1.08%\n",
      "iter 388: loss 3.2361, time 401.15ms, mfu 1.08%\n",
      "iter 389: loss 3.3967, time 400.88ms, mfu 1.08%\n",
      "iter 390: loss 3.2320, time 400.83ms, mfu 1.08%\n",
      "iter 391: loss 3.2800, time 401.02ms, mfu 1.08%\n",
      "iter 392: loss 3.3482, time 401.09ms, mfu 1.08%\n",
      "iter 393: loss 3.4308, time 397.31ms, mfu 1.08%\n",
      "iter 394: loss 3.2173, time 375.92ms, mfu 1.08%\n",
      "iter 395: loss 3.1316, time 393.73ms, mfu 1.09%\n",
      "iter 396: loss 3.3175, time 400.90ms, mfu 1.08%\n",
      "iter 397: loss 3.4404, time 399.28ms, mfu 1.08%\n",
      "iter 398: loss 3.1183, time 393.42ms, mfu 1.08%\n",
      "iter 399: loss 3.2300, time 367.86ms, mfu 1.09%\n",
      "iter 400: loss 3.3227, time 382.73ms, mfu 1.10%\n",
      "iter 401: loss 3.2176, time 397.96ms, mfu 1.10%\n",
      "iter 402: loss 3.1717, time 400.28ms, mfu 1.09%\n",
      "iter 403: loss 3.3201, time 399.91ms, mfu 1.09%\n",
      "iter 404: loss 3.2631, time 401.47ms, mfu 1.09%\n",
      "iter 405: loss 3.1356, time 400.81ms, mfu 1.09%\n",
      "iter 406: loss 3.1059, time 400.34ms, mfu 1.09%\n",
      "iter 407: loss 3.1295, time 400.03ms, mfu 1.09%\n",
      "iter 408: loss 2.8071, time 398.97ms, mfu 1.09%\n",
      "iter 409: loss 3.1004, time 400.46ms, mfu 1.08%\n",
      "iter 410: loss 2.9643, time 399.84ms, mfu 1.08%\n",
      "iter 411: loss 3.2905, time 399.95ms, mfu 1.08%\n",
      "iter 412: loss 3.2055, time 400.05ms, mfu 1.08%\n",
      "iter 413: loss 3.2146, time 400.01ms, mfu 1.08%\n",
      "iter 414: loss 3.2162, time 399.45ms, mfu 1.08%\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/s/bach/l/under/daives/cs445/assignments/nikhil-GPT/nanoGPT/train.py\", line 291, in <module>\n",
      "    X, Y = get_batch('train')\n",
      "  File \"/s/bach/l/under/daives/cs445/assignments/nikhil-GPT/nanoGPT/train.py\", line 115, in get_batch\n",
      "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
      "  File \"/s/bach/l/under/daives/cs445/assignments/nikhil-GPT/nanoGPT/train.py\", line 115, in <listcomp>\n",
      "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py config/train_config.py --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained until the loss no longer got better, let's generate 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-nikhil-gpt-first-try\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      " from any 10, but I'm trying to be doing so if I want that gradient descent shape to cancel out as the exact or is basically just looking at this, because I'll do the amount of values. And I'm moving in the output tasks. So now I see 100 minutes, I'll have to take a lot of samples, and then I can do I can just take the sum of zero for that by one. So let's create the weight matrix of X and right so now we have all my velocity is just trying to deal with this validation or 1. So more what I'm not going to do is this is basically going to be some single one and then i would be my hidden layer to find my only. I'm going to see that I'll go. So I see how to see if my X is the outputs are the n by my inputs. So we, this should be doing Q layer, the blue2, my inputs, the input is 0. So there was a single value that I'm going to be written with respect to be. So of this for each time, if I take the predicted function, for each of the normal distribution, I'm going to change the network. Right, and then that kind of a bunch of values of feature, I don't want to have to take like some square it to be k. So now I'll just have two operations like a single value of training training data. So if I'll multiply them of the training data to get to all my weight. So I take the test and the test sets of those values and I'm doing everything in a different classes, I'm just going to take like negative output. I'm just getting no different outcomes. So I'm getting something like this, you know, kind of say, this is what's a couple of different features. So then I'm trying to scale, I'm going to say, I can try as a sort of this to take a bunch of steps to measure of samples in a 10ative people. So now I can see why have a one thing an array of these derivatives and I'm just got in a weight. Okay, and I'm not going to have patches. So I'm looking at this error now I'm going to give me to keep a single column of inputs. And I'm going to be doing our weights. I'm just going to do this into some weights. And I have the value for each column, I\n",
      "---------------\n",
      "\n",
      " in the following values, but we're just try times Q function. Okay, and use the gradient over neural network, then the gradient descent. And so now we'll see if you know, and what the first weight is the same function that the current state. This is the value of the error. Of course, assuming that I have, then I'm actually going to activate on the negative unit network numbers just going to do a specific samples. I'm going to compute my probabilities. So a linear regression problems. So, now I'm going to expect the model, for more with the output output layer. So now I'm doing back to this two classes and this is going to be the values and then I were to do one. So. So I can try and then I can try to be fit to the sample to other step to to the activation function function. And so these are the gradient descent with an argument that is, this is going to be a single feature map. Right. So now so if it? Yeah, we're going to be from the output value for all of a whole lot, so I have to specify some other array of those columns using the value of the number of weights and then I've done linear values that I have to set the normal distribution as a one matrix. So now for now I have to have a fully connected layer. And then do it to be a generative words. So you can see that in a one sample. It's the probability function, I take the weights. Okay one value of the mean layer, this is going to be the derivative of the Q value. So if we do is basically say this three dimensional version of a weight between P of epsilon going to be the input because the input is the error times the input size that function as two plus one by the matrixithmsm of w. So I want to keep there. And how we can have to do that back in the moment to figure out to each dimensions. So what we talked about this thing we are the weights. And then I can see that I'm going to do the error table. I want to take a single hidden layer for some weight to do is just have a return the sum of samples that position, and then the training data is going to be a return. So now I'll get the row points remember that T squared values, is, I can, I take a function this to the same thing that I'm\n",
      "---------------\n",
      "\n",
      " for the second weightance matrix multiplication. So this is not just no. And so if it happens to be the same, I'm trying to be doing, you're going to get the average of the output distribution. So what we'll take in x sub k plus state. And I'm going, I want to make sure that am now. So, we can make this, so let's do is basically the return the Q function as you can do is the e. But if we're going to use this look at the gradient of the model to actually need to be my ground truth term and then I'll take my weight weight TD is what's the delta weights. You can see that's going to be in the sample. So this will do is going to be as the same formula. This is going to be the same otherality of the error, where I'm going to be 100% of the error over the target activation function. So if I need to take each outputs. My nets, update, I can use myh of my input size of my previous value I choose the classes, one. So then let me take got to create a state and that target model. So now you know if I have an current linear action of a linear model of these weights and then'm just going to take the the ground truth probability of the current state or this. So now I'm going to try to be the network. So the same row, I'm going to initialize the action across the sum to my state, my input. So I have the first action function, I'm doing this. So now I'm going to do. So if I'm going to take a high, I have a single single one plus one,, or is three by one thing, a function of this to train function. So I will specify the inputs and take the ground truth output unit these inputs. So I'm just looking at the output, then I can just have w just get going to be the input weights. So, these values, what and I can be just going to take. So, I can just do y, then then be the state and then first value that I need to get a number of distribution. So now I'm just trying to specify this is use the next state my minimum function so I can set up with my time that value of samples. And then I'm just just a mean of the current action to just going to choose the current state\n",
      "---------------\n",
      "\n",
      " for some convolutional layer, they don't know, you know what they're going to work for those things, so this might use all the weights. So where all you would see? What things do we have an error function. So I want this to use the gradient of the network. And so now that I will just do the actual sample and so we can make this covariance for the square and basically have the output. So we would take the output values of the prediction times the inverse values for this input and then number of the same as we'll call this logarithms, and then it's going to happen for every output layer. So what they have to do we'll take. Right. So we can do here. What me do is you'll go. So here is this first for the QDA and then I can do that I have to be the weights, and then can do this from the neural networks in this for this can basically say, it's just, i'm going to be going to take like a bunch of different values. And then I'm going to get an input in this into a bunch of those values. And the patch of weights are equal to the next sample and the weight steps that's going to be some input. It's going to be like what I'm just going to apply that to Q value more than this function to that's going to be important for that's going to be able to get to be the same as a different data. So I need to do to complete the same state or take that right now, I can turn this into the probability that by the testing data, and then I'll need to have a non-validits that would be doing the goal where I'm trying to look at the state of this, right, we can do this function and then I just take for a value, and then go to optimize terms of the number of neural network as to that a function, because what we were to be with those epochs. And then that's the QDA function that I moved for the partial derivative of these times the image right from the standard inputs. So it's going to be a sample. So this is going to be the weights. So there's a matrix of that I'm now, multiply by D two by two things that each one is basically going to create this two dimensional row for the first one set of the error. But I've already know this is basically a\n",
      "---------------\n",
      "\n",
      " in the neural network as for some samples, you can see that are more samples, and these are the first. So you don't really want that's really want the parameters, you can know this error. So for this is like a very similar log likelihood of the optimization is the same class and then do some probability of the bias and we can kind of. So let's take the forward pass and use this basically use this case, actually going to be a bunch of the same as, but you're going to use some neural net. So you can just run a model to actually have to the output of these things through the same image image. So I'm going to do this to give me a return but just's going to be defined. So, it's not the error in this case, if I have this, I'll have two columns of a two hidden layer seven, but I will use a different classes. And then I'm around the input and I can use the error with my indicator variables for a reward of X. So remember i'll see here the following weights. And so now I'm going to be going to be all the error. And then I can I can do Q rate of the sum informationbook. So then I'm trying to basically just going to do this function to read. I have to a neural networks, there at the total gradient. So if I'm going to get a one minus the sample times the value of the convolutional layer step, and now I will train in the next state. So this is, I'm going to be some error. And I have the Q relationship between these hidden layer set time. So I'm going to have just at the softmax function of ones. And then you can get to this. So this allows me to do is I I can print them into the error. So what I will do is I can the actual as a sample and take in the next gradient descent function with error. So then if I find my target samples for a sample. So I have some weights w, I'm trying to specify the likelihood of that classes of sample. And then I will say I're going to see a weight to take the error matrix of the V and then sigma. So now I'll use the expected probability of respect to the first values of my output of outputs. So so, this is basically the same thing, now the output. So, in my log likelihood of W with\n",
      "---------------\n",
      "\n",
      " first one is going to be a little bit. So now, the train return, now we have the targets, what we have to draw the data and its linear function. So we'll take a go into a great so we can use some convolutional network function actually be kind of. So we'll want to measure the test, minus the same as the neural networks. And then, the weights that's a big output. So now I'll have's going to run that's going to be a bit of that so the matrices in the fruit inputs, I'm going to be my prediction. We can also sum for a lot of the patch or so for that I'm going to move this image from the filter. So we're actually change the input to actually just into a single output, what's going to be going to be as the gradient. So 10, I'm trying to turn this. The targets, and then I'm getting the goal is going to create a different values. So yeah, this is, let's read the error so if we already have to do that by the first values and I can use that into aPy version. So I'm doing my weight to create two. And then I can need to be the weight matrix into the input size. There's where you're my output of X, this is that we have this one. But now just going to be a linear function to be how to do is I'll be the output of that output, I can see that I can go from the other time to solve this into the testing data set of a matrix. But now I'm not going to do the same time. So the gradient is we'll just just see we're just trying to get the output size of T at the actual results. And so then, I need to measure this all of w is going to be the weights. And I'm going to give me my patches and an arbitrary image and then I have the individual samples and then I have to predict, will take that, and then a linear function starts to be a single vector. So if I have get this class two days about this is going to be a force of a point and this is going to be one of the same class. So if I have a member of matrix that these hidden layer size that's a single output of that X by n and W is zero. So linear regression, this is going to be being going to be so this is\n",
      "---------------\n",
      "\n",
      " before I want to do my training, and then do is not going to be a model. And so now if I do one the neural networks want to get this data, I will make sure it to do I want to see what I should do is I'm going to say, I'm going to do also going to do something like I'm going to have an instance of my state one class difference between my by my entire weights and then so I did use that. So we're trying to plot the weight matrix for this as a output. So now I'm going to add a prediction of our state and the activation function will see here in cross time distribution of basically the second one thing is going to be some of them. So, if I use the training t, for this case of the goal. So, I need to you can you can see just just say, I'll use to see that my action. I can return the one. So basically I see if I'm now if I think I'm going to take a normal distribution. So on this is our different weights and then I'll just say we're just going to add an input, and then I want to take the final output into 10% of this into neural networks as I'm going to classify the output layer layer. So I'm going to take that row values of the next hidden layer or the sum of the classes. So now this is going to be the output distribution of the state values of each sum of those units and a seven by the units in this error with neural network. So a best sum being a total of those size of all four feature and I'm going to be going to have one by D. But and so let's got a seven by one is going to be the expected value of the number of samples of units in the number of weights, in this prediction plus the value. So that's what Z has to be all of matrix for the same point. So if I mean one input to the output I'm going to look at this, I'll take all that. So now I think about this. if I'm trying to this really very intuitive for other layers of right because we can see that if we're just looking at a test set into a bit of training data, and then you're trying to maximize the return of my two things to update. And then I'm going to add some time well for all of that the same time, basically in terms of all\n",
      "---------------\n",
      "\n",
      " into the training. So this is just going to be the training, the inputs will get the best size of the error in the data. And then these would be the network problem and then be all of the weights. So, I can have a bunch of features to what what we will need to get into the individual two things that I can do that in this to be in the weights. So if I have a bunch of samples. So for each sample a function that are going to be defined another output of by one. So, for that I'm trying to print out of this one and then two samples. And in the state. And so now I can see that my my two works closer for the weights. So this case of a filter so we have a good prediction. And then just do this, if I can see that's going to be a column to do here. So I'm you have in one step to that sort of them. And so what the same problem is is actually going to be evenly between the second two. So if I make that, then I have my convolutional layer, and then I'm interested in this one. So I'm not going to be seven by 10. And then I'm trying to update that to be very small set and then I'm get with this sample to all of the individual values. So, this is going to be the best likelihood of my weights. We can see, my, I appreciate we just take me to take the neural networks. So, right, this is the input size of this is already got things that we had that's not really nice we can see that a minute going to be able to be some of a neural networks. So, if I have a member of class or a bunch of kind of class, we've got in the other values that mean of the matrix of these are all all, are going to be doing in the weights. So now it's going to look at the probability that's the sum of the minimum so now I have my two of classes, each weight in the error direction of five. Let me get some probability of the probability of each value. So some n sub n is just just going to be how many of those values would be a one hot column of all of zero, and then I can take the class. So I'll also kind of the output of of our difference function should be the hidden layer of a function. So, if I need\n",
      "---------------\n",
      "\n",
      " for the gradient, but it's going to do, it's not necessarily I'm not in two. So we've got some class classes. So now don't need to look at this last time I could not necessarily want to actually have a square or a member of that's no problem. So the number of course if I could use the derivative of the distribution of the first one by the input size is. So let's use the indicator variableigma. So now I'm trying to print the same thing to solve the bias. So this is here, a position of doing here. So this would be the probability of weights, it's not not going to say, if I'll notice that I multiply how. So basically the individual distribution for the center of the weight matrix. So we're all the test set of a continuous relationship and then the features in that I have a state that. So you can see the same thing here is just to do maybe a significant by like what I want to do. So this, when I'll imagine, let's I'm trying to do sure that I'm looking at the prediction right so I will see that I can know do the indicator variable is it's going to be the input and then I'll multiply my hidden units in the output layers to do you multiply that has to be defined that into a constant. So we'll define H. And then I'll have an input with this function, and then the matrix for n by its samples. So this is going to basically taking the sum from an sum for my test over the QDA function would be going to be the same thing. And then I have the standardize the same thing for things that is going to do we're going to go. Then we could be like the same time. This is still because it's a number ofize it's going to import it a lot of time I'm going to be pretty well here. And so there's these are all all these samples that that's going to be, I can take some it to reshape it into a single weight two dimensions here. So we can do the input to scale the output of x right which weight. I'll take the end up and then I'll take that has my indicator variable, change with the number of samples. So I should get to the same list of my line. So we're to see that I have a different variables. I'll put. So let's get my samples. So\n",
      "---------------\n",
      "\n",
      " on the same to make the class labels in Python, but but if you do a continuous variables. But this to can fit a sudden we can just want to actually get a whole lot of cases. So now we can see a bit more well this. So the Q function that likelihood is not I'm closer to the inputs that as the expected value for each of a single value of the image. So this is just just a little bit of just these, and I will train, or all of our values, remember, do is trying to classify the convolutional nets. So in this case, this allows me to make this to calculate that I'm with a inputs. So I can then take my data, I can try to take the weight matrix. So now we just make some probability of the max of class distributed, I'm some values to multiply that's now, because I'm trying to do a single value to be a list of those patches. And I'm not necessarily this case to run this. And then the error, I'm going to make a single point, but also just a scale the two different size of our probability of the function as a particular sample. So, for an example, if I should have some function that's the entire sort of that sort of a linear job of the number of weights just a only very nice number of things. So what I've got the value is I'm trying to optimize for a seven by doing a bit for more that, right so there's a particular function that's just like a simple for the first one, I'm going to get a member of that you have a year of a function. That is basically say when your image. So here I'll do my last thing, I'm trying to do the same one and I'm going to take something like the and I would the same value times. And I'm going to do a negative weight. So this I'm not doing is going to take a discriminant function of the end up with respect to the logar action. And so I'll see that of my inputs. So this is of N H function is going to be a high errors. So I'm going to be going to be taking the two components, and then the QDA is going to set of Q function. We have the number of samples in a different derivative of course, because now I want to probably be my inputs, assuming that we'll do that I'll do. So\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "! python3 sample.py --out_dir=out-nikhil-gpt-first-try"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below lets you choose the starting \"prompt\" and the model generates from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: start = Answer the question: What's a neural network \\n Answer: \n",
      "Overriding: num_samples = 1\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "Answer the question: What's a neural network \\n Answer: -shine All but that is basically we can see so if I want that gradient is going to look at the second distribution of the number of the value, and then the neural networks equals zero or one. And that's going to be the output of the convolutional layers. So I'll have to take a state my samples, I can see it's going to be different weights, I actually always a single inputs. So let's just take a look at a more right so now we have all my velocity value of these two. And so now I can see this actually just kind of a continuous values. This is like a certain, I'm going to have a seven samples of weights, and then the second layer. I'm going to assume that's going to be the second moment estimates, I have my X, the outputs. So now I'm going to be going to have the weights in Q layer value of 0. And so I can say, I'm going to say I want to be one over the output value of the input samples over the. So of this was always as the training to the input. I just for each of the normal distribution, I'm going to change the network. Right, and then that did. And the same for the feature, I don't want to have to take like some square it to be k. So now I'll just have two operations like a single shapes of y. So let's say that I'll take a state of a use a nonlinear function of the same definition of the test and the test set and then's just do I'm going to define this function and training data. And so I'm going to put I'm just getting no different outcomes. So I'm getting something like this, you know, kind of say, this is what I'm on the number of interest. This. I'll print,, I'm going to say, I can try as a sort of this, you know, to do like a two variables in a 10 by seven by seven points. Yeah, then I have one, an array of these derivatives and I'm just working in a little bit likely to be able to use that I have patches. So I'm looking at this number of these two words where I don't want to be less than I'm going to predict the actual weight value. So, I'm just going to do this into some weights. Yeah. Right. And so this is I'm\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# get input from user\n",
    "prompt = input(\"Enter a prompt: \")\n",
    "\n",
    "! python sample.py --out_dir=out-nikhil-gpt-first-try --start=\"$prompt\" --num_samples=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d54705a7c51399f88ff26e5af010bf5b52b49f4051d63947538750a079d2bb3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
