{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_config.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "eval_interval = 100 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False \n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'nikhil-gpt'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'medium'\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out-nikhil-gpt-final\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 200\n",
      "Overriding: log_interval = 1\n",
      "Overriding: block_size = 64\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 2000\n",
      "Overriding: lr_decay_iters = 2000\n",
      "Overriding: dropout = 0.0\n",
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "using fused AdamW: False\n",
      "step 0: train loss 10.8506, val loss 10.8486\n",
      "iter 0: loss 10.8515, time 2977.32ms, mfu -100.00%\n",
      "iter 1: loss 10.8609, time 406.37ms, mfu -100.00%\n",
      "iter 2: loss 10.8261, time 406.78ms, mfu -100.00%\n",
      "iter 3: loss 10.8255, time 407.30ms, mfu -100.00%\n",
      "iter 4: loss 10.7988, time 407.47ms, mfu -100.00%\n",
      "iter 5: loss 10.7482, time 406.56ms, mfu 1.06%\n",
      "iter 6: loss 10.7508, time 407.11ms, mfu 1.06%\n",
      "iter 7: loss 10.7010, time 402.11ms, mfu 1.06%\n",
      "iter 8: loss 10.6371, time 403.84ms, mfu 1.06%\n",
      "iter 9: loss 10.5871, time 405.45ms, mfu 1.06%\n",
      "iter 10: loss 10.5555, time 400.66ms, mfu 1.06%\n",
      "iter 11: loss 10.5459, time 408.08ms, mfu 1.06%\n",
      "iter 12: loss 10.5294, time 405.99ms, mfu 1.06%\n",
      "iter 13: loss 10.4694, time 406.87ms, mfu 1.06%\n",
      "iter 14: loss 10.4535, time 407.90ms, mfu 1.06%\n",
      "iter 15: loss 10.4353, time 407.74ms, mfu 1.06%\n",
      "iter 16: loss 10.3989, time 405.93ms, mfu 1.06%\n",
      "iter 17: loss 10.3743, time 406.85ms, mfu 1.06%\n",
      "iter 18: loss 10.3042, time 404.63ms, mfu 1.06%\n",
      "iter 19: loss 10.2910, time 404.97ms, mfu 1.06%\n",
      "iter 20: loss 10.2437, time 405.87ms, mfu 1.06%\n",
      "iter 21: loss 10.2071, time 405.46ms, mfu 1.06%\n",
      "iter 22: loss 10.1597, time 406.46ms, mfu 1.06%\n",
      "iter 23: loss 10.0600, time 400.24ms, mfu 1.06%\n",
      "iter 24: loss 10.0307, time 405.57ms, mfu 1.06%\n",
      "iter 25: loss 9.9718, time 406.38ms, mfu 1.06%\n",
      "iter 26: loss 9.9080, time 405.91ms, mfu 1.06%\n",
      "iter 27: loss 9.8469, time 404.99ms, mfu 1.06%\n",
      "iter 28: loss 9.7997, time 405.80ms, mfu 1.06%\n",
      "iter 29: loss 9.7247, time 406.35ms, mfu 1.06%\n",
      "iter 30: loss 9.6750, time 404.29ms, mfu 1.06%\n",
      "iter 31: loss 9.5738, time 407.76ms, mfu 1.06%\n",
      "iter 32: loss 9.4963, time 406.48ms, mfu 1.06%\n",
      "iter 33: loss 9.4658, time 405.80ms, mfu 1.06%\n",
      "iter 34: loss 9.3727, time 405.74ms, mfu 1.06%\n",
      "iter 35: loss 9.3231, time 403.84ms, mfu 1.06%\n",
      "iter 36: loss 9.1851, time 401.51ms, mfu 1.06%\n",
      "iter 37: loss 9.1287, time 404.38ms, mfu 1.06%\n",
      "iter 38: loss 9.0320, time 404.42ms, mfu 1.06%\n",
      "iter 39: loss 9.0127, time 405.55ms, mfu 1.06%\n",
      "iter 40: loss 8.8688, time 397.60ms, mfu 1.07%\n",
      "iter 41: loss 8.8332, time 402.16ms, mfu 1.07%\n",
      "iter 42: loss 8.7070, time 402.41ms, mfu 1.07%\n",
      "iter 43: loss 8.5727, time 407.88ms, mfu 1.07%\n",
      "iter 44: loss 8.5900, time 403.05ms, mfu 1.07%\n",
      "iter 45: loss 8.3999, time 401.54ms, mfu 1.07%\n",
      "iter 46: loss 8.3079, time 406.24ms, mfu 1.07%\n",
      "iter 47: loss 8.2383, time 406.36ms, mfu 1.07%\n",
      "iter 48: loss 8.1495, time 393.21ms, mfu 1.07%\n",
      "iter 49: loss 8.0504, time 401.40ms, mfu 1.07%\n",
      "iter 50: loss 7.9807, time 405.02ms, mfu 1.07%\n",
      "iter 51: loss 7.8999, time 406.70ms, mfu 1.07%\n",
      "iter 52: loss 7.7787, time 406.97ms, mfu 1.07%\n",
      "iter 53: loss 7.7102, time 404.00ms, mfu 1.07%\n",
      "iter 54: loss 7.6327, time 407.69ms, mfu 1.07%\n",
      "iter 55: loss 7.5449, time 406.50ms, mfu 1.06%\n",
      "iter 56: loss 7.5051, time 404.53ms, mfu 1.06%\n",
      "iter 57: loss 7.4053, time 406.93ms, mfu 1.06%\n",
      "iter 58: loss 7.2506, time 407.03ms, mfu 1.06%\n",
      "iter 59: loss 7.1116, time 405.88ms, mfu 1.06%\n",
      "iter 60: loss 7.1003, time 402.58ms, mfu 1.06%\n",
      "iter 61: loss 7.1039, time 408.17ms, mfu 1.06%\n",
      "iter 62: loss 6.9482, time 408.10ms, mfu 1.06%\n",
      "iter 63: loss 6.9154, time 408.50ms, mfu 1.06%\n",
      "iter 64: loss 6.8275, time 406.48ms, mfu 1.06%\n",
      "iter 65: loss 6.6464, time 406.36ms, mfu 1.06%\n",
      "iter 66: loss 6.7552, time 406.56ms, mfu 1.06%\n",
      "iter 67: loss 6.5778, time 406.48ms, mfu 1.06%\n",
      "iter 68: loss 6.6494, time 406.53ms, mfu 1.06%\n",
      "iter 69: loss 6.3796, time 403.82ms, mfu 1.06%\n",
      "iter 70: loss 6.4578, time 406.41ms, mfu 1.06%\n",
      "iter 71: loss 6.3631, time 405.36ms, mfu 1.06%\n",
      "iter 72: loss 6.3589, time 408.06ms, mfu 1.06%\n",
      "iter 73: loss 6.1581, time 393.11ms, mfu 1.06%\n",
      "iter 74: loss 6.2382, time 405.90ms, mfu 1.06%\n",
      "iter 75: loss 6.1479, time 404.44ms, mfu 1.06%\n",
      "iter 76: loss 5.8256, time 406.93ms, mfu 1.06%\n",
      "iter 77: loss 6.2064, time 394.09ms, mfu 1.07%\n",
      "iter 78: loss 6.0117, time 406.57ms, mfu 1.07%\n",
      "iter 79: loss 5.8873, time 405.95ms, mfu 1.07%\n",
      "iter 80: loss 5.9623, time 404.63ms, mfu 1.07%\n",
      "iter 81: loss 5.8644, time 406.86ms, mfu 1.06%\n",
      "iter 82: loss 5.9959, time 404.68ms, mfu 1.06%\n",
      "iter 83: loss 5.9490, time 407.39ms, mfu 1.06%\n",
      "iter 84: loss 5.6851, time 407.04ms, mfu 1.06%\n",
      "iter 85: loss 5.6329, time 402.63ms, mfu 1.06%\n",
      "iter 86: loss 5.8241, time 407.53ms, mfu 1.06%\n",
      "iter 87: loss 5.9548, time 405.53ms, mfu 1.06%\n",
      "iter 88: loss 5.7548, time 405.72ms, mfu 1.06%\n",
      "iter 89: loss 5.6125, time 406.31ms, mfu 1.06%\n",
      "iter 90: loss 5.6041, time 405.75ms, mfu 1.06%\n",
      "iter 91: loss 5.7466, time 406.06ms, mfu 1.06%\n",
      "iter 92: loss 5.4920, time 406.85ms, mfu 1.06%\n",
      "iter 93: loss 5.6349, time 406.94ms, mfu 1.06%\n",
      "iter 94: loss 5.6873, time 406.83ms, mfu 1.06%\n",
      "iter 95: loss 5.4738, time 406.46ms, mfu 1.06%\n",
      "iter 96: loss 5.3364, time 406.28ms, mfu 1.06%\n",
      "iter 97: loss 5.4048, time 399.98ms, mfu 1.06%\n",
      "iter 98: loss 5.4132, time 401.61ms, mfu 1.06%\n",
      "iter 99: loss 5.3932, time 402.90ms, mfu 1.06%\n",
      "step 100: train loss 5.3686, val loss 5.4775\n",
      "saving checkpoint to out-nikhil-gpt-final\n",
      "iter 100: loss 5.3035, time 3863.15ms, mfu 0.97%\n",
      "iter 101: loss 5.2920, time 402.69ms, mfu 0.98%\n",
      "iter 102: loss 5.3965, time 405.60ms, mfu 0.99%\n",
      "iter 103: loss 5.4839, time 406.14ms, mfu 0.99%\n",
      "iter 104: loss 5.4878, time 407.04ms, mfu 1.00%\n",
      "iter 105: loss 5.2204, time 400.58ms, mfu 1.01%\n",
      "iter 106: loss 4.9540, time 405.97ms, mfu 1.01%\n",
      "iter 107: loss 5.0566, time 405.37ms, mfu 1.02%\n",
      "iter 108: loss 4.9688, time 402.72ms, mfu 1.02%\n",
      "iter 109: loss 5.1805, time 407.33ms, mfu 1.03%\n",
      "iter 110: loss 5.1514, time 406.19ms, mfu 1.03%\n",
      "iter 111: loss 5.0354, time 406.90ms, mfu 1.03%\n",
      "iter 112: loss 5.1106, time 407.11ms, mfu 1.04%\n",
      "iter 113: loss 5.2121, time 408.12ms, mfu 1.04%\n",
      "iter 114: loss 4.9262, time 394.45ms, mfu 1.04%\n",
      "iter 115: loss 5.3442, time 406.00ms, mfu 1.05%\n",
      "iter 116: loss 4.9029, time 404.80ms, mfu 1.05%\n",
      "iter 117: loss 4.9225, time 403.70ms, mfu 1.05%\n",
      "iter 118: loss 4.7833, time 394.61ms, mfu 1.05%\n",
      "iter 119: loss 5.0680, time 407.68ms, mfu 1.05%\n",
      "iter 120: loss 5.0895, time 407.68ms, mfu 1.05%\n",
      "iter 121: loss 4.9022, time 406.19ms, mfu 1.05%\n",
      "iter 122: loss 4.7630, time 405.96ms, mfu 1.06%\n",
      "iter 123: loss 5.0825, time 401.50ms, mfu 1.06%\n",
      "iter 124: loss 4.8974, time 405.87ms, mfu 1.06%\n",
      "iter 125: loss 4.9171, time 406.23ms, mfu 1.06%\n",
      "iter 126: loss 4.8052, time 407.52ms, mfu 1.06%\n",
      "iter 127: loss 4.7877, time 401.76ms, mfu 1.06%\n",
      "iter 128: loss 4.8404, time 405.82ms, mfu 1.06%\n",
      "iter 129: loss 4.8853, time 406.87ms, mfu 1.06%\n",
      "iter 130: loss 4.8059, time 406.16ms, mfu 1.06%\n",
      "iter 131: loss 4.7958, time 402.95ms, mfu 1.06%\n",
      "iter 132: loss 4.7464, time 406.79ms, mfu 1.06%\n",
      "iter 133: loss 4.6734, time 407.19ms, mfu 1.06%\n",
      "iter 134: loss 4.6941, time 407.25ms, mfu 1.06%\n",
      "iter 135: loss 4.7493, time 405.32ms, mfu 1.06%\n",
      "iter 136: loss 4.8183, time 405.55ms, mfu 1.06%\n",
      "iter 137: loss 4.5829, time 404.40ms, mfu 1.06%\n",
      "iter 138: loss 4.4728, time 401.48ms, mfu 1.06%\n",
      "iter 139: loss 4.8282, time 408.20ms, mfu 1.06%\n",
      "iter 140: loss 4.6841, time 402.96ms, mfu 1.06%\n",
      "iter 141: loss 4.4434, time 405.38ms, mfu 1.06%\n",
      "iter 142: loss 4.3922, time 407.17ms, mfu 1.06%\n",
      "iter 143: loss 4.3799, time 406.55ms, mfu 1.06%\n",
      "iter 144: loss 4.7164, time 405.76ms, mfu 1.06%\n",
      "iter 145: loss 4.3182, time 405.22ms, mfu 1.06%\n",
      "iter 146: loss 4.5075, time 404.04ms, mfu 1.06%\n",
      "iter 147: loss 4.3493, time 401.10ms, mfu 1.06%\n",
      "iter 148: loss 4.5753, time 405.16ms, mfu 1.06%\n",
      "iter 149: loss 4.4311, time 405.82ms, mfu 1.06%\n",
      "iter 150: loss 4.4884, time 403.22ms, mfu 1.06%\n",
      "iter 151: loss 4.4738, time 407.55ms, mfu 1.06%\n",
      "iter 152: loss 4.6057, time 401.87ms, mfu 1.06%\n",
      "iter 153: loss 4.5627, time 408.88ms, mfu 1.06%\n",
      "iter 154: loss 4.5368, time 407.05ms, mfu 1.06%\n",
      "iter 155: loss 4.4486, time 405.12ms, mfu 1.06%\n",
      "iter 156: loss 4.3841, time 406.81ms, mfu 1.06%\n",
      "iter 157: loss 4.3330, time 405.42ms, mfu 1.06%\n",
      "iter 158: loss 4.1301, time 406.55ms, mfu 1.06%\n",
      "iter 159: loss 4.3843, time 407.40ms, mfu 1.06%\n",
      "iter 160: loss 4.5079, time 405.71ms, mfu 1.06%\n",
      "iter 161: loss 4.2849, time 404.56ms, mfu 1.06%\n",
      "iter 162: loss 4.3644, time 404.86ms, mfu 1.06%\n",
      "iter 163: loss 4.3617, time 396.73ms, mfu 1.06%\n",
      "iter 164: loss 4.3397, time 407.55ms, mfu 1.06%\n",
      "iter 165: loss 4.2615, time 403.07ms, mfu 1.06%\n",
      "iter 166: loss 4.2522, time 405.62ms, mfu 1.06%\n",
      "iter 167: loss 4.1897, time 407.26ms, mfu 1.06%\n",
      "iter 168: loss 4.3908, time 407.80ms, mfu 1.06%\n",
      "iter 169: loss 4.3748, time 404.95ms, mfu 1.06%\n",
      "iter 170: loss 4.1683, time 405.81ms, mfu 1.06%\n",
      "iter 171: loss 4.4309, time 407.02ms, mfu 1.06%\n",
      "iter 172: loss 4.2292, time 403.84ms, mfu 1.06%\n",
      "iter 173: loss 4.2811, time 404.42ms, mfu 1.06%\n",
      "iter 174: loss 4.2172, time 403.71ms, mfu 1.06%\n",
      "iter 175: loss 4.2709, time 406.76ms, mfu 1.06%\n",
      "iter 176: loss 4.2169, time 400.60ms, mfu 1.06%\n",
      "iter 177: loss 4.4482, time 402.01ms, mfu 1.06%\n",
      "iter 178: loss 4.3670, time 401.34ms, mfu 1.07%\n",
      "iter 179: loss 4.1933, time 406.68ms, mfu 1.07%\n",
      "iter 180: loss 4.2475, time 403.93ms, mfu 1.07%\n",
      "iter 181: loss 4.1943, time 404.80ms, mfu 1.07%\n",
      "iter 182: loss 3.9071, time 405.73ms, mfu 1.06%\n",
      "iter 183: loss 4.1818, time 407.06ms, mfu 1.06%\n",
      "iter 184: loss 4.1646, time 401.63ms, mfu 1.07%\n",
      "iter 185: loss 4.2913, time 406.16ms, mfu 1.06%\n",
      "iter 186: loss 4.2768, time 405.26ms, mfu 1.06%\n",
      "iter 187: loss 4.4158, time 403.95ms, mfu 1.06%\n",
      "iter 188: loss 4.1857, time 392.25ms, mfu 1.07%\n",
      "iter 189: loss 4.1182, time 405.47ms, mfu 1.07%\n",
      "iter 190: loss 3.9151, time 404.25ms, mfu 1.07%\n",
      "iter 191: loss 3.9606, time 405.95ms, mfu 1.07%\n",
      "iter 192: loss 4.4339, time 407.48ms, mfu 1.07%\n",
      "iter 193: loss 4.1773, time 405.32ms, mfu 1.07%\n",
      "iter 194: loss 4.0955, time 405.65ms, mfu 1.07%\n",
      "iter 195: loss 4.1919, time 406.55ms, mfu 1.06%\n",
      "iter 196: loss 4.0580, time 399.29ms, mfu 1.07%\n",
      "iter 197: loss 3.9546, time 398.96ms, mfu 1.07%\n",
      "iter 198: loss 4.1048, time 406.50ms, mfu 1.07%\n",
      "iter 199: loss 4.1015, time 404.07ms, mfu 1.07%\n",
      "step 200: train loss 4.1112, val loss 4.5361\n",
      "saving checkpoint to out-nikhil-gpt-final\n",
      "iter 200: loss 3.8227, time 4014.48ms, mfu 0.97%\n",
      "iter 201: loss 4.0332, time 405.42ms, mfu 0.98%\n",
      "iter 202: loss 4.0630, time 406.37ms, mfu 0.99%\n",
      "iter 203: loss 4.1855, time 397.59ms, mfu 1.00%\n",
      "iter 204: loss 4.0064, time 400.23ms, mfu 1.01%\n",
      "iter 205: loss 3.9895, time 405.98ms, mfu 1.01%\n",
      "iter 206: loss 4.1673, time 401.47ms, mfu 1.02%\n",
      "iter 207: loss 4.2061, time 405.20ms, mfu 1.02%\n",
      "iter 208: loss 3.9811, time 405.65ms, mfu 1.03%\n",
      "iter 209: loss 4.0392, time 403.11ms, mfu 1.03%\n",
      "iter 210: loss 4.0420, time 404.41ms, mfu 1.03%\n",
      "iter 211: loss 3.9494, time 405.56ms, mfu 1.04%\n",
      "iter 212: loss 3.9697, time 381.61ms, mfu 1.05%\n",
      "iter 213: loss 4.0540, time 379.47ms, mfu 1.05%\n",
      "iter 214: loss 3.9031, time 406.05ms, mfu 1.06%\n",
      "iter 215: loss 3.9173, time 404.47ms, mfu 1.06%\n",
      "iter 216: loss 4.1643, time 405.80ms, mfu 1.06%\n",
      "iter 217: loss 3.8065, time 406.02ms, mfu 1.06%\n",
      "iter 218: loss 3.9777, time 403.14ms, mfu 1.06%\n",
      "iter 219: loss 4.1112, time 405.43ms, mfu 1.06%\n",
      "iter 220: loss 4.1585, time 406.24ms, mfu 1.06%\n",
      "iter 221: loss 3.6847, time 402.79ms, mfu 1.06%\n",
      "iter 222: loss 3.9654, time 407.16ms, mfu 1.06%\n",
      "iter 223: loss 3.8676, time 391.88ms, mfu 1.06%\n",
      "iter 224: loss 3.9900, time 387.45ms, mfu 1.07%\n",
      "iter 225: loss 3.8805, time 400.40ms, mfu 1.07%\n",
      "iter 226: loss 3.6990, time 406.15ms, mfu 1.07%\n",
      "iter 227: loss 3.9170, time 404.70ms, mfu 1.07%\n",
      "iter 228: loss 3.7684, time 397.63ms, mfu 1.07%\n",
      "iter 229: loss 4.0725, time 404.16ms, mfu 1.07%\n",
      "iter 230: loss 3.7534, time 404.65ms, mfu 1.07%\n",
      "iter 231: loss 3.7884, time 406.58ms, mfu 1.07%\n",
      "iter 232: loss 3.8730, time 405.74ms, mfu 1.07%\n",
      "iter 233: loss 3.8098, time 400.08ms, mfu 1.07%\n",
      "iter 234: loss 3.7450, time 404.45ms, mfu 1.07%\n",
      "iter 235: loss 4.0162, time 406.36ms, mfu 1.07%\n",
      "iter 236: loss 4.0422, time 397.89ms, mfu 1.07%\n",
      "iter 237: loss 3.8168, time 402.13ms, mfu 1.07%\n",
      "iter 238: loss 3.8745, time 406.36ms, mfu 1.07%\n",
      "iter 239: loss 3.8509, time 405.84ms, mfu 1.07%\n",
      "iter 240: loss 3.9355, time 406.53ms, mfu 1.07%\n",
      "iter 241: loss 3.9593, time 405.23ms, mfu 1.07%\n",
      "iter 242: loss 3.8576, time 406.30ms, mfu 1.07%\n",
      "iter 243: loss 3.8743, time 405.17ms, mfu 1.07%\n",
      "iter 244: loss 3.7791, time 403.93ms, mfu 1.07%\n",
      "iter 245: loss 3.8448, time 401.93ms, mfu 1.07%\n",
      "iter 246: loss 4.0187, time 406.02ms, mfu 1.07%\n",
      "iter 247: loss 3.8520, time 404.06ms, mfu 1.07%\n",
      "iter 248: loss 3.9120, time 400.91ms, mfu 1.07%\n",
      "iter 249: loss 3.7636, time 405.83ms, mfu 1.07%\n",
      "iter 250: loss 3.7701, time 404.34ms, mfu 1.07%\n",
      "iter 251: loss 3.7839, time 401.82ms, mfu 1.07%\n",
      "iter 252: loss 4.0236, time 406.68ms, mfu 1.07%\n",
      "iter 253: loss 3.7044, time 392.60ms, mfu 1.07%\n",
      "iter 254: loss 3.7781, time 406.28ms, mfu 1.07%\n",
      "iter 255: loss 3.8674, time 406.19ms, mfu 1.07%\n",
      "iter 256: loss 3.7775, time 406.51ms, mfu 1.07%\n",
      "iter 257: loss 3.7936, time 404.46ms, mfu 1.07%\n",
      "iter 258: loss 3.6757, time 405.79ms, mfu 1.07%\n",
      "iter 259: loss 3.6751, time 405.44ms, mfu 1.07%\n",
      "iter 260: loss 3.9531, time 404.55ms, mfu 1.07%\n",
      "iter 261: loss 3.7101, time 406.60ms, mfu 1.06%\n",
      "iter 262: loss 3.7694, time 405.82ms, mfu 1.06%\n",
      "iter 263: loss 3.6390, time 407.07ms, mfu 1.06%\n",
      "iter 264: loss 3.7326, time 405.22ms, mfu 1.06%\n",
      "iter 265: loss 3.6721, time 403.97ms, mfu 1.06%\n",
      "iter 266: loss 3.6520, time 405.05ms, mfu 1.06%\n",
      "iter 267: loss 3.6752, time 406.72ms, mfu 1.06%\n",
      "iter 268: loss 3.6562, time 406.71ms, mfu 1.06%\n",
      "iter 269: loss 3.8269, time 406.92ms, mfu 1.06%\n",
      "iter 270: loss 3.7240, time 407.69ms, mfu 1.06%\n",
      "iter 271: loss 3.7292, time 405.57ms, mfu 1.06%\n",
      "iter 272: loss 3.7759, time 405.76ms, mfu 1.06%\n",
      "iter 273: loss 3.5818, time 406.38ms, mfu 1.06%\n",
      "iter 274: loss 3.7994, time 404.73ms, mfu 1.06%\n",
      "iter 275: loss 3.6069, time 405.98ms, mfu 1.06%\n",
      "iter 276: loss 3.6483, time 405.25ms, mfu 1.06%\n",
      "iter 277: loss 3.7109, time 405.15ms, mfu 1.06%\n",
      "iter 278: loss 3.6067, time 394.98ms, mfu 1.07%\n",
      "iter 279: loss 3.5949, time 407.81ms, mfu 1.06%\n",
      "iter 280: loss 3.7259, time 409.05ms, mfu 1.06%\n",
      "iter 281: loss 3.7231, time 406.87ms, mfu 1.06%\n",
      "iter 282: loss 3.6785, time 407.96ms, mfu 1.06%\n",
      "iter 283: loss 3.7377, time 406.86ms, mfu 1.06%\n",
      "iter 284: loss 3.8457, time 406.54ms, mfu 1.06%\n",
      "iter 285: loss 3.5497, time 410.65ms, mfu 1.06%\n",
      "iter 286: loss 3.7257, time 406.19ms, mfu 1.06%\n",
      "iter 287: loss 3.5479, time 404.15ms, mfu 1.06%\n",
      "iter 288: loss 3.4477, time 405.67ms, mfu 1.06%\n",
      "iter 289: loss 3.7894, time 403.43ms, mfu 1.06%\n",
      "iter 290: loss 3.6307, time 403.11ms, mfu 1.06%\n",
      "iter 291: loss 3.5891, time 407.59ms, mfu 1.06%\n",
      "iter 292: loss 3.7443, time 404.18ms, mfu 1.06%\n",
      "iter 293: loss 3.5607, time 407.37ms, mfu 1.06%\n",
      "iter 294: loss 3.6333, time 406.56ms, mfu 1.06%\n",
      "iter 295: loss 3.6077, time 400.97ms, mfu 1.06%\n",
      "iter 296: loss 3.4619, time 404.47ms, mfu 1.06%\n",
      "iter 297: loss 3.5439, time 406.17ms, mfu 1.06%\n",
      "iter 298: loss 3.3973, time 403.39ms, mfu 1.06%\n",
      "iter 299: loss 3.5111, time 406.48ms, mfu 1.06%\n",
      "step 300: train loss 3.5920, val loss 4.4171\n",
      "saving checkpoint to out-nikhil-gpt-final\n",
      "iter 300: loss 3.6097, time 3685.21ms, mfu 0.97%\n",
      "iter 301: loss 3.3552, time 405.60ms, mfu 0.98%\n",
      "iter 302: loss 3.4456, time 402.81ms, mfu 0.99%\n",
      "iter 303: loss 3.5603, time 407.60ms, mfu 0.99%\n",
      "iter 304: loss 3.4808, time 404.30ms, mfu 1.00%\n",
      "iter 305: loss 3.4275, time 407.14ms, mfu 1.01%\n",
      "iter 306: loss 3.5648, time 407.58ms, mfu 1.01%\n",
      "iter 307: loss 3.5431, time 407.11ms, mfu 1.02%\n",
      "iter 308: loss 3.6286, time 399.96ms, mfu 1.02%\n",
      "iter 309: loss 3.5087, time 400.25ms, mfu 1.03%\n",
      "iter 310: loss 3.5156, time 404.56ms, mfu 1.03%\n",
      "iter 311: loss 3.5613, time 405.21ms, mfu 1.03%\n",
      "iter 312: loss 3.6315, time 404.25ms, mfu 1.04%\n",
      "iter 313: loss 3.5035, time 407.00ms, mfu 1.04%\n",
      "iter 314: loss 3.5335, time 406.75ms, mfu 1.04%\n",
      "iter 315: loss 3.5909, time 406.79ms, mfu 1.04%\n",
      "iter 316: loss 3.4904, time 398.90ms, mfu 1.05%\n",
      "iter 317: loss 3.5124, time 406.20ms, mfu 1.05%\n",
      "iter 318: loss 3.4419, time 406.50ms, mfu 1.05%\n",
      "iter 319: loss 3.3282, time 397.47ms, mfu 1.05%\n",
      "iter 320: loss 3.5972, time 405.22ms, mfu 1.05%\n",
      "iter 321: loss 3.4337, time 406.49ms, mfu 1.05%\n",
      "iter 322: loss 3.4110, time 406.59ms, mfu 1.06%\n",
      "iter 323: loss 3.3176, time 405.74ms, mfu 1.06%\n",
      "iter 324: loss 3.3993, time 404.46ms, mfu 1.06%\n",
      "iter 325: loss 3.5013, time 407.79ms, mfu 1.06%\n",
      "iter 326: loss 3.5233, time 403.50ms, mfu 1.06%\n",
      "iter 327: loss 3.3758, time 404.36ms, mfu 1.06%\n",
      "iter 328: loss 3.4855, time 406.77ms, mfu 1.06%\n",
      "iter 329: loss 3.4392, time 406.89ms, mfu 1.06%\n",
      "iter 330: loss 3.5173, time 407.54ms, mfu 1.06%\n",
      "iter 331: loss 3.3868, time 408.24ms, mfu 1.06%\n",
      "iter 332: loss 3.2923, time 404.64ms, mfu 1.06%\n",
      "iter 333: loss 3.4525, time 404.86ms, mfu 1.06%\n",
      "iter 334: loss 3.2827, time 402.84ms, mfu 1.06%\n",
      "iter 335: loss 3.4719, time 405.58ms, mfu 1.06%\n",
      "iter 336: loss 3.4761, time 403.90ms, mfu 1.06%\n",
      "iter 337: loss 3.4190, time 406.97ms, mfu 1.06%\n",
      "iter 338: loss 3.2895, time 401.05ms, mfu 1.06%\n",
      "iter 339: loss 3.6028, time 407.93ms, mfu 1.06%\n",
      "iter 340: loss 3.5621, time 407.64ms, mfu 1.06%\n",
      "iter 341: loss 3.3113, time 402.73ms, mfu 1.06%\n",
      "iter 342: loss 3.4434, time 406.43ms, mfu 1.06%\n",
      "iter 343: loss 3.3941, time 407.08ms, mfu 1.06%\n",
      "iter 344: loss 3.4427, time 394.52ms, mfu 1.06%\n",
      "iter 345: loss 3.5457, time 407.93ms, mfu 1.06%\n",
      "iter 346: loss 3.2324, time 406.53ms, mfu 1.06%\n",
      "iter 347: loss 3.4541, time 405.58ms, mfu 1.06%\n",
      "iter 348: loss 3.4718, time 408.36ms, mfu 1.06%\n",
      "iter 349: loss 3.3866, time 406.71ms, mfu 1.06%\n",
      "iter 350: loss 3.3451, time 406.67ms, mfu 1.06%\n",
      "iter 351: loss 3.5546, time 407.00ms, mfu 1.06%\n",
      "iter 352: loss 3.3634, time 404.85ms, mfu 1.06%\n",
      "iter 353: loss 3.4435, time 406.87ms, mfu 1.06%\n",
      "iter 354: loss 3.4824, time 405.87ms, mfu 1.06%\n",
      "iter 355: loss 3.5508, time 405.68ms, mfu 1.06%\n",
      "iter 356: loss 3.3503, time 407.46ms, mfu 1.06%\n",
      "iter 357: loss 3.2781, time 406.00ms, mfu 1.06%\n",
      "iter 358: loss 3.4257, time 406.07ms, mfu 1.06%\n",
      "iter 359: loss 3.5633, time 399.19ms, mfu 1.06%\n",
      "iter 360: loss 3.2737, time 404.77ms, mfu 1.06%\n",
      "iter 361: loss 3.3811, time 402.09ms, mfu 1.06%\n",
      "iter 362: loss 3.4445, time 406.43ms, mfu 1.06%\n",
      "iter 363: loss 3.3313, time 406.71ms, mfu 1.06%\n",
      "iter 364: loss 3.2804, time 406.67ms, mfu 1.06%\n",
      "iter 365: loss 3.4684, time 406.12ms, mfu 1.06%\n",
      "iter 366: loss 3.3690, time 406.40ms, mfu 1.06%\n",
      "iter 367: loss 3.2841, time 405.00ms, mfu 1.06%\n",
      "iter 368: loss 3.2400, time 397.22ms, mfu 1.06%\n",
      "iter 369: loss 3.2391, time 400.69ms, mfu 1.07%\n",
      "iter 370: loss 2.9105, time 405.27ms, mfu 1.07%\n",
      "iter 371: loss 3.2144, time 405.96ms, mfu 1.06%\n",
      "iter 372: loss 3.0922, time 406.07ms, mfu 1.06%\n",
      "iter 373: loss 3.4326, time 403.71ms, mfu 1.06%\n",
      "iter 374: loss 3.3194, time 404.55ms, mfu 1.06%\n",
      "iter 375: loss 3.3068, time 407.18ms, mfu 1.06%\n",
      "iter 376: loss 3.3525, time 404.77ms, mfu 1.06%\n",
      "iter 377: loss 3.3785, time 404.92ms, mfu 1.06%\n",
      "iter 378: loss 3.3508, time 406.37ms, mfu 1.06%\n",
      "iter 379: loss 3.2632, time 406.78ms, mfu 1.06%\n",
      "iter 380: loss 3.2097, time 405.49ms, mfu 1.06%\n",
      "iter 381: loss 3.2531, time 407.70ms, mfu 1.06%\n",
      "iter 382: loss 3.3491, time 406.78ms, mfu 1.06%\n",
      "iter 383: loss 3.2058, time 405.78ms, mfu 1.06%\n",
      "iter 384: loss 3.2854, time 407.34ms, mfu 1.06%\n",
      "iter 385: loss 3.4243, time 404.19ms, mfu 1.06%\n",
      "iter 386: loss 3.2982, time 405.32ms, mfu 1.06%\n",
      "iter 387: loss 3.3670, time 405.99ms, mfu 1.06%\n",
      "iter 388: loss 3.5772, time 402.02ms, mfu 1.06%\n",
      "iter 389: loss 3.3359, time 406.90ms, mfu 1.06%\n",
      "iter 390: loss 3.2565, time 406.22ms, mfu 1.06%\n",
      "iter 391: loss 3.2078, time 398.74ms, mfu 1.06%\n",
      "iter 392: loss 3.1557, time 404.40ms, mfu 1.06%\n",
      "iter 393: loss 3.2493, time 398.80ms, mfu 1.07%\n",
      "iter 394: loss 3.0501, time 394.03ms, mfu 1.07%\n",
      "iter 395: loss 3.2894, time 397.63ms, mfu 1.07%\n",
      "iter 396: loss 3.2020, time 401.28ms, mfu 1.07%\n",
      "iter 397: loss 3.1062, time 398.26ms, mfu 1.07%\n",
      "iter 398: loss 3.3592, time 398.94ms, mfu 1.07%\n",
      "iter 399: loss 3.3194, time 404.41ms, mfu 1.07%\n",
      "step 400: train loss 3.2369, val loss 4.4902\n",
      "iter 400: loss 3.2281, time 2553.62ms, mfu 0.98%\n",
      "iter 401: loss 3.3158, time 396.79ms, mfu 0.99%\n",
      "iter 402: loss 3.3268, time 405.04ms, mfu 1.00%\n",
      "iter 403: loss 3.2674, time 404.24ms, mfu 1.01%\n",
      "iter 404: loss 3.2624, time 406.65ms, mfu 1.01%\n",
      "iter 405: loss 3.2876, time 406.26ms, mfu 1.02%\n",
      "iter 406: loss 3.2705, time 405.83ms, mfu 1.02%\n",
      "iter 407: loss 3.2828, time 405.11ms, mfu 1.02%\n",
      "iter 408: loss 3.1103, time 402.22ms, mfu 1.03%\n",
      "iter 409: loss 3.1307, time 407.75ms, mfu 1.03%\n",
      "iter 410: loss 3.4292, time 406.11ms, mfu 1.03%\n",
      "iter 411: loss 3.1456, time 405.83ms, mfu 1.04%\n",
      "iter 412: loss 3.2149, time 407.24ms, mfu 1.04%\n",
      "iter 413: loss 3.2995, time 397.38ms, mfu 1.04%\n",
      "iter 414: loss 3.3326, time 400.05ms, mfu 1.05%\n",
      "iter 415: loss 3.1310, time 404.73ms, mfu 1.05%\n",
      "iter 416: loss 3.0924, time 404.29ms, mfu 1.05%\n",
      "iter 417: loss 3.2316, time 402.64ms, mfu 1.05%\n",
      "iter 418: loss 3.4882, time 404.63ms, mfu 1.05%\n",
      "iter 419: loss 3.2630, time 405.18ms, mfu 1.05%\n",
      "iter 420: loss 3.0716, time 404.15ms, mfu 1.06%\n",
      "iter 421: loss 3.1646, time 405.56ms, mfu 1.06%\n",
      "iter 422: loss 3.1058, time 404.37ms, mfu 1.06%\n",
      "iter 423: loss 3.0936, time 406.53ms, mfu 1.06%\n",
      "iter 424: loss 3.2197, time 406.14ms, mfu 1.06%\n",
      "iter 425: loss 3.1277, time 404.24ms, mfu 1.06%\n",
      "iter 426: loss 3.1208, time 404.39ms, mfu 1.06%\n",
      "iter 427: loss 3.0811, time 405.02ms, mfu 1.06%\n",
      "iter 428: loss 3.1375, time 406.40ms, mfu 1.06%\n",
      "iter 429: loss 3.1647, time 402.39ms, mfu 1.06%\n",
      "iter 430: loss 3.0476, time 405.92ms, mfu 1.06%\n",
      "iter 431: loss 3.1720, time 407.13ms, mfu 1.06%\n",
      "iter 432: loss 3.0837, time 402.15ms, mfu 1.06%\n",
      "iter 433: loss 3.3513, time 399.60ms, mfu 1.06%\n",
      "iter 434: loss 3.0677, time 407.29ms, mfu 1.06%\n",
      "iter 435: loss 3.0706, time 403.03ms, mfu 1.06%\n",
      "iter 436: loss 3.1356, time 406.59ms, mfu 1.06%\n",
      "iter 437: loss 3.0589, time 404.35ms, mfu 1.06%\n",
      "iter 438: loss 3.0659, time 403.16ms, mfu 1.06%\n",
      "iter 439: loss 3.1370, time 404.39ms, mfu 1.06%\n",
      "iter 440: loss 3.1963, time 406.77ms, mfu 1.06%\n",
      "iter 441: loss 3.0599, time 406.22ms, mfu 1.06%\n",
      "iter 442: loss 3.0453, time 405.96ms, mfu 1.06%\n",
      "iter 443: loss 3.2431, time 406.98ms, mfu 1.06%\n",
      "iter 444: loss 3.1513, time 405.66ms, mfu 1.06%\n",
      "iter 445: loss 3.2472, time 405.62ms, mfu 1.06%\n",
      "iter 446: loss 3.1336, time 406.39ms, mfu 1.06%\n",
      "iter 447: loss 3.0955, time 404.38ms, mfu 1.06%\n",
      "iter 448: loss 3.0897, time 406.20ms, mfu 1.06%\n",
      "iter 449: loss 3.0880, time 405.42ms, mfu 1.06%\n",
      "iter 450: loss 2.8770, time 397.94ms, mfu 1.06%\n",
      "iter 451: loss 3.1767, time 406.08ms, mfu 1.06%\n",
      "iter 452: loss 3.0042, time 404.83ms, mfu 1.06%\n",
      "iter 453: loss 3.2801, time 403.85ms, mfu 1.06%\n",
      "iter 454: loss 3.2151, time 405.54ms, mfu 1.06%\n",
      "iter 455: loss 3.1245, time 405.67ms, mfu 1.06%\n",
      "iter 456: loss 3.0228, time 406.41ms, mfu 1.06%\n",
      "iter 457: loss 3.1316, time 403.07ms, mfu 1.06%\n",
      "iter 458: loss 3.0418, time 407.28ms, mfu 1.06%\n",
      "iter 459: loss 2.8351, time 404.05ms, mfu 1.06%\n",
      "iter 460: loss 3.0509, time 405.38ms, mfu 1.06%\n",
      "iter 461: loss 3.0987, time 404.77ms, mfu 1.06%\n",
      "iter 462: loss 2.9565, time 391.46ms, mfu 1.07%\n",
      "iter 463: loss 2.9989, time 401.58ms, mfu 1.07%\n",
      "iter 464: loss 2.9417, time 406.57ms, mfu 1.07%\n",
      "iter 465: loss 3.0526, time 407.14ms, mfu 1.07%\n",
      "iter 466: loss 2.9154, time 406.64ms, mfu 1.07%\n",
      "iter 467: loss 3.0195, time 403.21ms, mfu 1.07%\n",
      "iter 468: loss 3.0542, time 404.83ms, mfu 1.07%\n",
      "iter 469: loss 2.8913, time 409.61ms, mfu 1.06%\n",
      "iter 470: loss 2.9099, time 405.06ms, mfu 1.06%\n",
      "iter 471: loss 3.1584, time 409.80ms, mfu 1.06%\n",
      "iter 472: loss 2.9244, time 405.87ms, mfu 1.06%\n",
      "iter 473: loss 3.0318, time 405.41ms, mfu 1.06%\n",
      "iter 474: loss 2.7958, time 401.05ms, mfu 1.06%\n",
      "iter 475: loss 3.0599, time 407.45ms, mfu 1.06%\n",
      "iter 476: loss 2.9913, time 404.14ms, mfu 1.06%\n",
      "iter 477: loss 3.0131, time 405.61ms, mfu 1.06%\n",
      "iter 478: loss 3.1097, time 406.60ms, mfu 1.06%\n",
      "iter 479: loss 2.9100, time 406.45ms, mfu 1.06%\n",
      "iter 480: loss 2.9407, time 405.22ms, mfu 1.06%\n",
      "iter 481: loss 3.0163, time 406.27ms, mfu 1.06%\n",
      "iter 482: loss 3.0858, time 407.14ms, mfu 1.06%\n",
      "iter 483: loss 2.9075, time 401.69ms, mfu 1.06%\n",
      "iter 484: loss 2.8986, time 408.43ms, mfu 1.06%\n",
      "iter 485: loss 2.9910, time 406.37ms, mfu 1.06%\n",
      "iter 486: loss 2.9749, time 405.15ms, mfu 1.06%\n",
      "iter 487: loss 3.0424, time 403.82ms, mfu 1.06%\n",
      "iter 488: loss 2.8689, time 401.32ms, mfu 1.06%\n",
      "iter 489: loss 2.8388, time 405.82ms, mfu 1.06%\n",
      "iter 490: loss 3.0682, time 406.82ms, mfu 1.06%\n",
      "iter 491: loss 2.8168, time 402.16ms, mfu 1.06%\n",
      "iter 492: loss 2.9809, time 402.82ms, mfu 1.06%\n",
      "iter 493: loss 2.9091, time 407.01ms, mfu 1.06%\n",
      "iter 494: loss 2.8336, time 401.80ms, mfu 1.06%\n",
      "iter 495: loss 3.0658, time 406.73ms, mfu 1.06%\n",
      "iter 496: loss 2.9416, time 406.19ms, mfu 1.06%\n",
      "iter 497: loss 2.8615, time 403.62ms, mfu 1.06%\n",
      "iter 498: loss 3.0140, time 404.29ms, mfu 1.06%\n",
      "iter 499: loss 2.9394, time 401.06ms, mfu 1.07%\n",
      "step 500: train loss 2.9301, val loss 4.7212\n",
      "iter 500: loss 2.9460, time 2575.60ms, mfu 0.98%\n",
      "iter 501: loss 2.8936, time 406.68ms, mfu 0.98%\n",
      "iter 502: loss 3.0630, time 404.83ms, mfu 0.99%\n",
      "iter 503: loss 2.9670, time 404.55ms, mfu 1.00%\n",
      "iter 504: loss 2.9015, time 406.21ms, mfu 1.01%\n",
      "iter 505: loss 3.0296, time 406.46ms, mfu 1.01%\n",
      "iter 506: loss 3.0420, time 391.85ms, mfu 1.02%\n",
      "iter 507: loss 2.8885, time 406.81ms, mfu 1.02%\n",
      "iter 508: loss 2.8572, time 403.95ms, mfu 1.03%\n",
      "iter 509: loss 2.9306, time 406.50ms, mfu 1.03%\n",
      "iter 510: loss 2.9324, time 406.01ms, mfu 1.03%\n",
      "iter 511: loss 2.8822, time 405.41ms, mfu 1.04%\n",
      "iter 512: loss 2.8601, time 406.21ms, mfu 1.04%\n",
      "iter 513: loss 3.0290, time 407.09ms, mfu 1.04%\n",
      "iter 514: loss 2.8749, time 404.90ms, mfu 1.04%\n",
      "iter 515: loss 3.0119, time 407.84ms, mfu 1.04%\n",
      "iter 516: loss 2.7828, time 405.87ms, mfu 1.05%\n",
      "iter 517: loss 2.9916, time 403.91ms, mfu 1.05%\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/s/bach/l/under/daives/cs445/assignments/nikhil-GPT/nanoGPT/train.py\", line 293, in <module>\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py config/train_config.py --out_dir=out-nikhil-gpt-final_dirty --compile=False --eval_iters=200 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started to overfit after 300 iteration checkpoint, but we ended with train loss 3.5920, val loss 4.4171. Let's finetune to see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_config.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "eval_interval = 100 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False \n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'nikhil-gpt'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'medium'\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: init_from = resume\n",
      "Overriding: out_dir = out-nikhil-gpt-final\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 200\n",
      "Overriding: log_interval = 1\n",
      "Overriding: block_size = 64\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 1000\n",
      "Overriding: lr_decay_iters = 1000\n",
      "Overriding: dropout = 0.2\n",
      "Overriding: learning_rate = 0.0001\n",
      "Resuming training from out-nikhil-gpt-final\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "using fused AdamW: False\n",
      "step 300: train loss 3.5943, val loss 4.4121\n",
      "saving checkpoint to out-nikhil-gpt-final\n",
      "iter 300: loss 4.4238, time 4247.79ms, mfu -100.00%\n",
      "iter 301: loss 4.3014, time 427.70ms, mfu -100.00%\n",
      "iter 302: loss 4.2378, time 416.52ms, mfu -100.00%\n",
      "iter 303: loss 4.1555, time 427.88ms, mfu -100.00%\n",
      "iter 304: loss 4.2411, time 427.78ms, mfu -100.00%\n",
      "iter 305: loss 4.1775, time 428.09ms, mfu 1.01%\n",
      "iter 306: loss 4.4647, time 427.90ms, mfu 1.01%\n",
      "iter 307: loss 4.2699, time 428.42ms, mfu 1.01%\n",
      "iter 308: loss 3.9988, time 422.00ms, mfu 1.01%\n",
      "iter 309: loss 4.3059, time 425.59ms, mfu 1.01%\n",
      "iter 310: loss 4.2158, time 428.65ms, mfu 1.01%\n",
      "iter 311: loss 4.0411, time 427.18ms, mfu 1.01%\n",
      "iter 312: loss 4.2522, time 424.63ms, mfu 1.01%\n",
      "iter 313: loss 4.1492, time 425.97ms, mfu 1.01%\n",
      "iter 314: loss 4.2442, time 427.87ms, mfu 1.01%\n",
      "iter 315: loss 4.3137, time 424.63ms, mfu 1.01%\n",
      "iter 316: loss 4.2533, time 427.52ms, mfu 1.01%\n",
      "iter 317: loss 4.2795, time 428.35ms, mfu 1.01%\n",
      "iter 318: loss 4.0422, time 422.98ms, mfu 1.01%\n",
      "iter 319: loss 4.0814, time 424.40ms, mfu 1.01%\n",
      "iter 320: loss 4.2258, time 426.98ms, mfu 1.01%\n",
      "iter 321: loss 4.1752, time 427.72ms, mfu 1.01%\n",
      "iter 322: loss 4.1673, time 426.83ms, mfu 1.01%\n",
      "iter 323: loss 3.9644, time 426.45ms, mfu 1.01%\n",
      "iter 324: loss 3.8481, time 426.24ms, mfu 1.01%\n",
      "iter 325: loss 4.1688, time 423.05ms, mfu 1.01%\n",
      "iter 326: loss 3.9865, time 420.26ms, mfu 1.01%\n",
      "iter 327: loss 3.8699, time 426.82ms, mfu 1.01%\n",
      "iter 328: loss 4.2018, time 426.70ms, mfu 1.01%\n",
      "iter 329: loss 4.0085, time 426.92ms, mfu 1.01%\n",
      "iter 330: loss 4.0570, time 426.77ms, mfu 1.01%\n",
      "iter 331: loss 4.0566, time 426.60ms, mfu 1.01%\n",
      "iter 332: loss 3.9180, time 426.76ms, mfu 1.01%\n",
      "iter 333: loss 4.1039, time 419.62ms, mfu 1.01%\n",
      "iter 334: loss 4.1426, time 427.09ms, mfu 1.01%\n",
      "iter 335: loss 4.2062, time 427.35ms, mfu 1.01%\n",
      "iter 336: loss 3.8571, time 424.06ms, mfu 1.01%\n",
      "iter 337: loss 4.0110, time 427.85ms, mfu 1.01%\n",
      "iter 338: loss 4.1866, time 425.46ms, mfu 1.01%\n",
      "iter 339: loss 4.3340, time 426.58ms, mfu 1.01%\n",
      "iter 340: loss 3.9772, time 425.95ms, mfu 1.01%\n",
      "iter 341: loss 4.1477, time 415.00ms, mfu 1.01%\n",
      "iter 342: loss 4.0982, time 416.52ms, mfu 1.02%\n",
      "iter 343: loss 3.9625, time 424.29ms, mfu 1.02%\n",
      "iter 344: loss 4.2025, time 426.98ms, mfu 1.02%\n",
      "iter 345: loss 3.9339, time 427.63ms, mfu 1.01%\n",
      "iter 346: loss 3.9022, time 426.61ms, mfu 1.01%\n",
      "iter 347: loss 4.1331, time 426.80ms, mfu 1.01%\n",
      "iter 348: loss 4.1029, time 425.78ms, mfu 1.01%\n",
      "iter 349: loss 3.9996, time 412.61ms, mfu 1.02%\n",
      "iter 350: loss 4.1684, time 424.29ms, mfu 1.02%\n",
      "iter 351: loss 4.2076, time 427.94ms, mfu 1.02%\n",
      "iter 352: loss 4.0644, time 424.01ms, mfu 1.02%\n",
      "iter 353: loss 4.0346, time 425.79ms, mfu 1.02%\n",
      "iter 354: loss 3.9573, time 427.20ms, mfu 1.01%\n",
      "iter 355: loss 3.9954, time 421.62ms, mfu 1.02%\n",
      "iter 356: loss 4.0200, time 422.32ms, mfu 1.02%\n",
      "iter 357: loss 4.2169, time 427.13ms, mfu 1.01%\n",
      "iter 358: loss 3.9783, time 427.10ms, mfu 1.01%\n",
      "iter 359: loss 3.9201, time 421.96ms, mfu 1.01%\n",
      "iter 360: loss 3.8340, time 424.11ms, mfu 1.02%\n",
      "iter 361: loss 4.0742, time 428.23ms, mfu 1.01%\n",
      "iter 362: loss 4.0412, time 422.24ms, mfu 1.01%\n",
      "iter 363: loss 4.2187, time 424.19ms, mfu 1.01%\n",
      "iter 364: loss 4.0448, time 426.61ms, mfu 1.01%\n",
      "iter 365: loss 3.9097, time 426.34ms, mfu 1.01%\n",
      "iter 366: loss 4.1312, time 425.04ms, mfu 1.01%\n",
      "iter 367: loss 4.0826, time 426.09ms, mfu 1.01%\n",
      "iter 368: loss 4.1327, time 428.22ms, mfu 1.01%\n",
      "iter 369: loss 3.9688, time 425.80ms, mfu 1.01%\n",
      "iter 370: loss 3.9547, time 416.78ms, mfu 1.01%\n",
      "iter 371: loss 4.0730, time 419.18ms, mfu 1.02%\n",
      "iter 372: loss 4.1250, time 420.78ms, mfu 1.02%\n",
      "iter 373: loss 3.9364, time 408.20ms, mfu 1.02%\n",
      "iter 374: loss 4.1483, time 425.17ms, mfu 1.02%\n",
      "iter 375: loss 3.9805, time 426.35ms, mfu 1.02%\n",
      "iter 376: loss 3.7635, time 425.20ms, mfu 1.02%\n",
      "iter 377: loss 4.2068, time 425.48ms, mfu 1.02%\n",
      "iter 378: loss 3.8872, time 424.22ms, mfu 1.02%\n",
      "iter 379: loss 3.8735, time 427.58ms, mfu 1.02%\n",
      "iter 380: loss 3.9997, time 421.71ms, mfu 1.02%\n",
      "iter 381: loss 3.8954, time 426.86ms, mfu 1.02%\n",
      "iter 382: loss 4.1616, time 426.97ms, mfu 1.02%\n",
      "iter 383: loss 4.0173, time 426.65ms, mfu 1.01%\n",
      "iter 384: loss 3.8164, time 424.36ms, mfu 1.01%\n",
      "iter 385: loss 3.8256, time 425.58ms, mfu 1.01%\n",
      "iter 386: loss 4.0302, time 426.03ms, mfu 1.01%\n",
      "iter 387: loss 4.0895, time 419.78ms, mfu 1.02%\n",
      "iter 388: loss 3.9338, time 425.11ms, mfu 1.02%\n",
      "iter 389: loss 3.8487, time 427.06ms, mfu 1.01%\n",
      "iter 390: loss 3.8520, time 425.91ms, mfu 1.01%\n",
      "iter 391: loss 4.1450, time 429.39ms, mfu 1.01%\n",
      "iter 392: loss 3.8813, time 423.02ms, mfu 1.01%\n",
      "iter 393: loss 4.0004, time 425.80ms, mfu 1.01%\n",
      "iter 394: loss 4.1005, time 419.72ms, mfu 1.01%\n",
      "iter 395: loss 4.1161, time 422.04ms, mfu 1.02%\n",
      "iter 396: loss 3.8708, time 423.67ms, mfu 1.02%\n",
      "iter 397: loss 3.9475, time 421.25ms, mfu 1.02%\n",
      "iter 398: loss 4.0180, time 424.23ms, mfu 1.02%\n",
      "iter 399: loss 3.9131, time 425.32ms, mfu 1.02%\n",
      "step 400: train loss 3.6515, val loss 4.3980\n",
      "saving checkpoint to out-nikhil-gpt-final\n",
      "iter 400: loss 4.0369, time 3794.25ms, mfu 0.93%\n",
      "iter 401: loss 3.9608, time 425.13ms, mfu 0.93%\n",
      "iter 402: loss 3.9594, time 422.66ms, mfu 0.94%\n",
      "iter 403: loss 4.0454, time 426.90ms, mfu 0.95%\n",
      "iter 404: loss 4.0386, time 425.36ms, mfu 0.96%\n",
      "iter 405: loss 3.9443, time 426.24ms, mfu 0.96%\n",
      "iter 406: loss 3.6484, time 427.80ms, mfu 0.97%\n",
      "iter 407: loss 3.8174, time 426.88ms, mfu 0.97%\n",
      "iter 408: loss 3.7379, time 428.49ms, mfu 0.97%\n",
      "iter 409: loss 3.7259, time 425.29ms, mfu 0.98%\n",
      "iter 410: loss 4.0294, time 423.63ms, mfu 0.98%\n",
      "iter 411: loss 3.8305, time 428.54ms, mfu 0.98%\n",
      "iter 412: loss 3.9202, time 418.32ms, mfu 0.99%\n",
      "iter 413: loss 3.9793, time 428.34ms, mfu 0.99%\n",
      "iter 414: loss 3.9500, time 425.13ms, mfu 0.99%\n",
      "iter 415: loss 4.0993, time 424.17ms, mfu 0.99%\n",
      "iter 416: loss 3.8184, time 426.02ms, mfu 1.00%\n",
      "iter 417: loss 3.9188, time 425.18ms, mfu 1.00%\n",
      "iter 418: loss 3.8214, time 425.56ms, mfu 1.00%\n",
      "iter 419: loss 3.9545, time 428.25ms, mfu 1.00%\n",
      "iter 420: loss 3.9034, time 422.06ms, mfu 1.00%\n",
      "iter 421: loss 3.9832, time 427.03ms, mfu 1.00%\n",
      "iter 422: loss 3.8562, time 426.94ms, mfu 1.00%\n",
      "iter 423: loss 4.0778, time 428.61ms, mfu 1.00%\n",
      "iter 424: loss 4.0095, time 426.99ms, mfu 1.00%\n",
      "iter 425: loss 3.9821, time 427.78ms, mfu 1.00%\n",
      "iter 426: loss 3.9215, time 427.23ms, mfu 1.00%\n",
      "iter 427: loss 3.9107, time 427.31ms, mfu 1.01%\n",
      "iter 428: loss 3.8533, time 428.19ms, mfu 1.01%\n",
      "iter 429: loss 3.9531, time 425.54ms, mfu 1.01%\n",
      "iter 430: loss 3.8147, time 431.32ms, mfu 1.01%\n",
      "iter 431: loss 4.0011, time 423.50ms, mfu 1.01%\n",
      "iter 432: loss 4.0125, time 422.50ms, mfu 1.01%\n",
      "iter 433: loss 3.8799, time 416.96ms, mfu 1.01%\n",
      "iter 434: loss 3.9006, time 423.96ms, mfu 1.01%\n",
      "iter 435: loss 3.9865, time 417.04ms, mfu 1.01%\n",
      "iter 436: loss 4.0127, time 424.17ms, mfu 1.01%\n",
      "iter 437: loss 3.8113, time 426.47ms, mfu 1.01%\n",
      "iter 438: loss 3.7982, time 426.04ms, mfu 1.01%\n",
      "iter 439: loss 4.0487, time 421.06ms, mfu 1.01%\n",
      "iter 440: loss 4.0139, time 424.35ms, mfu 1.01%\n",
      "iter 441: loss 3.7209, time 425.75ms, mfu 1.01%\n",
      "iter 442: loss 3.6687, time 424.99ms, mfu 1.01%\n",
      "iter 443: loss 3.7290, time 424.41ms, mfu 1.01%\n",
      "iter 444: loss 4.1363, time 422.58ms, mfu 1.01%\n",
      "iter 445: loss 3.6712, time 428.68ms, mfu 1.01%\n",
      "iter 446: loss 3.9160, time 427.58ms, mfu 1.01%\n",
      "iter 447: loss 3.7354, time 427.42ms, mfu 1.01%\n",
      "iter 448: loss 3.9003, time 418.07ms, mfu 1.01%\n",
      "iter 449: loss 3.8219, time 422.10ms, mfu 1.01%\n",
      "iter 450: loss 3.9600, time 426.70ms, mfu 1.01%\n",
      "iter 451: loss 3.9538, time 426.78ms, mfu 1.01%\n",
      "iter 452: loss 4.0529, time 427.65ms, mfu 1.01%\n",
      "iter 453: loss 3.9433, time 425.65ms, mfu 1.01%\n",
      "iter 454: loss 4.0024, time 426.67ms, mfu 1.01%\n",
      "iter 455: loss 3.9065, time 421.90ms, mfu 1.01%\n",
      "iter 456: loss 3.9317, time 426.23ms, mfu 1.01%\n",
      "iter 457: loss 3.8275, time 425.10ms, mfu 1.01%\n",
      "iter 458: loss 3.5992, time 427.16ms, mfu 1.01%\n",
      "iter 459: loss 3.8685, time 416.97ms, mfu 1.01%\n",
      "iter 460: loss 3.9966, time 427.28ms, mfu 1.01%\n",
      "iter 461: loss 3.8732, time 427.25ms, mfu 1.01%\n",
      "iter 462: loss 3.8716, time 426.20ms, mfu 1.01%\n",
      "iter 463: loss 3.8796, time 427.66ms, mfu 1.01%\n",
      "iter 464: loss 3.9291, time 422.11ms, mfu 1.01%\n",
      "iter 465: loss 3.7209, time 426.71ms, mfu 1.01%\n",
      "iter 466: loss 3.8280, time 424.45ms, mfu 1.01%\n",
      "iter 467: loss 3.8152, time 424.62ms, mfu 1.01%\n",
      "iter 468: loss 4.0377, time 423.57ms, mfu 1.01%\n",
      "iter 469: loss 3.9568, time 424.80ms, mfu 1.01%\n",
      "iter 470: loss 3.8142, time 427.92ms, mfu 1.01%\n",
      "iter 471: loss 3.9566, time 423.43ms, mfu 1.01%\n",
      "iter 472: loss 3.8137, time 424.96ms, mfu 1.01%\n",
      "iter 473: loss 3.8765, time 426.90ms, mfu 1.01%\n",
      "iter 474: loss 3.8335, time 425.33ms, mfu 1.01%\n",
      "iter 475: loss 3.8945, time 425.62ms, mfu 1.01%\n",
      "iter 476: loss 3.8740, time 423.60ms, mfu 1.01%\n",
      "iter 477: loss 4.0189, time 427.89ms, mfu 1.01%\n",
      "iter 478: loss 4.0179, time 426.82ms, mfu 1.01%\n",
      "iter 479: loss 3.8577, time 428.50ms, mfu 1.01%\n",
      "iter 480: loss 3.8378, time 426.28ms, mfu 1.01%\n",
      "iter 481: loss 3.8839, time 425.34ms, mfu 1.01%\n",
      "iter 482: loss 3.6126, time 421.49ms, mfu 1.01%\n",
      "iter 483: loss 3.8359, time 425.17ms, mfu 1.01%\n",
      "iter 484: loss 3.8509, time 425.93ms, mfu 1.01%\n",
      "iter 485: loss 3.9825, time 426.73ms, mfu 1.01%\n",
      "iter 486: loss 3.9695, time 425.46ms, mfu 1.01%\n",
      "iter 487: loss 4.0477, time 423.63ms, mfu 1.01%\n",
      "iter 488: loss 3.8761, time 426.59ms, mfu 1.01%\n",
      "iter 489: loss 3.8554, time 427.60ms, mfu 1.01%\n",
      "iter 490: loss 3.6386, time 426.28ms, mfu 1.01%\n",
      "iter 491: loss 3.7274, time 420.15ms, mfu 1.01%\n",
      "iter 492: loss 4.1289, time 427.34ms, mfu 1.01%\n",
      "iter 493: loss 3.8715, time 423.82ms, mfu 1.01%\n",
      "iter 494: loss 3.8758, time 423.69ms, mfu 1.01%\n",
      "iter 495: loss 3.9176, time 425.57ms, mfu 1.01%\n",
      "iter 496: loss 3.8294, time 419.87ms, mfu 1.01%\n",
      "iter 497: loss 3.7311, time 423.60ms, mfu 1.01%\n",
      "iter 498: loss 3.8652, time 425.51ms, mfu 1.01%\n",
      "iter 499: loss 3.8620, time 426.83ms, mfu 1.01%\n",
      "step 500: train loss 3.6140, val loss 4.3879\n",
      "saving checkpoint to out-nikhil-gpt-final\n",
      "iter 500: loss 3.6543, time 3750.12ms, mfu 0.92%\n",
      "iter 501: loss 3.8214, time 426.65ms, mfu 0.93%\n",
      "iter 502: loss 3.8014, time 427.25ms, mfu 0.94%\n",
      "iter 503: loss 3.9559, time 428.98ms, mfu 0.95%\n",
      "iter 504: loss 3.8429, time 429.14ms, mfu 0.95%\n",
      "iter 505: loss 3.8065, time 429.29ms, mfu 0.96%\n",
      "iter 506: loss 3.9362, time 430.47ms, mfu 0.96%\n",
      "iter 507: loss 3.9362, time 413.14ms, mfu 0.97%\n",
      "iter 508: loss 3.7364, time 424.53ms, mfu 0.97%\n",
      "iter 509: loss 3.8084, time 427.30ms, mfu 0.98%\n",
      "iter 510: loss 3.8551, time 426.13ms, mfu 0.98%\n",
      "iter 511: loss 3.8423, time 427.12ms, mfu 0.98%\n",
      "iter 512: loss 3.7843, time 426.04ms, mfu 0.99%\n",
      "iter 513: loss 3.8672, time 427.29ms, mfu 0.99%\n",
      "iter 514: loss 3.7574, time 425.57ms, mfu 0.99%\n",
      "iter 515: loss 3.7768, time 423.28ms, mfu 0.99%\n",
      "iter 516: loss 4.0479, time 427.35ms, mfu 1.00%\n",
      "iter 517: loss 3.6655, time 426.74ms, mfu 1.00%\n",
      "iter 518: loss 3.9098, time 425.14ms, mfu 1.00%\n",
      "iter 519: loss 3.9587, time 425.98ms, mfu 1.00%\n",
      "iter 520: loss 4.1068, time 424.65ms, mfu 1.00%\n",
      "iter 521: loss 3.5744, time 425.84ms, mfu 1.00%\n",
      "iter 522: loss 3.7772, time 416.25ms, mfu 1.01%\n",
      "iter 523: loss 3.7775, time 426.46ms, mfu 1.01%\n",
      "iter 524: loss 3.9567, time 425.89ms, mfu 1.01%\n",
      "iter 525: loss 3.7938, time 428.88ms, mfu 1.01%\n",
      "iter 526: loss 3.6801, time 427.46ms, mfu 1.01%\n",
      "iter 527: loss 3.8225, time 426.22ms, mfu 1.01%\n",
      "iter 528: loss 3.7590, time 428.30ms, mfu 1.01%\n",
      "iter 529: loss 4.0318, time 425.93ms, mfu 1.01%\n",
      "iter 530: loss 3.6847, time 427.88ms, mfu 1.01%\n",
      "iter 531: loss 3.7341, time 425.39ms, mfu 1.01%\n",
      "iter 532: loss 3.7736, time 425.51ms, mfu 1.01%\n",
      "iter 533: loss 3.7330, time 426.95ms, mfu 1.01%\n",
      "iter 534: loss 3.7513, time 428.22ms, mfu 1.01%\n",
      "iter 535: loss 3.8907, time 429.19ms, mfu 1.01%\n",
      "iter 536: loss 3.9575, time 416.61ms, mfu 1.01%\n",
      "iter 537: loss 3.7924, time 406.46ms, mfu 1.02%\n",
      "iter 538: loss 3.8686, time 425.33ms, mfu 1.01%\n",
      "iter 539: loss 3.8243, time 415.98ms, mfu 1.02%\n",
      "iter 540: loss 3.9808, time 415.78ms, mfu 1.02%\n",
      "iter 541: loss 3.9692, time 419.78ms, mfu 1.02%\n",
      "iter 542: loss 3.8729, time 426.37ms, mfu 1.02%\n",
      "iter 543: loss 3.8945, time 438.93ms, mfu 1.02%\n",
      "iter 544: loss 3.8395, time 432.00ms, mfu 1.01%\n",
      "iter 545: loss 3.8256, time 402.36ms, mfu 1.02%\n",
      "iter 546: loss 3.9786, time 426.11ms, mfu 1.02%\n",
      "iter 547: loss 3.8554, time 425.42ms, mfu 1.02%\n",
      "iter 548: loss 3.8846, time 426.87ms, mfu 1.02%\n",
      "iter 549: loss 3.8141, time 402.67ms, mfu 1.02%\n",
      "iter 550: loss 3.8030, time 414.31ms, mfu 1.02%\n",
      "iter 551: loss 3.8505, time 426.42ms, mfu 1.02%\n",
      "iter 552: loss 4.0470, time 422.61ms, mfu 1.02%\n",
      "iter 553: loss 3.7873, time 434.85ms, mfu 1.02%\n",
      "iter 554: loss 3.8164, time 412.84ms, mfu 1.02%\n",
      "iter 555: loss 3.9651, time 421.97ms, mfu 1.02%\n",
      "iter 556: loss 3.7999, time 428.03ms, mfu 1.02%\n",
      "iter 557: loss 3.8561, time 425.89ms, mfu 1.02%\n",
      "iter 558: loss 3.7645, time 426.09ms, mfu 1.02%\n",
      "iter 559: loss 3.7427, time 426.27ms, mfu 1.02%\n",
      "iter 560: loss 4.0831, time 425.04ms, mfu 1.02%\n",
      "iter 561: loss 3.7523, time 423.23ms, mfu 1.02%\n",
      "iter 562: loss 3.8399, time 421.41ms, mfu 1.02%\n",
      "iter 563: loss 3.7156, time 427.69ms, mfu 1.02%\n",
      "iter 564: loss 3.7811, time 427.79ms, mfu 1.02%\n",
      "iter 565: loss 3.7453, time 423.77ms, mfu 1.02%\n",
      "iter 566: loss 3.7391, time 426.63ms, mfu 1.02%\n",
      "iter 567: loss 3.7765, time 419.04ms, mfu 1.02%\n",
      "iter 568: loss 3.7520, time 421.05ms, mfu 1.02%\n",
      "iter 569: loss 3.9399, time 415.69ms, mfu 1.02%\n",
      "iter 570: loss 3.8298, time 428.05ms, mfu 1.02%\n",
      "iter 571: loss 3.8962, time 425.55ms, mfu 1.02%\n",
      "iter 572: loss 3.8720, time 425.47ms, mfu 1.02%\n",
      "iter 573: loss 3.6573, time 422.87ms, mfu 1.02%\n",
      "iter 574: loss 3.9469, time 424.09ms, mfu 1.02%\n",
      "iter 575: loss 3.7624, time 419.65ms, mfu 1.02%\n",
      "iter 576: loss 3.7919, time 415.03ms, mfu 1.02%\n",
      "iter 577: loss 3.8792, time 419.54ms, mfu 1.02%\n",
      "iter 578: loss 3.7367, time 427.96ms, mfu 1.02%\n",
      "iter 579: loss 3.7308, time 427.83ms, mfu 1.02%\n",
      "iter 580: loss 3.8512, time 428.45ms, mfu 1.02%\n",
      "iter 581: loss 3.8235, time 425.96ms, mfu 1.02%\n",
      "iter 582: loss 3.8383, time 420.51ms, mfu 1.02%\n",
      "iter 583: loss 3.9020, time 425.25ms, mfu 1.02%\n",
      "iter 584: loss 4.0463, time 426.97ms, mfu 1.02%\n",
      "iter 585: loss 3.7075, time 426.30ms, mfu 1.02%\n",
      "iter 586: loss 3.8778, time 426.06ms, mfu 1.01%\n",
      "iter 587: loss 3.7174, time 427.54ms, mfu 1.01%\n",
      "iter 588: loss 3.5809, time 428.51ms, mfu 1.01%\n",
      "iter 589: loss 3.9985, time 424.74ms, mfu 1.01%\n",
      "iter 590: loss 3.8925, time 420.61ms, mfu 1.01%\n",
      "iter 591: loss 3.7756, time 426.13ms, mfu 1.01%\n",
      "iter 592: loss 3.8920, time 414.29ms, mfu 1.02%\n",
      "iter 593: loss 3.7843, time 426.11ms, mfu 1.02%\n",
      "iter 594: loss 3.8730, time 425.39ms, mfu 1.02%\n",
      "iter 595: loss 3.8283, time 417.17ms, mfu 1.02%\n",
      "iter 596: loss 3.7111, time 426.09ms, mfu 1.02%\n",
      "iter 597: loss 3.8075, time 425.44ms, mfu 1.02%\n",
      "iter 598: loss 3.6346, time 427.48ms, mfu 1.02%\n",
      "iter 599: loss 3.7239, time 422.76ms, mfu 1.02%\n",
      "step 600: train loss 3.5778, val loss 4.3774\n",
      "saving checkpoint to out-nikhil-gpt-final\n",
      "iter 600: loss 3.8041, time 3731.92ms, mfu 0.93%\n",
      "iter 601: loss 3.6178, time 427.70ms, mfu 0.93%\n",
      "iter 602: loss 3.6895, time 427.16ms, mfu 0.94%\n",
      "iter 603: loss 3.8279, time 401.72ms, mfu 0.95%\n",
      "iter 604: loss 3.7567, time 418.12ms, mfu 0.96%\n",
      "iter 605: loss 3.7333, time 424.52ms, mfu 0.97%\n",
      "iter 606: loss 3.7681, time 426.83ms, mfu 0.97%\n",
      "iter 607: loss 3.8292, time 423.75ms, mfu 0.98%\n",
      "iter 608: loss 3.9015, time 414.59ms, mfu 0.98%\n",
      "iter 609: loss 3.8115, time 420.29ms, mfu 0.99%\n",
      "iter 610: loss 3.8189, time 422.64ms, mfu 0.99%\n",
      "iter 611: loss 3.8421, time 422.12ms, mfu 0.99%\n",
      "iter 612: loss 3.9686, time 422.75ms, mfu 1.00%\n",
      "iter 613: loss 3.8053, time 419.03ms, mfu 1.00%\n",
      "iter 614: loss 3.7947, time 427.56ms, mfu 1.00%\n",
      "iter 615: loss 3.9065, time 416.04ms, mfu 1.00%\n",
      "iter 616: loss 3.7605, time 417.97ms, mfu 1.01%\n",
      "iter 617: loss 3.7841, time 423.32ms, mfu 1.01%\n",
      "iter 618: loss 3.7302, time 418.91ms, mfu 1.01%\n",
      "iter 619: loss 3.6585, time 425.77ms, mfu 1.01%\n",
      "iter 620: loss 3.9963, time 426.96ms, mfu 1.01%\n",
      "iter 621: loss 3.7431, time 428.85ms, mfu 1.01%\n",
      "iter 622: loss 3.6997, time 423.91ms, mfu 1.01%\n",
      "iter 623: loss 3.7096, time 426.87ms, mfu 1.01%\n",
      "iter 624: loss 3.6727, time 428.43ms, mfu 1.01%\n",
      "iter 625: loss 3.8211, time 418.33ms, mfu 1.01%\n",
      "iter 626: loss 3.9516, time 419.13ms, mfu 1.01%\n",
      "iter 627: loss 3.6978, time 420.83ms, mfu 1.01%\n",
      "iter 628: loss 3.8410, time 420.25ms, mfu 1.02%\n",
      "iter 629: loss 3.8302, time 429.45ms, mfu 1.01%\n",
      "iter 630: loss 3.8383, time 426.93ms, mfu 1.01%\n",
      "iter 631: loss 3.7669, time 427.64ms, mfu 1.01%\n",
      "iter 632: loss 3.7142, time 416.31ms, mfu 1.02%\n",
      "iter 633: loss 3.8100, time 429.08ms, mfu 1.01%\n",
      "iter 634: loss 3.5920, time 428.13ms, mfu 1.01%\n",
      "iter 635: loss 3.8264, time 424.55ms, mfu 1.01%\n",
      "iter 636: loss 3.7941, time 427.20ms, mfu 1.01%\n",
      "iter 637: loss 3.7706, time 417.31ms, mfu 1.01%\n",
      "iter 638: loss 3.6504, time 418.07ms, mfu 1.02%\n",
      "iter 639: loss 3.9948, time 421.60ms, mfu 1.02%\n",
      "iter 640: loss 3.9419, time 426.11ms, mfu 1.02%\n",
      "iter 641: loss 3.7682, time 424.88ms, mfu 1.02%\n",
      "iter 642: loss 3.8573, time 422.63ms, mfu 1.02%\n",
      "iter 643: loss 3.7992, time 423.69ms, mfu 1.02%\n",
      "iter 644: loss 3.9043, time 427.82ms, mfu 1.02%\n",
      "iter 645: loss 3.8703, time 419.78ms, mfu 1.02%\n",
      "iter 646: loss 3.6363, time 426.59ms, mfu 1.02%\n",
      "iter 647: loss 3.8140, time 426.92ms, mfu 1.02%\n",
      "iter 648: loss 3.7909, time 427.58ms, mfu 1.01%\n",
      "iter 649: loss 3.8015, time 428.19ms, mfu 1.01%\n",
      "iter 650: loss 3.7199, time 425.56ms, mfu 1.01%\n",
      "iter 651: loss 4.0021, time 427.00ms, mfu 1.01%\n",
      "iter 652: loss 3.7902, time 427.42ms, mfu 1.01%\n",
      "iter 653: loss 3.8646, time 428.03ms, mfu 1.01%\n",
      "iter 654: loss 3.9417, time 427.10ms, mfu 1.01%\n",
      "iter 655: loss 4.0320, time 419.63ms, mfu 1.01%\n",
      "iter 656: loss 3.7297, time 425.36ms, mfu 1.01%\n",
      "iter 657: loss 3.8077, time 426.46ms, mfu 1.01%\n",
      "iter 658: loss 3.8541, time 427.45ms, mfu 1.01%\n",
      "iter 659: loss 4.0079, time 413.64ms, mfu 1.02%\n",
      "iter 660: loss 3.7298, time 425.11ms, mfu 1.01%\n",
      "iter 661: loss 3.8342, time 428.03ms, mfu 1.01%\n",
      "iter 662: loss 3.8098, time 426.50ms, mfu 1.01%\n",
      "iter 663: loss 3.7942, time 426.67ms, mfu 1.01%\n",
      "iter 664: loss 3.7347, time 427.98ms, mfu 1.01%\n",
      "iter 665: loss 3.8719, time 427.15ms, mfu 1.01%\n",
      "iter 666: loss 3.8917, time 427.00ms, mfu 1.01%\n",
      "iter 667: loss 3.8323, time 425.50ms, mfu 1.01%\n",
      "iter 668: loss 3.7071, time 426.00ms, mfu 1.01%\n",
      "iter 669: loss 3.6791, time 426.92ms, mfu 1.01%\n",
      "iter 670: loss 3.3763, time 426.34ms, mfu 1.01%\n",
      "iter 671: loss 3.7822, time 428.03ms, mfu 1.01%\n",
      "iter 672: loss 3.5974, time 427.70ms, mfu 1.01%\n",
      "iter 673: loss 3.9451, time 424.73ms, mfu 1.01%\n",
      "iter 674: loss 3.8083, time 423.47ms, mfu 1.01%\n",
      "iter 675: loss 3.8672, time 417.60ms, mfu 1.01%\n",
      "iter 676: loss 3.9112, time 424.87ms, mfu 1.01%\n",
      "iter 677: loss 3.8663, time 425.70ms, mfu 1.01%\n",
      "iter 678: loss 3.8948, time 428.18ms, mfu 1.01%\n",
      "iter 679: loss 3.7295, time 411.94ms, mfu 1.02%\n",
      "iter 680: loss 3.6611, time 427.43ms, mfu 1.02%\n",
      "iter 681: loss 3.7512, time 426.96ms, mfu 1.01%\n",
      "iter 682: loss 3.8183, time 420.72ms, mfu 1.02%\n",
      "iter 683: loss 3.7353, time 425.89ms, mfu 1.02%\n",
      "iter 684: loss 3.8687, time 426.12ms, mfu 1.01%\n",
      "iter 685: loss 3.9314, time 428.31ms, mfu 1.01%\n",
      "iter 686: loss 3.8008, time 419.33ms, mfu 1.02%\n",
      "iter 687: loss 3.9391, time 425.90ms, mfu 1.01%\n",
      "iter 688: loss 4.0900, time 422.42ms, mfu 1.02%\n",
      "iter 689: loss 3.8774, time 424.26ms, mfu 1.02%\n",
      "iter 690: loss 3.8691, time 430.76ms, mfu 1.01%\n",
      "iter 691: loss 3.8033, time 426.12ms, mfu 1.01%\n",
      "iter 692: loss 3.6368, time 425.30ms, mfu 1.01%\n",
      "iter 693: loss 3.7513, time 422.86ms, mfu 1.01%\n",
      "iter 694: loss 3.5259, time 428.39ms, mfu 1.01%\n",
      "iter 695: loss 3.8362, time 428.38ms, mfu 1.01%\n",
      "iter 696: loss 3.7611, time 426.89ms, mfu 1.01%\n",
      "iter 697: loss 3.7098, time 427.60ms, mfu 1.01%\n",
      "iter 698: loss 3.9017, time 427.58ms, mfu 1.01%\n",
      "iter 699: loss 3.8714, time 427.07ms, mfu 1.01%\n",
      "step 700: train loss 3.5537, val loss 4.3515\n",
      "saving checkpoint to out-nikhil-gpt-final\n",
      "iter 700: loss 3.7073, time 3732.79ms, mfu 0.92%\n",
      "iter 701: loss 3.8547, time 426.46ms, mfu 0.93%\n",
      "iter 702: loss 3.7920, time 426.70ms, mfu 0.94%\n",
      "iter 703: loss 3.9327, time 393.69ms, mfu 0.95%\n",
      "iter 704: loss 3.9060, time 420.14ms, mfu 0.96%\n",
      "iter 705: loss 3.9791, time 426.53ms, mfu 0.97%\n",
      "iter 706: loss 3.8226, time 429.12ms, mfu 0.97%\n",
      "iter 707: loss 3.9108, time 425.48ms, mfu 0.97%\n",
      "iter 708: loss 3.6901, time 427.22ms, mfu 0.98%\n",
      "iter 709: loss 3.7492, time 426.42ms, mfu 0.98%\n",
      "iter 710: loss 4.0297, time 424.53ms, mfu 0.98%\n",
      "iter 711: loss 3.7081, time 420.21ms, mfu 0.99%\n",
      "iter 712: loss 3.7641, time 427.16ms, mfu 0.99%\n",
      "iter 713: loss 3.9223, time 428.98ms, mfu 0.99%\n",
      "iter 714: loss 3.9229, time 426.18ms, mfu 0.99%\n",
      "iter 715: loss 3.8462, time 422.77ms, mfu 1.00%\n",
      "iter 716: loss 3.7933, time 426.20ms, mfu 1.00%\n",
      "iter 717: loss 3.7832, time 423.90ms, mfu 1.00%\n",
      "iter 718: loss 4.0537, time 405.34ms, mfu 1.01%\n",
      "iter 719: loss 3.8987, time 427.32ms, mfu 1.01%\n",
      "iter 720: loss 3.6970, time 428.34ms, mfu 1.01%\n",
      "iter 721: loss 3.6723, time 427.97ms, mfu 1.01%\n",
      "iter 722: loss 3.7179, time 427.27ms, mfu 1.01%\n",
      "iter 723: loss 3.6405, time 421.79ms, mfu 1.01%\n",
      "iter 724: loss 3.7955, time 425.75ms, mfu 1.01%\n",
      "iter 725: loss 3.6768, time 425.05ms, mfu 1.01%\n",
      "iter 726: loss 3.8473, time 427.79ms, mfu 1.01%\n",
      "iter 727: loss 3.7690, time 426.52ms, mfu 1.01%\n",
      "iter 728: loss 3.8823, time 426.64ms, mfu 1.01%\n",
      "iter 729: loss 3.7350, time 427.38ms, mfu 1.01%\n",
      "iter 730: loss 3.6752, time 428.45ms, mfu 1.01%\n",
      "iter 731: loss 3.8485, time 427.78ms, mfu 1.01%\n",
      "iter 732: loss 3.7939, time 425.91ms, mfu 1.01%\n",
      "iter 733: loss 3.9017, time 426.76ms, mfu 1.01%\n",
      "iter 734: loss 3.8676, time 427.61ms, mfu 1.01%\n",
      "iter 735: loss 3.8916, time 427.47ms, mfu 1.01%\n",
      "iter 736: loss 3.6634, time 427.44ms, mfu 1.01%\n",
      "iter 737: loss 3.7467, time 428.43ms, mfu 1.01%\n",
      "iter 738: loss 3.6512, time 429.18ms, mfu 1.01%\n",
      "iter 739: loss 3.8068, time 427.70ms, mfu 1.01%\n",
      "iter 740: loss 3.8295, time 427.33ms, mfu 1.01%\n",
      "iter 741: loss 3.7766, time 428.69ms, mfu 1.01%\n",
      "iter 742: loss 3.8399, time 415.76ms, mfu 1.01%\n",
      "iter 743: loss 4.0150, time 426.93ms, mfu 1.01%\n",
      "iter 744: loss 3.8010, time 427.39ms, mfu 1.01%\n",
      "iter 745: loss 3.9114, time 425.35ms, mfu 1.01%\n",
      "iter 746: loss 3.7675, time 426.02ms, mfu 1.01%\n",
      "iter 747: loss 3.7417, time 424.67ms, mfu 1.01%\n",
      "iter 748: loss 3.9233, time 424.61ms, mfu 1.01%\n",
      "iter 749: loss 3.7569, time 423.67ms, mfu 1.01%\n",
      "iter 750: loss 3.5310, time 428.41ms, mfu 1.01%\n",
      "iter 751: loss 3.9158, time 429.05ms, mfu 1.01%\n",
      "iter 752: loss 3.6416, time 427.89ms, mfu 1.01%\n",
      "iter 753: loss 3.9853, time 429.51ms, mfu 1.01%\n",
      "iter 754: loss 3.9520, time 425.89ms, mfu 1.01%\n",
      "iter 755: loss 3.9940, time 426.94ms, mfu 1.01%\n",
      "iter 756: loss 3.7374, time 429.39ms, mfu 1.01%\n",
      "iter 757: loss 3.9233, time 418.89ms, mfu 1.01%\n",
      "iter 758: loss 3.8977, time 422.08ms, mfu 1.01%\n",
      "iter 759: loss 3.5831, time 426.70ms, mfu 1.01%\n",
      "iter 760: loss 3.8235, time 426.46ms, mfu 1.01%\n",
      "iter 761: loss 3.8248, time 426.51ms, mfu 1.01%\n",
      "iter 762: loss 3.6082, time 422.19ms, mfu 1.01%\n",
      "iter 763: loss 3.7769, time 426.97ms, mfu 1.01%\n",
      "iter 764: loss 3.7363, time 417.84ms, mfu 1.01%\n",
      "iter 765: loss 3.8444, time 419.32ms, mfu 1.02%\n",
      "iter 766: loss 3.7129, time 427.54ms, mfu 1.01%\n",
      "iter 767: loss 3.6554, time 425.52ms, mfu 1.01%\n",
      "iter 768: loss 3.8600, time 427.69ms, mfu 1.01%\n",
      "iter 769: loss 3.5710, time 426.84ms, mfu 1.01%\n",
      "iter 770: loss 3.5471, time 424.93ms, mfu 1.01%\n",
      "iter 771: loss 3.8906, time 426.58ms, mfu 1.01%\n",
      "iter 772: loss 3.7732, time 426.85ms, mfu 1.01%\n",
      "iter 773: loss 3.6932, time 427.56ms, mfu 1.01%\n",
      "iter 774: loss 3.5031, time 426.94ms, mfu 1.01%\n",
      "iter 775: loss 3.8041, time 428.38ms, mfu 1.01%\n",
      "iter 776: loss 3.8177, time 428.38ms, mfu 1.01%\n",
      "iter 777: loss 3.7847, time 428.06ms, mfu 1.01%\n",
      "iter 778: loss 3.8687, time 427.82ms, mfu 1.01%\n",
      "iter 779: loss 3.7787, time 427.32ms, mfu 1.01%\n",
      "iter 780: loss 3.7256, time 427.11ms, mfu 1.01%\n",
      "iter 781: loss 3.9424, time 427.48ms, mfu 1.01%\n",
      "iter 782: loss 3.8407, time 426.58ms, mfu 1.01%\n",
      "iter 783: loss 3.6044, time 426.33ms, mfu 1.01%\n",
      "iter 784: loss 3.6164, time 424.18ms, mfu 1.01%\n",
      "iter 785: loss 3.9461, time 428.03ms, mfu 1.01%\n",
      "iter 786: loss 3.8537, time 427.19ms, mfu 1.01%\n",
      "iter 787: loss 3.9987, time 427.45ms, mfu 1.01%\n",
      "iter 788: loss 3.6729, time 426.99ms, mfu 1.01%\n",
      "iter 789: loss 3.6024, time 417.37ms, mfu 1.01%\n",
      "iter 790: loss 3.8363, time 426.95ms, mfu 1.01%\n",
      "iter 791: loss 3.7432, time 424.61ms, mfu 1.01%\n",
      "iter 792: loss 3.8986, time 427.87ms, mfu 1.01%\n",
      "iter 793: loss 3.6328, time 426.99ms, mfu 1.01%\n",
      "iter 794: loss 3.6066, time 426.95ms, mfu 1.01%\n",
      "iter 795: loss 3.8416, time 427.52ms, mfu 1.01%\n",
      "iter 796: loss 3.6962, time 423.42ms, mfu 1.01%\n",
      "iter 797: loss 3.7202, time 428.01ms, mfu 1.01%\n",
      "iter 798: loss 3.8812, time 426.04ms, mfu 1.01%\n",
      "iter 799: loss 3.7872, time 426.34ms, mfu 1.01%\n",
      "step 800: train loss 3.5090, val loss 4.3923\n",
      "iter 800: loss 3.7938, time 2576.28ms, mfu 0.93%\n",
      "iter 801: loss 3.7913, time 428.07ms, mfu 0.93%\n",
      "iter 802: loss 3.7774, time 428.40ms, mfu 0.94%\n",
      "iter 803: loss 3.8728, time 429.79ms, mfu 0.95%\n",
      "iter 804: loss 3.7801, time 428.07ms, mfu 0.95%\n",
      "iter 805: loss 3.7977, time 425.38ms, mfu 0.96%\n",
      "iter 806: loss 3.9218, time 429.21ms, mfu 0.96%\n",
      "iter 807: loss 3.6667, time 423.07ms, mfu 0.97%\n",
      "iter 808: loss 3.6747, time 424.26ms, mfu 0.97%\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/s/bach/l/under/daives/cs445/assignments/nikhil-GPT/nanoGPT/train.py\", line 293, in <module>\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py config/train_config.py --init_from=resume --out_dir=out-nikhil-gpt-final_dirty --compile=False --eval_iters=200 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=1000 --lr_decay_iters=1000 --dropout=0.2 --learning_rate=1e-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finetuning for 400 more epochs, we end with train loss 3.5537, val loss 4.3515. only slightly better than before, but better nonetheless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-nikhil-gpt-final\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      ". So you can see that they have a square right so if I want that gradient is going to look at this second column that is basically just a lot more weights. And then you can see this values that's that are not the probability of the probability that it's not 100. So I have the answer, I'm going to get a general's a particular step down to take the sum of zero. So, I'm going to take the same equation call this. And so now I'm going to take this value of these columns. And so now I can take this update z. This should be some function, this can basically take that I'm going to take the weights w and then the entire hidden layer, the stride length of the output layer. And then I can have a patch size, then have my input, the input hidden unit. This is, that's going to have the output. So, we also want to be my output layer. So now now this is just going to be a list, I'm just going to have a linear function. So of this is going to take a function the input matrix function. So, now, if I assume I have a function that I go a nonlinear function of that I want to be a linear model so if I don't want to get to take some different output, I need to just use the first day and I've seen before. So this is going to take a random action that I'll take a state of the current state, all my action. So I take the time action. So I'm going to use this function. If I'm going to take this state. And here is going to have a I'm just going to take that sample X with respect to the Q value, and then my current X. So this is going to be the next action. So I take. I'll print,, epsilon, then I'm going to predict the current action over my state and I need to set of state, in my state, and my goal, I'm trying to have my next state, and I'm trying to share the goal. So, I'm never going and I'm going to do this. So if I'm looking at this action, I'm going to take pretty small number of samples, and I'm going to predict the actual number of samples. And I'm just going to do this into some weights. And I have the value for each column, and\n",
      "---------------\n",
      "\n",
      " in the following values, but they're just a class. So we can see like this classifier over a linear unit might look at this. And so this is a better way that's, and what's the data that they're what I can see that I can see the same classifier. So what we can do I calculate it, look at this one, and I can just take my network and just do start my model. So, I'm going to compute my model. So a linear model of the number of inputs, I'm going to print now take for each patch size and then I want to do my samples that back to find two classes and this is going to be the second one. We were going to predict the output. So I can try the current position of the epsilon value. And so here's going to be the prediction function. And so these are going to be basically, the one that's going to be the number of steps that's. Right. So now so if it's it's going to be a set from the output value and then the input and for, and then it doesn't kind of a bunch of columns that, but we're looking at an values. So, you know, let's have some other values that are say a one I'm trying to get to stack the input and then try to the input. So do you can see that to take some way to make sure this model so you can just just work in the bottom right, but we're going to be one and you get the bottom, but there's a dog or something like a things like not very close to the first. Look at the actual samples so we know that we go into the data set up with this data because we're going to be the best actions are that I'm going to fit to this data set and calculate the samples. So I want to pick up with a model from the training data percent of this is a moment. So, I can do this, I can create some data set the training data set of different hidden units it by different operations. So we can have some dummy data to predict all any data. And so they've had to standardize these. So I'll see that this data, the first class is the same class, I'm going to have a set of different logar values that we have to be some n output, and then, and then we have this case, in this case I'm\n",
      "---------------\n",
      "\n",
      " for the second weight. And then I have some value of the goal. And so if I'm going to be doing this back to update the shapes of the input, the error. And then I'll have a single value. And then I can see that the same state. And I'm going to take the linear function. I'll use the error value is going to update the T, I want to take the return the current state as my current state is going to be the best case that I can use this function to keep that and state. So I need to have my ground truth term that I'm going to move for the goal, and what's going to do is I can do is I reached this state. So I'm going to be this. So I'm going to have a action.6 and then I'm going to have my goal. And I don't want to find 100 positive of my action over my state. And then I will compute the action from my action. My action. So, I can use my action of my state space is I'll take I choose the state action one state and then I actually take my action that current state and my action. And then it's pretty close to a table. So now this is going to take a return more of my Q function times the weights the ground truth probability of the current state value value. So now the reinforcement learning learning, I'll end that, I'm going to be going to get the reinforcement being going to be the current state. And so in this case, you can see if I have the return, the update the current error to the value and my return. So I can take the return, take, have now know, the action of 0, Y is going to take the sum of p of the action. So now we can print the Q function for every step. So this is these this will be a square I'll print out that, then define a weight matrix w, and going to be the input, actually sigma sub n by the sum of x is just going to be T plus, and then print it by N by the product. One, we'll just solve for a weight vector. So this now I'll call this sum on this value here. Right. The next state is going to be the input. And so I can see this here's going to be the dimension. And so I'll print out in the first column to choose the sum of\n",
      "---------------\n",
      "\n",
      " to the convolutional neural network. But when we had before we have to each your environment. So we've got two classes? And then the patch. So let's collect, the outputs. And then we have an error between n by d. So now if I take this values, we're going to be able to be the output, where I will do, and multiply them by seven by two by one by 10 by D minus two by seven by seven by things, on the second layer values for this input and then D by n by two by seven units, and then I'm trying to predict it into a single output. That's going to do this bias. So then we'll take the sum to choose the goal and then take those values of the target values of that are going to be one for the Q value. And then at this will allow me to be the weights to be an arbitrary individual matrix from the derivative of a single vector or the sample. So now I want to be an matrix. So, let's see that we can do this. And then what's take this function. So in this will multiply by X times X, W. So now we're going to have this sample xilde. So now this is going to be all weights times w is going to take the second weights. And so we can use Q value multiplied by this function, Q function, and then I have some function that's going to run this function that and then the right, and then it's going to be the partial derivative of that convolutional net to be able to turn this into the probability that by the natural activation function. So now I need to have a non-t variable that gives me this function. And then I have a single value, it's going to be the sum for another patch, and then I just take for all n by z equals x and then multiply that by n by 1 minus 1 to mu a matrix, the square x times 1 minus the hidden layer. So, what I have this here's this is one this is going to be the sum times the image, but then we'll just create it sum distribution by D minus the number of samples. Right, this is the same sample and then if I have a sum over my four, so this is going to be one of the inputs is going to be sum of these row for the sum of that. So now this is I've already going to build the log\n",
      "---------------\n",
      "\n",
      " in the neural network as for some type of neural network. So, if we have the patch of hidden layers. So you don't really want that prediction, it's optimized for an neural network. So now, for this is like a very similar log likelihood. And then we want to get an output target, we have a bias and we can kind of. So let's look at this. And then I basically use this case, what I can do is I'm going to get to use a neural network to use some neural net. So you can see that we have a bunch of different inputs of those these things these filters are different things. So now, the first ways are going to go into a different parameters in the samples that should be some more more time. And then you can do it to be a different way to use that you can see a bunch of different values in the other units and map. So, you can use some sort of these different types of different weights, so you can just represent a whole lot of different element of different values in the target value. And then you want those values of X and then here, as the weight w, the weight matrix I can do is going to be the activation function, and then then then update it basically just going to do this function that you know what the output is we do is we'll see what it's going to be a probability that's going to be the sample is the probabilities of the overall probabilities, and the probability of the probability of class, the probability is our class one is, we have a member of class K. And so we'll do if I have some two dimensional vector that sample one by n by m by m. And then all n by x, then you can get all this. So this is now this will get the probability of x1, the jars. So now I will now, that we get equal to w at the sum for all w given v times w. And so then for all these times x, for all d1, I can give me w, x, here's k. And you know, of x plus one, right now what we're going to have times x sub x sub n times tilde times w and sigma. So now I'll use this just define the natural log function. So this is a function to update that is the value that's the same thing, now the output. So, in the log likelihood times W is\n",
      "---------------\n",
      "\n",
      " to this data, that is going to be the training data. This is going to be some hidden layer. Let me look at the code that's say, what I want to fill between the goal. I'm trying to figure out of these samples, and I end up on the world. So instead I want to measure the test, I can start at the same state. And then, I have a state 1. OK, so now I'll have this function.backed the goal is going to be going to be the next, I'm going to define the state epsilon error for the next state, then I'll plot all my random action. And then I'll just divide that by my state. So, what we're going to do is I actually just just going to be take my state I can do is I'm going to take my action 10 state I'm trying to turn my state over my state. And then I'm going to pass it through this over my action. So yeah, this is, let's read out pretty small actions. So now I'm going to take my current state. And so I can I'm trying to fit to the Q function weight to next state. And then I'm going to be the next state. And then I'm going to call this next my action is try to define the state to have this from the action over my state. And then then now I'm going to do the cross error. I'm going to be in this case again, I can define my state is I'm going to basically replace the best action as a matrix, I'll take the return. So I need to take the same state times the next return, I need to know, I calculate the next state and I measure that update the rest of the same thing, and my sample. So this is going to be a state and I'm going to take the value of X. And so it's going to be the number of classes. And now here's going to take the current state, and then I have to have to the output of these ones in the appropriate number of inputs, it to be the current state. So I can use the action and then use the same return that I have this to predict, and then I'm going to have some value of the size that is going to be the gradient. So I'm trying to solve for every linear regression, in this case, what I get. I'm going to\n",
      "---------------\n",
      "\n",
      " before I want to do that function, and then I'll show the forward pass a model. And so now if I do one the neural networks, I'm going to use my network. Okay, you have no change the same thing to do that my network architecture. So I can find that delta. So also use we have a single output from the input that output is going to have the weights, which is going to be my output layer so then T plus k by one by w. So that are the square sample. So now if I to the output, I have some sample. So I have the sum my predictions, I can choose the input distribution of that's the inputs them is going to be meaningful, and then my targets. So the target is the error for every sample of the output. So, if I have you look at the inputs, sorry, T, I can see the error value is going to be the sum of W, right, then the sum over the log of x weights,, and then I can be the two and then I'm going to take the output of the sum of the same way of all of F, the input, 1, the derivative of this is going to be going to be the activation function. So if I draw the next layer. So I have a square function that row. So now if I want this, I have this dimensionality of all this is going to be the output one, it will be pretty far as we're trying to a nonlinear function. So this is going to be difficult to be in terms of scalarithm of this case, it's not the gradient. And that we have to take this function. So, let's generate a single output. So, we see the expected value, I can see what I'm going to be using X and w and W plus the value. So that's going to be sub x sub n times the sample. And so where I'm going to be the number of inputs represents the output sample. So this is going to be that. So now I think about this. And so now I'll see this is my prediction in this function, because this is a value of the sum of that x one. So what I put this out which is going to be very close to the prediction. So now here's basically the discriminative model. So now I'll see if I have this is the sum every column of this for the number\n",
      "---------------\n",
      "\n",
      " into this convolutional layers. So how many nodes are the softmax is what we'll do we have a neural network. So if you have these filter that you can use this, there's a bunch of convolutional layer. So we have a look at what we're using the actual weights, and I'll have that I can do is I can actually be in the weights that into a linear function. So what I just was the same as a function that's going to be defined another output of the x. So, for k times N by zero times X times 1. And then two, I can see that basically multiply that by four by d. And then here is the sum for the weights. So this should be a exponent so now I'll have basically take all the gradient. So, if I'm going to take the individual values of the update all my prediction. So now you have n by K by d parameter. So now I apply what the same function to the only update that is, I'll do is I can plot the activation function, then every input my. So what I get is I can do is I can apply it to do that and then I should look at this. And then I'm trying to update that to update the sum for that by all my gradient, this is going to be the gradient descent. So what this is I'm going to do is I'll do I'm going to have my Q function to update the Q function so I'm going to update my state times the epsilon greedy function, what do is I'm looking at all my action is I want to do I'll use this going to update. And then I'll define this. So now, if I have the value of my objective is the next action'm just going to take a take my current reward. And then I have the state target times all T sub T, this gives me my state. So now what I'm going to do is I'm going to get a next state. Right. So I'm going to take, I'm going to take those five combinations of W in the end of random action, I'm going to get rid of the best action. So now I'll calculate the states and I'm going to replace the current time, I need to take the derivative of s and then when I'll plot the right, and then I have the target should, 1, 1 minus 0 and 1, and 1. So\n",
      "---------------\n",
      "\n",
      " for the gradient, but it's going to be, it's not necessarily necessarily going to be two. So what I need to do is I want to use that takes a look at this last time. So let's now I'll do is I'll do this if I have no small samples and then I put a certain number of samples and samples and then I'll go to the same as a three samples. Right. Right. So, if I need, so let's run this. So if I think that if I have three, this here, a position,000 samples, and then I'm going to look at this. So I'm looking at say, if I'll notice that I take a. I put the goal down the left side of the left side, and then the end up here. So I want to figure out the best one that I'm going to want to use the training data minus what I'll do is I want to get the return. And so now we can see our current state action pair of s and then I'll take is going to be the time step for the current prediction. This is this, because I can now do this in all the state samples from state. So now we think of these two models, I have 0,000 to do 2, A, 1. So that I can standard deviations, we'll define H. And then I'll call my update my next state, and then the end up and then I'm going to run my goal. I'm trying to optimize an sum for my test over the Q step. So I can have these features in sigma. So I can take my table, and then for the update my weights for k. So now QNET, this will take the softmax function. So now I need to minimize the actual number of weights and then I have to start a function. And I'll define that as we're here as this case, for these weights and then these samples that are going to be the weights in a single vector it should be the, right? So now I can plot the same thing that. So I'll see the for all the right, if I can my inputs are going to be the transpose of those patches. So I can change all sample, in order to take some column that to do this list of my means. So we'll see that that I have the appropriate number of units and then take the inverse of outputs. So now we\n",
      "---------------\n",
      "\n",
      " on the same thing that the class is basically the first thing that I'm going to pick out about this data to be like a sudden. So now this is actually like this is a couple of points in the data, the data. So let's just see that this class. So you'm going to try to see what we do is I can use it to use the same as the properties. We don't have to use the data to try these, and see that we're going to have our data, you can see these. So the same way you're going to be in this case, this case, you can see what I want to make with a force up to optimize the different filters. So, I can see that are just as a bunch of samples that are the value and we're interested in the actual predictions and W. And then multiply that's now we'll take the sum of the rows and then X minus the indicator variables, 1 minus d equals x times the this case, and then our output w sub k, and we can print y so v. So here is just a z is two by represented as our probability that's T plus 0. This is the probability of W. So if I define a probability of class two dimensional array. So now now we have linear function here we have weights, just add a matrix or indicator variables in a derivative of n by two samples over k. So I finally, we think about that's a probability for the probability that first sample. So I also going to be that's a simple for n, each individual samples. And then I'll print them into the other k by n by zero. That is going to be the image. So here's one that's the number, I'm going to divide the input. So this is, I want to be the sum for each of inputs that times the weights, but now I'm going to take the look at all my individual samples for the input to predict things, a single output. And then I'm trying to predict the next in the target values, and then calculate the joint probability of the actual values are the sample. And so if I need some of X are two values that we can see that we'll see that in the number of values is going to be a linear function. We have the number of samples in a different samples that we, because we go into data probably need to review this as the next state and I'll take the same time\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "! python3 sample.py --out_dir=out-nikhil-gpt-final_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-nikhil-gpt-final\n",
      "Overriding: start = my wife\n",
      "Overriding: num_samples = 5\n",
      "Overriding: max_new_tokens = 100\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "my wife is any moment, but I'm trying to be giving you like a couple of people are going to look at the second day or work. So, if I try a problem, you know what I have two features that are moving in the other things that is a 10 dimensions, and then I have the answer, I'm going to get to get a little bit more complicated features. And it's actually always a lot of a little bit more than it's going to be able to be very long\n",
      "---------------\n",
      "my wife we don't want to be able to deal with this. So if I have more what I'm not going to do is this function that's like this because I had this point or something that's the entire data, it's going to be able to be very good for all. So, if I have a little bit of what the in this case I'm going to see that I'm going to be doing Q old with the Q. So if I take the derivative of 0, Y,\n",
      "---------------\n",
      "my wife was not quite a chat, I'm just going to do free to be able to find this for your time, you know, you know, just for you can see, if you can see that the number of rows or something, and then that kind of imports or in NumPy, if you don't want to get to take like some reason, you're not gonna be getting better or more or even though it's a lot of training data and maybe it's a lot about the data or\n",
      "---------------\n",
      "my wife to be the assignment to make sure that you need to do the time. So if you're going to use this, you need to make sure that when you do this time here. Any questions, you have the problem. Let's look at like a very similar information about this, you know, kind of say, this is what's a couple of different features are the number. But we've seen, one, you can see these different patches that we're going to see that you know,\n",
      "---------------\n",
      "my wife to infinity as a coin is in a group. And so for example, if I have four people tend to be the first time, and I'm looking at in a little bit likely to do that I'll get a sudden it to be able to look at this data. If I am it seems pretty good. Okay, so now I'm going to say I'm doing a random action, I'm just going to do this in the probability that C is, and then I don't have a\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# get input from user\n",
    "prompt = input(\"Enter a prompt: \")\n",
    "\n",
    "! python sample.py --out_dir=out-nikhil-gpt-final_dirty --start=\"$prompt\" --num_samples=5 --max_new_tokens=100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaner Dataset\n",
    "Based on the output of the model, it seems like the dataset might have some grammatical errors. Let's try to clean the dataset and see if we can get better results. We did this simply by putting it through a google doc, and accepting all of the grammar suggestions. There's too much data to fix it manually, but this will hopefully improve things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 224,061 tokens\n",
      "val has 25,106 tokens\n"
     ]
    }
   ],
   "source": [
    "! python3 ./data/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_config.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "eval_interval = 100 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False \n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'nikhil-gpt'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'medium-clean'\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: out_dir = out-nikhil-gpt-final_clean\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 200\n",
      "Overriding: log_interval = 1\n",
      "Overriding: block_size = 64\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 2000\n",
      "Overriding: lr_decay_iters = 2000\n",
      "Overriding: dropout = 0.0\n",
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "using fused AdamW: False\n",
      "step 0: train loss 10.8531, val loss 10.8524\n",
      "iter 0: loss 10.8401, time 6697.73ms, mfu -100.00%\n",
      "iter 1: loss 10.8405, time 402.37ms, mfu -100.00%\n",
      "iter 2: loss 10.8432, time 403.96ms, mfu -100.00%\n",
      "iter 3: loss 10.8336, time 405.32ms, mfu -100.00%\n",
      "iter 4: loss 10.8050, time 400.18ms, mfu -100.00%\n",
      "iter 5: loss 10.7733, time 404.74ms, mfu 1.06%\n",
      "iter 6: loss 10.7291, time 405.64ms, mfu 1.06%\n",
      "iter 7: loss 10.6929, time 403.46ms, mfu 1.06%\n",
      "iter 8: loss 10.6739, time 405.59ms, mfu 1.06%\n",
      "iter 9: loss 10.6103, time 406.04ms, mfu 1.06%\n",
      "iter 10: loss 10.5773, time 405.77ms, mfu 1.06%\n",
      "iter 11: loss 10.5243, time 406.39ms, mfu 1.06%\n",
      "iter 12: loss 10.5173, time 403.84ms, mfu 1.06%\n",
      "iter 13: loss 10.4710, time 405.82ms, mfu 1.06%\n",
      "iter 14: loss 10.4656, time 406.04ms, mfu 1.06%\n",
      "iter 15: loss 10.3851, time 397.83ms, mfu 1.07%\n",
      "iter 16: loss 10.3934, time 394.36ms, mfu 1.07%\n",
      "iter 17: loss 10.3839, time 396.71ms, mfu 1.07%\n",
      "iter 18: loss 10.3385, time 403.41ms, mfu 1.07%\n",
      "iter 19: loss 10.2745, time 403.44ms, mfu 1.07%\n",
      "iter 20: loss 10.2313, time 404.53ms, mfu 1.07%\n",
      "iter 21: loss 10.2013, time 403.60ms, mfu 1.07%\n",
      "iter 22: loss 10.1402, time 401.64ms, mfu 1.07%\n",
      "iter 23: loss 10.0766, time 401.00ms, mfu 1.07%\n",
      "iter 24: loss 10.0088, time 404.47ms, mfu 1.07%\n",
      "iter 25: loss 9.9993, time 403.87ms, mfu 1.07%\n",
      "iter 26: loss 9.9070, time 401.41ms, mfu 1.07%\n",
      "iter 27: loss 9.8715, time 402.68ms, mfu 1.07%\n",
      "iter 28: loss 9.8094, time 398.87ms, mfu 1.07%\n",
      "iter 29: loss 9.7484, time 398.12ms, mfu 1.07%\n",
      "iter 30: loss 9.6513, time 404.97ms, mfu 1.07%\n",
      "iter 31: loss 9.6086, time 395.98ms, mfu 1.07%\n",
      "iter 32: loss 9.5601, time 404.79ms, mfu 1.07%\n",
      "iter 33: loss 9.4902, time 404.17ms, mfu 1.07%\n",
      "iter 34: loss 9.4141, time 404.88ms, mfu 1.07%\n",
      "iter 35: loss 9.2801, time 404.36ms, mfu 1.07%\n",
      "iter 36: loss 9.2252, time 405.66ms, mfu 1.07%\n",
      "iter 37: loss 9.1125, time 404.58ms, mfu 1.07%\n",
      "iter 38: loss 9.0422, time 406.69ms, mfu 1.07%\n",
      "iter 39: loss 8.9786, time 404.83ms, mfu 1.07%\n",
      "iter 40: loss 8.9278, time 404.29ms, mfu 1.07%\n",
      "iter 41: loss 8.8120, time 406.05ms, mfu 1.07%\n",
      "iter 42: loss 8.6974, time 397.27ms, mfu 1.07%\n",
      "iter 43: loss 8.6198, time 404.63ms, mfu 1.07%\n",
      "iter 44: loss 8.5073, time 404.94ms, mfu 1.07%\n",
      "iter 45: loss 8.4604, time 405.85ms, mfu 1.07%\n",
      "iter 46: loss 8.2924, time 400.97ms, mfu 1.07%\n",
      "iter 47: loss 8.2539, time 405.15ms, mfu 1.07%\n",
      "iter 48: loss 8.1037, time 401.23ms, mfu 1.07%\n",
      "iter 49: loss 8.0232, time 392.87ms, mfu 1.07%\n",
      "iter 50: loss 7.9998, time 404.67ms, mfu 1.07%\n",
      "iter 51: loss 7.9071, time 405.18ms, mfu 1.07%\n",
      "iter 52: loss 7.7435, time 404.77ms, mfu 1.07%\n",
      "iter 53: loss 7.7595, time 405.37ms, mfu 1.07%\n",
      "iter 54: loss 7.5383, time 406.81ms, mfu 1.07%\n",
      "iter 55: loss 7.5763, time 404.75ms, mfu 1.07%\n",
      "iter 56: loss 7.4196, time 403.78ms, mfu 1.07%\n",
      "iter 57: loss 7.3451, time 405.39ms, mfu 1.07%\n",
      "iter 58: loss 7.2373, time 402.52ms, mfu 1.07%\n",
      "iter 59: loss 7.1657, time 404.87ms, mfu 1.07%\n",
      "iter 60: loss 7.1490, time 405.78ms, mfu 1.07%\n",
      "iter 61: loss 7.0309, time 403.71ms, mfu 1.07%\n",
      "iter 62: loss 6.8923, time 404.30ms, mfu 1.07%\n",
      "iter 63: loss 6.9592, time 405.95ms, mfu 1.07%\n",
      "iter 64: loss 6.8350, time 404.30ms, mfu 1.07%\n",
      "iter 65: loss 6.7253, time 405.69ms, mfu 1.07%\n",
      "iter 66: loss 6.6263, time 405.63ms, mfu 1.06%\n",
      "iter 67: loss 6.6834, time 401.97ms, mfu 1.07%\n",
      "iter 68: loss 6.5543, time 407.17ms, mfu 1.06%\n",
      "iter 69: loss 6.4657, time 404.62ms, mfu 1.06%\n",
      "iter 70: loss 6.3939, time 407.08ms, mfu 1.06%\n",
      "iter 71: loss 6.3500, time 406.89ms, mfu 1.06%\n",
      "iter 72: loss 6.1114, time 406.30ms, mfu 1.06%\n",
      "iter 73: loss 6.1984, time 402.79ms, mfu 1.06%\n",
      "iter 74: loss 6.2937, time 408.94ms, mfu 1.06%\n",
      "iter 75: loss 6.1385, time 406.34ms, mfu 1.06%\n",
      "iter 76: loss 5.9618, time 404.40ms, mfu 1.06%\n",
      "iter 77: loss 6.0567, time 404.93ms, mfu 1.06%\n",
      "iter 78: loss 5.9524, time 407.03ms, mfu 1.06%\n",
      "iter 79: loss 5.9480, time 406.20ms, mfu 1.06%\n",
      "iter 80: loss 5.9459, time 405.22ms, mfu 1.06%\n",
      "iter 81: loss 5.6397, time 404.30ms, mfu 1.06%\n",
      "iter 82: loss 5.7196, time 405.43ms, mfu 1.06%\n",
      "iter 83: loss 5.8843, time 405.05ms, mfu 1.06%\n",
      "iter 84: loss 5.7683, time 405.49ms, mfu 1.06%\n",
      "iter 85: loss 5.6184, time 406.49ms, mfu 1.06%\n",
      "iter 86: loss 5.7175, time 404.86ms, mfu 1.06%\n",
      "iter 87: loss 5.8007, time 396.14ms, mfu 1.07%\n",
      "iter 88: loss 5.7140, time 403.61ms, mfu 1.07%\n",
      "iter 89: loss 5.7785, time 405.19ms, mfu 1.07%\n",
      "iter 90: loss 5.4781, time 403.99ms, mfu 1.07%\n",
      "iter 91: loss 5.5762, time 398.68ms, mfu 1.07%\n",
      "iter 92: loss 5.6093, time 397.24ms, mfu 1.07%\n",
      "iter 93: loss 5.6476, time 403.44ms, mfu 1.07%\n",
      "iter 94: loss 5.8031, time 401.32ms, mfu 1.07%\n",
      "iter 95: loss 5.5187, time 403.81ms, mfu 1.07%\n",
      "iter 96: loss 5.4627, time 403.10ms, mfu 1.07%\n",
      "iter 97: loss 5.4206, time 402.18ms, mfu 1.07%\n",
      "iter 98: loss 5.3690, time 403.74ms, mfu 1.07%\n",
      "iter 99: loss 5.2414, time 405.67ms, mfu 1.07%\n",
      "step 100: train loss 5.3563, val loss 5.4669\n",
      "saving checkpoint to out-nikhil-gpt-final_clean\n",
      "iter 100: loss 5.6188, time 3667.90ms, mfu 0.97%\n",
      "iter 101: loss 5.3472, time 406.90ms, mfu 0.98%\n",
      "iter 102: loss 5.1724, time 404.87ms, mfu 0.99%\n",
      "iter 103: loss 5.0451, time 377.29ms, mfu 1.01%\n",
      "iter 104: loss 5.3674, time 402.65ms, mfu 1.01%\n",
      "iter 105: loss 5.2865, time 405.03ms, mfu 1.02%\n",
      "iter 106: loss 5.0915, time 405.24ms, mfu 1.02%\n",
      "iter 107: loss 5.0314, time 405.81ms, mfu 1.03%\n",
      "iter 108: loss 5.1837, time 394.10ms, mfu 1.03%\n",
      "iter 109: loss 5.0752, time 402.19ms, mfu 1.04%\n",
      "iter 110: loss 5.1656, time 404.42ms, mfu 1.04%\n",
      "iter 111: loss 5.0011, time 405.56ms, mfu 1.04%\n",
      "iter 112: loss 4.9892, time 404.94ms, mfu 1.04%\n",
      "iter 113: loss 5.1602, time 406.71ms, mfu 1.05%\n",
      "iter 114: loss 5.0578, time 405.16ms, mfu 1.05%\n",
      "iter 115: loss 5.0197, time 406.40ms, mfu 1.05%\n",
      "iter 116: loss 4.9672, time 405.11ms, mfu 1.05%\n",
      "iter 117: loss 5.0797, time 402.87ms, mfu 1.05%\n",
      "iter 118: loss 5.0253, time 404.20ms, mfu 1.05%\n",
      "iter 119: loss 4.9318, time 404.55ms, mfu 1.05%\n",
      "iter 120: loss 4.8954, time 405.83ms, mfu 1.06%\n",
      "iter 121: loss 4.8257, time 404.72ms, mfu 1.06%\n",
      "iter 122: loss 4.8717, time 404.18ms, mfu 1.06%\n",
      "iter 123: loss 4.7494, time 405.16ms, mfu 1.06%\n",
      "iter 124: loss 4.8566, time 404.97ms, mfu 1.06%\n",
      "iter 125: loss 4.6796, time 405.04ms, mfu 1.06%\n",
      "iter 126: loss 4.8540, time 402.45ms, mfu 1.06%\n",
      "iter 127: loss 4.8071, time 401.50ms, mfu 1.06%\n",
      "iter 128: loss 4.6580, time 406.58ms, mfu 1.06%\n",
      "iter 129: loss 4.8535, time 404.35ms, mfu 1.06%\n",
      "iter 130: loss 5.0277, time 404.25ms, mfu 1.06%\n",
      "iter 131: loss 4.6754, time 405.19ms, mfu 1.06%\n",
      "iter 132: loss 4.5530, time 403.04ms, mfu 1.06%\n",
      "iter 133: loss 4.4753, time 394.17ms, mfu 1.07%\n",
      "iter 134: loss 4.5055, time 404.52ms, mfu 1.07%\n",
      "iter 135: loss 4.6836, time 404.20ms, mfu 1.07%\n",
      "iter 136: loss 4.4277, time 406.72ms, mfu 1.07%\n",
      "iter 137: loss 4.7589, time 405.37ms, mfu 1.06%\n",
      "iter 138: loss 4.7795, time 405.22ms, mfu 1.06%\n",
      "iter 139: loss 4.5887, time 405.61ms, mfu 1.06%\n",
      "iter 140: loss 4.7074, time 406.77ms, mfu 1.06%\n",
      "iter 141: loss 4.5793, time 405.72ms, mfu 1.06%\n",
      "iter 142: loss 4.4273, time 404.69ms, mfu 1.06%\n",
      "iter 143: loss 4.5984, time 404.68ms, mfu 1.06%\n",
      "iter 144: loss 4.5414, time 404.99ms, mfu 1.06%\n",
      "iter 145: loss 4.5724, time 407.07ms, mfu 1.06%\n",
      "iter 146: loss 4.5020, time 404.67ms, mfu 1.06%\n",
      "iter 147: loss 4.3650, time 404.36ms, mfu 1.06%\n",
      "iter 148: loss 4.5121, time 404.93ms, mfu 1.06%\n",
      "iter 149: loss 4.5264, time 405.40ms, mfu 1.06%\n",
      "iter 150: loss 4.6940, time 405.18ms, mfu 1.06%\n",
      "iter 151: loss 4.6460, time 402.80ms, mfu 1.06%\n",
      "iter 152: loss 4.5155, time 405.68ms, mfu 1.06%\n",
      "iter 153: loss 4.4137, time 403.93ms, mfu 1.06%\n",
      "iter 154: loss 4.4877, time 405.02ms, mfu 1.06%\n",
      "iter 155: loss 4.4263, time 404.64ms, mfu 1.06%\n",
      "iter 156: loss 4.4473, time 404.44ms, mfu 1.06%\n",
      "iter 157: loss 4.4109, time 405.71ms, mfu 1.06%\n",
      "iter 158: loss 4.6935, time 391.75ms, mfu 1.07%\n",
      "iter 159: loss 4.4891, time 403.53ms, mfu 1.07%\n",
      "iter 160: loss 4.2719, time 404.51ms, mfu 1.07%\n",
      "iter 161: loss 4.4103, time 404.52ms, mfu 1.07%\n",
      "iter 162: loss 4.2890, time 404.87ms, mfu 1.07%\n",
      "iter 163: loss 4.3041, time 405.91ms, mfu 1.07%\n",
      "iter 164: loss 4.2008, time 404.97ms, mfu 1.07%\n",
      "iter 165: loss 4.4191, time 404.61ms, mfu 1.07%\n",
      "iter 166: loss 4.5535, time 404.45ms, mfu 1.07%\n",
      "iter 167: loss 4.3172, time 403.86ms, mfu 1.07%\n",
      "iter 168: loss 4.2813, time 404.51ms, mfu 1.07%\n",
      "iter 169: loss 4.3815, time 406.21ms, mfu 1.07%\n",
      "iter 170: loss 4.3748, time 404.45ms, mfu 1.07%\n",
      "iter 171: loss 4.2840, time 400.40ms, mfu 1.07%\n",
      "iter 172: loss 4.3814, time 404.51ms, mfu 1.07%\n",
      "iter 173: loss 4.2449, time 404.70ms, mfu 1.07%\n",
      "iter 174: loss 4.1325, time 404.80ms, mfu 1.07%\n",
      "iter 175: loss 4.1666, time 404.24ms, mfu 1.07%\n",
      "iter 176: loss 4.1869, time 403.19ms, mfu 1.07%\n",
      "iter 177: loss 4.2740, time 400.72ms, mfu 1.07%\n",
      "iter 178: loss 4.1425, time 399.81ms, mfu 1.07%\n",
      "iter 179: loss 4.1367, time 405.52ms, mfu 1.07%\n",
      "iter 180: loss 4.3249, time 399.99ms, mfu 1.07%\n",
      "iter 181: loss 4.1268, time 398.21ms, mfu 1.07%\n",
      "iter 182: loss 4.2167, time 401.88ms, mfu 1.07%\n",
      "iter 183: loss 4.1723, time 397.83ms, mfu 1.07%\n",
      "iter 184: loss 4.1380, time 404.88ms, mfu 1.07%\n",
      "iter 185: loss 4.0819, time 403.76ms, mfu 1.07%\n",
      "iter 186: loss 4.3021, time 404.67ms, mfu 1.07%\n",
      "iter 187: loss 4.1227, time 406.50ms, mfu 1.07%\n",
      "iter 188: loss 4.2927, time 406.08ms, mfu 1.07%\n",
      "iter 189: loss 4.2279, time 406.58ms, mfu 1.07%\n",
      "iter 190: loss 4.0927, time 405.13ms, mfu 1.07%\n",
      "iter 191: loss 4.3021, time 405.08ms, mfu 1.07%\n",
      "iter 192: loss 4.0734, time 403.36ms, mfu 1.07%\n",
      "iter 193: loss 4.1239, time 405.71ms, mfu 1.07%\n",
      "iter 194: loss 4.1867, time 404.35ms, mfu 1.07%\n",
      "iter 195: loss 3.9392, time 405.79ms, mfu 1.07%\n",
      "iter 196: loss 4.0595, time 404.98ms, mfu 1.07%\n",
      "iter 197: loss 4.1856, time 404.23ms, mfu 1.07%\n",
      "iter 198: loss 3.9665, time 404.73ms, mfu 1.07%\n",
      "iter 199: loss 4.2100, time 403.55ms, mfu 1.07%\n",
      "step 200: train loss 4.0886, val loss 4.6056\n",
      "saving checkpoint to out-nikhil-gpt-final_clean\n",
      "iter 200: loss 4.1524, time 3643.95ms, mfu 0.97%\n",
      "iter 201: loss 4.2851, time 402.34ms, mfu 0.98%\n",
      "iter 202: loss 3.9735, time 406.61ms, mfu 0.99%\n",
      "iter 203: loss 4.2096, time 377.21ms, mfu 1.00%\n",
      "iter 204: loss 4.0308, time 394.69ms, mfu 1.01%\n",
      "iter 205: loss 4.0834, time 406.93ms, mfu 1.02%\n",
      "iter 206: loss 4.2143, time 405.54ms, mfu 1.02%\n",
      "iter 207: loss 4.0446, time 402.66ms, mfu 1.03%\n",
      "iter 208: loss 4.3481, time 406.61ms, mfu 1.03%\n",
      "iter 209: loss 4.0005, time 404.95ms, mfu 1.03%\n",
      "iter 210: loss 4.1784, time 404.22ms, mfu 1.04%\n",
      "iter 211: loss 3.9747, time 404.78ms, mfu 1.04%\n",
      "iter 212: loss 4.0297, time 404.73ms, mfu 1.04%\n",
      "iter 213: loss 3.8843, time 402.54ms, mfu 1.04%\n",
      "iter 214: loss 4.0056, time 405.00ms, mfu 1.05%\n",
      "iter 215: loss 4.0359, time 405.27ms, mfu 1.05%\n",
      "iter 216: loss 4.0630, time 400.67ms, mfu 1.05%\n",
      "iter 217: loss 4.0624, time 403.58ms, mfu 1.05%\n",
      "iter 218: loss 3.8878, time 403.21ms, mfu 1.05%\n",
      "iter 219: loss 3.9320, time 401.22ms, mfu 1.06%\n",
      "iter 220: loss 4.0481, time 404.33ms, mfu 1.06%\n",
      "iter 221: loss 3.9953, time 403.11ms, mfu 1.06%\n",
      "iter 222: loss 3.8375, time 403.37ms, mfu 1.06%\n",
      "iter 223: loss 3.9192, time 405.85ms, mfu 1.06%\n",
      "iter 224: loss 3.8731, time 398.06ms, mfu 1.06%\n",
      "iter 225: loss 3.8011, time 405.37ms, mfu 1.06%\n",
      "iter 226: loss 4.0980, time 393.82ms, mfu 1.06%\n",
      "iter 227: loss 3.9926, time 404.43ms, mfu 1.07%\n",
      "iter 228: loss 3.9576, time 404.80ms, mfu 1.06%\n",
      "iter 229: loss 3.9455, time 405.06ms, mfu 1.06%\n",
      "iter 230: loss 3.9674, time 405.53ms, mfu 1.06%\n",
      "iter 231: loss 3.8634, time 404.59ms, mfu 1.06%\n",
      "iter 232: loss 3.8527, time 405.45ms, mfu 1.06%\n",
      "iter 233: loss 3.7754, time 404.12ms, mfu 1.06%\n",
      "iter 234: loss 3.9976, time 402.35ms, mfu 1.07%\n",
      "iter 235: loss 3.8453, time 405.73ms, mfu 1.06%\n",
      "iter 236: loss 3.6788, time 406.15ms, mfu 1.06%\n",
      "iter 237: loss 3.7703, time 405.17ms, mfu 1.06%\n",
      "iter 238: loss 3.8684, time 404.59ms, mfu 1.06%\n",
      "iter 239: loss 3.8255, time 398.51ms, mfu 1.07%\n",
      "iter 240: loss 3.8362, time 404.66ms, mfu 1.07%\n",
      "iter 241: loss 3.8721, time 404.81ms, mfu 1.07%\n",
      "iter 242: loss 3.7371, time 405.04ms, mfu 1.07%\n",
      "iter 243: loss 3.8720, time 403.63ms, mfu 1.07%\n",
      "iter 244: loss 3.8120, time 404.18ms, mfu 1.07%\n",
      "iter 245: loss 3.7742, time 404.70ms, mfu 1.07%\n",
      "iter 246: loss 3.9522, time 402.53ms, mfu 1.07%\n",
      "iter 247: loss 3.6460, time 405.10ms, mfu 1.07%\n",
      "iter 248: loss 3.8756, time 406.02ms, mfu 1.07%\n",
      "iter 249: loss 3.7606, time 398.89ms, mfu 1.07%\n",
      "iter 250: loss 3.8442, time 404.14ms, mfu 1.07%\n",
      "iter 251: loss 3.8782, time 404.77ms, mfu 1.07%\n",
      "iter 252: loss 3.6674, time 404.18ms, mfu 1.07%\n",
      "iter 253: loss 3.6113, time 404.22ms, mfu 1.07%\n",
      "iter 254: loss 3.7393, time 404.46ms, mfu 1.07%\n",
      "iter 255: loss 3.9138, time 404.09ms, mfu 1.07%\n",
      "iter 256: loss 3.6898, time 404.42ms, mfu 1.07%\n",
      "iter 257: loss 3.6169, time 404.87ms, mfu 1.07%\n",
      "iter 258: loss 3.7656, time 404.73ms, mfu 1.07%\n",
      "iter 259: loss 3.6858, time 397.86ms, mfu 1.07%\n",
      "iter 260: loss 3.7835, time 402.44ms, mfu 1.07%\n",
      "iter 261: loss 3.7008, time 405.05ms, mfu 1.07%\n",
      "iter 262: loss 3.8247, time 404.92ms, mfu 1.07%\n",
      "iter 263: loss 3.7636, time 402.04ms, mfu 1.07%\n",
      "iter 264: loss 3.8897, time 403.49ms, mfu 1.07%\n",
      "iter 265: loss 3.7526, time 405.43ms, mfu 1.07%\n",
      "iter 266: loss 3.6926, time 404.68ms, mfu 1.07%\n",
      "iter 267: loss 3.7706, time 405.05ms, mfu 1.07%\n",
      "iter 268: loss 3.7317, time 405.76ms, mfu 1.07%\n",
      "iter 269: loss 3.8366, time 406.15ms, mfu 1.07%\n",
      "iter 270: loss 3.7688, time 403.14ms, mfu 1.07%\n",
      "iter 271: loss 3.6397, time 405.90ms, mfu 1.07%\n",
      "iter 272: loss 3.8462, time 405.21ms, mfu 1.07%\n",
      "iter 273: loss 3.5721, time 406.59ms, mfu 1.06%\n",
      "iter 274: loss 3.4755, time 401.55ms, mfu 1.07%\n",
      "iter 275: loss 3.7162, time 405.60ms, mfu 1.07%\n",
      "iter 276: loss 3.5352, time 402.38ms, mfu 1.07%\n",
      "iter 277: loss 3.5841, time 404.46ms, mfu 1.07%\n",
      "iter 278: loss 3.6848, time 403.11ms, mfu 1.07%\n",
      "iter 279: loss 3.6461, time 405.19ms, mfu 1.07%\n",
      "iter 280: loss 3.9178, time 404.86ms, mfu 1.07%\n",
      "iter 281: loss 3.6895, time 402.94ms, mfu 1.07%\n",
      "iter 282: loss 3.6338, time 403.64ms, mfu 1.07%\n",
      "iter 283: loss 3.5677, time 407.45ms, mfu 1.07%\n",
      "iter 284: loss 3.7574, time 405.00ms, mfu 1.06%\n",
      "iter 285: loss 3.6211, time 405.09ms, mfu 1.06%\n",
      "iter 286: loss 3.6640, time 403.34ms, mfu 1.07%\n",
      "iter 287: loss 3.7759, time 408.89ms, mfu 1.06%\n",
      "iter 288: loss 3.3812, time 398.14ms, mfu 1.07%\n",
      "iter 289: loss 3.8030, time 406.40ms, mfu 1.07%\n",
      "iter 290: loss 3.6862, time 402.95ms, mfu 1.07%\n",
      "iter 291: loss 3.7518, time 403.42ms, mfu 1.07%\n",
      "iter 292: loss 3.7007, time 405.22ms, mfu 1.07%\n",
      "iter 293: loss 3.8504, time 404.56ms, mfu 1.07%\n",
      "iter 294: loss 3.7369, time 403.35ms, mfu 1.07%\n",
      "iter 295: loss 3.5599, time 398.55ms, mfu 1.07%\n",
      "iter 296: loss 3.4309, time 402.77ms, mfu 1.07%\n",
      "iter 297: loss 3.5455, time 401.74ms, mfu 1.07%\n",
      "iter 298: loss 3.7807, time 391.28ms, mfu 1.07%\n",
      "iter 299: loss 3.5224, time 387.27ms, mfu 1.08%\n",
      "step 300: train loss 3.5905, val loss 4.5120\n",
      "saving checkpoint to out-nikhil-gpt-final_clean\n",
      "iter 300: loss 3.6137, time 3628.95ms, mfu 0.98%\n",
      "iter 301: loss 3.6372, time 406.31ms, mfu 0.99%\n",
      "iter 302: loss 3.5542, time 405.97ms, mfu 1.00%\n",
      "iter 303: loss 3.6860, time 375.35ms, mfu 1.01%\n",
      "iter 304: loss 3.8524, time 404.26ms, mfu 1.02%\n",
      "iter 305: loss 3.7057, time 398.55ms, mfu 1.02%\n",
      "iter 306: loss 3.6429, time 399.03ms, mfu 1.03%\n",
      "iter 307: loss 3.5257, time 401.37ms, mfu 1.03%\n",
      "iter 308: loss 3.5040, time 401.54ms, mfu 1.04%\n",
      "iter 309: loss 3.8378, time 402.92ms, mfu 1.04%\n",
      "iter 310: loss 3.5810, time 403.92ms, mfu 1.04%\n",
      "iter 311: loss 3.6122, time 405.05ms, mfu 1.04%\n",
      "iter 312: loss 3.4172, time 404.99ms, mfu 1.05%\n",
      "iter 313: loss 3.5407, time 404.95ms, mfu 1.05%\n",
      "iter 314: loss 3.4711, time 406.66ms, mfu 1.05%\n",
      "iter 315: loss 3.5168, time 401.88ms, mfu 1.05%\n",
      "iter 316: loss 3.6068, time 404.52ms, mfu 1.05%\n",
      "iter 317: loss 3.4686, time 403.83ms, mfu 1.05%\n",
      "iter 318: loss 3.6676, time 409.66ms, mfu 1.05%\n",
      "iter 319: loss 3.5398, time 390.77ms, mfu 1.06%\n",
      "iter 320: loss 3.4512, time 403.89ms, mfu 1.06%\n",
      "iter 321: loss 3.5801, time 405.86ms, mfu 1.06%\n",
      "iter 322: loss 3.5220, time 405.23ms, mfu 1.06%\n",
      "iter 323: loss 3.4871, time 406.05ms, mfu 1.06%\n",
      "iter 324: loss 3.5742, time 405.49ms, mfu 1.06%\n",
      "iter 325: loss 3.5128, time 405.24ms, mfu 1.06%\n",
      "iter 326: loss 3.4737, time 404.33ms, mfu 1.06%\n",
      "iter 327: loss 3.4825, time 404.61ms, mfu 1.06%\n",
      "iter 328: loss 3.4022, time 404.85ms, mfu 1.06%\n",
      "iter 329: loss 3.6131, time 396.54ms, mfu 1.06%\n",
      "iter 330: loss 3.5886, time 402.83ms, mfu 1.06%\n",
      "iter 331: loss 3.5431, time 404.59ms, mfu 1.06%\n",
      "iter 332: loss 3.6481, time 402.35ms, mfu 1.07%\n",
      "iter 333: loss 3.5567, time 405.89ms, mfu 1.06%\n",
      "iter 334: loss 3.4996, time 405.55ms, mfu 1.06%\n",
      "iter 335: loss 3.3966, time 399.89ms, mfu 1.07%\n",
      "iter 336: loss 3.3545, time 404.61ms, mfu 1.07%\n",
      "iter 337: loss 3.3013, time 405.26ms, mfu 1.07%\n",
      "iter 338: loss 3.3482, time 402.90ms, mfu 1.07%\n",
      "iter 339: loss 3.4026, time 402.36ms, mfu 1.07%\n",
      "iter 340: loss 3.3346, time 397.38ms, mfu 1.07%\n",
      "iter 341: loss 3.3670, time 404.08ms, mfu 1.07%\n",
      "iter 342: loss 3.5170, time 403.58ms, mfu 1.07%\n",
      "iter 343: loss 3.2917, time 400.28ms, mfu 1.07%\n",
      "iter 344: loss 3.4990, time 405.04ms, mfu 1.07%\n",
      "iter 345: loss 3.4214, time 407.02ms, mfu 1.07%\n",
      "iter 346: loss 3.2527, time 406.90ms, mfu 1.07%\n",
      "iter 347: loss 3.2356, time 404.33ms, mfu 1.07%\n",
      "iter 348: loss 3.4187, time 404.72ms, mfu 1.07%\n",
      "iter 349: loss 3.4954, time 405.06ms, mfu 1.07%\n",
      "iter 350: loss 3.5588, time 404.90ms, mfu 1.07%\n",
      "iter 351: loss 3.5453, time 404.26ms, mfu 1.07%\n",
      "iter 352: loss 3.4450, time 406.09ms, mfu 1.07%\n",
      "iter 353: loss 3.4694, time 403.19ms, mfu 1.07%\n",
      "iter 354: loss 3.3141, time 396.82ms, mfu 1.07%\n",
      "iter 355: loss 3.4263, time 403.37ms, mfu 1.07%\n",
      "iter 356: loss 3.4822, time 401.29ms, mfu 1.07%\n",
      "iter 357: loss 3.2684, time 402.31ms, mfu 1.07%\n",
      "iter 358: loss 3.3158, time 404.73ms, mfu 1.07%\n",
      "iter 359: loss 3.3163, time 402.88ms, mfu 1.07%\n",
      "iter 360: loss 3.4476, time 401.46ms, mfu 1.07%\n",
      "iter 361: loss 3.2240, time 405.21ms, mfu 1.07%\n",
      "iter 362: loss 3.4545, time 404.81ms, mfu 1.07%\n",
      "iter 363: loss 3.3331, time 404.69ms, mfu 1.07%\n",
      "iter 364: loss 3.3397, time 402.24ms, mfu 1.07%\n",
      "iter 365: loss 3.2497, time 396.68ms, mfu 1.07%\n",
      "iter 366: loss 3.3583, time 403.23ms, mfu 1.07%\n",
      "iter 367: loss 3.3049, time 404.81ms, mfu 1.07%\n",
      "iter 368: loss 3.5719, time 404.22ms, mfu 1.07%\n",
      "iter 369: loss 3.2199, time 404.84ms, mfu 1.07%\n",
      "iter 370: loss 3.3359, time 404.00ms, mfu 1.07%\n",
      "iter 371: loss 3.2147, time 404.46ms, mfu 1.07%\n",
      "iter 372: loss 3.3478, time 403.04ms, mfu 1.07%\n",
      "iter 373: loss 3.2819, time 404.70ms, mfu 1.07%\n",
      "iter 374: loss 3.3834, time 403.75ms, mfu 1.07%\n",
      "iter 375: loss 3.2291, time 404.77ms, mfu 1.07%\n",
      "iter 376: loss 3.1356, time 406.83ms, mfu 1.07%\n",
      "iter 377: loss 3.1606, time 405.71ms, mfu 1.07%\n",
      "iter 378: loss 3.3127, time 405.64ms, mfu 1.07%\n",
      "iter 379: loss 3.2585, time 403.07ms, mfu 1.07%\n",
      "iter 380: loss 3.4717, time 404.18ms, mfu 1.07%\n",
      "iter 381: loss 3.3859, time 405.15ms, mfu 1.07%\n",
      "iter 382: loss 3.1719, time 404.85ms, mfu 1.07%\n",
      "iter 383: loss 3.2828, time 404.18ms, mfu 1.07%\n",
      "iter 384: loss 3.2328, time 404.89ms, mfu 1.07%\n",
      "iter 385: loss 3.4118, time 404.78ms, mfu 1.07%\n",
      "iter 386: loss 3.4071, time 403.27ms, mfu 1.07%\n",
      "iter 387: loss 3.3407, time 404.63ms, mfu 1.07%\n",
      "iter 388: loss 3.2107, time 403.25ms, mfu 1.07%\n",
      "iter 389: loss 3.3935, time 407.29ms, mfu 1.06%\n",
      "iter 390: loss 3.4016, time 400.92ms, mfu 1.07%\n",
      "iter 391: loss 3.1263, time 405.15ms, mfu 1.07%\n",
      "iter 392: loss 3.1613, time 403.98ms, mfu 1.07%\n",
      "iter 393: loss 3.2021, time 404.76ms, mfu 1.07%\n",
      "iter 394: loss 3.3728, time 405.15ms, mfu 1.07%\n",
      "iter 395: loss 3.2172, time 397.46ms, mfu 1.07%\n",
      "iter 396: loss 3.1208, time 404.05ms, mfu 1.07%\n",
      "iter 397: loss 3.1794, time 406.11ms, mfu 1.07%\n",
      "iter 398: loss 3.3678, time 402.63ms, mfu 1.07%\n",
      "iter 399: loss 3.2134, time 403.68ms, mfu 1.07%\n",
      "step 400: train loss 3.2211, val loss 4.5729\n",
      "iter 400: loss 3.3401, time 2531.78ms, mfu 0.98%\n",
      "iter 401: loss 3.3129, time 403.45ms, mfu 0.99%\n",
      "iter 402: loss 3.2624, time 405.39ms, mfu 0.99%\n",
      "iter 403: loss 3.1938, time 405.48ms, mfu 1.00%\n",
      "iter 404: loss 3.2643, time 401.88ms, mfu 1.01%\n",
      "iter 405: loss 3.2947, time 405.00ms, mfu 1.01%\n",
      "iter 406: loss 3.4652, time 403.65ms, mfu 1.02%\n",
      "iter 407: loss 3.0962, time 405.17ms, mfu 1.02%\n",
      "iter 408: loss 3.3555, time 406.41ms, mfu 1.03%\n",
      "iter 409: loss 3.1835, time 400.33ms, mfu 1.03%\n",
      "iter 410: loss 3.2309, time 405.13ms, mfu 1.03%\n",
      "iter 411: loss 3.2731, time 405.08ms, mfu 1.04%\n",
      "iter 412: loss 3.1427, time 405.73ms, mfu 1.04%\n",
      "iter 413: loss 3.1829, time 404.29ms, mfu 1.04%\n",
      "iter 414: loss 3.0959, time 405.89ms, mfu 1.04%\n",
      "iter 415: loss 3.2321, time 406.38ms, mfu 1.05%\n",
      "iter 416: loss 3.1718, time 404.88ms, mfu 1.05%\n",
      "iter 417: loss 3.2555, time 406.89ms, mfu 1.05%\n",
      "iter 418: loss 3.2077, time 405.89ms, mfu 1.05%\n",
      "iter 419: loss 3.0878, time 405.74ms, mfu 1.05%\n",
      "iter 420: loss 3.0345, time 404.87ms, mfu 1.05%\n",
      "iter 421: loss 3.2329, time 400.65ms, mfu 1.05%\n",
      "iter 422: loss 3.2347, time 404.11ms, mfu 1.06%\n",
      "iter 423: loss 3.1188, time 404.63ms, mfu 1.06%\n",
      "iter 424: loss 3.2219, time 404.95ms, mfu 1.06%\n",
      "iter 425: loss 3.1264, time 405.19ms, mfu 1.06%\n",
      "iter 426: loss 3.1094, time 405.60ms, mfu 1.06%\n",
      "iter 427: loss 2.8486, time 406.26ms, mfu 1.06%\n",
      "iter 428: loss 3.1220, time 405.98ms, mfu 1.06%\n",
      "iter 429: loss 3.2571, time 405.45ms, mfu 1.06%\n",
      "iter 430: loss 2.9870, time 403.23ms, mfu 1.06%\n",
      "iter 431: loss 3.2682, time 402.50ms, mfu 1.06%\n",
      "iter 432: loss 3.3397, time 404.39ms, mfu 1.06%\n",
      "iter 433: loss 2.9918, time 405.75ms, mfu 1.06%\n",
      "iter 434: loss 3.0661, time 393.10ms, mfu 1.07%\n",
      "iter 435: loss 3.1340, time 404.50ms, mfu 1.07%\n",
      "iter 436: loss 3.2343, time 405.05ms, mfu 1.06%\n",
      "iter 437: loss 3.2438, time 406.06ms, mfu 1.06%\n",
      "iter 438: loss 3.0082, time 404.06ms, mfu 1.06%\n",
      "iter 439: loss 3.2701, time 406.75ms, mfu 1.06%\n",
      "iter 440: loss 3.2120, time 404.09ms, mfu 1.06%\n",
      "iter 441: loss 2.9391, time 405.49ms, mfu 1.06%\n",
      "iter 442: loss 3.1477, time 403.99ms, mfu 1.06%\n",
      "iter 443: loss 3.1699, time 403.29ms, mfu 1.06%\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/s/bach/l/under/daives/cs445/assignments/nikhil-GPT/nanoGPT/train.py\", line 293, in <module>\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py config/train_config.py --out_dir=out-nikhil-gpt-final_clean --compile=False --eval_iters=200 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_config.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "eval_interval = 100 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False \n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'nikhil-gpt'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'medium-clean'\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: init_from = resume\n",
      "Overriding: out_dir = out-nikhil-gpt-final_clean\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 200\n",
      "Overriding: log_interval = 1\n",
      "Overriding: block_size = 64\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 1000\n",
      "Overriding: lr_decay_iters = 1000\n",
      "Overriding: dropout = 0.2\n",
      "Overriding: learning_rate = 0.0001\n",
      "Resuming training from out-nikhil-gpt-final_clean\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "using fused AdamW: False\n",
      "step 300: train loss 3.5753, val loss 4.4757\n",
      "saving checkpoint to out-nikhil-gpt-final_clean\n",
      "iter 300: loss 4.3678, time 3952.36ms, mfu -100.00%\n",
      "iter 301: loss 4.2478, time 410.95ms, mfu -100.00%\n",
      "iter 302: loss 4.1935, time 414.00ms, mfu -100.00%\n",
      "iter 303: loss 4.3290, time 390.55ms, mfu -100.00%\n",
      "iter 304: loss 4.2093, time 418.18ms, mfu -100.00%\n",
      "iter 305: loss 4.2013, time 415.85ms, mfu 1.04%\n",
      "iter 306: loss 4.1349, time 416.91ms, mfu 1.04%\n",
      "iter 307: loss 4.2466, time 420.03ms, mfu 1.03%\n",
      "iter 308: loss 4.2070, time 414.74ms, mfu 1.04%\n",
      "iter 309: loss 4.1858, time 418.36ms, mfu 1.03%\n",
      "iter 310: loss 4.1316, time 418.65ms, mfu 1.03%\n",
      "iter 311: loss 3.9070, time 414.45ms, mfu 1.03%\n",
      "iter 312: loss 4.1251, time 420.41ms, mfu 1.03%\n",
      "iter 313: loss 4.0248, time 414.13ms, mfu 1.03%\n",
      "iter 314: loss 4.3375, time 412.33ms, mfu 1.04%\n",
      "iter 315: loss 3.9657, time 407.12ms, mfu 1.04%\n",
      "iter 316: loss 4.2644, time 413.14ms, mfu 1.04%\n",
      "iter 317: loss 4.2902, time 416.42ms, mfu 1.04%\n",
      "iter 318: loss 4.0388, time 419.48ms, mfu 1.04%\n",
      "iter 319: loss 4.0822, time 407.58ms, mfu 1.04%\n",
      "iter 320: loss 4.1834, time 407.68ms, mfu 1.04%\n",
      "iter 321: loss 4.1587, time 417.25ms, mfu 1.04%\n",
      "iter 322: loss 4.1330, time 417.60ms, mfu 1.04%\n",
      "iter 323: loss 3.7829, time 417.26ms, mfu 1.04%\n",
      "iter 324: loss 4.1478, time 418.97ms, mfu 1.04%\n",
      "iter 325: loss 4.0594, time 413.38ms, mfu 1.04%\n",
      "iter 326: loss 3.8980, time 416.92ms, mfu 1.04%\n",
      "iter 327: loss 4.1251, time 414.64ms, mfu 1.04%\n",
      "iter 328: loss 4.0915, time 417.95ms, mfu 1.04%\n",
      "iter 329: loss 4.0539, time 415.58ms, mfu 1.04%\n",
      "iter 330: loss 3.9455, time 417.27ms, mfu 1.04%\n",
      "iter 331: loss 4.1057, time 418.38ms, mfu 1.04%\n",
      "iter 332: loss 4.1105, time 415.56ms, mfu 1.04%\n",
      "iter 333: loss 4.0621, time 418.22ms, mfu 1.04%\n",
      "iter 334: loss 4.3113, time 418.26ms, mfu 1.03%\n",
      "iter 335: loss 3.9958, time 416.24ms, mfu 1.03%\n",
      "iter 336: loss 4.0772, time 414.50ms, mfu 1.04%\n",
      "iter 337: loss 3.9448, time 415.58ms, mfu 1.04%\n",
      "iter 338: loss 3.8434, time 417.26ms, mfu 1.04%\n",
      "iter 339: loss 3.9052, time 417.23ms, mfu 1.03%\n",
      "iter 340: loss 3.9420, time 417.27ms, mfu 1.03%\n",
      "iter 341: loss 4.0509, time 417.43ms, mfu 1.03%\n",
      "iter 342: loss 4.0119, time 419.38ms, mfu 1.03%\n",
      "iter 343: loss 4.0061, time 418.22ms, mfu 1.03%\n",
      "iter 344: loss 4.0268, time 418.31ms, mfu 1.03%\n",
      "iter 345: loss 3.9441, time 418.83ms, mfu 1.03%\n",
      "iter 346: loss 3.9534, time 418.47ms, mfu 1.03%\n",
      "iter 347: loss 4.0870, time 418.85ms, mfu 1.03%\n",
      "iter 348: loss 3.8594, time 419.84ms, mfu 1.03%\n",
      "iter 349: loss 3.8159, time 414.58ms, mfu 1.03%\n",
      "iter 350: loss 4.0971, time 418.14ms, mfu 1.03%\n",
      "iter 351: loss 4.1260, time 418.18ms, mfu 1.03%\n",
      "iter 352: loss 3.9527, time 417.58ms, mfu 1.03%\n",
      "iter 353: loss 4.2204, time 417.48ms, mfu 1.03%\n",
      "iter 354: loss 3.9146, time 418.31ms, mfu 1.03%\n",
      "iter 355: loss 4.0826, time 415.95ms, mfu 1.03%\n",
      "iter 356: loss 3.8475, time 417.73ms, mfu 1.03%\n",
      "iter 357: loss 3.9825, time 418.22ms, mfu 1.03%\n",
      "iter 358: loss 3.9832, time 418.38ms, mfu 1.03%\n",
      "iter 359: loss 4.0239, time 417.89ms, mfu 1.03%\n",
      "iter 360: loss 4.0232, time 415.27ms, mfu 1.03%\n",
      "iter 361: loss 4.0534, time 416.66ms, mfu 1.03%\n",
      "iter 362: loss 3.9283, time 412.12ms, mfu 1.03%\n",
      "iter 363: loss 3.9223, time 412.17ms, mfu 1.03%\n",
      "iter 364: loss 3.9635, time 415.95ms, mfu 1.03%\n",
      "iter 365: loss 3.9415, time 417.58ms, mfu 1.03%\n",
      "iter 366: loss 3.9169, time 418.86ms, mfu 1.03%\n",
      "iter 367: loss 4.0840, time 418.14ms, mfu 1.03%\n",
      "iter 368: loss 4.0047, time 417.62ms, mfu 1.03%\n",
      "iter 369: loss 4.1253, time 417.23ms, mfu 1.03%\n",
      "iter 370: loss 3.9784, time 417.76ms, mfu 1.03%\n",
      "iter 371: loss 3.9026, time 418.08ms, mfu 1.03%\n",
      "iter 372: loss 3.7153, time 417.05ms, mfu 1.03%\n",
      "iter 373: loss 3.9023, time 412.45ms, mfu 1.03%\n",
      "iter 374: loss 3.9542, time 417.72ms, mfu 1.03%\n",
      "iter 375: loss 3.9382, time 417.89ms, mfu 1.03%\n",
      "iter 376: loss 3.8192, time 417.98ms, mfu 1.03%\n",
      "iter 377: loss 3.8991, time 416.83ms, mfu 1.03%\n",
      "iter 378: loss 3.9194, time 415.82ms, mfu 1.03%\n",
      "iter 379: loss 3.8226, time 420.48ms, mfu 1.03%\n",
      "iter 380: loss 3.9509, time 419.07ms, mfu 1.03%\n",
      "iter 381: loss 3.5733, time 418.86ms, mfu 1.03%\n",
      "iter 382: loss 3.8017, time 417.45ms, mfu 1.03%\n",
      "iter 383: loss 3.9268, time 418.55ms, mfu 1.03%\n",
      "iter 384: loss 3.9268, time 417.56ms, mfu 1.03%\n",
      "iter 385: loss 3.7947, time 409.45ms, mfu 1.03%\n",
      "iter 386: loss 3.8868, time 406.59ms, mfu 1.04%\n",
      "iter 387: loss 4.0611, time 413.05ms, mfu 1.04%\n",
      "iter 388: loss 4.0264, time 416.03ms, mfu 1.04%\n",
      "iter 389: loss 4.0107, time 408.61ms, mfu 1.04%\n",
      "iter 390: loss 3.8779, time 419.14ms, mfu 1.04%\n",
      "iter 391: loss 3.9912, time 418.85ms, mfu 1.04%\n",
      "iter 392: loss 4.0367, time 418.01ms, mfu 1.04%\n",
      "iter 393: loss 3.9055, time 416.96ms, mfu 1.04%\n",
      "iter 394: loss 4.0623, time 418.67ms, mfu 1.03%\n",
      "iter 395: loss 3.9458, time 415.89ms, mfu 1.04%\n",
      "iter 396: loss 4.0467, time 414.72ms, mfu 1.04%\n",
      "iter 397: loss 4.0009, time 410.19ms, mfu 1.04%\n",
      "iter 398: loss 4.0192, time 416.29ms, mfu 1.04%\n",
      "iter 399: loss 3.7967, time 416.82ms, mfu 1.04%\n",
      "step 400: train loss 3.6428, val loss 4.4814\n",
      "iter 400: loss 4.0236, time 2571.87ms, mfu 0.95%\n",
      "iter 401: loss 3.9581, time 418.18ms, mfu 0.96%\n",
      "iter 402: loss 3.9645, time 418.04ms, mfu 0.96%\n",
      "iter 403: loss 3.6108, time 416.28ms, mfu 0.97%\n",
      "iter 404: loss 3.9931, time 416.83ms, mfu 0.98%\n",
      "iter 405: loss 3.9909, time 418.03ms, mfu 0.98%\n",
      "iter 406: loss 3.7635, time 418.00ms, mfu 0.99%\n",
      "iter 407: loss 3.7433, time 417.09ms, mfu 0.99%\n",
      "iter 408: loss 3.8575, time 419.49ms, mfu 1.00%\n",
      "iter 409: loss 4.0049, time 419.33ms, mfu 1.00%\n",
      "iter 410: loss 4.0030, time 417.10ms, mfu 1.00%\n",
      "iter 411: loss 3.8236, time 417.36ms, mfu 1.01%\n",
      "iter 412: loss 3.8455, time 417.56ms, mfu 1.01%\n",
      "iter 413: loss 4.0010, time 417.59ms, mfu 1.01%\n",
      "iter 414: loss 3.9338, time 418.01ms, mfu 1.01%\n",
      "iter 415: loss 4.0023, time 413.55ms, mfu 1.02%\n",
      "iter 416: loss 3.8645, time 407.32ms, mfu 1.02%\n",
      "iter 417: loss 3.8682, time 418.55ms, mfu 1.02%\n",
      "iter 418: loss 3.9109, time 418.08ms, mfu 1.02%\n",
      "iter 419: loss 3.9493, time 417.97ms, mfu 1.02%\n",
      "iter 420: loss 3.8579, time 414.03ms, mfu 1.02%\n",
      "iter 421: loss 3.8906, time 417.73ms, mfu 1.02%\n",
      "iter 422: loss 3.9354, time 418.71ms, mfu 1.03%\n",
      "iter 423: loss 3.8654, time 418.13ms, mfu 1.03%\n",
      "iter 424: loss 3.9223, time 418.50ms, mfu 1.03%\n",
      "iter 425: loss 3.7406, time 417.22ms, mfu 1.03%\n",
      "iter 426: loss 3.9828, time 417.81ms, mfu 1.03%\n",
      "iter 427: loss 3.9211, time 419.08ms, mfu 1.03%\n",
      "iter 428: loss 3.6732, time 417.22ms, mfu 1.03%\n",
      "iter 429: loss 3.8561, time 419.12ms, mfu 1.03%\n",
      "iter 430: loss 4.1223, time 419.29ms, mfu 1.03%\n",
      "iter 431: loss 3.8003, time 418.71ms, mfu 1.03%\n",
      "iter 432: loss 3.7702, time 417.97ms, mfu 1.03%\n",
      "iter 433: loss 3.7838, time 415.56ms, mfu 1.03%\n",
      "iter 434: loss 3.7740, time 414.40ms, mfu 1.03%\n",
      "iter 435: loss 3.9131, time 417.42ms, mfu 1.03%\n",
      "iter 436: loss 3.7368, time 416.31ms, mfu 1.03%\n",
      "iter 437: loss 4.0139, time 418.35ms, mfu 1.03%\n",
      "iter 438: loss 4.0658, time 418.65ms, mfu 1.03%\n",
      "iter 439: loss 3.8759, time 413.16ms, mfu 1.03%\n",
      "iter 440: loss 3.9521, time 413.49ms, mfu 1.03%\n",
      "iter 441: loss 3.9415, time 417.96ms, mfu 1.03%\n",
      "iter 442: loss 3.7632, time 417.90ms, mfu 1.03%\n",
      "iter 443: loss 3.9036, time 417.39ms, mfu 1.03%\n",
      "iter 444: loss 3.8512, time 417.75ms, mfu 1.03%\n",
      "iter 445: loss 3.9221, time 419.21ms, mfu 1.03%\n",
      "iter 446: loss 3.8880, time 419.10ms, mfu 1.03%\n",
      "iter 447: loss 3.8071, time 418.47ms, mfu 1.03%\n",
      "iter 448: loss 3.9387, time 416.35ms, mfu 1.03%\n",
      "iter 449: loss 3.8539, time 413.05ms, mfu 1.03%\n",
      "iter 450: loss 4.0157, time 418.56ms, mfu 1.03%\n",
      "iter 451: loss 4.1390, time 419.64ms, mfu 1.03%\n",
      "iter 452: loss 3.9324, time 418.25ms, mfu 1.03%\n",
      "iter 453: loss 3.7820, time 419.79ms, mfu 1.03%\n",
      "iter 454: loss 3.9637, time 419.72ms, mfu 1.03%\n",
      "iter 455: loss 3.8413, time 416.79ms, mfu 1.03%\n",
      "iter 456: loss 3.8340, time 414.61ms, mfu 1.03%\n",
      "iter 457: loss 3.9651, time 417.55ms, mfu 1.03%\n",
      "iter 458: loss 4.0912, time 415.99ms, mfu 1.03%\n",
      "iter 459: loss 4.0021, time 418.11ms, mfu 1.03%\n",
      "iter 460: loss 3.8822, time 418.01ms, mfu 1.03%\n",
      "iter 461: loss 3.9112, time 418.49ms, mfu 1.03%\n",
      "iter 462: loss 3.8293, time 414.80ms, mfu 1.03%\n",
      "iter 463: loss 3.8423, time 416.77ms, mfu 1.03%\n",
      "iter 464: loss 3.7512, time 417.41ms, mfu 1.03%\n",
      "iter 465: loss 4.0039, time 412.68ms, mfu 1.03%\n",
      "iter 466: loss 4.0345, time 417.55ms, mfu 1.03%\n",
      "iter 467: loss 3.9390, time 417.84ms, mfu 1.03%\n",
      "iter 468: loss 3.7964, time 418.56ms, mfu 1.03%\n",
      "iter 469: loss 4.0031, time 416.34ms, mfu 1.03%\n",
      "iter 470: loss 3.9210, time 418.13ms, mfu 1.03%\n",
      "iter 471: loss 3.9578, time 418.04ms, mfu 1.03%\n",
      "iter 472: loss 3.9443, time 416.70ms, mfu 1.03%\n",
      "iter 473: loss 3.8695, time 417.64ms, mfu 1.03%\n",
      "iter 474: loss 3.7633, time 417.62ms, mfu 1.03%\n",
      "iter 475: loss 3.8731, time 407.42ms, mfu 1.03%\n",
      "iter 476: loss 3.8722, time 417.21ms, mfu 1.03%\n",
      "iter 477: loss 3.8756, time 418.03ms, mfu 1.03%\n",
      "iter 478: loss 3.7556, time 416.73ms, mfu 1.03%\n",
      "iter 479: loss 3.7914, time 415.12ms, mfu 1.03%\n",
      "iter 480: loss 3.9147, time 417.52ms, mfu 1.03%\n",
      "iter 481: loss 3.7541, time 418.17ms, mfu 1.03%\n",
      "iter 482: loss 3.9883, time 417.46ms, mfu 1.03%\n",
      "iter 483: loss 3.8683, time 417.61ms, mfu 1.03%\n",
      "iter 484: loss 3.8522, time 418.17ms, mfu 1.03%\n",
      "iter 485: loss 3.7840, time 418.04ms, mfu 1.03%\n",
      "iter 486: loss 4.0213, time 416.95ms, mfu 1.03%\n",
      "iter 487: loss 3.8465, time 414.23ms, mfu 1.03%\n",
      "iter 488: loss 3.9999, time 409.13ms, mfu 1.04%\n",
      "iter 489: loss 3.9812, time 413.23ms, mfu 1.04%\n",
      "iter 490: loss 3.8094, time 418.23ms, mfu 1.04%\n",
      "iter 491: loss 4.0433, time 417.44ms, mfu 1.04%\n",
      "iter 492: loss 3.8236, time 417.53ms, mfu 1.03%\n",
      "iter 493: loss 3.9312, time 418.42ms, mfu 1.03%\n",
      "iter 494: loss 3.9809, time 415.92ms, mfu 1.03%\n",
      "iter 495: loss 3.7423, time 417.14ms, mfu 1.03%\n",
      "iter 496: loss 3.8338, time 415.51ms, mfu 1.03%\n",
      "iter 497: loss 3.9431, time 416.12ms, mfu 1.03%\n",
      "iter 498: loss 3.7468, time 418.84ms, mfu 1.03%\n",
      "iter 499: loss 3.8679, time 417.62ms, mfu 1.03%\n",
      "step 500: train loss 3.5951, val loss 4.4653\n",
      "saving checkpoint to out-nikhil-gpt-final_clean\n",
      "iter 500: loss 3.9388, time 3657.82ms, mfu 0.94%\n",
      "iter 501: loss 4.0696, time 421.35ms, mfu 0.95%\n",
      "iter 502: loss 3.8363, time 412.34ms, mfu 0.96%\n",
      "iter 503: loss 3.9147, time 389.14ms, mfu 0.97%\n",
      "iter 504: loss 3.8313, time 404.36ms, mfu 0.98%\n",
      "iter 505: loss 3.9011, time 405.95ms, mfu 0.99%\n",
      "iter 506: loss 4.0147, time 412.95ms, mfu 1.00%\n",
      "iter 507: loss 3.8348, time 417.80ms, mfu 1.00%\n",
      "iter 508: loss 4.1729, time 406.26ms, mfu 1.01%\n",
      "iter 509: loss 3.8078, time 418.79ms, mfu 1.01%\n",
      "iter 510: loss 3.9745, time 415.78ms, mfu 1.01%\n",
      "iter 511: loss 3.7918, time 414.81ms, mfu 1.01%\n",
      "iter 512: loss 3.8212, time 418.44ms, mfu 1.02%\n",
      "iter 513: loss 3.7739, time 417.48ms, mfu 1.02%\n",
      "iter 514: loss 3.9475, time 418.45ms, mfu 1.02%\n",
      "iter 515: loss 3.8631, time 418.29ms, mfu 1.02%\n",
      "iter 516: loss 3.9139, time 418.69ms, mfu 1.02%\n",
      "iter 517: loss 3.8937, time 418.03ms, mfu 1.02%\n",
      "iter 518: loss 3.7908, time 416.88ms, mfu 1.02%\n",
      "iter 519: loss 3.8345, time 418.06ms, mfu 1.02%\n",
      "iter 520: loss 3.9232, time 416.83ms, mfu 1.02%\n",
      "iter 521: loss 3.8698, time 418.10ms, mfu 1.02%\n",
      "iter 522: loss 3.7229, time 418.63ms, mfu 1.03%\n",
      "iter 523: loss 3.8210, time 417.99ms, mfu 1.03%\n",
      "iter 524: loss 3.8181, time 418.25ms, mfu 1.03%\n",
      "iter 525: loss 3.7199, time 417.91ms, mfu 1.03%\n",
      "iter 526: loss 3.9477, time 415.69ms, mfu 1.03%\n",
      "iter 527: loss 3.9232, time 417.80ms, mfu 1.03%\n",
      "iter 528: loss 3.8806, time 408.04ms, mfu 1.03%\n",
      "iter 529: loss 3.8198, time 417.83ms, mfu 1.03%\n",
      "iter 530: loss 3.8615, time 418.59ms, mfu 1.03%\n",
      "iter 531: loss 3.8161, time 418.42ms, mfu 1.03%\n",
      "iter 532: loss 3.7960, time 417.67ms, mfu 1.03%\n",
      "iter 533: loss 3.7062, time 419.75ms, mfu 1.03%\n",
      "iter 534: loss 3.9284, time 418.58ms, mfu 1.03%\n",
      "iter 535: loss 3.7969, time 416.63ms, mfu 1.03%\n",
      "iter 536: loss 3.6495, time 417.90ms, mfu 1.03%\n",
      "iter 537: loss 3.7484, time 411.97ms, mfu 1.03%\n",
      "iter 538: loss 3.8963, time 419.32ms, mfu 1.03%\n",
      "iter 539: loss 3.7966, time 419.04ms, mfu 1.03%\n",
      "iter 540: loss 3.8163, time 417.67ms, mfu 1.03%\n",
      "iter 541: loss 3.8210, time 418.19ms, mfu 1.03%\n",
      "iter 542: loss 3.7406, time 417.24ms, mfu 1.03%\n",
      "iter 543: loss 3.8581, time 417.62ms, mfu 1.03%\n",
      "iter 544: loss 3.9028, time 417.85ms, mfu 1.03%\n",
      "iter 545: loss 3.7774, time 415.64ms, mfu 1.03%\n",
      "iter 546: loss 3.9335, time 420.83ms, mfu 1.03%\n",
      "iter 547: loss 3.6901, time 408.56ms, mfu 1.03%\n",
      "iter 548: loss 3.9119, time 410.96ms, mfu 1.03%\n",
      "iter 549: loss 3.7457, time 417.97ms, mfu 1.03%\n",
      "iter 550: loss 3.8677, time 418.97ms, mfu 1.03%\n",
      "iter 551: loss 3.9149, time 416.69ms, mfu 1.03%\n",
      "iter 552: loss 3.7481, time 403.16ms, mfu 1.04%\n",
      "iter 553: loss 3.6749, time 413.35ms, mfu 1.04%\n",
      "iter 554: loss 3.7890, time 417.95ms, mfu 1.04%\n",
      "iter 555: loss 3.9285, time 416.79ms, mfu 1.04%\n",
      "iter 556: loss 3.8064, time 417.54ms, mfu 1.04%\n",
      "iter 557: loss 3.7330, time 413.72ms, mfu 1.04%\n",
      "iter 558: loss 3.8169, time 417.78ms, mfu 1.04%\n",
      "iter 559: loss 3.7890, time 411.54ms, mfu 1.04%\n",
      "iter 560: loss 3.8723, time 414.79ms, mfu 1.04%\n",
      "iter 561: loss 3.8003, time 415.29ms, mfu 1.04%\n",
      "iter 562: loss 3.9173, time 409.05ms, mfu 1.04%\n",
      "iter 563: loss 3.8422, time 416.82ms, mfu 1.04%\n",
      "iter 564: loss 4.0096, time 412.89ms, mfu 1.04%\n",
      "iter 565: loss 3.7867, time 417.93ms, mfu 1.04%\n",
      "iter 566: loss 3.8220, time 408.92ms, mfu 1.04%\n",
      "iter 567: loss 3.8942, time 414.04ms, mfu 1.04%\n",
      "iter 568: loss 3.8186, time 412.77ms, mfu 1.04%\n",
      "iter 569: loss 3.9656, time 412.59ms, mfu 1.04%\n",
      "iter 570: loss 3.8410, time 412.98ms, mfu 1.04%\n",
      "iter 571: loss 3.8163, time 420.67ms, mfu 1.04%\n",
      "iter 572: loss 3.9649, time 418.25ms, mfu 1.04%\n",
      "iter 573: loss 3.7416, time 418.51ms, mfu 1.04%\n",
      "iter 574: loss 3.5985, time 418.17ms, mfu 1.04%\n",
      "iter 575: loss 3.8088, time 416.57ms, mfu 1.04%\n",
      "iter 576: loss 3.7016, time 406.79ms, mfu 1.04%\n",
      "iter 577: loss 3.7242, time 417.48ms, mfu 1.04%\n",
      "iter 578: loss 3.8212, time 417.60ms, mfu 1.04%\n",
      "iter 579: loss 3.7348, time 419.22ms, mfu 1.04%\n",
      "iter 580: loss 4.0546, time 418.90ms, mfu 1.04%\n",
      "iter 581: loss 3.8124, time 420.27ms, mfu 1.03%\n",
      "iter 582: loss 3.7841, time 419.30ms, mfu 1.03%\n",
      "iter 583: loss 3.7472, time 420.15ms, mfu 1.03%\n",
      "iter 584: loss 4.0041, time 419.16ms, mfu 1.03%\n",
      "iter 585: loss 3.8568, time 417.78ms, mfu 1.03%\n",
      "iter 586: loss 3.8391, time 417.91ms, mfu 1.03%\n",
      "iter 587: loss 3.9744, time 419.69ms, mfu 1.03%\n",
      "iter 588: loss 3.6291, time 417.43ms, mfu 1.03%\n",
      "iter 589: loss 3.9944, time 418.76ms, mfu 1.03%\n",
      "iter 590: loss 3.9342, time 418.36ms, mfu 1.03%\n",
      "iter 591: loss 3.9674, time 418.01ms, mfu 1.03%\n",
      "iter 592: loss 3.9112, time 416.39ms, mfu 1.03%\n",
      "iter 593: loss 4.0367, time 416.20ms, mfu 1.03%\n",
      "iter 594: loss 3.9878, time 419.27ms, mfu 1.03%\n",
      "iter 595: loss 3.7580, time 418.49ms, mfu 1.03%\n",
      "iter 596: loss 3.7011, time 417.90ms, mfu 1.03%\n",
      "iter 597: loss 3.8095, time 418.06ms, mfu 1.03%\n",
      "iter 598: loss 3.9728, time 418.32ms, mfu 1.03%\n",
      "iter 599: loss 3.7503, time 418.15ms, mfu 1.03%\n",
      "step 600: train loss 3.5831, val loss 4.4665\n",
      "iter 600: loss 3.9141, time 2566.69ms, mfu 0.94%\n",
      "iter 601: loss 3.8984, time 418.93ms, mfu 0.95%\n",
      "iter 602: loss 3.7771, time 417.81ms, mfu 0.96%\n",
      "iter 603: loss 3.9190, time 417.13ms, mfu 0.97%\n",
      "iter 604: loss 4.1271, time 418.04ms, mfu 0.97%\n",
      "iter 605: loss 3.9462, time 416.70ms, mfu 0.98%\n",
      "iter 606: loss 3.8959, time 418.89ms, mfu 0.98%\n",
      "iter 607: loss 3.8331, time 409.46ms, mfu 0.99%\n",
      "iter 608: loss 3.8165, time 416.92ms, mfu 1.00%\n",
      "iter 609: loss 4.0808, time 416.78ms, mfu 1.00%\n",
      "iter 610: loss 3.8357, time 418.16ms, mfu 1.00%\n",
      "iter 611: loss 3.9224, time 416.33ms, mfu 1.01%\n",
      "iter 612: loss 3.7093, time 415.93ms, mfu 1.01%\n",
      "iter 613: loss 3.8586, time 418.30ms, mfu 1.01%\n",
      "iter 614: loss 3.7208, time 406.68ms, mfu 1.02%\n",
      "iter 615: loss 3.8294, time 414.64ms, mfu 1.02%\n",
      "iter 616: loss 3.9191, time 417.96ms, mfu 1.02%\n",
      "iter 617: loss 3.8021, time 418.45ms, mfu 1.02%\n",
      "iter 618: loss 3.9732, time 418.20ms, mfu 1.02%\n",
      "iter 619: loss 3.8528, time 403.72ms, mfu 1.03%\n",
      "iter 620: loss 3.7699, time 418.98ms, mfu 1.03%\n",
      "iter 621: loss 3.8316, time 420.04ms, mfu 1.03%\n",
      "iter 622: loss 3.8404, time 417.20ms, mfu 1.03%\n",
      "iter 623: loss 3.8251, time 420.31ms, mfu 1.03%\n",
      "iter 624: loss 3.9043, time 421.35ms, mfu 1.03%\n",
      "iter 625: loss 3.8431, time 417.47ms, mfu 1.03%\n",
      "iter 626: loss 3.8097, time 419.88ms, mfu 1.03%\n",
      "iter 627: loss 3.8131, time 419.17ms, mfu 1.03%\n",
      "iter 628: loss 3.7421, time 418.56ms, mfu 1.03%\n",
      "iter 629: loss 3.9178, time 419.32ms, mfu 1.03%\n",
      "iter 630: loss 3.9786, time 420.41ms, mfu 1.03%\n",
      "iter 631: loss 3.8596, time 418.98ms, mfu 1.03%\n",
      "iter 632: loss 3.9840, time 418.71ms, mfu 1.03%\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/s/bach/l/under/daives/cs445/assignments/nikhil-GPT/nanoGPT/train.py\", line 289, in <module>\n",
      "    logits, loss = model(X, Y)\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/s/bach/l/under/daives/cs445/assignments/nikhil-GPT/nanoGPT/model.py\", line 188, in forward\n",
      "    x = block(x)\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/s/bach/l/under/daives/cs445/assignments/nikhil-GPT/nanoGPT/model.py\", line 111, in forward\n",
      "    x = x + self.attn(self.ln_1(x))\n",
      "  File \"/usr/local/anaconda3/2022.08/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/s/bach/l/under/daives/cs445/assignments/nikhil-GPT/nanoGPT/model.py\", line 75, in forward\n",
      "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py config/train_config.py --init_from=resume --out_dir=out-nikhil-gpt-final_clean --compile=False --eval_iters=200 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=1000 --lr_decay_iters=1000 --dropout=0.2 --learning_rate=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-nikhil-gpt-final_clean\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "\n",
      " Any questions about the policy. And you have your path so if I want it's a little bit. You want to handle a little bit data and an time to do this, you know, this for this. They're not going to be doing this is not going to be going to have a lot of different properties of the samples, and the value to be able to predict different weights that are actually always a single vector. So let's create the first one thing. And so that is the sum all my input values of these are done with this. So if I take more what I'm going to be the output of each of the target weight for all k and xt times k. So, then the sum now I have the softmax function. And then I can have a e to the derivative of course, what the term is the n is,, that I'm going to do is I can draw the sum for the output of all y. So now now this is just going to be a list, I'm just going to have a linear function. So of this is going to take a function, I'll kind of sort of say, which is the second set that I don't go to a probability of them that kind of a bunch of different of feature, and then I want to have to take a nice square, and then I can use the first day and the next one. So I can do is I've got these filters that. So what of them are the first, what all I can define and I take the test data. So for some input values and I'm doing everything. So now, let's do this. So I'm going to put I'm just getting this, I'll take a standardizable. So if I'm going to be X. So finally, what I'm going to do is I'm going to reshape this,, and then I can say, I can try to do that to see what I'm taking my inputs and then I'm going to just trying to do for each weight, and then I'll just have an array of my derivatives and then I can put in a weight matrix by the value for every output that should have individual hidden layer and that weight, and then I will feed to look at the input size. So if I're going to take the output of these things that I can I'm just going to be initialized weights, I'll take the output convolutional layer, I can\n",
      "---------------\n",
      "\n",
      " Yeah, you can compare a grade. Same for you. So we can see like this classifier over a linear unit that's going to tell you you know, if you're going to be enough to have a three dimensional image has a car, but like 10% moment, but you're interested in the first sample, and then it, then I'm almost going to activate on the image or just a 10 H. And then it's going to be a sum to be able to get a linear regression problems. So, a shallow line is kind of making a larger model, if we've got an input to the same value of back to the two classes, this is going to be the second one. We were going to predict the output. So this is that it seems to be the output of the value. And so mostly pretty good to go off the forward pass. But these are the gradient descent, we're not going to be able to update the following thing that we can take my prediction of my number of weights. So we'll use that for derivatives to be all all all of these features for, and then have these one and basically have some functions and you can say we're looking at an values of these are being not the moment for the number of outputs. What is the output I'm trying to get to take the input and then the prediction as a bunch of weight. So now I'm going to plot the input sizes, and so now, we just have a single two weights and then take the weights. And then I'm going to take an output and then I can plot the training data which will not going to do is the bias of the data. So what we have is we go into the data, right, this is because we can do is we have a linear model that in this case that normal distribution. And so now we can use the value and then the training data to use how we can have to do that back to run function to figure out in the same function, but it's going to be the sigma, which is going to make sure that I'm going to be the same thing that I want to take this. And so now we had to do is just have a return the sum that and then we'll calculate the log of the likelihood, and then in the logistic regression, right, therefore, this value in the front at the output, so in, I can define this this case, in this looks like this\n",
      "---------------\n",
      "\n",
      " gallon. No, it's going to have to be a random of. And so if it's going to be a little bit more than the shapes of, you're going to be similar thing in the data, right. And so here, in multiple things that you've got different values, and then, you know, let's take all all the same thing. So, you can see you want to use the height of the same as you can do is have a set of the case that, and then I'll assign the gradient and then you can actually need to have my ground truth and then multiply by the target, for the indicator variables and what's the way that I think and in this is the number of samples and then for all my convolutional layers and their image. What's either the image. So I'm going to have to be defined by D plus one dimensional array of 100 by one, and then I'll run my return by seven by 10 images from each outputs. My line. So, I can use my tensor here is I'll just look at this value, I've got like a particular unit. So I want to do like some layer for some input. So now I take the output column that class. So, we're going to be doing this. So, what's the sigmoid function to be the output of this is the other outputs, and the output of sigmoid function, is going to be the input, the gradient being going to be the number of inputs, and then we want to be the first action function, the own values. So where we are we can just done. So if I do this in the training points, what we're going to have to be,, only random input to take the output of p of the bias. So now we can print the sample, which is the ground truth output is these this will be a square. So now what we'll then define a weight matrix of the weights times the nonlinear function, actually take the delta values, be and then our output of going to be the weights, and then we'll plot the error, and the state, and then we'll make a weight matrix. So this will just take my sum of the sum over the output is use the next state of the function so I can set of the sum of that into D plus one over the layer, and then a n. And now now the logistic regression is the sum of\n",
      "---------------\n",
      "\n",
      " I want to do a lot of time. And I will describe the notebook. And so I's going to be the probability of the value of the joint probability of where all of, the entropy. And then I have an error between the shape of the error with respect to the individual values. And so we can see that we'll see the actual sample and the class, and then the probability for the square value is going to be the probabilities of s. And then you know, we're going to have to use this class, and make a high value. So if we have a little bit of sigma, remember, that is the above and a sigmoid function. So X we just take the sum of X. So now here is sigma, the sum of all P of sigmoid function for the Qleigh. And then we can compare this to p between the sigmoid of the s of W. And so now we will do this. And then we can actually take the sum of the sigmoid function is going to be squared one. And then what's this is in this problem.module. So now is the logistic regression. So now we're going to have this function to use the derivative of this function. So for all the gradient of the value for some function, so we can't want to predict Q value of the raw function, Q function. So now I have some function for the activation function. So I've actually calculate the right, and now if I have a single squared error function f times the output of the sum of n for each formula for the probability of the probability over all, and that is the sum of the weight, we can use of this function. And then where these are now I can use the linear function and then I can define my current state, and my gradient of my prediction function, and then I will run that, and specify the gradient so this I can define a function, because what x function is going to have the actual value for all that's the QDA and my weight this is going to be the original times the image and then the standardize my neural network as I'll do this as the bias. So I can just have my inputs and then take the log of the target times my current state. How many things. So if I take a single output. So I take the row for the first one, 1, I'll see I've already just apply that as a\n",
      "---------------\n",
      "\n",
      " Now, if I'm doing some of these three, right, it's not going to be the amount of them. So if you find some reason here, it feels like this. But it's the range of the other things like a couple of things that are like I'm going to get an issue in some other. So, so we can kind of. So, if I take this for other things I basically have this case, if I define a bunch of movement. And if I say you're going to be some of zeros and then I can see we have a bunch of things like the same jar. So now now I take our different action plus one and then take the target values. So basically this mean of these two things that I'm going to take the number of inputs and then do this output. And then I have my output of other weights. And then I take of these filters. So I'll take x and then then the hidden layer of the input, I have my indicator variables of a seven by two. So remember i'll see here I'll take my prediction z sub k by k. So now here, I have this is the sum for all I can do the first of the sum of these, and then I can take a values, and then do this function that, I want to define a function of X minus K times the gradient. So if I'm going to be a normal distribution for the actual terms of the output. And so the step, the weights are effectively, in the output is our hidden layer,, and then the gradient be some error. And I have the individual weights, and I'll set my outputs, and then when I have two inputs, the input half times one, you can see that filter. And then the first one represents the first column of the scalar values in the error graph. And then I can use that for the actual layer. So this should be the next gradient. So now I will do then I'm going to have my patches, I'm going to have some weights w, I'm trying to specify the sum you that. So now I'm going to take this data and y and 1 times the same thing and the same thing we can be going to just do is going to be all of x and therefore I can have the number of classes. So now here, so, this is some of samples of weights, the output of classes, and then I would assume that a\n",
      "---------------\n",
      "\n",
      " I don't want to be short these two. Right, you're not doing, a lot like you. Let me do it's draw more complicated and you've been able to fill the edge. So I can do some meaning that it is not a bunch of types of people are the classes, you know, you can perform a lot of different parameters of different ways of. And then, if I had a lot of different thing that I have to have different edge. So, if you were, a bit of techniques like the data, you know, there are basically, you got a different by like points like like, I'll see all of the first column or w. And what I will look at example, then, what we're going to do is I actually just just set of these samples, these two samples, four. Right, so if I'm looking at a. I have 0. Now what I've got this is the same outputs with y equals one. So, I will take all shape of those two classes, and then I can use my tanh function, right, what I'm going to do is I can do is I will do for every Q function over one by W. And then I can define this for the y dot backwards. So now is going to be the next my action. So, this is going to have this, what I'll just use the output values so now I can do is I'll look at this. So if I'm going to create again, I can define this function the error to solve this into the sum of my inputs over all right means over my input is the weights I will get the input, the gradient is going to be your hidden layer is going to calculate the log likelihood of the T. So now this is going to be, this function, I'm going to get a sample of x and s. And I'm going to give me my model and it's a learning rate of sigmoid function and the sum for every step, and then I'll look at a linear function. So the output of these ones in the appropriate value of this class two. So this can collect the output. So now I will take this, the bias matrix that by the same to just make some matrix, and then to the output layer, which size that's also the output of that filter to give me the input to input. So we get this in X, this class, the only so now I\n",
      "---------------\n",
      "\n",
      " before I definitely still had a little bit, and it's not not well. I'm trying to select the answer for the link where I'm getting to get. So I will take a problem. So, I want to see what I should do is I'm going to be able to calculate the first class between 0. So, the difference from the input that I'm going to have the weights, which is going to be the output. So let's use that. So we can see for that I can be able to do this. So if I look at these two things that you're just going to give me a bunch of neural networks, you the parameters that's the inputs and then go through the bias and then have an arbitrary value and the output of the error for every output of the output. So, if you have you apply you know all those weights in the target values. This is a linear linear function that can be a one. So basically, you're going to have to have some weights, you know, and you need to do on this to update them. So, if those individual values, you just define my predictions. So, the input, this is the negative two. So now, where this is going to be doing is now if the same. And so for the set, you're going to be getting to be able to visualize this, I can do something like this as well, I can do is I want to do it as I'm going to be able to take this over the same forward pass for those dimensions and then use a way with the error for every hidden layer, and then it becomes not the gradient. And so if I have any of the same and so let's add it to this to the Q term that the value of the input to the output layer. So the QDA is the return this as the neural net. So that's what we will do here is we get here. So we're where we had it's going to be the gradient represents the output for an error, and it's all that. So we actually think about the probability if I'm going to have this. So we see that we saw this this error, I'm running the training accuracy and then they're going to put this into the training training validation and do this to train using the training validation here. So here I'll just return this for the output. So if I have this is the same validation and basically in the same thing\n",
      "---------------\n",
      "\n",
      " You don't know, especially you want to scale people who know, you know what you'll note in zoom. You could do is you guess this so you can go through the idea, you know, what we'll do is I can do is I don't to do is I will need to get into a lot of things that you can do that in this to control in fact, please see if you're doing for your image. So for each of a bunch of things like you can be learned that first time. So now you have no other things about zero, but don't have like, two samples. And in the assignments are not going to be getting different types of features. You could see that you have straight out right, you know, so the assignment one is going to be pretty important. So if you're applying operations to create some weight size, and all filters in the convolutional layers in the same action that gives me the previous layer. So the same thing we go. So I'm trying to make some activation function, if I have that, then I have my convolutional layers, and then I'm going to train and then'm going to have to give me seven inputs. So I'm going to have these two inputs to hold these, then and then apply my outputs, and I'll see the sum for the inputs to every hidden layer. So this function, I'll initialize all I have some matrix. And then I appreciate we just going to be able to actually doing a relatively useful for the input. And so I can compute the joint level of weights. So now I'm just using the output of all of the same state, and the right, which is the weights. So now if I have the actual values of the input term and I'm just going to take a single step. So what I can do is I'm going to take, we can use this gives me an weights. So now what I'm going to do is I'm going to get up the minimum that state that will do is I've got each weight in the error for those five by five by one, one. So the output of these are two by n sub n by K. So now this is the gradient of the same sample. So this is going to be the value of the sum of my indicator variables. So I'll also kind of the output of the hidden layer. And, so I can do a function. So, if I need\n",
      "---------------\n",
      "\n",
      " Yeah, if you can't want to be a lot of things that you're going to know two. So we're dealing with something that. So, we're looking at the code, let's see that you can do the negative time. So this is going to be the time we have already. But if I have a nine, if I have a column of it's one by one. And it. Right. Right. So, if I need, so here's true, that is the same thing? And so I have this is going to be the output. So now I'll define this array of that weights, it's a bunch of inputs and then I could compare them to two values to. So basically the same distribution as the center of these features of these value and the output layer. So here I want to figure out the sum of that, then I can want to use the training sample minus what I'll do is I want to get the columns for what I want to do. These are the number of samples, and then I'll take is going to be the end up for the train, right, and then I'm going to give me the validation and the train samples and then I will print the training test over those training data. So let's basically just do you can see that we're going to look at the testing data is just define the training data and then the network and then I'll see how much it's going to be something that is not like this data and then I'm going to be an error for my test accuracy. So let's run this. So I can run sigma. So I can take a table, basically create this is going to be black and then I'm going to be the tanh function that seems to be a. So now this is a single number of hidden layer, this is going to be used to be big one value. And then that here is the activation function for that is not going to be that input. And that's just a by the same thing. And so, let's just take a list of the same thing. So now I'll see, for all the right, if I can just take the end up and then I see it that I've been doing the predicted with the number of samples. So I should be a two of classes, if I just do this, to use that to use the next value and I'll put. So this, now I to the probability\n",
      "---------------\n",
      "\n",
      " on the same time, the class is not nine, but but they are pretty well, it's a little bit at like a sudden that, you know, you're getting on a couple of points. Okay. All right, so here's this is going to get a day, so now I'm going to have to generate my matrices. So it's going to take this and then I'll just use my validation values and then multiply by these and then I can train, or all my training samples, remember from all these. And then I'll try this to see here in this case, and then I'll just use the first solution to make some training data set to optimize the training training data data because I can see that I'm going to fit a training data set to the test accuracy is going to get the testing data. So, if I'm going to do that I'm trying to do in this again to be a list of data that's going to be an error this case, and then I will want to take a model that I know, so I'm going to take the same thing that this is represented as our function that's going to be this sample. So, for order to fit my weights are going to just on the data and then I want to find this linear function here. So if I just add a function or the activation function. So what I've got the value update x is I finally up with a probability of 2 minus T for T times x2 so 2. I'm going to be x times 1 minus 0, 1 and 1, 1. And so now I can just use the function. So now this for the gradient is going to be some function. Now I'll take my sum of X by x times W sub n1. So this is the probability of X dot shape and I'll create y y times x and then I can then z will do the sample is the log is I'll see the update of w times the probability of the sum of the X and the logar values. And so now I can take the sum as terms of this operation of the input by each sample now have w x, so v is going to be the output of w w is two where i can do is I have to have two columns of X and then I have the number of samples in each sample, and then, six, and then Z and then my input, give me the output, I'll take every element of\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "! python3 sample.py --out_dir=out-nikhil-gpt-final_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Dirty\" dataset\n",
      "Overriding: out_dir = out-nikhil-gpt-final_dirty\n",
      "Overriding: start = So basically, what you want to do is if you\n",
      "Overriding: num_samples = 5\n",
      "Overriding: max_new_tokens = 100\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "So basically, what you want to do is if you know any questions, but that's basically we're not so if I want to observe that you know, you look at the training data that data, the training data. If you have some data, you have the data that you observe that there's 100 points in 10 dimensions, and then that they're not going to be the samples, and then there's multiple things like sometimes I'll have a different splits that are different training or you want to take a look at a more complicated than 20,\n",
      "---------------\n",
      "So basically, what you want to do is if you have rid of these values in a bunch of points in terms of features. Right. So, in a continuous problem, so this case, we can see for all of this point or something that's the entire data, you have three inputs, so, 1. And then we can see that see how the inputs are my X are the outputs are the n, and then that are going to have the weights that, and then the input. And so effectively the fruit, it's like this two\n",
      "---------------\n",
      "So basically, what you want to do is if you want to modify's you know, you know, if you're going to be able to be able to try to use to take the network kind of sort of say, you know, I'm going to change the network to use the training data that kind of control the data. So I have my weight matrix multiplication by very interesting for every patch size, but you know, these are pretty fast and things are doing linear regression, this is going to be doing these filters that are going to be doing\n",
      "---------------\n",
      "So basically, what you want to do is if you end up to get to alling. And so you're worried about your code that you're going to use the gradient descent in a bigger network. But you know, you're not going to have a couple of things. So, if you look like, the information about classification, you know, kind of say, this is going to be on next time. So if I notice that's like,, one, you can say, I can see that like, say this, you know,\n",
      "---------------\n",
      "So basically, what you want to do is if you know, it's just looking in a group, you end up with a particular group, but it's actually an example, but you know, but it's in a whole lot of options and you can use that to go through a typical than that you can see what I will do is look at the code that I might have to perform a regression problems. But you know, we want to see that it's like this, but you need to show is very close to the QDA and really\n",
      "---------------\n",
      "\"Clean\" dataset\n",
      "Overriding: out_dir = out-nikhil-gpt-final_clean\n",
      "Overriding: start = So basically, what you want to do is if you\n",
      "Overriding: num_samples = 5\n",
      "Overriding: max_new_tokens = 100\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 7.23M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "So basically, what you want to do is if you know any of the remainder of the properties we're not so if I want to observe that you know, you want to handle a little bit of the entire time, because the neural networks equals neural networks for this. So what you're going to have 100 points in 10 dimensions, and then I'll use and then I'm going to print the hidden units to be the activation function. And then I actually trying to be the product of those two samples. And then I'll create a weight matrix,\n",
      "---------------\n",
      "So basically, what you want to do is if you have rid of these values here to say, there's a bunch of classes. And so now in the first moment, probably this case, they're going to be represented in the way of things. So, we're going to learn a jar, which's very good data at squares. So see how much the gradient is orthogonal to this is a lot of, that's like, I should be a member of probability of blue. So if I take the column is 0, Y squared\n",
      "---------------\n",
      "So basically, what you want to do is if you want to get's going to be a bunch of study that. But there's a bit of them has some reason, it's not very positive. So, if you have the probability of these 10% number of rows and something, and then that kind of inputs or in the same feature, and then we want to have to take some different values, and then which output is going to be more things that that we're going to be nice five and four. So now we'll take a single\n",
      "---------------\n",
      "So basically, what you want to do is if you end up to get to all the end up and you know the time and the experiments for some reason. So I'm doing everything in a bunch of things like, yes. So I'm going to put I'm just getting no different outcomes. So I'm getting something like this, you know, kind of say, finally, what I'm going to do is I'm going to start trying to have, I'm going to say, I can try and do that to see what you can do\n",
      "---------------\n",
      "So basically, what you want to do is if you know, like a coin up in a lecture, you don't make sure that you have to do is you know, you know, like, but you got a couple of things like the project. So, that you're going to be able to look at this data. If you're going to use to keep in this data that. So you're going to keep in each time. And I'm just trying to do this functions as we'll show is, the value for this is that's\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# get input from user\n",
    "prompt = input(\"Enter a prompt: \")\n",
    "\n",
    "print(\"\\\"Dirty\\\" dataset\")\n",
    "! python sample.py --out_dir=out-nikhil-gpt-final_dirty --start=\"$prompt\" --num_samples=5 --max_new_tokens=100\n",
    "\n",
    "print(\"\\\"Clean\\\" dataset\")\n",
    "! python sample.py --out_dir=out-nikhil-gpt-final_clean --start=\"$prompt\" --num_samples=5 --max_new_tokens=100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
