{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal for CS445"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daniel Ives & Kenny Hotra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to built *nikhil-GPT*, a language model that can talk about machine learning concepts. This will be achieved using OpenAI’s Whisper speech recognition to create textual data for our model. In constructing this model, we will attempt to answer whether it is possible to train a model on class content and have it effectively discuss the concepts discussed in said class. In pursuit of this question, we will have these additional questions.\n",
    "\n",
    " - How accurate is Whisper in generating transcripts from speech?\n",
    " - What model size of Whisper will produce acceptable results? With 20+ hours of audio data, it will be important to pick the smallest effective model so that the model can be trained in a reasonable amount of time.\n",
    " - Once we build the final model, how can we interface with it? Can we set up a ChatGPT-like experience or will we have to limit the user’s ability to interact with the model due to its constraints?\n",
    " - At a high-level, how do transformers (the type of network behind Whisper and GPT) work?\n",
    " \n",
    " One hypothesis we can make about the data is that filler words such as “um” and “uh” in addition to any background noise could result in our language model adopting these words into its responses. This would result in the model having a distinct tone from similar chatbots such as ChatGPT. The broader hypothesis is that the quality of the input data will determine the quality of the final model. If our transcripts are filled with inaccuracies and filler words, so will our model. Another hypothesis regarding the model is that if it is implemented properly, the model will be effective in answering questions and explaining questions regarding course content. However, due to the specificity of the data, the model would likely fail to appropriately answer anything outside of course content similar to how a classifier or regression model would perform poorly on data that is notably different from the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use Andrej Karpathy's [minGPT] (https://github.com/karpathy/minGPT) library to create the model. For training data, we’ll use transcripts from all the lectures in this class. The lecture recordings do not contain transcripts, but they do allow us to download them. Our plan is to use OpenAI's [Whisper](https://openai.com/research/whisper) to turn the audio into full transcripts. This data collection phase will be a large part of the project, but is a vital first step to getting a good model. \n",
    "\n",
    "Because all of the complex coding will already be done for us, our final report will also include some explanation on how both Whisper and minGPT work, as well as an analysis of the final result. For a quantitative measure for our model’s performance, we will use BLEU scores as a value to assess the quality of output text. We also will give the same prompts to ChatGPT by openAI and compare the results, although we expect ChatGPT to vastly outperform our model. The goal is to gain a better understanding of how transformers work, which is the basis for Whisper and minGPT.\n",
    "\n",
    "There are three major phases for this project:\n",
    "1. Obtaining the training data\n",
    "    * Download videos\n",
    "    * Run Whisper to convert audio to transcripts\n",
    "    * Manual clean up\n",
    "    * Explain how Whisper works in the report\n",
    "2. Building a training the model\n",
    "    * Use minGPT with our training data\n",
    "    * Explain how transformers/minGPT work in the report\n",
    "3. Analyzing and creating the report\n",
    "    * Come up with prompts to figure out the best way to interact with the model\n",
    "    * Quantitatively assess the model’s responses using BLEU scores\n",
    "    * Analysis of final result, comparison to ChatGPT, etc.\n",
    "\n",
    "While we’ll both certainly contribute to all parts of this project, we’re each going to “own” one part and do the majority of work on it. Daniel will be in charge of downloading the recordings and building the model using minGPT. Kenny will be in charge of running Whisper and assessing responses using quantitative values. In terms of the final report, we’ll make equal contributions, and the overall contribution will be 50/50 between the two of us.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Results\n",
    "\n",
    "We foresee a couple possible outcomes:\n",
    "\n",
    " * The model works well and can provide information about machine learning concepts.\n",
    " * The model is unintelligible and useless, either because the data isn't very good, there isn't enough of it, or the model is too complex for the data.\n",
    " * The model sometimes makes a little bit of sense, but is nothing like the huge LLMs that OpenAI has trained.\n",
    "\n",
    "To be honest, outcome 3 is probably the most likely outcome, but the ideal would be outcome 1. If it ends up being outcome 2, we will still have learned about transformers and how they work, so it won't be a total loss. Additionally, comparisons to ChatGPT can bring about insights as to what parameters or features result in a more useful chat model.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "In order to ensure that this project can be done within the given time limitations, we have constructed a timeline with tentative dates for when we anticipate completing certain tasks.\n",
    "\n",
    " * End of Week 1 (4/7): Obtain audio files from all lectures up to a certain date (At least through 4/4, “19 Reinforcement Learning for Two Player Games”).\n",
    " * Start of Week 2 (4/10): Use Whisper to generate transcripts, creating our training data.\n",
    " * End of Week 2 (4/14): Manually comb through and clean up training data to the best of our ability.\n",
    " * End of Week 3 (4/21): Use minGPT to generate our model. This may take a substantial amount of time depending on the size of the dataset and the model architecture(s) tested.\n",
    " * End of Week 4 (4/28): Analyze the performance of our model and create a report.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
