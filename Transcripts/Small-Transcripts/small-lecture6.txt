 Okay, let's get started. Sorry, I'm running late. Late bringing our faculty candidate back from lunch. All right, so today I'm going to finish up the introduction to neural networks notebook and then time permitting. I'll start the next one, which I think is Adam optimizer. If I remember correctly, so what I'll do, I'll just recap kind of gradient descent where we left off last time and then continue to the end with demonstration and the code. So assignment one is due a week from today for most of you. So I hope you all have made progress. I had some people come to office hours. So clearly some of you are working on it. So I just recommend that, you know, everybody take advantage of office hours if you need any help, because things are only going to get more complicated from here. So if there are any other any questions before I get started, let me share the screen to zoom. All right, no questions, but the material or general class procedures. All right, so let's get going then. Here it is. All right, so just recall how we train my gradient descent is pretty much the same neural networks as training in linear operations in that we are taking the error between some prediction y and some target t and then you square it and then you sum that squared error for all of our samples. So the only trick here is that we may have some n number of samples for each sample. We want to predict some k number of outputs, right? So now you're having a many to many correspondence. And so for every sample, for every output, you need to compute the equivalent error between your prediction of that output for that sample to the target value. So just to clarify a point, the targets, this is just a question that came up in office hours, the targets are the things that's known, right, known data that you're trying to optimize the model to predict. So target t, y is the output of a function. So if we assume that there is some function f of x parameterized by w, x is your input, that function outputs y, your task is then to solve for w. The trick with neural networks is that there may be a nonlinear function. And your goal is to optimize the weights for a nonlinear function. And we do that by basically performing linear operations and then taking some nonlinear function and taking the output linear operation as the input to the nonlinear function. The nonlinear function is an activation function. The one we're working with currently is tan h. There are others that I will go into within a few lectures. But for the moment, we can assume that the tan h function is the nonlinear function of choice. And it has some nice properties, such as being linear near zero and less linear as the inputs go to more extreme values. So the trick because the error is not a linear function of the parameters, we have to set the derivative equal to zero and solve for them. But you can do gradient descent in the same way. So the only thing is you just have to update the equivalent weights for every input to the said weight matrix, be that v or w, for everything that that weight matrix is intended to output. So as we saw, the update, the delta rule for v, the hidden layer weights, is pretty straightforward. So if we break down what happens here with the d edv, we end up with something that's fairly familiar. So for example, there's two outputs, if you look at one output here, the d edv is simply the partial derivative of the squared error, right, with respect to v. We then can use the chain rule to break down what everything in this term actually means in terms of other values in our in our equation. So just recall that z is at this point, the output of the hidden layer is a scalar value that's not intended to be inherently meaningful. But when the network is optimized, the output of this hidden layer should be useful information for the computation in the subsequent layer, which is in this case, for a two layer neural network, the output layer. The what is z is actually the is actually h of a, where a is the scalar value that comes out of the linear operation, we perform a nonlinear operation, in this case, tan h over it. So therefore, we're going to use the partial derivative of tan or the derivative of tan h to compete the partial derivative, which is going to be one minus tan h squared. So similarly, we have a constant, we have the error, we have the weights, we have the part of the derivative of the activation function, we have the input. When we add another output, the only real difference is that we now have to sum the squared errors for each output. So this is just a recap of what we did last time. So what we end up with is that the delta rule for the hidden layer, where there are two outputs of this hidden layer, we need to sum that error for each output. And in this case, if we were only trying to optimize the weights for a single output node, so the function remains the same. So we have this error term, z, which is the input to that layer, and then a constant. So now the delta rules involving the, or the delta rules involving the deltas look like this, we have the learning rate for the output layer, the error term, the input. This can, the error term can just also be written, you know, delta sub 1, 0, or 0. And so then for the hidden layer, the error term is going to be the sum of the errors for each of the individual outputs times those respective weights. Okay, so that was just a recap of what we talked about Tuesday. Any questions after that refresher? All right. Now the full version of back propagation is going to look something like this. So take, just look at a single weight here. Let's take v sub d1, which I think this is essentially d0 here. This one here looks like maybe a typo. So what we, if we're looking at just how we update this single weight given some set of outputs that depend on the output of this node, right, this should look familiar compared to the one above here. The only difference is we're now dealing with an arbitrary number of outputs. So what we do here is this error term is going to be output y1 minus the equivalent target, and then output y2 minus t2, and output y, yk minus tk, and this should be, you should be k's, they're not 2s. I didn't make this figure, I'll try and see if I can Photoshop it to be correct. So what we do then is these things will need to be summed, and then we will take, so if you look at the function here, the effect that we're doing here is now this generalizes to whatever number of outputs we actually have. So this is t1 minus y1 times w1 plus t2 minus y2 times w2, and so on and so on until we get to tk minus yk times wk, right. So we only have to use the derivative of the activation function once, because we're simply looking at how to update one way in one node. So here this h is going to be activation function that produces z, and so all of these then will need to be summed, multiplied by 1 minus the activation function, or the derivative of the activation function squared, times the input to this node. So now the only real thing that I need to do here that is different with multiple nodes is I'm going to be doing this at scale for every single one of these. So as you can imagine, this starts to grow somewhat out of control if I were to try to do it, you know, element-wise, using these update rules. So what we're going to do then is we're simply going to apply matrix multiplication and matrix operations to do this. So here we have the update rule for an individual weight v sub jm. I'm going to take the current value of that minus the learning rate times the error derivative with respect to that weight. Same thing for the output layer weights, and we saw how these terms here will break down into the full formula depending on whether I'm looking at a hidden layer weight whose update rule depends on what happens in the output layer, or I'm simply looking at the output layer weight, where the only real thing that I have to calculate is the error in that output term. So the reason that neural networks get more complicated is really entirely because of these hidden layers. So what this means then is that you can represent a purely linear function as a neural network that doesn't have any hidden layers. So if I have, if instead of this, if these yellow nodes weren't here, that are going straight from input to output, it would be the same as a linear function. I'm simply just trying to optimize the weights here, because the assumption is that there's no non-linearity, this is a regressor, there's no non-linearity being applied here. So there's no non-linear function whose derivative I need to take in order to update. So one thing you, what this means then is that one neat trick you can do is that you can actually create neural networks that you can use to just do like retrogression, and you can, if you set up the code correctly, you can use it the exact same way and basically just say I want to have no hidden layers in this, and it will perform the same as a linear model. Which means that a linear model is a good baseline, and also when we get to certain assignments, you can set up your neural network in that way to compare the neural network to effectively a linear model with no significant changes to the code, except for specifying some of those hyper parameters about the hidden layers. So we took these original update rules, we saw the operations that lead us to these expressions for doing gradient descent. So right here, here's the element-wise error term times the relevant weight. One thing here now is for simplicity's sake, before we ignored the addition of the bias column just to show how the math worked, but now we can add it back in order to make the matrix operations work correctly. So why are we doing this? If we're just looking at a single, how to update a single weight, it doesn't matter whether that weight is sort of a meaningful weight or a bias weight, the update is the same. If I want to make these into matrix operations, remember the math works out much more cleanly if I have this bias column so that I can assume that w0 times x0 is simply w0 times 1, because that first column x0 for every sample is 1, and so we can turn this into basically your standard end-dimensional linear function with some sort of y-intercept. So hence being the z tilde and the x tilde, if you remember from last time, we used the storted notation to signify some weight matrix that comes with the addition of a bias column. So if we look at these, let's work on driving this result using the output layer weights. So here this is kind of our standard formula for computing the total squared error for all samples for the output. So here what I will do then is I will simply take the derivative of both sides. This allows me to bring this negative 2 down in front. So now I'm simply summing over for all n for all k the error terms and then multiply that by the derivative of the output with respect to the weight. So what is y sub nk prime? Well this is equal to for all m, which remember that's the output of our hidden layer, that's the weights that are in our layer. So for all m I'm going to take weight m prime k prime times z nm prime. And so now this works out eventually, I'm kind of skipping through the math here, but eventually I take the derivative of both sides here. I can then take the derivative of the thing that we just saw here. So we're driving this. This is the how it can be rewritten. So I can put that into the partial derivative function there and then eventually this will kind of cancel out to this part here. So now we have basically a constant times 1 over n times 1 over k times the sum for all n, remember that's all samples, times the sorry the sum for all n of the target n sub k or sort of target t sub nk minus y sub nk. So that is the target value for that sample for that output minus the predicted value for that sample for that output times the relevant inputs to the output layer. So I apologize for alternating the input and output. Z is going to be that input to the output layer for sample n for output of the hidden layer m. So m here is just one of those columns in that hidden layer weight matrix. This is not something that is again a meaningful like scalar value that has any real interpretation. It's simply some sort of feature that will inform what t sub nk would be. Okay, so we can think of this as if I had a single layer, just some linear function where I have kind of random outputs or stochastic outputs or arbitrarily derived outputs. This part of the neural network, this hidden layer that output z will allow me to approximate those values. Okay, now the intuition there is that those values are somehow useful numerical features for predicting the true output that we're actually interested in. So questions about that intuition. Okay, so that was just for the output layer. So now the harder one of course is going to be optimizing those hidden layer weights because we are dependent upon how wrong those outputs of the hidden layer weights were when predicting the final output. So again, you can think of it as a function over x produces z and a function over z produces y. I'm trying to update the appropriate, I'm trying to get the appropriate values of z to map from x to y. But you'll notice that z does not directly involve y even though it does directly involve x. So I have to use some error in y to correctly update z for the appropriate weights for processing x. So therefore I need to know, you know, how many outputs, how wrong are all the outputs that derive from a particular term in z. So we'll begin with the same formula. So the error formula is the same. And the derivative is going to be the same as above. Now, knowing that y sub nk is equal to the formula given previously. So then I can substitute substitute this in for z. So remember z is going to be the output of the hidden layer over some function of x. Right. So if I take v, my hidden layer weights times x, my inputs, and then I apply some function h to it, this is going to be equivalent to z. So now we can rewrite this. And so now if I'm looking at basically what is, I'm trying to calculate what dz, dv is, I can take all this whole monster here and stick it in where I'm trying to calculate z. Okay. So now I'll just, because this is really gnarly and hard to read, I will just let h of vx equal v equal to a. Yes. The d, this one here, this is going to be the number of things I measure about x. So we'll talk about x as being an n by d matrix, meaning that every x is like a number, is some sample number n. And then there's some thing about that I'm measuring d and I have d things. Okay. All right. So let a equal h of big v times b, big x. And so then I can just, I'll use a in place of this, this normally equation there. So now I can rewrite this as below. So then after some simplification, I end up with the following equation. So negative two times one of our n times one of our k times the sum for all n times the sum for all k of again, the error term here, times the sum for all m, that is all outputs of the hidden layer of w sub m prime k prime times effectively dz, because we wrote z is this dz d a, where a is the input to z times x. Okay, so let's now work backwards. Remember what all these things refer to x is the inputs, a is going to be some weights times x, right. So that's occurs both in the numerator and the denominator of this derivative. The trick is that in the numerator, applying some activation function on the near function h to this, the tilde is there because that output has to have this column of ones appended to it, so that it can be an argument to another linear, another linear operation in the next layer. That next layer is parameterized by weights w, right. And so every w is going to take in n, like we can call them inputs, right. This is just inputs of the hidden layer, not the same as the raw inputs to the network. But there are m things that go into w, and for each of those m things, we're going to produce k things, okay. So m by k matrix. So that's this part. And then this part here is effectively what you should be familiar with now error term, average overall samples and all outputs. So to summarize, error function that you should be familiar with to just square an error. And then the derivative of that error with respect to each weights depend is going to differ depending on which layer that weight is in. Okay, so we can have arbitrarily large networks where all the hidden layers have update functions that are some form of this, this one here at the bottom, here I'm kind of mousing over, and then the output function is going to just be a version of this for this to update the output weights. All right. So that was back propagation, that is, how do we update those individual weights based on prediction error? But how do we get that prediction in the first place? This is going to be the forward pass. So when you're writing a neural network, basically have the forward function, which is I'm going to use my network in its current state. I'm going to, and if I'm in training, I'm going to assume that current state is not optimized, and that's going to give me some error that I can use to better optimize those weights using the backward pass. So the forward pass, this is effectively, right, remember what this is, this is going to be big matrix V times big matrix X. This is just broken down in terms of the individual elements. Then I apply h some activation function over this gives me z, right. So each individual element of z, z, for sample n, and then quote, hidden output m will be defined by this. And then finally, I'm going to take all of that at a column of out of one on front, if necessary. And then for each element in that updated matrix, I'm going to multiply that by the equivalent weight, and then take the sum over that for all m, that is all things that go into the up, the up with it. And the backwards pass is given by the equations that we derived previously. Okay. So that was all the mathematics, which took us probably in total an hour to get through between Tuesday and today. So you can go through this and this is, I believe, the most math heavy notebook in terms of like individual equations that I have written out. But if you are interested in how all this comes together, you can review this. But you don't have to actually worry about this so much for doing the assignments, which I think is going to be a relief to most people. So let's actually go through the process of turning the mathematical equations into functional Python code. So the first thing we're going to need to do is right, we've been looking at these individual scalar operations. I'm operating over some individual input sample for some individual measurement about that sample, right, that's going to be x sub and D. I also have, let's say, z sub and m, which is going to be some function over some individual x times some weight v, right, this is going to produce some value and m, which is going to be for every in every input sample of all n, I want to produce m measurable scalars about it, right, so these are just unit list numbers effectively. And then finally, I'm going to take that whole thing for each one of those apply some activation function to it, the tan h function. And then for each of those outputs, I'm going to pass it into another set of weights that's going to give me the k things I actually care about. Then I use the error to optimize both weights in the NW. Of course, what I said in in lecture three is basically don't use for loops, because that they're slow. And it just introduces a lot more opportunity for error. So we're going to take advantage of matrix multiplication. So the first thing we're going to do is convert these individual scalar operations into matrix matrix expression. So if you have the version of how we define z sub and m as this I basically want to try and get rid of this this the sum operation here by combining things into rows and vectors or rows and columns. So we have, you know, h for the sum arbitrary arbitrary column or arbitrary row times x for some arbitrary column. Okay, so now as long as these things match up, then I can start to rewrite my equation. So z sub and m will basically be I'm going to take I don't care about the row, I'm just going to take all of the m in whichever row it is times all of the n rows in whichever column it is. This allows me to actually perform a single matrix operation to get one term of the output matrix. I'm just going to use the commutative property swap these two things around here. And so now for every row and every column, I'm going to collapse this into one big matrix. So x tilde is going to be my input with the bias and then v tilde is just going to be my my hidden layer weights. As long as these things are the right shape, they're going to multiply because all I have to do is make sure that these two inner terms are the same, right? So the star here is going to be some variable. And as long as these two have the same number of items, I can perform the whole thing as a single matrix operation. So now I do that, if I just perform this nonlinear function, that's just going to operate element wise, right? So I'll take the tan h of now every element of this resulting matrix and that will give me z. And then the same thing basically applies there. So I just want to get rid of the sum sign. And so all I need to do is make sure that I can rewrite my operation so that I have some input that has the same number of columns as the other thing has the same number of rows. And so if I can manage that, then I can collapse both z and w into big matrices too. And so now I have z tilde times w and that's going to give me y. So this part should be pretty straightforward. I'm just looking to make sure that for every individual element, I have a matrix of the appropriate shape that can be multiplied to something else. The backwards pass for v looks, well, is more complicated. So we just have to do some more manipulations of the individual numbers in order to get the matrices to be in the right shape. So here's the individual operation. I'm trying to get rid of these sum signs. I can get rid of the sum over k by basically taking the column k to be some arbitrary number. So this will allow me to do that. I'm just going to rewrite those stars here. The only thing I need to do to make sure, sorry, what was that? It wasn't me. I've never heard that sound like Peter before, so I was a little worried. Oh, press here to power off the projector now. Press here to keep the projector on. I guess the projector must have been on for a long time. We will hope that it doesn't die before class is over. If it does, we have to spend another day on neural networks. Okay, so backwards pass. So what I'm trying to do is I'm trying to get rid of the sum signs. I can get rid of the sum over k by turning this into just some arbitrary column. The only thing I need to do there is I need to make sure to transpose the weight matrix here. So because I don't have any commutative property that I can just use to swap these two things to make sure that the dimensions align instead, because I'm going to take this to be a column matrix minus the column matrix times the column matrix. The result that this is going to be a column matrix. And so then I will, sorry, my bad, this is a row matrix minus a row matrix times a row matrix. So the result of this will be a row matrix. So in order to turn this into a column matrix, I'm just going to have to transpose it. In NumPy, you do not actually have to do this for individual rows, remember, because if I have a one-dimensional array, I can just multiply them together. And then using the MatMul operator, it will give me a single number. So NumPy will basically course the shape for you, but you're almost never going to have single row or column matrices in dealing with neural networks. So you can assume that this is always going to be the case. All right. So the next thing I want to get rid of now is the sum over n. So what do I do there? So this remains the same as before. And so now I need to do something with the n's to turn them into arbitrary rows. So if I rewrite these as stars, I just have to make sure that I'm doing this basically for all n, for each individual n in that row. Similarly, because I again don't really have a nice, what I want to do is I want to make sure that the inner dimensions of this thing aligns with this thing. This is not really easy. I have to compute all of this in order to figure out whether the dimensions are right. But I can already tell that I'm going to have, I've got n items that have m individual corresponding columns. And then I have multiplied that by something that also has n rows for j columns. So in order to get basically the two n size dimensions on the inner, all I'm going to do is just transpose this whole thing. Right. So the shapes of this line up such that I end up with n by m. And so if I transpose it, then I'm going to have m by m, which is going to multiply nicely. Okay. So I'm almost there, except I have this problem in that the right side has subscripts written in terms of mj or m and some other elements, whereas the left side is jm. So I have to make sure to line these up because I can't take v sub jm and then add something that is in terms of something sub mj. Because then I would want the dimensions are going to line up and I'll be adding or subtracting like the wrong element from the in the update matrix from the wrong element in the thing to be updated. Okay. So then what I will do here is so I have this is the update function according to what I had computed previously. So what I will do is first I'm going to transpose it. Okay. Now this allows me to swap the individual dimensions. And then what I can do is I can treat for v for every row j every column m, then I will be able to take the learning rate times one over n times one over k. So for all inputs and all outputs times the transpose input times the whole error function times the transpose weights, element wise multiplied by the derivative of the activation function. So the end result now is now I've got things all lined up. I've got I can treat these arbitrary members elements of the of the matrix just as individuals. So if I can handle them all the same, I can collapse everything into a big matrix. So given this I can take so v is going to be some new hidden layer weights. And for that I'll take the previous hidden layer weights plus the learning learning rate times one of random over k times the transpose input times the error times the output times one over one minus z in this case the derivative of the of tan h. So this w with the carrot is going to be w without the constant input row. So this is just going to be sort of raw w or w tilde was w with the with the bias. And so now the backwards pass for w works out rather similarly. So the first thing I'm going to get rid of is the sum. This is pretty easy because I only have one sum to get rid of so we can basically do it the same way. Again, all I need to do is transpose the resulting array that results from taking the error term for every row and then multiply that by z by z tilde. Again, my subscripts are a little bit inverted. So the right side has mk. So the left side is mk, but the right side has k and m. So then what I will do is I'll take these two terms and just switch them using the commutative property. And so now this allows me to turn these into arbitrary rows or arbitrary columns. And therefore I can collapse now everything in this into a big matrix. So w is going to be o w plus the learning rate times whenever n times whenever k times the transpose input to the hidden layer. So this is now playing the same rule as x tilde did in the above. So these two are kind of playing equivalent rules here. And then this is just the raw error. My t, all my t's minus all my y. And so now the shapes of these should work out correctly. And so now all together in math, what is z? z is some function h applied over bias x times v. And then I apply, I append a free append a bias to my z, multiply that by w, I get y. And then the update for v is going to have to incorporate a couple of other things, right, not only the inputs and the error, but also the weights, things that are to be updated in the subsequent layer. And then the derivative of the activation function, the nonlinear function, is being applied. And then w is kind of the same, just simplified. Basically the only thing different, this is, this can be considered, the w update can be considered a special case of the v update, where the activation function is just a linear past, x equals x. What's the derivative of y equals x? It's one. So this is just the same as multiplying by the derivative of y equals x, which is one. And there are no weights. There are no subsequent weights to be updated. So I don't need to add, I don't need to multiply by anything here. Okay. So that was the mathematics. Now how do we do this in Python? So your code is going to look something like this, right? This is not necessarily going to be exactly what you're going to do in assignment two, but this is generally the shape of the code. So first of all, I'm going to take an input x and a target t. What I want to do is I want to do this forward pass, right? That's these two things, or the first two things up here is a z and y. So what I'm going to do for that is I will just add ones to x. So this is this add ones function, can be assumed to be some function that's just going to put a constant column of ones on the front of your input matrix. Okay. So I'll just call that x one. This allows me to keep things straight. So then I will multiply x one and v. This is going to give me what I was previously referring to as a, I take the 10h my nonlinear function, this is going to give me z here, right? So z equals h of x v is that's what's happening in these first two lines. I have to add a column of ones on the front of z. So I'll apply that same add ones function. And then I take this z one, multiply it by w. This gives me my output y. So now for the backprop step, I'm going to do gradient descent on the derivative of the squared error with respect to each weight to update v and w. So first thing, I'm going to reuse t minus y in a bunch of places. So I'll just calculate that as the error. And so now if you look at these two columns here, or these two rows here, you should be able to see that they're basically just reproducing in Python what we did here. So what is v? It is the previous value of v plus some learning rate times x one transpose times the error times w transpose and then element wise multiplied by the by the derivative of the 10h function. And then w is going to be the same thing. So why am I not really using one over n or one over k? Well, I know the sizes of n or k, so this is just a constant. So I don't really need to worry about that. All right, questions. All right. So the above equation is going to be like a single gradient descent step. So we're going to be using all of our training data in x and t to calculate the gradient of e with respect to the weights. So we don't want to use the for loop. So we're going to actually use instead of writing the for loop, we're going to be using the full gradient. So here's an example. This is just dummy data. So this data doesn't signify anything. What I'm going to do is I want to create a nonlinear function that I can fit in neural network to. So if you recall from like assignment three, if this loads, so we had like we did some exercises where we had this function that was at best only vague, the at best vaguely linear. This got a long notebook. Like this one here, right? So we created some function that this is really a nonlinear function, right? Because I created some some dummy data and then applied a polynomial over it. So we know that that's likely to be a nonlinear function. And so now my goal is instead of just fitting a line to that, which is a pretty lousy fit, I want to try and fit a nonlinear function to it using a neural network. So there. So for example, here's a function that has clearly some nonlinearities in it with the use of sign. And I'm going to try and fit a neural network to this using nonlinear functions. So I will apply some noise to make things a little bit more interesting. So this F slot is going to be a random variable drawn from the standard normal distribution between with a range of negative 10 and 10. Okay, so let's see what it looks like. So let me do my imports. So first I'll make some training data and I'll create some helper functions. So this is, I'm just going to create some training data that is where the inputs are evenly spaced across some some domain. And then I'll take the this function and apply that to that to that data to create this nonlinear function. And I'll do the same thing with some testing data, where I have just different inputs that are randomly, they're sampled differently. So these are generated using the linspace function. The X test is basically, I take the train data and add a little bit of noise to it, right? Not so much that the testing data is going to be completely out of distribution, but enough that it's close, but it hasn't been seen before. That's just the idea here. And so that is, even though this is dealing with random, random inputs here, this is what you should expect for pretty much any type of machine learning problem if you want it to behave well. That is the training, the training data and the testing data should be more or less resemblance, right? If I'm trying to use my network fitted to weather to classify miles per gallon, it's not going to work very well. And the same is true for neural networks as well. Okay, so now I'm going to add this helper function, make sure I'm running a resell. So I'll add this helper function, add ones. And so this is just doing what we did previously in notebook three, just in a convenient wrapper function. So I'll standardize standardization in this case is the same as we did for the near regression. So compute means and standard deviations over the training data, and then I will standardize both the training data and the testing data using those computed means and standards. Take this. Now I apply the add ones function to both my training and my testing data. I'll do this right now so that when I get to the evaluation phase, my testing data is the shape that I might never expect. So generally good practice is just to make sure that you have your data all set up in advance, before you before you're going to use it, and then just make sure that you're not accidentally like inputting the testing data into your training function or something. Okay, so previously we did our weight initialization in in assignment one and in lecture three using just initializing them all to zeros. So here with the neural network, we're going to actually initialize the weights to random values. So if all the weights are initialized to zero, all those hidden units will earn identical weight updates. And that would be as if we have only like one hidden units. Okay. And so basically you cannot reliably get your network to fit when you are initializing your weights to zeros with most network architectures. You could also, you could do assignment three using random initialization of the weights. It's just that a couple things that happened one, your code may not pass the automated grader because we have no expected values assuming that you initialize using zero. But also, even if you even if that weren't a factor, different initializations will take longer or different times to converge, because you don't know exactly what the initialized weights are. And so you don't know how many gradient update steps you need to get to the same level of root mean squared error. So with neural networks, this is a pitfall in that if you have different weight initializations, you may get different results. Usually, if the data is such that the network architecture is such that it can converge, you don't end up with a radical difference in many cases. But generally it's good practice for reproducibility to do things like set a constant random seed to make sure that you're initializing effectively to deterministic values. All right, so now let's set the parameters of our of our neural network. The first thing I want to specify is how big is this hidden layer? Right, so I have n samples and I have d things that I'm I want to measure about this, and I have k things that I want to predict. But somewhere in there, there's a transformation by some dimensionality m, right? I basically have to decide the size of that. So I'll use 20. The number of layers that you choose has certain implications for the results of your training based on the nature of the data. And normally people arrive at the appropriate number of hidden layers or hidden units by some sort of empirical evaluation or trial and error grid search. There are more sophisticated techniques that you can use that are somewhat outside the scope of this class, but there are some toolkits that will do things like hyper parameter search for you. All right, for the moment, we'll just say, okay, we'll use 20 hidden hidden units. I'm going to specify two learning rates, one for the one for the hidden layer and one for the output layer. So, you know, you can initialize these to be constant, you can use some techniques based on the sample size if you want. So now we're going to initialize v and w randomly, right? So I will initialize in this case from the same distribution. All I need to do is make sure that they're the right shape. So I have x train dot shape one, that's the number that's d, that's the number of things measured about each sample plus one for the bias column. And then the output of this has to be the same number of hidden units, right? So the output shape of this should be hidden units. And then I add one to that. And then the output of the output size is the, this outer dimension of w. So set the number of epochs, in this case, I'll do 100,000. I'll collect a couple of things for plotting training and testing error. So now here's the, here's sort of the real operation. So I will perform the forward pass and the training data. Remember, I've already applied add ones to x train. So that's done for me. Multiply that by v, take the tan age of that, apply the add ones function to the output, that gives me z one, and then y equals z one times w. And so that is basically what we are doing up here. Okay. Next thing is the, I'll compute the error, right? So this is going to be t train s minus y. So these are the training targets and then the prediction. So now the backwards pass, these are just the two functions that I created before. So the only thing that's really different here is I'm using x train s, instead of just like x one. So you just have to make sure that you're using the right, the right data. Usually if you're putting in, if you've assigned multiple variables to different splits of the data and you use the wrong one, it'll probably just throw some shape error. That should be pretty easy to spot. All right. So now, after this, this allows me to do the forward pass in a single line. So if we break this down for the, to predict the test, I can just take the add ones function, apply that over the output of the linear operation here. So if you work outwards, this is x times v plus with the ones, take the tan h, add ones again, multiply that by w. And now this allows me to compute the error relative to the testing data. All right. So now then what I can do is I can collect the error traces for plotting. And then all of this, this is just a matplotlib functions. All right. So now we run this and we can actually see this network working in real time. So here we can see the root mean squared error on the y-axis plotted versus the training epochs on the x-axis. And this will run for about a hundred thousand training epochs. And so you can see that the training error is much lower. But we also see some significant progress in the testing error. Nonetheless, the testing error kind of appears to bottom out at around maybe .25, whereas the training error is much lower. So this, this network should be able to fit to this data decently well. We can look and see what the predictions are. Right. So here we have training samples. So this is the, I'll see plot the actual and predicted given the training data, the testing data and the model for every x. Right. So if I run this again, we can actually watch this, this second one. Oh, maybe not. I guess this does that. I thought this was gonna animate in real time. I guess I didn't set it to do that. So what we see here is now this is gonna be the, the blue line is gonna be the training. Okay. I guess it is moving a little bit faster than that. So what we see here now, I guess the point that I'm trying to make here is blue line is the training data. The green line is the fit to the training data. This is the model. So you can see that the blue line and the green line are pretty close. Right. So the green is the predicted values of the blue line is the actual values and the orange line is the testing data, which overall is still pretty good. Like it's a decent fit. But it's not quite as good. So we can see how this, this higher testing error is being reflected in the, in the outputs. Okay. Final thing is what are the different, what are the different units actually predicting? So they will just look at the output units, or sorry, the hidden units. And so for X, there's like 20 lines here. You can only really see a few of them because most of them are basically these lines clustered near y equals zero. But you can see how like for some of these units, for different values of X, they're outputting different values. So these hidden units, if we look and just see sort of how many of these are not just clustered here at y equals zero, you know, one, two, three, four, five, six, seven, eight, maybe. There's maybe eight units that are probably doing most of the work, whereas the bulk of the units are either always outputting just a very small value. And then the sum like this one here, whatever it's optimized to is basically a right around zero. It's changing its output from very positive to very negative. This is going to be after the application of the tan h function, because that is going to bound us between negative one and one. All right. So that's the end of intro to neural networks. Questions. I'll go quickly through, I'll start Adam at least before we dismiss. All right. Any questions about neural operations? Yeah. How do you decide the right plot for? Yeah. I mean, it depends on what you're trying to visualize. Right. So for example, if you're plotting training and testing loss or error, you know, this is something that you'll see commonly. So you'll see training time and then the error or the loss function on the y axis. This allows you to show that your network actually is converging and where and how well it's able to be optimized. So it really, you know, there's so many things that you could measure about a neural network and its output. So it's really dependent upon what it is trying to show. I'll have some examples of say plotting hidden unit outputs in multiple dimensions later, I think in notebook, maybe seven. So I guess there's no, there's no real like rules for this in that it's dependent upon the data and it's also dependent on what you're trying to show with the data. Because depending on how you visualize the data, it may kind of tell a different story that makes your point or a different point better or worse. So I think the most I'll say right now is like during this class, you will have multiple opportunities to see different types of plots and you can see which ones maybe are intuitive to you and which ones are not. And then in like when in your final project, you'll have the opportunity to kind of experiment with how you want to visualize the data if it's a classification problem versus a regression problem versus something else. All right, any other questions? Yeah. Yes. Yeah. So I think I mean, you'll definitely know if you're wrong. If you see it not converging, there are some things that weird things can happen, especially as your networks are larger in that and your input sizes grow larger in that you may see your network doesn't converge for like a long time. And then actually like I think I have maybe I have an example. Let me see if I am okay. Maybe we'll discuss this and I'll do Adam tomorrow or Tuesday. So like here's some research that I'm doing and basically we're like trying to predict some outputs from an experiment from data that we gathered in the lab and with the last plot, one thing that we see, we obviously see things like this. So for example, if I'm looking at I'm plotting four things here, right? So we have a training set a validation set and for each one I'm plotting accuracy and the loss. So this is a classification problem. We'll talk about loss functions as opposed to error functions later, but just take this to be, you know, classification error and then accuracy. Right. So the accuracy is actually sample accuracy, which means that even though I have this nice smooth curve as the loss goes down, which is what I want to see, there's a long time where that is not actually reflected in the classification accuracy because basically it's a not a regression problem. It's basically is the sample being predicted correctly or not. And so because of that, we see these kind of jagged leaps where it goes up a lot and then in fact the out the validation accuracy goes down again. And then it sort of converges better and better in these in these stepwise increments. Right. So it's like it's getting, I don't know, 70% of it was like 85% of the samples here. And it's really not getting any more than 85% of the samples. And then suddenly it starts getting 88% of the samples and then eventually it gets up to like 97 or something like that. So you can plot, you know, different metrics and the different packages will allow you to do that pretty easily. So you can plot like accuracy loss, even, you know, other metrics like f1 precision that we talked about earlier. So again, you know, depending on the nature of your task, this is a regression function, you probably want to see a lot of these nice curves and you should see them both in, you know, in like, well, I guess probably we're going to be using error for that. So you should see kind of the error curve start to decline. And it should be pretty much consistent with a couple of exceptions that we'll go into later, where is it, if it's classification, you're going to want to look at like measures like accuracy, or some of these more information theoretic precision recall type measures. So I guess one thing you can do, we'll use PyTorch and you may use TensorFlow on your projects, but there's like a lot of built in metrics that you can use. And so most of these packages basically allow you to create a validation metric in your training, just with like the change of a string. And then you can plot all these things and see like which ones are actually useful information to present. Okay. All right, other questions? Yeah. How do you decide how many hidden layers include the network? Great question. Short answer is no one really knows. Long answer is there's a number of techniques you can use. So one of the network, one of the notebooks we're going to be doing is like how to find the parameters. So you can do things like grid search, basically specify a bunch of different options you want to try and try all of them and see like, according to say your error metric or accuracy metric, which one is best. There are some rules or some, I would say rules of thumb, I guess, that you can consider based on the sizes of your input. So if you have n by d inputs, the more d's you have, the more dimensions, generally the larger network you're going to need. So effectively what's going on here is I look at the architecture of this network that I created is basically four hidden layers that are 128 units, 64 units, 64 units and 32 units. Okay. I ultimately arrived at these empirically in that I tried different things and saw what worked better. But why did I choose 128, for example, as opposed to 64 in that input layer? Well, it's because the raw dimensionality of the input was something like 780. So if I take 780 dimensions and project it down to say 64 dimensions, I'm losing a lot of information. So effectively what we're doing at every hidden layer, the network sizes, the layer sizes, the dimensionality of the output, which means that you're projecting whatever comes into that layer into that dimensionality. So if I have 20 hidden units, I'm going to be projecting whatever comes into that into 20 dimensions. So what if your input is less than 20 dimensions? Well, it'll project it with the addition of noise because there's no other way to do it. It's basically going to optimize some weights because it has no information available to optimize them. What happens if I put in way more than 20 things, going to have to project that down to 20 dimensions, and so I could lose information that could be useful. So in this case, I'm trying to effectively classify three classes using 780 dimensional inputs. So big information bottleneck, but I don't want to go too fast. So projecting in this case down from 780 to 128 sort of turned out to be the happy medium between 64 and 256. People usually use nice multiples of two for computational properties. Sometimes you might just use decimal numbers. So it's not unheard of to see 20 hidden units, 10 hidden units, 100 hidden units, whatever. And sometimes people will find that some weird number just works for the grade like 347, which is that a prime number that might be a prime number? And it just seemed to work better than 346 or 348. Case in point, I guess, if you're you heard of the BERT model, you may have this like this, it's a major NLP model. So basically, you've heard of chat GPT, I assume, okay. So GPT is a sort of one family of big NLP models, and they use what's called the decoder layer of the transformer, meaning they're basically good for generation. BERT is sort of the is a bidirectional encoding of representations from transformers. And the logo was always BERT from Sesame Street. And that's that only uses the encoder layer of the transformers basically meaning that it's taking raw input turning it into an n dimensional representation. That, what that does is the BERT model is basically taking, you know, words turning them into 768 dimensions. But they use it, they do it using stacked encoders. And so that is you have one feedforward network, another feedforward network, another feedforward network, and so on. How many encoders do they use? How do you decide how many encoders to use? Well, BERT uses 12. Why 12? Well, they try to bunch of other stuff and 12 work best. So there are, you know, there's probably good reasons why 12 works better than say 11 or 13. But it's not entirely clear why 12 works better than say 16. It's probably a happy medium between robustness of the representation and compute time. So all that is a long way of saying everybody has their favorite techniques. There's some really smart people who have probably found some evidence based ways to explore this space. Sometimes it requires an enormous amount of resources to do this exploration. So some folks just do trial and error or grid search or, you know, what have you. Alrighty, what else? All right. Let me start the Atom notebook, I guess. I doubt we're going to have time to go through that. Of course, it might take me 15 minutes to find my folder. All right. Where does this work? There it is. Okay. So, all right. Gradient descent with Atom. So I guess just in brief in the last 15 minutes. So who's Atom? Atom is about a person. Atom is a function. So there are many ways to descend a gradient. What we've talked about so far is basically looking, trying to find the direction of steepest descent and taking little steps in that direction. Well, this seems pretty inefficient. Right? Maybe I could take a bigger step. But as you've observed, one way to take a bigger step is to have a larger learning rate. Right? The learning rate is eventually the step down the gradient. But if your step is too big, you may find yourself stepping back and forth over that optimum. You don't want that. But let's think about if I'm standing on the edge of a canyon and trying to find my way to the bottom. If I know that I'm at the very lip. Right? I could probably take a big step. And then like I'm not going to overshoot the minimum. So that's fine. So maybe I can be smarter about how big a step I'm taking or basically scale the step that I'm taking using some metric. So there are other ways of finding error gradients that will lead to fewer steps before we get to optimal weights. So this thing, one of them is called Adam. And there are various Adam variants that are in use. So this one is sort of the original one from 2014, I think. So Adam stands for adaptive movement estimation. So you can kind of see where we're going with this. If I'm really far from the gradient or from the minimum, I can justify moving faster. If you're getting closer, maybe I should be slowing down. Right? So there are a couple of more in-depth gentle introductions you can read. There's the additional paper here by Kingman Ba and then Jason Brownlee has this nice kind of gentle introduction to Adam optimization. But basically, for general purpose gradient descent algorithms, I want to collect all the weights into a neural network into one vector. So this way you can actually use arbitrary gradient descent algorithms for an optimization problem as long as I know what hyperparameters I need to apply where. So let's say we assume these weight matrices V and W. So to collect all of these, we can run this pack function. So now what I'll do is I'll take V and W, flatten them both, and then just stick them together. So just use the H stack function. So let's just test this before going any further. So what I can do is I'll create some numbers. These are just arbitrary numbers just to represent hidden layer weights. And then some to represent output layer weights. So if I print them, these are obviously not good weights for any real reason. They're just illustrative. So if I have V, which is a five by two and then W, which is a three by six, if I flatten all of them, and again, you'll observe that these matrices would not multiply. You have to make sure that the outputs are the right shape. So I need to flatten them. This is just going to give me an array of zero through nine and then an array of 20 through 37. And if I run the pack function, we can see that the output is what I expect. I basically just have a single array that contains all those numbers. Okay. So now imagine that we sent this weight vector off to some gradient descent optimizer, and then it returns some new weight values. I need to unpack those into the right shape for V and W. So I'll create an unpack function that is a good do more complicated than the pack function, because it reshapes W back into the constituent matrices. So of course, for that, I need to specify what shape those are. And then this will create that into the number rows, number of columns for each one, and then reshape, basically split the full, the flattened, the flattened pack vector at where, according to the shapes, the two different weight matrices should be, and then reshape them appropriately. So if I define that and then I pack V and W, and then I run unpack, this will get me back my, my original one. All right. So how is V defined? Right? That is when we run a cell that invokes V, what happens? So first thing, what I can do is set, what I do is here is I'll set W zero equal to 2000. And then if I look at V, you will see that that first element is now set to 2000. So what happened? I actually didn't recalculate W. So why did the value change? So if I do, if I create this V two, which is basically a, an instance of V, then if I run, if I show V two and W, I still see this 2000 here. That's right. Because I just effectively clone V. Now let me set W zero equal to 1000. So wait a minute. V two didn't change. Right? Why not? Because the unpack method is going to create these output matrices that preserve the same places in memory as the vector that was used to create them. So if I change W, you change V. So this can be useful for efficient usage of memory to make sure you're not overwriting something you don't need to overwrite. So if I print W, right, this is that packed vector, I can see 1000 there. So V two didn't change because it was effectively making a deep copy. And but if I change W and then I print the packed vector, we see the same contents here now. So I need to update W in place in order to do this. So if I just do, you know, in place multiplication, that will work. But if I do W equals W times point one, that will not. Right? So now W has a 100 in the first position. And similarly, now so does V. Right? So this is basically accessing the same point in memory. Another way to update arrays, update values without creating new memory is to basically just do a slice over the entire array. So what I'm doing here is now just saying for every element in W, take W times 100. And so now if I print W, this will give me 10,000 in the first position and, you know, 100 times everything else. And then the in W, you will see that now the in W have all been multiplied by 100. Okay, so all of that, that's just how you create your format, your weight matrices, so that it is appropriate for doing this kind of general optimization using gradient descent. So all right, in the last eight minutes, real briefly, intro to Adam. So Adam is short for adaptive movement estimation. It's just spelled Adam like the name, it's not an acronym. It is pretty straightforward to implement. It is computationally efficient. Little memory requirements provided that you are creating your matrices in the appropriate way, or such not just creating deep copies everywhere. It's invariant to diagonal rescale of the gradients. And it's well suited to problems that are large in either in the data or in the parameters. And so it's generally quite efficient as we'll see, presumably on Tuesday, we can get Adam based optimization to converge a whole lot faster than SGD. Okay. It is also appropriate for non stationary objectives and for problems with very noisy or very sparse gradients. And the hyper parameters have an intuitive interpretation and typically don't require a lot of tuning. So SGD maintains a single learning rate for all weight updates. And this doesn't change during training. So remember the learning rate is the step. Adam combines the benefits of these two of two extensions to SGD one is a grad or adaptive gradient, which this will maintain a learning rate for every parameter. And this improves on performance with sparse gradients, and then RMS proper root mean square propagation, which also maintains per parameter learning rates. And these are based on the average of recent magnitudes. So that is how quickly a weight is changing. And that means that the algorithm does well off like these online or non stationary problems. So Adam provides the benefits of both using these things called the first moment and the second moment. So if any of you have taken physics, this may be familiar to you. First moment is the average of recent magnitudes of the gradients for the weight in question. So that's just the mean of recent, the recent values. And then the second moment is just the square of that. So this is just the uncentered variance. So formulas that say if I have I'm going to take like the past four would look something like this. So this is again, related to the concept of the moment in physics. So these are expressions involving the product of a distance and a physical quantity. So basically this accounts for how the mass of the object is actually arranged. So the first moment of mass is the center of mass. And the second moment of mass is the rotational inertia. So just think of this in terms of if my weights were quantities in space, right, if those were masses, then the center, the first moment would reasonably be the center of mass, right? They're all distributed kind of unevenly and like a potato, it wouldn't necessarily be the center of material. But you know, rather center of mass. And then the second moment is going to be its variance. So you think of the first moment as probability distribution as the mean and second moment has its variance. So you can have raw variance or centered variance. And for Adam, the second moment is raw. So what Adam does in the last few minutes is it basically calculates an exponential moving average of the gradient and the square gradient controlled by these two parameters, beta one and beta two. And these are decay rates. So that is, if you think about, if I'm on the lip of the Grand Canyon, I might run down the trail at first, right, because it's really steep. And then I'll slow down as I get to the bottom. So how much do I slow down as I approach the bottom, or as I think I'm approaching the bottom, would be controlled by these two parameters. And so these are just the decay rates of these moving averages. And so exponential decay will describe the amount of reducing an amount by 60% over time. And so the time here is training epochs. So the first moment of calculation will look something like this. So M, which is some value for the moment, equals beta one times M plus one minus beta one times the error. So here what I'm going to do is I'm just going to look and see this controlling parameter. How much do I need to reduce my momentum or my movement? So all right, I will end there for today. And then on Tuesday, we'll pick up with the Adam implementation. All right, thanks, everybody. I'll see you next week.