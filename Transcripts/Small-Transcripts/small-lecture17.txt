 I'm not going to be able to. I get there and I know that. I'm going to have to go. Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Oh, Okay. Welcome back everyone. Hope you enjoy any good break. So, Hi, let me start with the schedule where we're going from here. Okay, so what's going to happen, like I mentioned before break, I'm going to be gone next week. What I'm going to try to do is actually get us a day ahead. Mostly today, because we have a review of softmax and CNN and they kind of always build in after spring break, but usually, we don't really need to spend the whole class on that if any. And so I'm going to try and basically get through notebook 16. So I can move 17 to Thursday and then do 18 remotely next Tuesday. And then, ideally, we can all effectively take the 30th off, because I will be traveling, and you all can have a free day I guess if we, if we manage to do that so hopefully we can that's my goal. So, okay, so then next Tuesday, I will be in California, but I will just do lecture remotely so you can come to the room and attend on your laptop is really wanted to. But you can just, you know, watch from your house or wherever, or if you can all come in here and someone's projector you have a watch party for lecture. Huh. How am I supposed to know where you. You do that and then you like write a reflection on how this was a different experience for you, two pages minimum and you get some extra credit. Okay. Yeah, and you can't use chat GPT to write it for you. Okay, so a three is due tonight I. One thing you should know about a three that I discovered. People have mentioned there's some you having some issues with the automated greater and some things like you're seeing to be doing it right and maybe it's giving you some different answers so in some cases. I discovered how many of you are using an Intel or an M one Mac to do your homework. Okay, in some cases and this might be the case with other types of newer processors as well there's some slight differences in the architecture of the chip set that actually cause some slight discrepancies in the way it ends up performing great decent operations. So, I sorry, I'll send you all the discussion that we first kind of found this last year. And we, we seem we think we can't put the way to address it. What this means that it might take a little bit longer to get your a three grades back because we have to sort through some of these issues. But I just want to warn you so if you are worried that the other greater is maybe not is giving you things and you can't track it down I recommend you come talk to me or send me the issue first so I can maybe review but also know if you're using a new Mac. Sometimes if you're using like Python 310. This may be a slight issue. So, I wouldn't necessarily get like to upset about that if you're like losing points on one of the issues and it seems to be like there's a slight discrepancy yes in the back. So, I think we are grading using 3.9. So, if you want to just be assured that you're writing into the same version that we're grading on use 39. That's not necessarily going to change anything with like the architecture of the chip set or something like that. So that's also an issue. Unfortunately, you know, well, unfortunately technology continues to evolve. And so our notebooks that were written back in like 2019 or whatever are not necessarily like fully automatically compatible with everything that's out right now yes. Probably not. No, if you're getting full points the other greater, especially, even if you're using like a newer computer, most likely will be the same for us. If you want. One thing that may help is like, you know, if you're a concern you can submit along with your assignment take like a screenshot of the auditor make sure that the output is like present in the notebook. So if you're going to be like, people don't don't do this never clear your output before you submit because then we have to rerun the whole thing and we're not going to see what you actually run. So like, you know, run the auditor show us that your output is, you know, you're getting full points. We'll run it again, of course, but if you notice some of these issues then that at least tells us that like we can check with you ask like what your, what your system settings are. And you know, a machine that has those same settings or as close as we can get. So yeah, apologize for that that is an issue mostly with like the GPU part of the code. But it's one of these things with this was just newer machines and the way that they handle certain types of instructions. Today, I'll lecture in whatever form it takes, which is hopefully mostly just and also assign assignment for assignment for tends to be the trickiest one for a lot of people. Fair warning is a lot of different moving parts to plug together I'll get to that when we, when we assign it. And then on Thursday project proposal so I know some of you come talk to me about your ideas. So that's a good thing to do. So you, if you have an idea and you want some preliminary feedback on and I am available. The proposal. I'll assign that and then you will have, you know, little over a week I guess to write your proposal, send it back to me, and I'll have time to give you feedback and I'll give you a little bit of a summary of any changes or clarifications. Okay, so that being said, does anybody have any major issues that you want to review regarding classification, convolutional neural nets, or the softmax function. And if you do. Let's take a few minutes to talk about that before I go and select her. If you don't, then I will start reinforcement learning. I think you'll feel pretty good about softmax. Remember, it's just like you're just computing error in the same way. You just now you're subtracting probabilities from probabilities, convolutional nets. You basically have a patched version of your image that allows you to simulate kind of the scanning function of the quote I over the image. And it will allow you train your filters to have higher outputs when they both match certain things in the input, and then with a set of properly optimized filters you're able to do classification for things that have different types of image level features. Okay, any questions as we go through, you know, the homework of course we can come back and review this material in office hours. But hopefully I think everybody's at least somewhat confident on this. So I'm going to say to say no, I'm going to stress that once more. If you're not confident about it is probably other people in the class who are equally not confident, but less less brave than you so if you speak up you're probably doing your classmates a favor. But I'm assuming from the silence you all are completely expert at the softmax function and convolutional nets and you're going to have no problem with assignment for. So, I have no choice but to take you at your word so I'm going to start to lecture 16. That's Simon for there we go. Okay. So, this is the beginning of our unit on reinforcement learning so we'll have about four lectures on this. So, the introduction we're going to do is just sort of the the simple will call tabular reinforcement learning so no neural networks involved at this stage. And then we'll show how you can use neural networks. So, reinforcement learning can, for some people be very intuitive and for some people it can be very not intuitive and it might be kind of how you've been conditioned to think about machine learning. There are a couple of different ways to think about reinforcement learning one is that you are training and using your model at the same time. So, this is sort of similar way to think about this is this is sort of like supervised learning except you're not getting all your samples at once. This is basically you're in the quote world, and you have to explore the world and that process of exploration is what gives you the actual samples. Right. And so, this is in some ways it's like learning by trial and error, which makes sense, because a lot of human learning, especially you know when we're say infants learning to manipulate things is done by trial and error. Right, so the example that I gave last year was when my daughter was like kind of learning to eat with the spoon. Right. And so she likes the taste of applesauce. And so if she loads loads of the spoon with applesauce or reloaded the spoon with her with applesauce for her, and she sticks like the handle in her mouth she doesn't get the applesauce but she sticks the spoon part in her mouth she gets the applesauce. So, that's the taste of applesauce so basically the agent has things that make it quote happy or sad, and it wants to make it wants to be the most happy it can be, or at least, at least sad it can be way in the way you specify your problems is analogous to that. So, this is sort of, you know, trial and error learnings like this agent world model where the agent explores the world and learns about what types of inputs, give it positive reinforcement, or positive rewards, and what types do not and it tries to maximize the positive rewards or minimize the negative rewards. So, for example, I assume we all know how to play tic-tac-toe. So if you look at a board setup that looks like this of this is the current state of the board where x and O has made two moves each, and your x, and you're trying to choose the best action right you have a number of different possibilities and this is not, I guess this is not totally exhaustive, or actually maybe not as exhaustive. So, we have five open squares on our grid, and you can place your x in any of them. Of course, depending on where you place the x is going to be, you know, a better or not as good move, right. So if you look at this, you know, this is sort of a qualitative judgment, but if you look at the ones that are outlined in green, these are good moves. So, for example, if I put x here, as x I now have two paths to victory, right. So if O puts their blocks made by going here, I can still win by going there. Similarly here, right, there's one path to victory, so maybe it's not as good as this one, maybe this is like obviously the best one, but this one is still pretty good, because like O could block me there, but then I still have like a couple of other places that I could go. The one maybe is not as good, right. So this is not really, it's not really obvious like how I set myself up for victory here because I put x there, and I have to fill out like at least two more squares before I can win. And if I don't really open another path to victory without an additional move. And like maybe these are marginally good moves, but like not this one, this one is actually very good. I'm not sure why this circle is dark green. I would say like this one and this one are like obviously the best moves. And like these two are okay, but maybe not the obvious best and this one is probably like one of the worst moves you can make. So, basically, these may not, these moves may not like immediately lead you to victory, but you can see how they lead you to victory in like the within the next move, or you can see how like this one for example, it's not immediately clear how in the next say and moves you're going to, you're going to be able to win the game. So all these can be represented as quote. So, we have a set of possible states will denote this s and all these can be discrete. So, for example, in tic-tac toe, that is the card now you have s is going to be less than infinity. So we have tic-tac game positions right so at most, even the board is empty there's like nine possible moves that every player can make. The position in a maze so it's in the kind of a grid world, I can move like one step in every direction, but depending on the size of the maze and how many squares are blocked off is going to be a finite places, a set of places that I could go. Sequence of steps in a plan so there's a defined goal. And I'm trying to figure out how do I get from where I am to the goal. There's going to be and this there's a set of discrete steps I can only choose a finite you know one of these defined things I can't really like do multiple things at once or something like that. Then this would be, you know, a finite set. These can also be continuous values right so these could be joint angles of robot arm, or this could be like the angle of an inverted pendulum page. For example, or you can have like the position lost even race car right if I'm trying to control a vehicle. I have continuous values that I can pull at like any given time and they're not necessarily like integers, or they could be, you know, they could be just like fractions to a specified decimal. You can also like parameter values for a network routing strategy or something so any any type of basically planning problem planning control problem can be realized as a reinforcement learning problem where you have to define the states that you can be in. Now when you're in a state you must decide what action you take in that state that actions going to move you to other states, and depending on the correlation between states and actions, you know an action could keep you in the same state or can move you to a different state. And the actions can also be discrete values, right so the next moves are the where I see the X's in red. And so this is going to if this is my state here, each of these actions going to put the board into a different state. Right and that's going to then dictate which actions are more or less advantageous in that state. So, in the in the maze, where I am is part of what I need to know but also I need to know where could I go. Right. And so that's going to be, you know, from where from where I am now, what possible moves are available to me could be there's a wall on one side so I can't move there. Right that might restrict the set of actions that I have available to me. So, there might be arrangements of sequence of steps in a plan so for example, there might be multiple ways to get to the goal from where I am. Right, it doesn't necessarily matter which one of these things I do first at this state as long as I do, you know some subset of them that gets me to a place where I can then move toward the goal. So, you can also be continuous values right so if I'm my inverted pendulum or my robot arm does a torque that you're going to apply to like the pendulum or the joint. And that's going to be continuous value. So, you know, I think in terms of this project that we're working on to control inverted pendulum using a joystick to the joystick right that would apply a torque, and that's going to be a continuous value. So, in the race car example right so if I'm trying to actually control the car, if I know where I am, and I'm trying to like, move, learn how to like drive on a racetrack. I'm going to have to steer the car and apply a certain amount of acceleration in order to move the correct way along the track and not say, you know, drive off the racetrack or drive off the road or something like that. So, in order to do that, you have to have a network routing strategy right there is certain parameter values that you're going to need to set in order to route traffic through your network. And so these are going to also be continuous. So, what we want to do then is given a state and a set of possible actions I want to choose the action that's going to result in the best possible future from the state. So, in order to do that. So, you see, I have to have some way of representing that future outcome. So, for example, what should this, what should this value represent so in tic tac toe I might want to determine if I make this move. What's the likelihood that I'm going to win the game from this position so for example we can see if you know the rules of tic tac toe, if I am, if I make this move, and I'm X. I'm able to tell that by doing this does 100% chance that I'm going to win the game. Like, no matter what does at this point, I'm going to have at least one path to victory. Right, whereas for like one of these, maybe it's more likely than not that I would win the game but it's not certain. Right, I might have say 67% chance of winning and a 33% chance of it being a draw or something. I think I'm going to be able to quantify that somehow. In the maze is going to be like how far am I to the goal, right so I know where the goal is. And I want to be able to figure out like what the number of steps is is going to take me to reach the goal if I am in place p, and I make move M. So planning basically these this might be like efficiency and time and cost or something right there may be multiple ways of getting to the solution. I probably want to choose the one that's going to be like the most efficient, right, or has some some added some benefit. So maybe if I'm like playing, you know, Pacman or something right I might want to choose the path is going to like to pick up the most pips or something like that. There are multiple things that you might be wanting to balance. I might need to might have some energy constraint right so I want to choose the move that's going to allow me to conserve energy the best race car right time to reach the finish line, but also taking into account that if I make certain moves, I could say flip the car over right and that would of course delay me getting to the finish line and never frowning maybe your metric is throughput so all these things. What you what you decide to measure is going to be again critical to determining what type of strategy actually arrive at. So I'll show if time permitting I'll show an example at the end or basically like by measuring certain things we can learn a different type of task. And of course, that's contingent upon the ability to measure that thing if you can't measure that thing you have no way of quantifying your, your, your progress toward that that particular goal. So, with these correct values these multi step decision problems can then be reduced to the single step decision problems that is I'm just going to look at the set of possible actions I'll pick the one that has the best value. So this is going to be guaranteed to find the optimal multi step solution so that dynamic programming problems basically, I've got some multi step problem, and I'm going to be looking for the optimal sub solution by solving for the optimal, the optimal global solution for the sub solution in every step. So that is, if I'm if I know where I'm at right now, and I can find what the optimal solution is for my current situation, then I can be guaranteed that whatever I choose now is going to be part of the ultimate best solution that I write that. And so the, the cost of a single action is going to be what we call the reasons, or the reward of that action from that state. And so then the value of that state action pair is basically the expected value of the full return so I have a full sequence. I have it each, each step I'm at some state I take some action for each of those state action pairs and get my reward for my reinforcement. And then at the end I'm going to get the full return as the sum of all of those rewards. So for example, if I look at this example by starting one of these states, and then I get small r for every reinforcement, and then the return is going to be the sum of all those. But if you look at this just if you take if you take a look at this you'll see that these numbers don't add up left right. And so if I, if I'm here. It looks like my reinforcement is point to my return is point nine. So, that doesn't really make sense but he added backwards that is right to left it does. Right. Why is that. Well that's because I'm trying to get to the end here, and I use that by trying to predict the sum of all reinforcements that I'm going to get when I get to the end. And so that is, if I'm here, then maybe the optimal action that allowed me to get here would be going from this previous state. Right, let's assume this is just like one path to the lattice that we'll see in a moment. But basically, if I add this up right to left, all of a sudden, I can see that if I do, if I followed this path that I'll get the total return of point nine according to the sum of all these little reinforcements. I mean like some state s sub t at time t upon taking a sub t from the state I'm going to get the one step reinforcement, and then the next state so if I find here, and I take this action I'm going to get into this next state I'll have the reinforcement this case of point three. And so now I can continue this until I reach some goal state case steps later. And so then the return r sub t starting from state s sub t should then be the sum of all of my reinforcements. So that's why this works. So effectively, what I'm trying to do is if I'm in any of these steps that precede the goal, I want to be able to predict what the best action is and I'm going to do that by estimating the sum of all possible actions that I can take from this state and try to figure out the best sequence. So then I'll use the best returns to choose the best action so this will become a little bit clearer if you look at this example. So, five this lattice here. My goal is to get to one of these end states, right and this that let's assume these states are actually all the same. So they're actually be a closure here but we're not showing that. So if I'm looking at this. If I go along this path, then my return is going to be point nine. If I go along this path, my return is going to be point eight. If I go along this path, my return is going to be point six. So, which of these is the best path to take the first one. I think the first one. You all agree. What if I'm trying to minimize costs rather than maximize the reward. So, the last one right so really depends you know are you are you maximizing or minimizing that is what does your reward or cost function actually represent so for example if it's the race car and you're measuring time, you probably want to minimize time. Whereas if you're playing Pac-Man, and you're measuring like the number of pips you collect, you might want to maximize that. Or if you have like, are you trying to conserve some resource or you're trying to gain some like extrinsic reward or something like that. So if it's like energy that you want to minimize, then you might want to take like this bottom path. Right. So let's say that this is actually a cost. What is the energy required to move between each of these states. And so this is interesting because in this case, the first reinforcement you're going to get is actually the highest value. Right, so if you want to minimize the total return, it doesn't necessarily make intuitive sense to take the path that has the highest reinforcement, if the reinforces actually a cost. But you can see that from this state, you then actually can get to a sequence that allows you to take a bunch of really kind of small cost steps and in one case that has no cost in exchange for taking a big cost step at the first time step. And so this actually ends up being the best the best path to take if you're trying to minimize, say expenditure some resource. Right, so conversely, if you're trying to maximize, if you're in this state, it might make sense to take this because you get like the largest reinforcement. If you think that's a reward that might seem like the best action. But ultimately, you're going to get the lowest return. Right. So you may want to take one of these sort of lower reward first steps, because you can figure out that you're actually going to get a higher reward later on in the sequence. So, to do that we need to know these values. Right, so I need to have some sense of what's going to happen if I'm in some state and I take some action and needed to try to estimate this for basically all combinations states in action so where do these values come from. So, we have a couple of options run one is I can write the code to calculate them. So, this usually isn't possible, or if it is possible it's not really a problem you need to solve reinforcement learning. Right, I can kind of brute force my way through it, or use, you know, some other type of solution and you can probably do this for tic-tac-toe, because it's a fairly constrained problem, and there's a relatively limited set of things that you can do that will. Typically count for all those. You could use dynamic programming. This is also usually not possible because it requires knowledge of the probabilities of the transitions between all the states for all the actions and if you don't have that, then it becomes a lot more difficult to infer what those rewards would be. So you can compare this to things like the hidden Markov model. This is a type of model that we use in natural language processing to try and figure out like, what's the probability you say moving from state x one to state x two or from state x two back to state x one. And then we also have these things like one of my quote observations that I've ever stepped. So you can use, you know, certain types of dynamic programming algorithms to solve certain types of sequence problems. But again, if you can do this, your problem is probably constrained well enough that you don't need to be doing reinforcement learning for this. So a true reinforcement learning problem is one that you are looking for examples. You're basically looking for lots of examples of both solving the problem successfully and not solving the problem successfully. So what I want to do is I want to see for my inverted pendulum if I'm trying to balance. I want to look and see like, given a an angle and velocity of the pendulum, what moves the same kind of balance to give a try to balance like a pen on your hand or something like this. Right. So I'm trying to keep this balance and I do like really bad job. And then over time, maybe I become better, better and better at it. And I learned that by figuring out if I can feel see the pendulum. I know that I should move and I should maybe make a certain movement that's like small or large, etc. And I do this a whole lot. Right. So in some sense, you know, learning to walk, you know, is some sort of reinforcement learning or at least some kind of reinforcement learning feedback happening between the brain and the body, we can teach AI agents to learn to quote walk by running them through an obstacle course. And they sometimes come up with these really weird walks like, you know, they're not necessarily by people and you can have like a three limb to Asian this sort of has this weird triangular walk. And so it learns to actually move through through the environment by this by this example sampling. So basically these examples are going to be represented as these five tuples so that as you have the state where I am the action, what I do, the reinforcement like the return the the reward that I got from making that action that state, and then where I end up. So that is the next state and then the next action. Yes. We have a hybrid that could be the best reinforcement for it. So you're trying to maximize. Yeah, yeah. Right. Well, it depends. Again, you have to perform your problem. So if you're assuming that you're trying to maximize the return, then yes, the one that has the highest return is typically going to be, you know, at least an instance of a good plan, possibly the best plan. It gets more complicated as you have like more continuous scenarios, but generally yes now if you're trying to minimize of course then you're basically just inverting that it's going to be like the lowest return. So there are a couple of different techniques, broad families techniques we can use. And so I'll, we'll focus on temporal difference, we'll also talk about what you're going to do a demonstration at the end time permitting. So basically, in Monte Carlo sampling what I'm going to do is I'm going to take an average of the observed returns, and then assign that to every state action pair. So that is I have a basically some value function that takes in my state in my action. And this is going to approximate the mean of the return from that state and that action. Right. So, in order to do this, I need to actually wait until the episode concludes that is I either reach a goal or I fail, like I time out or something. I can't wait before I update my state action pair variables. So for example, I'm trying to solve a multi sequence problem. And I semi randomly explore the world, and then I reach my goal that I can see that I got my goal and I can see, oh hey this is actually a really good sequence of events. I want to do more like this in the future. Or if I say I've got a time limit of like you can take 10 steps and then you're done. I'm going to be 10 attempts to solve the problem and I try to move through the world. And in 10 attempts I do not reach the goal. Right, so then you can see that and say, well, this was not a good sequence of events. I should do less of this in future. Now, the trick is that in maybe a sub optimal sequence of events maybe I made the first three steps of my 10 were actually optimal. So from where I started, those took me directly toward the goal and then I moved off track or something right so doesn't necessarily mean that all of the everything in that sequence was bad just means the sequence overall was bad. Similarly, if I do reach the goal, I might have started off bad, and then I stumbled upon, you know, a path toward the goal. So again, just just like the opposite. I mean, I don't necessarily do not necessarily want to adhere to everything in that sequence, because there might be a better way, right, maybe I reached the goal in like seven steps. But I actually the best path could have gotten me there in four. Right, so maybe I don't want to continually just exploit the strategy that I happen to stumble upon, because there might be a better one. Okay, now, Temple differences the other strategies to basically what I'm going to do is I have the value function I'm going to evaluate that over the next state next action pair that is, given where I end up if I take this action, that I look at all the possible actions that I can take in that, in that state, and I'll sample each of those actions in turn and then put that action and that then my current my next state into the value function. So I'm going to use that as an estimate of the return from the next state, and then we're going to update the current state action values that is value of s of t a sub t. So that is the value function of s of t a sub t is going to be approximately equal to the return from the next, the sorry, the reinforcement at the next step, plus the value function at the next step which is an approximation of the entire return. So that is this is small r, this is an approximate approximation of big R. And now what this does is then I can update my state action pair immediately after taking the action. So I can basically see, I took this action from the state, and I have a pretty good estimate of how good or bad it was. So I can see if this is going to get me closer to the goal or not. So, if you take a look at this, this little graph here, I'm trying to estimate the return are from state B. So this is where I am right here. And basically, I've got a situation where I can move from one of these states through state C, and then from state C, you can either move to state w or L w versus when L is loose so pretty straightforward scenario I want to end up in w. And I have some examples. So that is I have, I started a, I go through C, and I go to L, and I do that 100 times. So basically 100 times I go from a to see and then I lose. So I have a lot of examples of failure. And then 100 and 101st time I go from a to see to w. And the second time I start in B, I go to see and I go to w. So this is a contrived example. Right, so I have one example that shows me what the estimate of the return from state B is I'm trying to figure this out. What this would look like for these two methods is basically from Monte Carlo every example starting in state B leads to a return of one. This is trivial, because I have one example starting in state B, but it leads to return of one, right so every example and starting state be of each return of one. The mean of this is one which is a break a prediction of a win. Now the temporal difference method which show me that the reward from C to be a zero and then from state C I have 100 words that are negative one and two that are one. So in this case, what this would end up being is a value of negative point nine six. So this is a very likely loss. So, temporal difference takes advantage of the cash experience given in the state that I learned for state C, whereas Monte Carlo is going to take just the samples that have the entire sequence including my my start so basically what about what about what do I do in this in in this situation, if I go from be, if I started be, you know, if I'm trying to win what type of method might I use. Right Monte Carlo right in this case is pretty trivial, because I've got my Monte Carlo method gives me the only, the only only prediction of a win starting in state B. So, this is not necessarily like how you would actually choose you would have a better distribution of your actual samples. Any questions so far. See where we are. Okay. All right, so let's take a very simple maze example like a stupidly simple maze example to the point that you look at this image and you think like an image fail to load. But here's our quote maze, where G in the corner of these walls represents the goal. So, I'm, I can have, I can be any position here will still will assume there's a grid. And I can, I need to decide whether to move up down left or right. So, to do this we need an estimate of the number of steps needs to reach the goal. This would be big R the return. We need to formulate this in terms of the reinforcements that is the law. So first of all what reinforcement do we get for making a move. So for every move, it's going to be one. So, if you don't really know if any of these moves are going to get us closer to the goal, then big R is going to be the sum of those to produce the number of steps to the goal from each state. So now you can see that if I have a bunch of cells in this that represent where my agent is. It's going to look sort of like one of those little mind sweeper grids there's like a number associated with how close. How many steps it's going to take me to get from here to the goal. I want to basically traverse the shortest path. So, the Monte Carlo sampling will assign this value as an average of the number of steps from the goal to the goal from every starting state. And the temporal difference will update the value, based on one plus the estimated next value. So, now if our question is whether we do Monte Carlo update or temporal difference. Let's look at this comparison on this maze problem. So, we've talked about this value functions that is the V of s and a. So, how do we actually represent this function. So, the simplest way to do this is actually just as a table so you can imagine that let's we take this maze example. I have a state that would be represented as some cell say numerical cell, and then action that's going to be one of those four things up down the left right. So, the value of that should be some representation of how many steps it should take me to get to the goal from the state, given that I take this action. So we'll take this tabular function. And we will actually write this function called q function as this table so this state action value function is basically, you take in both the state in the action and the value is the prediction of the expected of future reinforcements in the maze problem. This future reinforcements are basically a sum of a bunch of ones each one representing a step to the maze. So terminology q comes from this thesis by sky called Watkins University of Cambridge. And so what we're going to do is we're going to select our current belief based on the optimal action is in the current states by taking the arg max of the q function, or the argument of the q function depending on if you're maximizing or minimize. Right. So again, what we will do is, I think this for the maze problem. I believe there's some typos here like it says arg max we actually mean argument, because we're trying to minimize distance. But typically, you can think of another way to another way to do this is you can just realize your cost is negative rewards. Right. So now if I think like it costs me to take a step and therefore the return for taking a step is negative one. All of a sudden I just turn an argument problem and arg max problem so typically I'm going to be talking about in terms of minimizing. But usually, most people are going to are trying to maximize. And you, if you have this problem of you've got a cost function rather than reward. It's pretty easy to just like invert your reward values, and suddenly you turn your course into a cost. So, we're going to represent the q table for this maze world which is going to be. So I have the q function of s sub t and some action. I'm trying to find the argument for a that gives me the best value. So, if we have argument of these possibilities, let's say, this is the goal, and s represents the state. So I have effectively a set of q values for this state, and each of the possible actions up right down or left. Right. And so I'm basically going to evaluate this q function for each of these combinations, and then choose the value of the action that is the right hand side of the second argument. And that gives me that in this case, minimum value or maximum value if you if you are treating them as as cost. So, that is we can let the current state be a position in x y coordinates, and the action would be integers 12123 or four. So, if I'm looking for the element of 123 or four, that's going to minimize the value of this function, where my state is represented as these two values x and y. So if I assume a grid. Then I can have say, zero zero one zero one one or something like that you know you can start from the bottom left if you want. So let's try to do this in Python. So first, we need to know how we can actually implement this q function. So we know we know what the arguments are already it's going to be, you know, a tuple consisting of an x y coordinate and then a single integer representing the value. So, we know we can enumerate all the states, which is going to be a finite set of out of 100 possible possible positions, and then we can do it all the actions. So, we can do this in finite which is for. And so then we can calculate the new state from the old state in an action, and then represent all of these state action combinations in finite memory. So, a fairly nice compact table that we can use. So we'll just store the q function in table form. So, we're going to use three dimensions x and y you could in principle, use two dimensions right so I could just say enumerate these zero one two three four or five six seven eight nine and 10 next row 11 or next row 1011 12 so on so on. It is maybe just a little bit cleaner easier to visualize if I keep them separate so that's what we're going to do. But it wouldn't be too difficult to switch between these two representations. So we'll have a three dimensional q table where I'm going to have one dimension is x second dimension is why these two dimensions represent states and the third dimension is the action. So you can think of this as kind of this cube or I've got each of these states on the x y plane and as I go every level deeper. That's representing like what the q table is going to look like for each of those different actions. Now the actual value is going to be the steps toward the goal. So we're going to be minimizing. So we're going to be moving just above in order to get as closer to G I'm going to be moving either right or down. So we have some intuitive representation of like what correct q values should look like which allows us to calculate them. So, how can we represent this three dimensional table of q values in Python. So if we have x or x and y have 10 possible values and there are four actions. So this table basically three dimensional NumPy array that is a 10 by 10 by four array. Okay, so now we can represent this three dimensional table as a NumPy array. So how should we initialize this table. This above line is going to initialize all the values as zero. So think about what effects this is going to have for the q values for actions as they're updated to estimate steps toward the goal. So what's what's going to happen is basically all of my actions are going to have the lowest possible q value that is zero. And so this is going to force the agent to try all the actions from all the states, because it has no notion of what might be better than the other. So it's basically going to start like here. Every possible action that it could take is going to be zero. It won't know what the possible return is unless till it takes that action so going left is just as good as going down. So this is lots of what we call exploration. Okay, questions. So now updating the q value using the temporal difference updates will look something like this. So I'm in a state. I take an action, I get some sort of return. And then I'm in a new state and I can see which possible actions are available to me. So then I can calculate the TD error. So that is the return at this, this next time step, plus the value function that is the q function. And then I'll use this, I'm going to take the q value for the next state next action pair. So track the q value for the current state current action pair that is where I was and what I just did. And then I use it to update the q value that's stored for s&t so it's going to be some cell in this three dimensional array that has the value in it that's by default initialize to zero, and I'll update that with some new value. So if I, for example, start here, and I move to the left, then this should give me I'll have the return, which is one, right, in this case, my return is always one. And then I'm going to have the q value for this, which is going to be different steps toward the goal, which in this case, let's assume you're like a seven by seven group just count them so we'll have 1234567. Next let's say I get a q value for the next day next action that's seven, compare this to where I was 123456. Okay, so now I have one plus seven minus six. So then that's going to be that'll give me the value that allows me to update the q table there. And so then how much do I update it. Well, I don't necessarily want to update it just with the raw value I'm going to scale that by something. So I'll have a constant here row is going to be some learning rate. And then I could say, well, maybe this is not like an atrocious thing to do but it's suboptimal. So I want to maybe discount this one a little bit. 0.1. 0.01 whatever. This role is just a scale factor between zero and one. And then I take that, and then I add it to the current value of the q function at that at that point. And so that's going to give me the new value. And it's going to look something like this. So let's say the position is going to be represented as a two dimensional array, or sorry to value array. And so then return this or reinforcing this case is always one. So I'll take my q old, which is going to be basically the q value using my old state right this is a to two elements. And then my old action, and then q new is going to be the q value of my current state and my actions. So remember, what we're doing here is this is after I've already taken the action so I'm updating q new as my current state, because this is where I am now, yes. What would be your forwarded time? What I just did, what I just did to get here. Yeah. Yeah, so keep in mind like the q new here is the values at times of t plus one, whereas q old is at the values of time step t so you have to you think ahead and kind of one step in time. I'm but I want to update the values of the table where I just was, which I think of as being the quote current time step. I'm just like projecting forward seeing, where will I be if I make this action. How good or bad is this I'm going to use that value to update my values where I am now yes. Do you want row to be zero? Yeah, actually you know what this should probably be. So if you would, if row is zero you would never learn, right so you would have no update. Yeah, so this is probably more accurate. Okay, other questions? Right. Okay, so here we are. And so now how much, how much do I update so my TD my temporal difference error is basically this part here. So again, I'm trying to figure out what my expected return is compared to the q value of where I currently am. And then I'm going to take my current value, and then update it by my learning rate times this error function so now this should start looking pretty familiar right I have an error term, I have a scale factor of how much I update it. And then I have some value that is being updated. Okay. So, this is going to be performed for every pair of steps that is t and t plus one until you get to the final step. Of course that has to be handled differently, because there's no next state, like once we get to the goal and done. And so then the update, given as previously, is going to become basically I'm just going to update my current q value with instead of taking some error from the subtracting the q value for the next state from the current state, I'm just going to take the next return minus or the next reinforcement minus my current q value. So in Python this becomes adding a test for the goal yeah. In this case yeah. In this particular example. Yeah, so it's not not necessarily always going to be one. But aren't we trying to the goal. Yes, it has to be. Well, and because we're taking steps through through a maze, and these are discrete. So basically, imagine if I had a two dimensional maze where sometimes I have to jump on top of a box. Right. That might have a cost or something of two. Or if I have a continuous, I can decide how much I move. Maybe let me be like playing mini golf or something right, and you can either hit it hard, or softly, right you go to like, what is it like on mountain avenue they have like this mini golf courses. And that I suck at. And so like you try to figure out like how much do I need to put right so they could be. I have a force of like one, you know, Newton or whatever. And then I have a force of like point two. So these, you always have to think about what your, what the problem we're trying to solve and how that is represented in this case, it's discrete steps through a maze. So it's step one square in any direction. So it's got a cost of. It is not necessarily always known but you basically get that from the environment by making the action. Okay, so I'll, I will show you a block stacking example. And at the end, and I'll explain how that's going so I'll try and get through this thing. And now I need to check and see if I'm at the goal so in this case, the maze of its representatives and character rain have a letter G at a goal position. All I'm going to do is just check and see if where I'm going has letter G at it. And if it does then I use my, my kind of goal update rather than my normal update. Okay. So now to choose the best action for some state x y I just need to look at the argument for the Q function at the row that contains where I'm at, given my, given my current state. And then if we store the available actions as this array, then I can just update the state based on the action by looking at a, where a is the argument of the Q function. Okay. So this is this agent environment interaction loop that is the agent in the environment, it does something it gets some sort of feedback from that and uses that to decide what is best to do the next time. So we have to implement the following steps that is I initialize Q, I choose some sort of non goal state me done randomly. And then I repeat the following. If I'm at the goal, then I update my Q function with the temporal difference error that is one minus the Q function or R minus Q function. And I take a new random state says I'm at the goal I'm done. I reinitialize and I try to solve the problem again. Otherwise, I'm not at the goal, I select the next action. I'm not at the first step that I'm going to update the queue old with the temporal difference error within this case is one plus Q new minus Q old. I'll shift the current state and the action to where I where that action landed me, and I'll move those to the old ones, and then I'll apply the action to get to the new state. So I attend maze is going to look something like this in Python so initialize my Q table, choose a random starting position, and then for however many times I want to train. I'm going to execute the following so if I'm at the goal, then I'm going to update my Q function using one minus Q old, and then I'm going to reinitialize. I'm going to initialize my state. Otherwise, I'm going to select the next action. If, and then I'm going to update my Q old using this formulation of temporal difference error. I'm going to shift my current state of my current action to my old ones, and then apply my action to get to my new state. So all perform an RL problem you need to know the following things. You have to have the state space that is in this case it's the size of the maze what is the extent of things that I could that I could possibly where I could possibly be. Then the action space, what are the things I could possibly do. So in the in the maze example doesn't even move you can make in bouncing an inverted pendulum the state space is going to be you know, have the angle and angular velocity would say of the pendulum, and the action space is like how can I move my my hand or what torque and I apply to my pendulum to keep it balanced reinforcement for every state action, or at least a method to calculate it's this can be shoved off to another function. But you have to have some method of extracting this so you can get very sophisticated with this like in the example of a show you actually draw that directly from simulator. So you don't have the, you don't have any knowledge of it ahead of time but you basically use like a representation of world physics to figure it out for you. And so this can be either a cost or reward so usually we just call everything reward and you invert it if it's a cost. And you also need to know whether you're minimizing or maximizing returns. So depending on this, you know you can just apply like a negative reward there are some subtle differences in behavior with more complicated problems but generally this holds. Okay, so the Python solution in the maze problem looks like the following. So we'll start with text file that specifies this 10 by 10 maze looks like this. So now print the maze so here's my representation and I have some walls here and I have my goal so basically I'm going to start somewhere in the white space and have to find my way to G. So I'll convert this into a machine readable format of course. So I basically flatten this into a list. So I'll turn this into a 12 by 12 array, it's 12 by 12 because we have a 10 by 10 maze with walls on all sides. Right, so we're just representing this is basically places that the agent cannot go. This is mostly just for for readability right I could represent this without these walls here. It would just be pretty confusing to look at. Alright, print it out again to make sure we didn't screw it up. Great that looks good. So, I'll need some functions. I'm going to create one's going to draw this queue service over this two dimensional state space. And then one to select an action, given the queue surface. So that's what these are. This is really just drawing functions not going to go through that for time. So we can start these arrays that holds the queue table and updates it using temporal differences, and then we're going to use one that will update the queue table using Monte Carlo. So remember the difference between temporal difference Monte Carlo, temporal difference I get to update my values every time I make an action and see what happened Monte Carlo I have to wait until I get to the end. And then I can see, based on an average of how good every action was I'll update that. So we need to have four possible actions for each position. These are represented changes in rows and columns so we have plus one and minus one. And then I'll set the queue value for the invalid actions to infinity. And so an invalid action is going to be basically where I run into a wall. So if I end up here, right, going right should be an invalid action because there's wall there. So, by doing that I can basically say that this is by saying this has a cost of an infinite cost I'm never going to take this because I'm not I'm not allowed to. So now for some parameters so we'll run 100,000 interactions, that is 100,000 updates, I'll let my learning rates be point one and then I also have this epsilon. This is a random action probability. So what's the random action. Well, let's imagine that I explore. And I end up at the goal. So I could take one step forward one step back one step forward one step back one step forward one step back and one step down. That step down lands me at the goal. Right. So I need to take like three forward back steps and not a dog trying to find a place to nap. So, and before I, before I step down. Instead, it's a possibility that maybe I took seven steps and I could have taken one. Right, so this random action allows me to basically pop out of some sufficient strategy that's still some optimal. And so, but I can do that by taking a random action it might get might allow me might maybe take like some random actions that never get me to the goal, but there's a chance I'm going to get a better, a better solution. So now I need to keep history trace of positions and reinforcements and then I need to use that to update the Monte Carlo version of Q. So I'll store this trace of x, y and a. And so now this is just for initialization display. So now let me initialize. So here's my start position. And then this is my, my first action that's just the index of where I'm going to look for my for my actions. I'll keep track of the trials and the number of times that I've hit the goal. So now we can see okay if I found the goal I'll perform the goal update, if not I'll perform the regular update, and then the Monte Carlo update is looking, I'm going to look into trace, and then I'm going to update this based on an average of all of those, all of those costs. So if I'm done I randomly start new position. If I'm not at the goal, then I pick the next action. This can be done randomly. If I choose a random value that's less than epsilon otherwise I'll choose the best action from the Q function. And then every 100 steps I will print. So, let's watch this run. Here's the TD Q policy. Here is the Monte Carlo Q policy. This is what it's done. The most recent trials that read obviously is the goal. And then this is going to be the number of steps per goal at each trial. So just take a look at this and see you know what do you think is better at solving this temporal difference or Monte Carlo. If I'm here I'm trying to get to the goal. The hour represents like what the best action is for this cell. So say I start here. Follow the arrows. Great. Let's start in the same position in Monte Carlo. That's this one. Right. Okay. I'm going to grab. I got stuck. So, pretty clearly for this, this type of problem the temporal difference is a better solution. You can see, obviously in the Monte Carlo I find plenty of places where if I start at some trajectory, I end up, you know, in a place where I just sort of go back and forth if I adhere to the Q policy. I might be able to pop out of this by doing a random action, but there's no guarantee. Okay, so now let me show you a quick example of Monte Carlo learning for different tasks. So let me. All right, let me run this environment. This is basically a unity simulator of block stacking. So me on the sometimes it crashes hopefully won't crash. If it does crash, because I show a video or something. So here's my goal for 300 steps. Okay, so we have two blocks. My goal is to stack the. There he goes. Okay, my goal is to stack C block on top of the B block and what it's doing is it's selecting an action relative to the surface of the B block. You can see that there are times where it's basically jumping around and moving, you know, it's not even touching the surface of the B block. And sometimes it does. So we can see here in these updates every so often one of these. So, the rollouts will pop out showing showing me like what the reward at the mean reward that it's been getting so this is using Monte Carlo learning. What this is doing is for the first 100 steps. It's really just randomly exploring. So you can see if I randomly explored the space, what looks like it's a viable solution versus what's not, and then the actual learning starts. So you can see here is kind of out there exploring the space is really to sort of verify and that there are no good moves out here. And then eventually it might sometimes it takes a while to converge. So I'm not sure it's going to get there in 300 steps. But eventually it will kind of going off spinning off in space. You can see it doing its thing there. Eventually it might find. Oh, hey, here's a good action. This actually gives me a good reward. I should try more of this. Let's see if we're going to get there. I did not. Okay, let me try again. Usually I run for like 500 but sometimes that it as it starts to succeed it takes longer and this on this takes like a bit more time. So you can see it exploring. Okay, not learning a whole lot, or at least it's learning what bad moves are it found something. So it should try to do more like that in the future but right now it's still in the random exploration phase. So let's see if it. We do 100 steps of random exploration, still there. Okay, so now it's found it may be found a couple of actions that might be pretty good or at least closer to optimal. So you can see now it's starting to stack. So now it does some more exploration. It tries some some more stuff. We can see that it's getting me and reward of roughly 37, and in this case the reward is like 1000 for stacking the right the first time and then as a discount of 100 each time. So I'm not sure it's actually going to learn anything here. There we go, starting to get something. So now I can see it's starting to kind of learn a bit. And then it's, we'll see what the reward is. So here now mean reward is like 32, it should start to get a slightly higher rewards here so 31.2. And now it's like 68.4. So now it seems to kind of start to converge on some sort of viable solution. So this is an example of Monte Carlo learning because it has to terminate the episode in this case it gets 10 attempts, or it stacks successfully. And so once it reaches the goal or times out will basically look at all the actions that it took and see how good or bad they are. This is using a particular type of method called a deterministic policy gradient so it's actually using a neural network to optimize the weights. So in particular, there are two neural networks one called an actor and one called a critic. So as you can imagine, the actor chooses an action, and it also predicts how good the things the action will be. And then the critic basically says, yeah, you're great or boo you suck based on the action. And so the actor tries to make better actions, the critic tried to get better at predicting how good the actors actions are. And so then both of those losses flow backwards into those respective neural networks to optimize those weights. So this should terminate relatively soon and we can see what the final word was, and we can try and evaluate this briefly. So it probably could have done with training for like a little bit longer. So you can see had a mean reward of 169. So it kind of. Yeah. In this case it's 1000. So basically if it were, if we're perfectly stacking the first time you would get a reward of 1000. So if it's, if it, the way the reward is set up here it's 1000 for stacking perfectly the first time, minus 100 for every additional time you get a reward of 900, and then like 800 the third time and so on. Then it gets a reward of negative one for missing the block entirely, and then a reward of nine for touching the block, but not stacking successfully. So, so again we're using the world physics here. So basically if it, if it stacks off center but it stays stacked, that would be a reward in the hundreds depending on which attempt it was on. So that would be the same. So if it stacks and stays, yeah, perfectly yeah yeah. But of course you're more likely to stack perfectly the first time if you put it exactly centered. Right so if it trains long enough, you know it should, it should approximate that. Questions. Yeah. So in this case, we think about if we know the maximum reward is 1000. Right. So we can think that on average. So probably, given what you know about the reward shape. Alright, 1000 for the first attempt 900 for the second attempt. So basically between this number falls between what would be a good stack between the eighth and ninth attempt. So probably on average, the model as it stands right now would probably stack successfully somewhere between that eighth and ninth attempt, maybe a little bit better, because there's a lot of noise in this and so when you actually evaluate the model sometimes it performs better than it appears to at the end of training. But again it's all, it's all about you have to understand like what the values in your reward actually signify. And so this in this case the reward value were chosen quite deliberately to kind of encourage it toward a solution with the appropriate amount of exploration like once it finds solution, it gets a very high signal saying I should be trying more of these things. So would it be right to assume that the reward is no, it's not very efficient learning. That may be the case. So you can you can use like these reward shaping strategies to specify like what types of things you want to encourage the agent to do. But you could just as well train this with, say a reward of negative one for failing the stack and just a reward of one for stacking. Yeah, it might take like a little bit longer to converge possibly, but you might actually converge like a better solution so for example, with like the off center stack it might get a really high reward and that's as good as it ever gets so it's like oh well this is a really good solution it's not like perfect, but I'll keep this whereas with a more with kind of a rewards of lesser magnitude you might encourage it toward like a more like a more perfect solution that might take longer. So, yeah, so you're you'll because you store the action at every step so if you see the action is represented in zero to 1000s basically in this case about values that are closer to 500 minutes closer to getting it centered. So, in this one we have 578 and 530. So, the reward is nine. So it got it on the block but maybe it didn't quite stack it fell off. And so you keep track of all of those. So, so you can see the value of the values and each of the states, and you can say for this action that I took I got a reward of nine, just like not terrible but also didn't get me what I want for this reward. This action I got a reward of like 600. Right, so this is better than this other one that got me only a reward of nine. So like you're getting information from the policy. Yeah, until the end. Yeah. Other questions, good questions. Yeah. So, so you're getting a good observation. Yeah, reward minus one. Yeah, is that the highest thing you deduct from the reward. In this case, yes. Yeah, so this case, if it just sort of plops the block out there in space and it never touches the destination blocks that were the negative one. So when you saw like the block kind of moving around in space, it's really kind of exploring the space and learning like there are no good actions here in this region. So eventually I want to try and move out of this region. Okay. Other questions. One last question. Yeah, it's one. Okay, go ahead. So let me get the actions over there, the one which has minus one reward and the one which has reward. Yeah, they won't pretty much have like, about 500. Yeah. And if I remember, you mentioned that if they're about 500, they get very good. Well, so I mean, keep in mind that like, imagine a large space where the block is in the center, right, that space is defined is bounded at zero and 1000. So the value is very close to zero or just like way off in space, that is very close to 1000 or way off in space values close to 500 are going to be close to the block. The way that I've constructed the action space here is the block is like really small. Because it's, I'm not sure about like the span of the block in the in the action spaces but it's very, very small. So basically you want, if I picked 500 500 exactly, it should stack. We also add, we also make this more difficult actually because we add a little bit of physics noise to it. So if you imagine this is a virtual environment, I can be hyper precise. So if you actually stacking things, when you release it there's a slight motion, right. So basically we add a little bit of a jitter or a push. So sometimes, even if it stacks well. There's a bit of noise there that actually like sort of simulates this release and it actually ended up falling off. So there's a number of things we've done to make this problem kind of harder and more realistic. That makes it you know, at least, it's still pretty easy for you to learn to solve but like, you know, at least somewhat challenging. Alright, I better talk about assignment for before we adjourn. So, what you're going to be doing here is classification of hand drawn digits so you're going to be given a solution to assignment to similar to what you've seen before. And then you need to extend it into a neural network classifier class. This is using emnis but it's not a convolutional net yet this is still fully connected net will do convolutional that's next. And then you're also going to need to define the confusion matrix function. So here's the neural network class should look a lot like what we've seen before. So, what similar to some of your your a two solutions. Then we have we got our optimizers we got an instance of neural network. So what you need to do, then is you can test using these functions. You need to extend the neural network class. So inherits from the snow network classifier inherits from neural network. Again, you'll have access to all the same functions, but you need to override the train function the error f function the gradient f function and the use function. In addition, you also need to define make indicator bars and softmax functions. All of these are given in some form in previous notebooks is just this you're going to have to pull from. I want to say 910 and 12 probably, I will double check that. But there's a couple of different places you're going to look for some of these different things. So, you want to look in the implementations of classifiers that we've done look at how we override the train function and the error function gradient function use function, and then create versions of the make indicator bars and the softmax function for done before. So we can do just create a sample test of this new class so basically what this is what this is doing is is just classifying random numbers into instances of class 01 or two depending on whether they're less than 25 between 25 and 75 or between, or greater than 75. So, basically, you can test that function using using this. So, you can see everything like this, and these values are just offset so you can see them both so we just we show T plus five just so they don't overlap perfectly. Then for the hand drawn digits. So you can download and this this deep learning site goes down a whole lot but you can also get it from here. So, this is a page at the Washington CS department. If you have trouble with this I have amnesty pickle saved so like if some for some reason either these sites work. Just let me know I'll put it up on canvas. The file is already partitioned. So open it. You've seen this before who 2000 training samples 10,000 valentess samples. So, these are these, these train classes these are the digits. If we look at those 784 columns we now know that those are the pixels right so if we split all of them up, you'll see that we just plot the values and we have a bunch of things between zero and one. So, you have pixel intensities for the image right so you have pixel intensity of zero, and then suddenly we start to see actual pixels that are non zero. And remember these are laid out in rows. That's why I had this periodicity in this plot. So you can rearrange them into a square. So let's reshape them look at the numbers. We print this image we can see it's a five, give you a function to turn it into a grayscale color map. And then you can pop the negative image. Okay, so let's look at the first 100 images and plot them using the labels as titles so these are the first 100 images. So you want to check the proportions of each digit see the roughly even we found that they are roughly 10% belongs to each each class. So they're all very, very class close to point one. So now let's try the first experiment so training neural net five hidden units in one layer and a small number of epochs. As you can train your neural network classifier using this these these settings and see what the final training error is. So now you have to run some longer experiments so first you have to write code to do the following so you have to try five different hidden layer structures doesn't really matter what they are. So you have to use the same method and choose you have to train that for each one training network for 500 epochs collect the percent of samples classified for the given train validate and test partitions. So create collect these into a pandas data frame. You want to log the times this is going to be how long the network took to train in seconds. So we've done that before you see you start a timer, and then subtract the end time from the start time or the start time and time sorry. And what you're going to do is you're going to retrain a network using the best hidden layer structure as judged by the percent correct on the validation set. So basically you you've done a network search using the percent correct of the validation set use that to figure out which of your network architectures is the quote best one. So then you retrain this network and then use this network to find the find several images where that it gets wrong. So you look in the test set and see where the networks probability the crash class is closest to zero. So these are the images for which your best network actually does the worst on. So then you draw these images and you discuss why your network might not be doing well for those images for example, are there a bunch of fourth a book like nine just something like that you know depending on how your network is performing. Then you need to write a confusion matrix function. So this is going to return the confusion major any classification problem as a pandas data frame. We've shown this in lecture 12. So this needs to take in two arguments the predicted class and the true class. So it should look something like this, where you take in y classes and T test and output something like that. If you want to do some of the coloring, you can use what we showed in lecture. I think to do that. Okay, so 50% for the correct code 50% for the experimentation and discussion will have the greater I have not put this up yet I will do that this evening. So this is the same, you know, same procedure. So put this in your, in your folder with your, with your notebook and run it. Finally, you can do extra credit for combining the train test and validate partitions into two matrices. So I'm going to go back to Ray Lou and then a single value of learning rate number of epochs compare several layer architectures by applying the cross flow validation as defined in lecture 10, and then show the results and discuss which architectures you find works best and how you determine this. Okay, we're out of time. So let you go. I'll go back to my office and have office hours in a few minutes.