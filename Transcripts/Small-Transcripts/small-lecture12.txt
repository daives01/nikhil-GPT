 Yeah, I want to grab it. Yeah, well, I got it. All right. All right, guys, let's start. So I appreciate you. Hardie souls who came out today. I actually was not expecting this many people. I was like, you know, a dozen to 20 people in the room, which is twice as much as it's not actually be here. So, Yeah, thanks for, thanks for the rat. Let me wait for me to catch. Okay, there we go. All right, so what we're going to do today is I will just go ahead and share the screen. I don't think I really have. I don't know if you have any questions, except that assignment to is due today for most of you. So if you haven't requested if you need an extension. And your default due date is today, I'm sorry it is now too late. So please make sure that you get your assignments submitted. So, so I'll, let's do today, read for assignment one if you choose to do that are going to be next Tuesday and that's also the same day we're going to assign assignment three. So I'd say around now is probably when things start to get pretty, pretty intense as far as the workload for this class. So, it's about it as far as class announcements. So what we're going to do today is I'll just I'll finish up the classification with logistic regression lecture. And then I believe we are exactly on track for where we need to be is the 2823. Okay, let me get through this and I'll see I think that notebook 11 is like pretty, pretty short. So, see even get through that today because we are now a little behind. Okay, any questions. Nope. All right. So, just a refresher on last time. So, we're doing classification of course and so we talked about that in terms of probabilities, what we want to do is we basically have a bunch of classes. And then for example we want to predict the probability that it falls into each of your annual classes or your classes. And so then what we have to do is we basically have to maximize the likelihood of seeing the data that we actually have according to the class distribution that we've got so remember the data course in this case is going to is going to include the samples and whatever numerical features define each of those samples and then a label for each for each class and remember the labels now going to be an indicator variable or a one hot vector. Where it's all zeros except for a single one where in the index corresponding to the class that the sample belongs to this can also be thought of as a probability, right so it is a 0% chance the ground truth is basically, you know sample sample and is K or is not K for whatever class K. In other words, for the correct class K it is 100% likely to be that class and 0% likely to be anything else. So think of this as probabilities everything is zero or one. I'm now trying to minimize the distance between my prediction and the ground truth, which is now just a probability distribution, right and being a probability distribution has to sum to one. So, we have the data likely that we want to maximize now. So this is going to be we have some function g. It's basically going to be my prediction function for my sample parameterized by the weights K for every class K, and then divide that by the sum of the prediction values for that for every class So we define the gradient with respect of the law like that with respect to to W, and we end up with update rules for each individual weight w this should look kind of superficially familiar to what you're what you already know from doing the regression with neural networks that is I'm trying to update an individual weight. And so I need to take the previous value of that weight and then add something to it to move it in the right direction that something is going to be some constant learning rate was to say alpha. And then this is going to be the sum of all my errors for my for my samples right so wj j corresponds to class, I want to better optimize the weights corresponding to that class I need to effectively figure out how wrong is my prediction for every sample in my data set for every sample and. So the one I'm going to do is I'm going to have some target value, this is going to be that indicator variable where all zero is the single one in the right place. For sample and for that class J so basically what this means is that now for this sample and at index J there should be a one, and there should be all zeros other. So then what I want to do is I want I'm going to subtract from that my prediction. Right so this should be this function G, I'm going to technically have a different function G for every class J. And this will tell me the like the class, I see to do this for every class. And then of course I have the other term there being that input. So pretty standard optimization function at this point in that I have some learning rate I have some error term, and I have some input right and these are the three things plus the weight that I need to use to. optimize the value of that weight to the correct value. So, of course we're doing this in going to be doing this in Python and so we want this to be some some level of optimized for speed. So if we have my update rule for w sub J or J is some class as given above. We can see and we check out the well the expected shapes of these arrays are going to be right so remember, we're dealing with these weighted sums of inputs. And so, we're going to be adding this constant one to the front of each sample so what's the dimensionality of X sub n, it's going to be D plus one where D is the number of measurements or features for that sample, plus one, which is that's my bias column. And then, by one, because I'm only doing with a single sample right now. w sub J should also be D plus one by one with this is just a just a single a single class. And so that then T sub n J minus G sub J of X of n is going to be some scalar. So again this is my, this is my error measurement how wrong am I. And so I need that to be expressed in terms of a scale of value. So this all works, but you'll notice that the sum is over n, and then each term in the product is going to have any components so that means we can rewrite these as matrices, and then do it all as a dot product to basically do all my computations across the entire data set at once. So just to work this out we'll kind of do something similar to what we did when we worked out the update rules for neural networks using matrices. So that is we can remove the sum and basically just replace the subscript n for all samples with just an arbitrary placeholder which is use this star. So I'm going to remove this sum right here. And so now for whichever value of n. I'm just going to be updating the associated value of T sub J for that sample. So consider that remember we have we want to set up all our, our data as these big matrices are where my role rows are the samples. And then my columns would get are going to be the targets. So in this case, the target is going to be, you know, a n by k array for k classes. So, what are the shapes of each piece here. So this is going to be T minus G, this is going to be n minus one so or s or n by one. So for every sample I should get a scalar value saying how wrong this prediction is. And so now x, right, likewise I removed the end and I'm just using this kind of placeholder star. So this should be this is going to be basically my big x my big collection of all my inputs. And so this should be n for n samples by D plus one, where D is the number of features for each sample plus one to the bias. And so then w sub J as above is going to be D plus one by one. So this is going to work now if we transpose x and then just pre multiplied and then we just define G is a function that accepts x. So if you look at this, and then here below, they're basically just identical, having removed the sum only all you do is transpose my, my big x to make sure that my shapes line. So now what we've got is we basically have an arbitrary update rule for all of my x for a single class J. So, now we've been able to effectively get rid of the summation over n. And so now I want to get rid of my other subscript which is this J. In other words, we're going to try and make this expression work for all the W's. So now we've successfully gotten rid of n. Let's do the same thing to get rid of J will just replace J with our placeholder. And so likewise, every time I see a J here, I and I just replaced with the star. So now I have nicely end up with sort of T sub star star, which, if you remember how we handle this and the intro to neural network lecture this is basically just saying, I'm going to replace this with a big matrix of all of my T's. So here the star star means I can account for any row and any column. So now this is just going to be big T matrix of all my target values. So, if W star which is basically now just W, this is going to be D plus one times K. So now think about the, what we're trying to map to is going to be probabilities over K classes. So the output should be of dimensionality K. What are we trying to map to those K classes, your size D plus one for the bias so this is going to be D plus one by K T T sub star star is just big T remember so this is going to be N by K, how many samples do I have and and they could be each of them in the classes so N by K. And now G of X is going to be N by K minus one. So this should, this is going to predict my, my samples, and then for each of them is going to predict you know some some class. So actually, why is it called on. This is incorrect. My mistake. No, times. Right. So, G of X is N by K. So now T minus G of X should also be N by K of course we have you know N by K minus and by K equals and by K just element wise. And so, T or sorry X transpose times T minus G is now going to be D plus one by K. And so now X transpose times T minus D plus one is D D plus one by K and so now we have the update equation for all my W's being for any W I'm going to take the previous value of that weight plus learning rate times the input times the error term. So far so good. So, a will be some constant or alpha be some constant. T is my N by K target indicator variables, and then G of X is the prediction function over X over the values X. So this what does what does G what does you look like basically. So we defined for K from one through big K. And so we said that this function will just take it to be the shape of e exponentiated to the W times X. So we do the exponentiation, because we basically need to end up with a subtraction, we need to be able to do T minus G. But previously, if you remember we worked in the world of logarithms. So we can't necessarily easily subtract. I can there's nothing that I can take the logarithm of a subtraction very easily right I take the logarithm of a division right now I can do I can subtract the logarithms, but actually want to be subtracting the scalar values itself. So to get around that only do is just going to expand shape both sides. Now I can actually do a subtraction. So the sub cave X is going to be my prediction function for that class K divided by the sum of the prediction outputs for all of my classes. Then I can change these to handle all the samples X. So now instead of for a particular class K. And for a particular sample and I'm just going to take f of X parameterized by W. So I'm back in my familiar territory of I have some inputs X, and I'm trying to map to the right outputs. My job is now to optimize for the right weights that will let me do that. So I can just say that f of X parameterized by W is e raised to the X times W. So now g of X is going to be f of X parameterized by W divided by basically the sum over all the rows for X parameterized by W. So I basically have one one class, and then I divide by the song. So given training data X, which as you recall is n by D plus one, and then class indicator variables T there and by K, we can then perform the following expressions. Perform the operations with the following code. So first what we need to do is we need to create this function to get indicator variables from the class labels because you might find that your class labels are not set up in these one hot vector representations in fact they're just like labeled ABC or 123. So you need to translate them into an appropriate indicator variable format. So basically I'm just trying to go from this you know 12213 to 100010001001001001001001001 for how many classes you got. You might just work notifications, keeping at me. Oh, God, you have to do that. Okay. So everybody just ignore that please. So what you're going to do is we're going to find this make indicator variables function. This will take in the, the, the T column matrix that this is just going to be like a two dimensional matrix it's n samples by one. And then what I will do is I'm just going to pretty much just reshape this into into the into a two dimensional matrix. It's not already, and then just take those individual values and map them to the appropriate index in a vector that's otherwise all zeros. So let's take the demonstration let's take the above sample right here. Reshape it right and then I will run it through the make indicator variables, and this gives me the same thing. Right so these are now the above indices as indicator variables. So now for how many things am I trying to predict it's going to be however many classes I've got and what I want to predict is now the probability that my sample falls into each of those classes, and by dividing by the sum of all predictions I can ensure that I'm going to get a value that's a basically a percentage out of one. So I'll define G that does all of that here so now I have G of X of W. And so I'll have my, my F which is going to be I'm going to exponentiate X times W. And then I'm going to take the denominator and then I'll return my G's my G values, which is going to be that f's value divided by the denominator. So the function is also sometimes called the softmax function, as you probably have heard of so softmax function is this common final layer activation function. And now as you see what it does is it takes your scalar values and maps them all into some probability distribution that sums to one. Like the sigmoid, right which converts things into a value normalized between zero and one. The softmax does that to accept it does it makes sure that that that that's some is normalized so that all values sum to two one. So, as you can see the sigmoid is useful for binary classification right if everything is going to be between zero and one. And if I have a class of interest all I need to do is sigmoid some value and it's going to tell me the probability between zero and one that it falls into that class of interest. So this is useful for two class problems, because software because sigmoid squishes everything between zero and one which means that to get the probability of the other class I just do one minus the sigmoid. So, softmax of course I can't use the sigmoid function for multi class problems, because I have a class of interest, all that's going to do is tell me what's the probability that this is not a member of that class, which is a binary classification problem. If actually want to know which class it falls into I have to be able to normalize all the probability so that such that they sum to one. So, if you're not maybe we'll review this after spring break. The softmax is a generalization of the sigmoid function for multi class problems and it's a fairly simple derivation to show that that is not I believe in this notebook but we can get to that. So softmax function is basically as you may have seen, you know, exponentiate x times w is that as you take your final output aside, prior to the softmax, exponentiate it and divide by the sum of all explanations. So, w can be formed with code such as this so make my indicator variables. I will define the weights in the appropriate shape, define some learning rate, and then for every step in my specified number of epochs. I will then take the softmax of x and w right in this function, we're multiplying x and w. And so then we can use my update rule to better optimize weights. So, I'm going to put this in a nutshell for a very simple linear classification problem is how I can use the softmax function and so I'm doing neural networks, what I'll do is I'll just have the appropriate insertion of hidden layers to handle non linearity and then do backprop in a very similar way. So, that's the conclusion so far. I'm curious, why is everybody grounded to this side of the room. It's even when people are here, that side of the room is much sparser for some reason. If you all come in through that door and it's just closer. Anyway, okay. We have an unbalanced distribution in this class right if I just want to predict like where someone sitting in the class is almost always going to be on this side because sorry, it's very empty over there. So here's code for applying linear logistic regression to the Parkinson's data so we're still in the world of linear linear equations here, because I don't have my hidden layers to allow me to insert non linearities. So I'm, I can still use the softmax function of course to squish everything into the appropriate probability distribution, but what it does is just taking a matrix multiplication of X and W, which is still a kind of standard linear linear equation that we have been doing since the beginning of class. So, we'll do the Parkinson's data set again. So let me read in the data. So this should look familiar 195 samples of 34 features each. So then I will do the same type of data cleaning that I did before. So of course I don't want to include the status, which is my output label in my inputs, and neither do I want to include the names that's not helpful feature. So I drop those. And then I slice off the status column this becomes my targets. So now I can see that I've got 22 input features in X. These are the names of all those features. I have a single output that status. So standardization function, as you've seen before, right, so I'm just going to compute some means and standard deviations and standardize the inputs. I'm now also going to import the QDA LDA implementations that we used in the previous lecture. This will allow me to compare the performance of logistic regression to the two previous methods that we use, namely QDA and LDA. So now to generate our training data and our validation and testing partitions. What we're going to do is we're going to partition data into folds on a class by class basis. So this is kind of similar to what we did earlier in that we're going to define like how many folds I want. And then I'm going to use one of them to be testing data, and then one of them to be validation and then the rest to be training data. And the only wrinkle here is that I need to make sure that I have approximately the same portion of samples from each class. So this function does this, this is called generate stratified partitions. So what this will do is it's going to generate my sets for training validate and test for both my inputs and my targets. And before I evolve, if I don't want validation data, I can set validation defaults, and it's going to give me a dictionary that's keyed by the class label so they want to retrieve say the training and the validation sets for class zero. It's going to be set up in a pretty intuitive way. So what I'm going to do here is I'm just going to basically shuffle all of my rows, and then I'm going to pull out the appropriate set for each fold, and then I'm going to partition my folds, or I'm going to have, I'm having partition my data in my folds, I'm going to segment off which collection of fold I want to use for the one to use for training validation and test. Basically, this just does all that. So in the this part here this is just going to make sure that I'm roughly balanced across all of my classes. And then this part will generate the test and optional validation fold, and then return the rest as train. So, if you run this, what we'll do here is I will print. These are each of my folds. And then we'll see that the first number this is like my number of training samples and the number of validation samples and the number of testing samples. And then this will be the, the number here is the partition or the percent of the partition that is class zero. If you remember that we have the healthy samples and the Parkinson's samples and it's not a balanced data set, but we want to see the university roughly the same proportion of each class across all of my folds. So we can do this in this trick using NP dot mean, right so this is going to be computing the arithmetic mean across the specified axis. And what this does to train equals zeros is going to turn everything to either true or false. And so, if this is true, it's going to be a one of its false it's going to be a zero and NP dot mean will handle that automatically. So these are the individual samples. So what that means then is that NP dot mean of a equals equals zero is going to be the proportion of samples in a whose values are equal to zero and just replace this constant here with whichever whatever thing you're interested in. So now what we can do is you can write a function that's going to iterate all the possible ways of making train validation and test sets from n partitions that I specify, and then I'll also train my qda lda and now my logistic regression models over this data. Okay, so what I'm going to do is I'm going to save time just using this the break statement to stop execution after just one run of cross validation. And then to use all runs what I have to do is I'm going to have to calculate the mean errors or accuracies over all cross validation runs. This is what you're going to be doing in a three. So, everybody know the term cross validation who's not familiar with this term. Okay, some somebody that's fine. So basically cross validation. I have a data set. And I, maybe it's small, or maybe I just want to make sure that I'm not just getting a lucky split. What I'm going to do is I'm going to basically partition it into these folds, and I'm going to hold one out as the thing to be tested on. Now remember, maybe that particular split is lucky for some reason. But depending on how you can just sort of depending on your random seed or other factors, you can get a particular testing split that just happens to perform well right if you hold that testing data out. The remaining training data is just very indicative of that testing data there's nothing really in the testing data that is maybe unusual from the perspective of the training data. So it's in that case it's quite likely that you would get a very good test performance. Right. But we don't know that you just segmented out like the 20% of the data that had that property. So, if you stop there, you can report a really high test result but then someone else who runs your code partitions a different split, or they have a different random seed they get different rows and that split and all of a sudden you're showing like 96% test accuracy, and they get 68 or something. And they're like, Whoa, what's going on this is way lower. So what you want to do, or one thing that you can do is to take that test split and rotated different one each time, and then average all those results. So for example, if you had some test split that was like okay got 96% on this one, and then 76% on the next one. And then like 86% across the remaining three just to make my mental math easier where you end up with a average test accuracy of 86%. And so that's much more realistic than either the high end the 96 or the low end the 76. And so that in that way, you'll get a more accurate picture of the actual performance of your your model trained over these different splits in the testing in the training data. Clear. Cross validation can also be useful technique. When you have a small data set. So for example, it may take. Let's take this sample we have 160 or 195 samples right this is not huge it will do fine for these linear methods. But let's say I have a more complicated problem that requires a neural network. What it requires more samples to actually converge to an appropriate model and so you need some, you know, you need like 90% of those training samples to get your model to converge you hold at 10%, which in a sample that has 195 samples to begin with, you know, 19 test samples right and the results might not be all that indicative right again you have the sparsity problem. So you could end up with a with a lucky split problem. And so one way you can kind of ameliorate that is to cycle through these different testing splits. So, number of reasons why you might want to use cross validation, we will do that here. And so you're going to have to implement this in a three using similar methods. So we'll have this, this version of the run park function. It's going to be called run park log reg for the just a progression will also include the outputs of QDA and LDA. So, this will have with the prediction accuracy using all three of those. So, in order to run my generate stratified partitions. I specify how many folds I want on this case I'm not going to use validation. So I'm just going to generate training and test. And I now I compute my means and my standard deviations, I standardized my X trains and my X tests using those means and deviations. And then I will do all my pre processing upfront, I'll append to my, my column of ones. And this stuff for linear logistic regression is now I had this make indicator of ours function. Right so this T train and T test now gets transformed into T train I and T test I. So now I should have some one hot vector representation or indicator variable representation of all of my, all of my target samples. And now the rest is as we've seen. Right so here to this point, ignore this likelihood list for the moment. We just initialize our weights, specify some learning rate specify some training time. This is my forward pass because it's linear. I'm just running through the soft max. So you can imagine this being just a, a neural network with no hidden layers and a soft max on the output will be the same thing. And I do my backwards past my weight update as we shop as we as we saw. So what we'll do is now here I will convert the log likelihood likelihood. So if you remember, we were calculating the log likelihood of the data to make the mathematics easier. We need to convert that back to the actual likelihood, because that's what we're trying to to maximize. And so then that all what I will do is I will append this likelihood per sample to this to this list so I can plot it. So now what I can see is after I plot I can actually see the likelihood of the data as training as training proceeds. And then I will print the percent correct using the logistic regression method, and I'll do the same thing using the QDA and LDA code that we did before. And the rest is plotting, and then I will define this percent correct to turn my results into a coherent accuracy. So, let's run it. Okay, so this is the percent correct for each one. So the logistic regression is 89.2 train 78.9 test QDA and LDA have really high numbers. Or actually wait a second, this is the same because right. This is the issue. So, sorry, sorry about that. Okay, so here we go. Let's do this again. Okay, more like it. So, we see logistic regression and LDA have kind of similar performances although we actually see the logistic regression as a higher test accuracy this time. QDA we see the same thing that we saw last time we received this really high training training accuracy and the test accuracy is not as impressive, suggesting that QDA again maybe overfitting to this data. So these are the weights, and you'll see that basically the weights of the second column or just the negative of the weights in the first column, because this is a binary classification problem so if it's not one it's the other. We're using the softmax because that's a generalization. We could recast this problem and one that uses the sigmoid because again, one minus sigmoid of of x will give me the probability that the sigmoid of x is that x is not in the class of interest. So, this is a single sample. These the individual values these are standardized. So now the samples times the weights right this is going to be that first sample times w. These, this gives me some scalar value, right 1.23 negative 1.23. Okay, we can already see, you know, what what the outputs going to be. So, this is the first sample of the first class it's going to be in. But just to run everything to completion, what I'm going to, I'm going to exponentiate this right now this is this, this is basically e raised to these values. So now they're no longer negatives right 3.45 and point three. I sum the exponentials is now 3.7. So now I normalize these. So, this is now 9 to and point eight. So now these some to one I just do a sandy check here, right this does some to one and the arg max is going to be the first element or class zero. So this isn't this is a the sample belongs to class zero. You, once we got to this point you could probably tell what the answer is going to be. So, I'm thinking of the scalar value, because this is a binary classification problem. It's going to be one or the other. If it were a multi class classification problem it would not be necessarily so easy to see that. And also the softmax has the nice problem, the nice properties of making everything consistent with your error term relative to the probability value that you do need. So, you can sort of see what the answer is by the time we get here, but you actually need to softmax in order to get a meaningful error term that I can use to do things like weight updates or back propagation in neural networks. So, finally our charts here this is the, the likelihood of the data, according to the just a progression we can see that we end up with just like from point five you go up to play about point eight, most of that. And now here this plot you can see the outputs for each individual, each individual classification. What we will find is, in most cases, LDA and logistic regression are performing the same they're probably only two samples where LDA says one thing and logistic regression says the other QDA. Again, we have you know some overfitting in the, in the training data so it's not performing quite as well. So, we can run this again. More time. So, we can run this a few more times you see similar trends. And generally what we find here is that at least for this data, logistic regression can usually achieve a higher test accuracy the LDA, even though maybe the training accuracy is the same or slightly lower. So, this may be a problem where logistic regression is more suited. So, run this a few more times, and you can see similar trends as we have seen before. So, now the code above. This is using SGD in the gradient of the log likelihood. So, do we have a better way of doing gradient descent, right, and remember gradient space, a sense, not descent yet. So we can try Adam right we've demonstrated that Adam often converges faster with with fewer training epochs. But first you need to define your error function to be minimized and as gradient function. So, previously we were trying to ascend the gradient, but the function to be optimized, we actually want to try and minimize the gradients we can use a general solution, using any kind of gradient descent algorithm. So, in this case what we're going to do is we're going to create this function as the negative of the log likelihood. So this allows us to minimize the function. And the gradient function also needs to include the negative so here's the definition of log likelihood above and it's gradient below. Also written as matrices. Oh, nice. Oh, I hope I didn't interrupt something. Stop the girl. Okay, didn't break. Sorry about that. I think it stopped for me accidentally. So I'll define this second version of the function that will be using Adam and import the optimizers class that I'm going to use. So I'm going to use the optimizers class that I have before snacking specify which optimizer I want. And now what I've done here additionally is below the softmax function is I now define the negative log likelihood function, and then it's gradient. So I'm going to use these to get the the output of my gradient descent operation at each step. And so now this is the thing that I'm trying to minimize. And then what you're, what you may want to do is like convert this back into, you know, the likelihood of the data. So here what I'll do is I will take this to likelihood. Right. So I have the negative log likelihood. What I'm going to do is it's going to take the negative of NLL the negative log likelihood, exponentiate it. This gives me back into regular likelihood. So now I can define this likelihood trace that is going to be this going to instantiate an instance of Adam that takes the negative log likelihood as input and it's gradient and the training number training epochs learning rate. So I'm going to define this error convert f function to likelihood and so this error convert f is an argument is a member function of the atom optimizer. And so it's just going to call this here that I've defined. So it's automatically going to convert the negative log likelihood back to the just plain like me. Then the rest is the same right we just do just a regression QDA and LDA print this. So here this is written errors a little bit misleading just because of the way that the neural network class is written. This class is written to originally based off of the regression problems and trying to minimize error. So what I've done is I'm now printing the likelihood of the data instead. So just to know when you see error here, you'll notice that it actually increases sometimes. And that's because we're trying to maximize the likelihood of the data, which is what's being printed out here. Okay, so maybe a more, we could rewrite this to actually print the negative log likelihood and see it decline. But then it would sort of go opposite what the chart actually shows. So here we see the error or likelihood climb to some threshold of about point eight nine. And then we also see the, the training and test accuracy, and you can see here that using Adam at least in this case train accuracy is only 88.9 or test accuracy is actually 96, which is significantly better than the test accuracy for either QDA or LDA. And then we see, you know, similar different split here, but we're still we're still getting basically better trick test accuracy than the train accuracy using the just progression with Adam. So, just throw that out there to some of you. What would you do to change this to run SGD instead of Adam for the linear logistic regression. Just think about how the code is set up. We'll go back to where we instantiate that. I'll just do it a little bit. So, what would you do to use SGD instead. And the answer is probably a lot simpler than you think. So, I always use softmax for all classification problems. Right now I'm just, I'm asking this is a this is a lead into the next lecture just just to sort of telegraph where I'm going a little bit. I need to change a word, a single word here. What do you think. What do you think of the back. You agree. Yeah, okay. So basically, the we have this optimizer class that that contains implementations that Adam and SGD. So to do that, all we need to do is change SGD to Adam and then change the change the type signature of the function and whichever way it demands. Which brings me to the next lecture code reuse through inheritance. Before I do that. Any questions on logistic regression. So, okay, 11 code reuse by class inheritance this will be pretty useful to you and I think this is relatively short so like we'll let you go early which will be cool. So, what you have done say a to so far, what we had you do for example is you complete a neural network implementation. And then we say here's a cell. Copy your whole neural network implementation into that cell and make the following changes right and some of you doing that are probably thinking, this seems like a very inefficient way of doing things why am I doing this well. This is not a great way of doing things obviously. Instead, what we can do is you want to make relatively small modifications to the existing classes and functions. So what you can do is you can add arguments to select for the new behaviors like what type of activation function you want to use what type of optimizer you want to use. So, I'm going to do class inheritance to extend the original class so I assume that all of you are familiar with doing class inheritance in some way, right, maybe in Python, maybe not but presumably at least taking Java here. And so you've done class inheritance. So, what we'll do just an example here that uses the neural network class. So, to allow, say the 10 h or the real activation functions, you know, you had used this keyword to specify 10 h relu. And then you in that copy of your neural network class, you have to add the additional Ray Lou behaviors and the gradients and everything like that. So instead, let's, let's use class inheritance do the same thing. So first, I'm just going to write out the optimizers file. And then I'll put it in the same way. So I'm going to put the optimizers right here. And then I will put it in its own Python script. So now to use this class need to import it like we've done. And here is a I'm thinking about to give away the answer to a to Maybe I should just end class here. What do you guys think. I'm going to stop here. No. I feel like I shouldn't let this like, yeah. What's that. I, we're right where we need to be. And I think the due date slipped every every year a little bit. I'm going to have to calibrate them appropriately so I'm going to call it a day here actually. So I don't give away the answer. Okay, so you all get get get 30 minutes back so I will I will see you next week. Bye.