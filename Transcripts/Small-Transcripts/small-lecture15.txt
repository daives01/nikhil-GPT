 All right, let's start. So weather is absolutely awful, and like a bunch of people are feeling sick. So it's been a really good time of year. And today is my birthday and so my president to myself is having a short class. Thank you. I mean I'm turning like 36 which is like mathematically it's actually kind of interesting but legally it means nothing. And it just means that you're like that much closer to 40 so. Yeah. Okay, so, does anybody need to come to office hours like after class. Hey, yeah, I was gonna my other president to myself was canceling office hours but I sort of felt like kind of a jerk doing that. But if you don't need to, then I'll do that and I'll hold off hours in the regular time on on Thursday. So, alright, a one re grades are back for those of you who submitted them on everybody did at least slightly better. I'm just going to start with the five point deduction. So this seems to be said paid off nominally for most of you. And Cyrus finished the two grades I just need to review them and I will post them so I will endeavor to get to that in the next 48 hours at least. So, I'm going to find out about a three is due the Tuesday after spring break. So, you know, if I Thursday would be the last time to talk to me about it in person I will be online, if you need to reach me over break. But I may not be able to respond like very promptly on anyone and traveling. So just FYI, you do have some time but it's because of the intervene implications for accessibility. So, yeah, just make sure that you stay on top of that. Any questions. Anything. Yeah. I am not here the 20, what is it the 28th and the 30th, I think. And then, and then I'm going, yeah. Yeah, so I'll be here for that interviewing week. And I'll be accessible it's just done probably either do a remote lecture or recorded lecture those weeks and I'm on. Okay. Okay, any other questions. All right, cool. All right, so last time we talked about combinator convolutional neural networks so this is effectively just a network that includes a weight matrix called a filter. That way you multiply it by things that quote match that weight matrix should output a high value. That's like this high activation. But we use these high these handcrafted filters. And so of course the question is, you know, why do we waste our time, you know, hand crafting the filters isn't the point of machine learning to basically look at a bunch of data and figure out what matters automatically. So this lecture, and the next will actually be about performing the training operations on convolutional networks and how they differ from, say the standard feed forward, and what CNNs are able to do that feed forward networks are not able to do so. And just as we recall, we take our image we break it up into these patches. This allows us to do among other things kind of simulate the scanning continuous motion of the eye over an image. And then we can take each of those patches and try to tease out what the important information is in that patch for any particular class. So, this is a classification problem. We can use CNNs for regression problems are not going to cover that here, but we'll just treat this as an as an image classifier so this is sort of this canonical computer vision problem that we're going to be talking about here. So we're going to do is we're going to be taking our neural network classifier class that we had from before, and turn it into a convolutional version. So the classification part is the same, right I have k classes. And then I that means I have k output nodes and I run my softmax function over the values in those output nodes that gives me whichever one is going to be the most likely class I take the argmax, and that's the class output. So again, I have indicator variables, and my error is going to be the probabilities for all of my classes. And then we take that and subtract that from my indicator variables which is a bunch of zeros with a one in the place where the where the class is the correct answer. So this makes it 100% probability minus whatever probability. It predicts for that class do that for all the other classes and my goal is now to try to get close as close to that distribution of all zeros and a one, as I can for all samples. So, in this example, we're going to be just using a single convolutional layer to demonstrate how the mathematics works without getting too complicated. And then we'll assume that our samples are these two dimensional arrays. And we'll assume black and white images. So we'll have them be square, and have the same number of rows and columns. So, and then, maybe not this one but the next one we'll talk about how to do this using pipe torch, and using multiple convolutional layers. So, our neural network classifier CNN class consists of pretty much the same components as the existing neural network classifier class so we have our in it functions as the constructor. We need to more arguments in additional to in addition to the usual ones that we have in the CNN, in the neural network classifier. We start with the things that we've, we've had before the number of inputs, number of hidden per layer the number of outputs, and we have the activation function so the first three and this fourth one, those are the things that we've got already in our existing neural network classifier. So, if we get a convolutional neural network we have to add the patch size and the stride. So the patch size is given my input image. What's the size I'm going to break it up into in patches, and the stride is going to be how much overlap do I have between those patches that is how many in this case pixels, and I'm moving from patch to patch so I'm going to take a seven by seven patch I'm going to move seven pixels to the right, there would be no overlap between my patches. So we don't typically want that what you want is a stride like that's going to be lower than the patch size. So if I had a patch size of seven by seven and stride like the two. Once I clip out that seven by seven patch I move over to pixels clip out seven from that location, and keep going. So these are the two. These are in addition to the standard for arguments so the patch size that's going to be the same as that kernel or filter size, right so we want our filter to match the size of the patch that they multiply together so I can actually get a meaningful preview out of that. So to make the right number of weights right previously we had this kind of wake make weights and views function that would automatically you create take all your weights and squish them into an array. And then in our constructor we can create the appropriate size of the different matrices that allow us to perform this operation between layers. So this becomes a little bit more complicated in that we have to deal with the convolutional layers in a slightly different way so first I'll initialize the weights, and I'll just build this matrix of all the weight shapes. So I'll have my end inputs, I'll build this list of shapes. And then first we're going to build the shape of the weight matrix for the convolutional layers in this case we're just going to have one. Right now we'll just assume there's going to be like one convolutional layer followed by and fully connected layers. So shapes will be initialized with the following so you're going to have self dot patch size times self dot patch size plus one. And then it's going to also have the number of hidden layers in there so let's just focus on what this means first so this is going to be self dot patch size let's say it's a seven by seven patch right so this would be. So this is going to be a five by five, just for simple math so if this is a five will have five times five that's 25 plus one, and this one is our bias. So this should be a 26 by whatever the hidden layer size that I'm projecting into. So again remember what happens when we create a neural network I have an input size to that layer, and then an output size of that layer that's going to be you know, n dimensions into n prime dimensions, whatever those values are. So in this case because the convolutional net layer happens on the input. The input size should be the input to the entire network. So this is going to be my patch. So I'm going to take the patch size plus one for the bias. I'll take say a five by five patch string it out into 25 individual numbers, plus a single one more number for the bias, and that's the input that goes into the convolutional layer. Then the convolutional layer should for each of these inputs map it into dimensionality equivalent to the size of the next layer right so this should be the number of hidden units. So I'm going to take say it say I have 10 hidden units. I'm going to take my 26 dimensional input, and I'm going to somehow map that into 10 dimensions, and that's going to be the size that convolutional unit. And then the input size would be the square root of the number of inputs so if I if my number of inputs is 784, then my input size would be 28 business should be 28 by 28 image. And so then, and in would be the input size minus the self minus minus patch size, divided by self stride plus one. So what this does is going to basically tell me for the next layer, how many inputs I should expect to have into each unit, assuming that I'm segmenting my image according to the specified patch size and stride lines, allowing for making sure that I'm taking just the floor division to make sure that I always get an integer. And then I multiply that by the number of hidden units to make sure that I'm getting this that many samples, they go into each of those units. And then for each hidden layer in my end hidden's per layer, except for the first one. I'll then append the n number and number of inputs plus one. And then the number of hidden this is going to be the shape of that next transformation. So from this. This all this part highlighted this all deals with the convolutional layer. And then this deals with all of the, the fully connected layers. And I'm just going to specify kind of by fiat that there's only one convolutional layer. I just deal with it all here, and it specified once. And then I can use this for loop to deal with all the, the full connected layers. Yes. I think this is just basically a reflection of what's in the existing code. So probably, this doesn't necessarily need to actually be here. You probably just get away with deleting with moving it but this is currently what's in the neural network classifier code. It's just leftover. Other questions. Yeah. Can you make more than one layer? Yes, absolutely. Yeah. So for for the task that we're going to do, which is basically classifying basic shapes one convolutional layer is definitely enough for more. Let's just take RGB images for example you're going to have three channels going in. So you're going to want to, for example, some have something that's going to be able to select features, relevant features from each of those channels it gets a good good deal more complicated. So you can see my pooled or just did version of those input features, then feed it to another layer that's going to tease out like higher level features what you typically see happening is like for actual images, you do, you can see in the earlier layers, often things happen that are like, you know, you can see the features that kind of do things like those diagonal edges and basic shapes, then you combine those into more structural features so the deeper you go to the network the more structure you get that's how you can actually recognize the real images of you know people and and and things and real pictures in the world. Okay. Other questions. So, we'll define this make patches function. So what we're going to need to do is, we're out to have some way of automatically converting that input matrix into the patches. So this is going to be very similar to something that we saw in the previous lecture. So I specify is just my input, and then the patch size and then the stride length. So these are the things I need to actually segment the image. So what the trick here is that I am going to be passing in X as a flattened array, right because I need to have some fixed input size and then one initial term for the bias. And so then I need to turn this back into something that can actually be segmented horizontally and vertically. So for X for the shape of X, I will take the square root of that and that's going to tell me what are the dimensions of this image on each side so we will assume that all our images are square at this point, and even more complicated convolutional nets, usually you squish the image into a square shape. It just makes the math easier for arbitrary image sizes. So I'll compute the number of patches this is basically going to tell me how many for this patch size and stride length, how many images I should expect to have. And so then I can use that stride tricks library that I showed you last time to basically compute the actual image patches, you know, for, for every individual individual patch as we shape that into an appropriately sized array and then overturn that. So that now I can move through all of my samples, and then how I have all of my patches accessible for for example. So now that we have the make patches function we need to modify the use function. So, what I do here is, similarly, I just start with the standard on standardized X, I'll standardize them. I then convert those flat and samples into patches to remember that this X at this point is still a single dimensional array, representing the pixel values. Run this through make patches so now I have X patches. And so then I'll run that through the forward path which we'll see next. And this will actually perform the convolution operation over over the patches and then return the, the predictions, I then run my softmax over that last element in the prediction so that's why is here is basically the output of every layer like we've done before it's accumulating this list. So I take the last element of that list this should be the output of that final layer, run the softmax layer over it. This should give me the actual probabilities for every class. And then I just take the arg max to actually tell me which which class index is predicted. So again, if we have 10 classes, if one of them shows up at as 11% that's the highest even if it's not very high is going to predict that. So, always good to look a bit more at kind of at your, your actual probability distribution rather than just relying on the output label. So, the forward pass. We actually have to mine a bit to handle the input as patches. So this also has to flatten the image from the, the output that we get from the convolution layer to feed it into the fully connected layer. So, a common tactic is not necessarily globally useful but one that is very common is to have some number fully connected layers after the convolution layer. In many cases this has been shown to improve accuracy but it does so at the expense of compute time you don't really always need to do this and often just having convolution layers, depending on the task can be enough. Nonetheless, it's very common to see these fully connected layers appended to the end of convolutional net. So in the forward pass what we'll do is, if I'm in my convolution layers, this is all kind of the same right this is the forward pass I have my regular activation or my, my 10 h activation, depending on which activation formula function I specified. So now if I'm in the convolutional layer which just in my example is known to be the first layer, then I need to find each sample into a vector to output that into the following fully connected layer. So that's what this does. So then for the other layers, once this is flattened, then it just gets fed into the next, the next layer, and then the same operations that we've seen before occur so I finally get like the last weights. And so then I can append the outputs of that last layer times that last weights plus the bias this was going to be that final prediction. Okay. So train is actually not that difficult to modify. So what we need to do is actually just create patches from x, because the way they make patches function instruction it's going to have them already in the right shape to feed into train. And so then I'll use that as the input matrix the optimizer calls and now the way this is written. We have this function arguments for the optimizer was previously it was going to be x and t for regression problem. So we have x and your ti or t indicator variables for classification problem. So now this is still classification problems we still use again, we still use t indicator bars. But now the inputs is going to be like the patched version of x. So the way that this is written. This is actually a very straightforward change. So we have to change f actually change nothing. So straightforward. So why do we not change anything in the error function. So what's the error in classification problem. There's misprobability is basically right so I'm just trying to compute a sort of a distance metric how long I wrong I am. This is going to be one of these terms is going to be 100% and I take that 100% minus some probability distribution if it says it's like 93.4, then it is you know what the 6.6 off and all those other numbers are going to be some some number of a distance. So I'm trying to minimize that distance in the convolutional net has anything changed about the classification aspect of it. It's the same right so the error term is the same with what gets what changes is the back problem part right this is the gradient so error is the same and that that final error term is just the the difference in predicted probabilities, but how we actually back problem through the network to account for this convolutional layers is quite different. So you got to do this here, once you move to pie charts we don't have to do this because we still can just use autograd and do you know lost on backwards. Nonetheless, I think it's very useful to understand exactly what's going on here. So, the, the back problem is going to step backwards with layers and you have to add this special case, when you reach the convolutional layers in this case the first layer. Going back to the fully connected layers is the same as we've been doing. But once you get to this convolutional layers you have to understand where exactly the error gets back propagated so if you consider that in a fully connected net. I've got some weight values that sort of live inside those nodes and I want to optimize those in the convolutional net. There's a weight matrix, like that filter is the weight that sort of lives inside the node. And then there's multiples to it gets applied to every patch of the input. So it's not just like a single component. And so you have to allow for all of those differences. So, here, basically the way we end up with that that that delta that's back prompt from the fully connected layer is going to have different values for each convolutional unit. And that's going to result result result from the application of those convolutional units to each patch, because when I apply the same kernel, same filter to different patches and you get to get a different value. And so then the delta that corresponds to each of the applications going to be slightly different. So, what I'll do is I'll sum those delta values, you know by multiplying each one for each convolutional unit by the values in the patch. And so the patch is different. It's going to result in a different output so I'm going to wait that error differently. So, in order to do this I'm going to reshape that delta matrix to the right form done like this. And so then I need to reshape the convolutional layer input matrix to a compatible shapes so I can multiply them together. And then I can reshape that input so this is that input turnings that you multiply by the error to actually get the amount by which you update the weights. The trick here is that because I have now a matrix of delta values. I need to have an equivalently shaped matrix for the convolutional inputs. So now you can calculate the, the derivative of the error with respect to the weights with the convolutional layer with the single matrix multiplication. The fully connected layers just work as they have been. Okay, so the only real trick here is just understanding that you have multiple applications of each patch of each filter to each patch, because the patches are different. The resulting values those feature maps are going to be different from the same filter applied to the different patches. And so then the, the error that you're going to get out of those features that come out of those different patches multiplied by that filter is going to be slightly different. I have to account for that. I do that by collecting everything into a matrix, and then making sure that my inputs are reshaped into the shape that is compatible with that matrix so I can do the whole thing in a single operation. All right, questions about the components of the CNI classifier. Okay. So, what we'll do is we'll in my code that's hidden from you. I'll define the neural network classifier as a new class that will extend the neural network classifier. And so then you can use it as in the following. So we'll do is we'll make some simple images these are either squares or diamonds. So first define my square. And then you can see here I've got a bunch of zeros and then some ones that define the shape of a square. Similarly for the diamond. So you can see here, give up closely and see those ones. I'll draw those images by defining this draw negative image function so I can see I've got an example of a square and I've got an example of a diamond, and these are centered within my within my image frame. So this works fine but right now if these were my only images will be my net would be really good at classifying squares and diamonds in this exact position which is not very useful to us. Right. If I'm trying to detect zebras. I want to take to zebra whether it's on the edge or in the center. So one of the key benefits of CNN's that they're invariant to translation. So all trains CNN will basically be able to pick up features corresponding to an image class whether it's in the center of the image whether it's off the edge, even if it's like a little further from from the camera or closer to it. As long as it's not like you know, so far away you can't see it, of course. Okay, so we're going to do is we're going to create a function that will generate a bunch of images like these are going to randomly shift them left and right and up and down. So I would challenge the fully connected nets, but not the convolutional net because remember the convolutional operator is sort of like having this pinhole camera that scans over the sea and it says oh I see a corner that looks like it might belong to a diamond there. That's important. It doesn't matter the exact place that it's that it matters that I see it. So, I'll define this make images class I'll make 20 or actually 400 black and white images the diamonds or squares and I'll sample a few of them to show. I'll create training data that has sort of randomly arranged squares and diamonds from classification task. So I'll create in this case 100 of each class. And then I will split that into train and test. So we get something like this right so each time I run this I'm going to get a slightly different set. Right. So we can see here. Yeah, so now I get decided to set up images you can see here that we've got your big squares that are nicely centered squares that are in the corner some squares and diamonds that are like tiny and shoved off to the edge. So at least this should challenge a fully connected net and allow me to demonstrate, you know, when a convolutional net is actually able to perform. Yes. So, if you were to introduce one that were cut off so I swear that it's a corner. Yeah, this network to be more. It might challenge this one somewhat I suspected probably wouldn't challenge it a whole lot because if you consider if the square is cut off in the corner. So we still have very square features if my two classes are square and diamond. If you imagine a square and a diamond that are cut off in the corner at the equivalent position. I'll have a square that looks like this, and then there's the frame, and the corner of the diamond might just be like a slanted line, right. And that could be enough basically I would end up with is possibly fee filters that are optimized to detect say, not the whole diamond but just the slanted line. And also then connects to something in the output layer that has a high activation when it gets for the diamond class, for example, so for this example, probably not for a real example, probably, right, especially if you have things that are like rotation, you know, I'm trying to classify animals from different perspectives, or even you know chairs there's a, probably heard of the image net data set it's a common you know 1000 class computer vision data set. And you know some researchers showed that the because the images are nicely cropped and framed, you're sort of getting your in chairs and cups and bottles and things, but they're all like this canonical pose. And so if I take a picture of a chair from like up here, all of a sudden, that doesn't show up an image that's an image that trained network doesn't recognize as a chair because it doesn't, it doesn't have the features of a chair. So cases like that you want more, you want some combination of more sophisticated network and maybe just like better training data, but in this case, if that were in the training data, it probably would be okay at that yeah. You can. There's a couple of ways to do this. So you can have an additional channel that say if you have depth data for example, that actually operates with the depth data so some like the gesture recognition stuff that we've trained here at CSU. They use the depth channel for that. One of my students actually defended master's thesis yesterday did a thesis on convolutional nets over 3d geometries. So actual meshes. And there's some very interesting properties of meshes that you have to account for to make them invariant for CNN. And they're still it's still challenging. And then also there are 3d CNNs. So that's the instead of a 2d convolution actually have a 3d convolutional operator. And that can be used for both 3d data and also say like multi channel like video data you can have the RGB channel and then a third channel that is like time. So you can actually look over like multiple frames. But these are way more compute expensive. Of course, the moment the moment I add another dimension. I'm basically going from like, you know, x to x squared to x cubed. And so every additional pixel basically, you know, I have to process that three times. Yeah. If I trained it on black on white and tested on white on black, it probably would have a lot of issues. Yeah. So again, we run the same issue that your training data must more or less resemble your testing data. So if I trained on images look like this and then I inverted this and used as the testing data. I suspect we'd have a lot of problems. If you trained it on both, then it probably would be able to accomplish classification of both, assuming that the network size is big enough to accommodate the filters required to optimize well enough to accommodate both of those features. So I suspect again if we had like the square diamond task, if we did that sort of by color version might have to increase the size of the hidden layer a little bit but probably wouldn't be too much of a big deal. Other questions, good questions. All right, so now we've got, we've created our CNN we've made the requisite updates to the patching the forward pass the use function and the back propagation. We've created training data that would challenge a fully connected net that is probably not going to challenge a CNN. So now we can actually try to train this. So our net has been defined to accept these two dimensional input matrices we need to flatten each image. So first what I'll do is I just look at this look at my some my train samples. So you can see like I've got 10 squares here. Let me take a sample inputs for train and test. I've got 200 train samples to test samples. These are 20 by 20 images each of them can contain 400 pixels. So what I want to do is flatten those. So what I'll do is I'll try two units of the convolution layer followed by one fully connected layer of two units. I use patch size of five and a stride of two. So what are my classes and just run NP unique over T train. So I've got two classes, diamond and squares, we have two output nodes. And so now what I will do is I will import my neural network. I will track how much time it takes to train. And so here, this is X train dot shape. Right. This should be that 400 number. This should be my flattened input size. These are my two hidden units and these are my, my output size. And then I specified my patch size and my stride life. So I'll train for 2000 epochs of learning rate of point one using Adam. Let's go for a minute or so. Not even that. And this is, this is our results. So took four seconds to train a perfect accuracy on the training data and pretty close to that on the testing data. So what I can do now is I can look at my individual samples and see what the probabilities are. So, these are the, this is what I predict. So class zero, these are squares class one, these are diamonds. This is the probability for each and then the blue line is the actual ground truth labels. So we can see this basically one sample in fact looks like the very first, maybe not very first note is the very first sample appears to be the very first sample and it got it got wrong, but got the rest right. So of the 20 testing samples, you got 19 correct. And one of them, it appeared to classify a, a square as a diamond with slightly above 90%. But in all other cases, it got it correct and the most, the one thing that got closest to being wrong was this one where I thought like it's about 19% likely to be a diamond and it's actually a square. So pretty good performance overall, it's a simple task, but this demonstrates, you know what a convolutional neural map can do. All right, so now let's see what our units actually learned, and I can do this by drawing images of the rate matrices. So first I'm going to look at the shapes. So I'm going to look at 26 by two. So this, this should be a five by five patch plus the one bias weight. So I'm not going to visualize the bias weight, not least because I can't turn this into a filter, I can't turn a 26 sample array into a square. So I'm going to lock up that bias weight reshape it into two five by five rays one representing the first unit one and the second unit. And we'll see what it actually learned. So. The square looks funny. Do you think there's simply something that it looks like a diamond right. Do you think it's actually reflecting the fact that it learned the shape of a diamond in that configuration. No, I'm not. Yeah. Right visually, and especially in this case because remember the diamonds are like are randomly distributed across the frame so this is just sort of coincidental it might be because of the weights the way the weights were initialized or other factors of the training. But let's not be misled into thinking that because we see the sort of diamond shape here that this is a this is necessarily like the unit that has a higher activation when it sees a diamond. It might be. And maybe there are a high portion of diamonds that are nicely centered from our maybe just got unlucky with randomization. But it's comparatively unlikely. Sorry, he was calling me. I'm not going to be funny again. Like this time of the day like my undergraduate university that calls and asked for money. Anyway, see you see we'll do to you to him for. Anyway, so what we can see here is that although we see a diamond isn't necessarily indicative of the fact that that filters like learning the diamond features in this configuration. So this is the weights. So this is the, the first unit this is this is going to be this one. So you can kind of see let's take like this first value this is like pretty low this is negative point six, whereas these two on either side of it or point five three and point five eight. So you can see that this corresponds to the values we see there so each hidden unit contains this five by five pixel filter. And then the individual pixels represent the value, you know, in the in that matrix. And unlike the last lecture where we had this very specific kind of edge. These units have been trained at the same time to rep to optimize for both squares and diamonds of different shapes in different places in the image. So then also, it's the it's a five by five filter over like a 20 by 20 image. And so as the as we move across those, those patches, these individual values are going to reflect some sub segments of the image. Okay. And so they do like pretty well at this task, but they only resemble a square diamond features visually. And if we train a CNN to detect dogs, would you record would you expect the filters in that network to visibly resemble dogs. Not really. Sometimes you will see when you say take the filter time the inputs times the filters the feature map not the filter itself. And you actually see like high activations on like features of a naturalistic input image so you can see if there's a filter that appears to activate more for certain types of edges you'll kind of see the sea of an image of like a German Shepherd or something in places where maybe there's like a diagonal edge for a certain type of filter that activates with that type of feature and you're sort of see that it matches those those features in the image. But as you get deeper and deeper into the network, those visualizations become like more and more, you know, obscure and opaque. So, kind of it's easier to visualize, maybe for those more intuitive features in the early part of a convolutional net but much not not so much later in the net. So, generally, these images these filters can be optimized for multiple things so both of these filters are optimized for squares and diamonds, you know, maybe some more than the other, but doesn't both at the same time. And that's generally true for all filters in a convolutional net they're all optimized for all classes to a certain to basically varying degrees. So you have some that are going to be more optimized for certain classes and less optimized for certain classes they should all activate a bit. At least like a non zero amount or mostly non zero amount, depending on what they're mostly up there most optimized for. Okay, so now let's repeat. We'll use for convolutional units and to fully connected layers after the convolutional layer so here we have using my four units and the two fully connected layers with 10 units each. And this time I'm going to train for 1000 epochs instead of 2000. I'll use Adam again at the same running rate. And so if you remember the last one took four points on things seconds. This took 5.8 seconds even though I trained for half the number of epochs right this is because it's a there's one there's twice as many convolutional units. There's also these two fully connected layers it's a bigger network there's more back propagation do this more operations. The test percent is still good, but it's actually not as good as the previous one it's 98.5 instead of 99.5. So the only thing that's missing is those probabilities look something like this. So there's some samples. It still seems to get that first sample wrong. In fact, you got it more wrong in terms of the actual probability of the incorrect class. And then there's two samples of diamond that is also getting along. So the training converges faster, but the generalization of the test data is actually not as accurate. I would say that you know perhaps is like overfitting to something in the training data. And that's quite likely in this case because the task is so simple that the bigger your network gets the more likely it is to learn kind of serious correlations in the data. So we will visualize what the four convolutional units learned looks like this. And then once again, you know we can see that there's no clear correlation between the actual physical shapes of the input images and anything that's being learned in these convolutional filters. So you can see, you know, where they're like for example in this, in this filter, these two pixels here represent individual values that filter that tend to have high activations, kind of regardless of what they're looking at. So that's a good probability to what's in this data. And then this one has like a high activation with, or is as a high value in the filter just like right in the middle, but not a whole lot of like visual correlation. So let's see how a fully connected non convolutional neural net would do. So, let's look at sort of the non patched version. So I go I to 200 samples training test. I'm using my CNM classifier I'll use my normal classifier. So you'll notice that I don't use the patch sizes and input I don't use the stride length. And also because this is an instance of neural network classifier not the CNN in that in it function it's not going to call like the make patches or anything. So the way you've written this code is that in the CNN, it's set up in a way that will automatically do the patching for you if you instantiate this type of networks I don't need to do that pre processing of the data before I instantiate the network and does it for me by virtue of the type of network that it is. So, if I have, you know, we run the convolutional version first and then I'll run the fully connected version. So, this is four hidden units and then to to fully connected means 99% test accuracy. If I do a fully connected net with the same, the same network architecture. So, one, you'll notice the trains a lot faster, right, because this the, the convolution means there are more operations that have to be done because I have to apply every patch every filter for every patch. So trains a lot faster, you know, two seconds versus five ish, but test fraction is only 83% as close to 99. So, that, and it looks like kind of a big mess. Right, there's a bunch of squares that are being misclassified as diamonds and a decent number of diamonds that are being misclassified as squares. So, what did our filters actually learn. So, we have this. So now take a look at this. So, what do you think it's tried to do here by looking at these filters. So, remember, our water water inputs look like we've got squares and diamonds randomly scaled and randomly moved about the frame. And it tries to optimize for all of the data at once. Right, that's what a neural network does. So, let's take a look at some of the things we might observe, for example, we might see, you know, there is sort of a line here of high values is also one at a similar position in this other filter. So, it might be that for example there could be a lot of squares that are in this region or some there's lots of horizontal lines in this region in the training data. And so it's learned that, well, there's probably something here at this location in the input. So, if I optimize to detect that, I will tend to get a higher accuracy so it may have learned effectively to look for a certain type of feature at a specific location in the input. Similarly, you might observe that like there's a diagonal line like here, and maybe another one there. So you can see some sort of diamond-esque features here. And so this might be another indication that a lot of diamonds happen to occur at approximately this location in the training data. And so it tried to optimize for that because by predicting that it got a better, it got less error and a better performance. If it doesn't occur in the test data, or don't occur in as significant a proportion, then it's going to incorrectly predict things that may be a square that has a pixel at this location is going to go, oh, well this is correlated with being a diamond in my training so I'm going to predict that I'm going to predict wrong. And so the variance to translation is one of these key features of the convolutional net that a fully connected net is not going to pick up on because it's trying to learn everything it can about the individual pixels and the data. And it generally just doesn't do a very good job because maybe if I had a bigger network, it would do a better job with a simple task like this. And generally speaking, it's not going to, there's not enough space in the network to represent the information at a per pixel level, where the actual position of the pixel actually matters. Any questions. So the, the fully connected net, you know, tends to overpip to the straining data. And so, but it tends to it fails to perform as well in the testing data so there are basically a few things that a convolutional net requires so it still requires fixed size inputs, like your fully connected net. But the reason it can do in variance to scale is because there is in, there's an imposed ordering on the pixels in that the neighborhood of a pixel is the pixels on either side. So you can actually learn basically relations between pixels at certain points and pixels around them regardless of the absolute position inside of the image. And so then there's also this, or this is this implicit ordering and then there's the neighborhood around the actual pixel so this allows us to use the striding off the striding operation the convolution and that filter to segment images according to the actual position relative to each other, regardless of the actual position inside the image. Okay, any questions. Yeah. So are we not considering the lower values like the black spots on the left side of each image. Yeah, they're pretty similar and they're probably close they're pretty close to zero I think, if I'm looking at like the edges. And in this case, this is also probably learning you know correlation with with the data in that we. We actually don't allow images to go off the edge for example. So, it's very likely that that first pixel on each side is empty in most samples. So it learned effectively that there is zero correlation or close to zero correlation with the edges and the class, because at most you might have like one pixel or one row or column of pixels if it's a square in that edge, but otherwise it's in the vast majority of samples it's an empty space. Okay. Other questions. Alright, so to summarize, convolutional nets use fewer overall weights, but they learn more generalizable matrices or filters. So we can say that the filters don't necessarily match the input features visibly, but we can train them to recognize multiple different types of features in sometimes very diverse input sets. And we can recognize them at the same time so these can be squares or diamonds straight lines diagonal lines curves, you know, as the network size increases the capacity for learning more types of filters also increases. So what we need to do is we need to take the image segment into patches stride over those patches we get a significant amount of overlap between the patches. And so now these filters can learn the difference and similarities between neighboring patches that have slight differences between them. So after you standardize your input values. So these, whether these just like intensity, or in RGB they're just color. So you're going to standardize them into a distinct range, convert that entire matrix into patches we have a function that does that for you. So you can use you do this so that your input is of the same format in both both functions. So now the input to the forward pass is the patched version, not the raw input. And so then if the outputs of a convolution layer is input to the fully connected there you flatten that into a single vector so now just becomes basically a feature representation that goes into the the fully connected layer. So the hardest part is back prop. So because we have multiple applications of each individual patch and each individual filter. And so those delta values are going to differ depending on the different features in the patch. And so there we're going to have like multiple values for every convolutional unit. And so that has to come from the application of each unit to each filter so we sum that from each unit to each patch, we sum that for all the delta values. So we have delta matrix reshape it into K by n, where n is going to be the number of units in the convolutional layer that you want to back prop. What's the other thing we need to multiply. So we have the error, we've got the features and we have the input. So then we reshape that input matrix to n by P, where P is the number of values in the patch so now K by n and n by P will multiply together to give me K by P. So P will give me the gradient of everything but the bias weights. So this is going to be the gradient associated with K classes, or K outputs, and then P patches. So then I'll sum every column that reach a data matrix delta matrix and get the gradient bias weights. So you compare this to backproping through a fully connected layer where all you need to do is multiply the inputs to the layer by the delta. And then we have no doctor matrix to reshape just open single values. So, final questions. Okay, I'm going to take you at your word that you don't need to see me in office hours. If you do I will be there but I'll be writing, but I will give you back 30 minutes and I'll see you on Thursday.