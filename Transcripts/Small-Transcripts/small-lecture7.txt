 Okay, let's get started guys. Your screen share. All right, so I'm going to tell you a stupid joke that I just heard. So everybody here speak French. How do you say the letters GPT in French. What does GPT mean in French? It means I farted. Okay, so basically if you do cat I farted in French, you get shot GPT. I'm going to say professor Louisville. If you go to my office hours, you see the meme on the cork board. It's like the Drake meme and it's like cat GPT. So I didn't realize this. Another did my wife, even though she didn't speak French, and has seen that meme. All right. Anyway, so now I'm going to tell you about talking about the dude named Adam. So let's go do that. So just a reminder that Assignment one due on Thursday. So if you do need an extension, I will need your reason for an extension by the end of the day tomorrow. And then I can get back to you. You can send it to be earlier of course, but then I can get back to you with with either the the ascent or a counter offer or denial if your reason is just bad. But so make sure that if you if you have that, you make sure you get that to me soon. Again, extending circumstances if you do get hit by a bus before class on on Thursday. Yeah, let me know. Right. So that obviously I can make it adjust accordingly. Again, once a reminder not to throw yourself in front of a bus to get an extension on a one I believe these the subsequent assignments are a good deal harder. So don't throw yourself in front of a bus for those either though. Alrighty, any questions about hope you all have made some progress in the assignment. Yes. I have a question about the assignment. Yeah, it's fine. So what the reason I have things like that there is just because what I'm most concerned with is basically are you able to do meaningful analysis I don't really care whether it's long or short some people are really good at doing like very dense but dense analyses and like they have you know, five sentences but they really capture the gist of what they've done in those sentences. But some people if I just say that some people tend to flail there are certain students who really do need a kind of guideline. And so that's why that's there is like you know, if you if you need a benchmark of where enough where you've done enough, you know, here's a reasonable I think if you've got this many sentences, it suggests that you've at least been able to put reasonable thought into it. I will of course read it to make sure you didn't just feed it to show you a bit to give me your answer. And because it is also good as generating you know, fluffy sentences on end. So make sure that you're of course writing your own sentences but that's what I'm really looking for is just strengthening our process and whatever it takes to get there that's important. Okay. Other questions. So you're talking about Adam. So you remember Adam from last week, Adam, he is a gradient descent optimization algorithm. So remember that Adam is not not actually an acronym it's short for adaptive moment estimation. Yes. Oh yeah sure. How's that. Okay. Yeah, so Adam is short for adaptive moment estimation. And it has the benefits of being straightforward efficient with little memory requirements. And it's very friendly to this method that we've done is basically stacking all of your your weights together, so that you can perform optimization on all of them at once right this helps in our in the efficiency of the computation. It's also appropriate for things like noisy or sparse gradients, and also the hyper parameters have an intuitive interpretation. What is that interpretation so moments in this case is moment as in momentum. So if you think about moving down the slope of your gradient if you're very far from the optimum. So if you're trying to reach the bottom, it may be wiser to move fast. Right, so if I can figure out that I probably have a long way to go, I'm just going to move quickly, because I can I don't expect to reach the optimum any point soon and so I'm not likely to start skipping back and forth over it so what I'm trying to get to with SGD. I'm just taking these tiny tiny steps with the goal of reaching the optimum and not not going, you're not going back up past the optimum. So this allows you to be a little faster. So you can basically take larger steps when you're far away from the the optimum, and then slow down when you're closer to the optimum and do it in a smart way and not just having say a decay and learning rate where you start fast and are deterministically moving slowly. Remember that the two key variables in Adam are the first moment and the second moment so this is basically the average of the recent magnitude so that is, if I've taken big steps recently, then it may be more reasonable for me to take a relatively large step now. The second moment is also like what's my rate of change over these recent samples so if I've been if I've not been changing probably not good and means I'm probably not approaching the optimum. But if I've been changing relatively little, it means that I can still take big steps and I don't need to really need to worry about slowing down anytime soon. If your first moment values are still large yeah. They don't. They don't. They can get stuck. They can get stuck. So this is why. So the question for those of you zoom how do how do these algorithms determine distinguished local from global minimum and the answer is they do not because this is where the data set balance is really important so one thing that happens commonly. I just didn't ask about this this morning is that if your data sample is way over balanced. And you have like if you're a classification problem you have three classes but you have 10 times as many members of class a compared to class B or C. And then your criteria is just like, if you're in training is just accuracy or something. It's really easy to basically say well I can't get a better answer then just classifying everything as class a it's going to give me 96% and I can't really do much better than 96% for class for three way classification tasks so why not just classify everything as class a if I have 1000 samples of class a and 20 samples samples each class B and C. Okay, so this can happen and gradient descent or Adam or any optimization algorithm will happily let you do that. So, Adam may have the benefit, as we'll see when you look at the last charts of sometimes allowing you to basically skip out of a local minimum and then climb that hill and then find basically a steeper path to descend down to a global minimum. So there is some benefit to basically allowing it to sort of overshoot what might be a local optimum and SGD tends to especially for learning would be very small, it'll find a local minimum to stay there because it can't go anywhere fast enough that lets you get out of that local minimum. Okay. So this leads me to sort of how do we, and how do we determine the quote speed at which I move through the gradient at every optimization step so if I've been moving very very fast. I can probably continue moving very very fast that is I can take bigger steps. If I've been taking big steps over say the past four training iterations, but the steps have been slowing down. So my rate of change is now going, you know, my, the derivative of my, of my step length basically across the gradient is declining. It may serve me to still take a relatively large step this time but maybe make it a little bit smaller. Right. So, what this suggests is that the gradient I'm moving along the gradient away this suggests that I'm still moving along a steep slope of the slope is getting slightly less steep. So maybe I am getting a little bit closer to my minimum and so then I should probably slow down a little bit so that I'm less likely to skip out of that, that bowl, right that minimum. Okay. So naturally this is closely related to the concept of moment along momentum and physics so basically we're looking at the distribution of values around around a mean or variance right so the probability distribution of these values that you're sampling at each step. Okay, so what we have with these tunable parameters, these beta values. And so this is going to be a decay rate for the first moment estimates, and then beta two is going to be an exponential decay rate for the second moment estimates. And then we also have a constant here to make sure that we are not divided, per main division by zero. So we still have a learning rate right to this alpha row. And so the learning rate that you specify that's a constant. So I'll just say, I'm going to do Adam with a learning rate of point zero zero one or something like that. And that value is not going to change. Unlike I can say some reinforcement learning algorithms. I've been doing that for a few months. But here I have these other values that will say, I can perform some some calculations over my first moment and second moment estimates that is sampled from the previous end training iterations to determine how I need to adjust my step size so my step size here is not a rigid constant rather something that I can tune a little bit based on these values. Okay, so any any questions on the atom recap and I'll go into the implementation. Cool with that. Alright. So to show this. Let's do what we have done. And I'll just kind of work on a dummy example. So I will make some training data this is going to use very much the same, the same formula that we use in notebook five this is basically the same function so I create some nonlinear functions. So I have some nonlinearities here by trying to have it model some variant of a sine function and making it difficult by adding some noise. So now I'm just trying to get my optimization to approximate this not your function. I'll make some testing data, the important things that they resemble the training inputs. So now I'll create my add ones column, or my add ones function that this will do is just be a generic way of adding that constant column of ones. And I'll specify my hyper some of my hyper parameters for my neural network in this case. The thing that I'm looking at is just what's the number of hidden units in my layer so I'm just going to assume I have a single hidden layer. And I'm going to create a number of units in that that will be that will allow me to introduce nonlinearities. Alright, so now if I create 10 hidden. So I'm going to look at memory of two sets of weights, the end of the year, right via the hidden layer weights I take my inputs, multiplied by those things. And this gives me some scalar values that have been applied on linear function to those now nonlinearized scalar values, then multiplied by the weights in w the output layer weights to actually give me the output values, this is still a regression problems I'm just trying to fit you to, you know, scalar values to scale their values. So now if I look at, you know, take the, the shape, basically just do prod of the shape this will tell me your two by in this case 10. So 20. What is NP dot prod. So this is just returned the product of an array of elements over a given axis. So in this case the array of elements was two and 10 is just the shape. And of course there's only one axis. So now what I can do is just confirm this is correct by just checking these two are equal right one plus one times and hidden wise at one plus one. Well we have one input. So this is my input value and x, and then I have the constant column of ones. Right, so there's going to be two inputs, one of which is going to be a bias, that's going to have a bias weight trained against that, and then there's the actual feature value is going to have some weight training against that. So I'm going to set the other parameters of my neural network so and hidden, I'll set my, my learning rate in this case to point 01. Then I can also just scale the learning rate in this case by the number of samples times the number of outputs. So you can do this to kind of optimize the, the learning rate before you start training to what you think might be at least a value in the appropriate range for the amount of data that you've got. So now then I'll initialize the weights to uniform the distributed values between, you know, these are normally distributed between negative one and point one or negative point one and point one. So now let me print the current state of my neural network. Right, so I've got a learning rate in this case point zero zero five seems like a normal learning rates kind of in that that range like, you know, one one hundredths down to 10 to the negative five or so. So typically, typical learning rate value. And then here my values currently of the NW right these was randomly initialized. So they don't really be anything right now. And they're also certainly not optimized to this to this function. So if you remember the pack function from earlier in the notebook on Thursday what that does is it will take my different weights from the different layers and pack them all into a single vector, so that I can apply optimization operations over all of them at the same time. And then we have the inverse of that which is the unpack function, I just have to supply the appropriate shapes that it won't unpack those arrays into, and it will give me the actual arrays back. So now I'm going to set my number of epochs or train in this case for 100,000. So I'll take these that this many steepest descent steps in the means where an error function. And then I'll do some sort of collection for plotting. So, finally, this is the meat of the operation here so this should look familiar to you already so we have our inputs x one multiply that by V apply a 10 h function to that that gives me Z and you to append and put a column one to the front of that again. So I'll do that by doubly that's going to give me my final output, and then I calculate the error, right just my ground truth target minus my actual predicted value, and then use these in the backwards past so just a reminder of the pieces of this operation. So Z is 10 h over the input, right this is going to be the derivative 10 h over the input. And so then the error. This is actually going to be the error term for this for the sample. So I'm going to transpose it to make sure that my weights are in the right in the right shape. And then the gradient in w is much simpler this is just going to be the input Z input to that layer times the error. I can pack my gradient values into a single vector using this, this function, and then w which are my packed weights, I can just optimize all of them at the same time by subtracting row times the gradient. And here is I've computed the appropriate gradients for the different layers separately. Right so I've done gradient of the ingredient of w. So, then what I do is I pack them into a shape that's going to be the same shape as all the weights, right, have basically have a gradient for each weight. So the gradient arrays are going to be the same shape as the actual weight arrays when I pack them they end up in the same shape as well. And so now now I have a single array I can do a single operation over them. So this allows me to be much more computationally efficient. It'll create error traces for plotting and the rest is just by plot. So, stick with this. And now we can see it start to converge. Right so you can see the train and test RMS E. So the train has converged down to a pretty low value and test is slightly above that. And after 10,000 samples. Basically you can see that we optimize entirely, you know, pretty early on. Right and then there's not a whole lot of improvement going on there. You'll notice a couple of things that's pretty subtle in this but you can sort of see little hook here at the bottom of this graph right. This is one of those cases where, due to the Adam. We use that here. This might be. We didn't actually use that here. Sorry, my bad. We're going to get to that later. So now we look at ignore everything I said like past 30 seconds. I'm going to go back to that in a moment. This is actually just a weird. This is not nothing to do with the gradient is like a weirdness in the plot looks like. So what's, what's the next thing here. So this is just going to be my actual predicted values and then according to the model. This is what it's predicted so it's maybe not very good. And then these are the hidden outputs for each unit right so each one of these represents the output of this unit. Depending on what the input is. Yes. This is just one layer. Yeah, this is currently using a single layer with 10 hidden units. All right, so now we're going to repeat this training loop using Adam this time. Previously, this is just sort of vanilla SGD as we've learned it already. And then the, the version with Adam so differences here. So same neural network right still 10 hidden layers or sort of 10 hidden units one layer. I'm not going to specify these other values so small epsilon just to prevent dividing by zero, right tiny number. So we have these two beta values. So let's just go back up refer to what beta one beta two are again. So beta two is the exponential decay rate for the first moment estimates, and beta two is the exponential to carry for the second moment estimates. So in this case, these values are set are set to be point nine point nine nine nine. So we can see that effectively I'm going to have a kind of a 10% decay for the first moment estimates, and then a really really small decay for the second moment. And row here I'll just set to point zero zero one. So, same thing as before, I'm just going to initialize my weights randomly. Now I'm going to specify these two other things. So, mt and BT and then beta one t and beta two t. What are those we'll see those in a moment. So these are going to be bias corrected moment estimates that I'll use to basically update the those beta one and beta two values. So, up until this point, look at the highlighted code. This is the same. So, standard operations input times weights supplied 10 h at a column of ones, more for that by output weights that gives me the value, then take that value subtract it from the target because we are use those error values to make the Alright, so now I need to approximate the first and second moments right these are going to be estimates about how fast I've been going down the gradients so far. So think of it as like, you know, momentum and acceleration or something like that. So, what I'll do is I'm going to look at the gradient. This is going to look at the gradient, the error gradient with respect to w, because I want to look at how fast I'm kind of getting down. Or services is all of them together. So, maybe when we look at how fast I'm moving down my my gradient defined by all of my weights. And so what I'm going to do is I'm going to take beta one times empty so the kind of existing movement estimate moment estimates, and then incorporate the decay value. So remember when the first moment the is kind of the, the mass, quote unquote, sort of the center of mass the center of the public distribution, and then the is going to be the second moment of the variance. So I'm just going to take the square of my gradient, and then I will use the beta two value to optimize that BT. So I'm going to use these beta one T and beta two teas to basically correct for bias. So this is going to be so beta one T is just going to multiply that value by the currently the pre calculated, or the set value beta one, do the same for beta two, and then m had and v had or corrected estimates. So in this case I'm going to take empty divided by one minus beta two T and analogously for for BT. So now how do I actually perform weight update so the formula slightly different here. So you still have the same components, except instead of the error, I'm going to be updating based on these moment estimates. So the measurements are kind of not averages but derived from the previous end set of updates. So again if I've been moving really really fast my error is really really large. I can justify taking a bigger step. If my error is pretty small, it seems to be moving that direction, I should take a smaller step or I should take at least the last big step. So what I'll do here is I'll take m hat divided by the square of the hat and epsilon just in case the hat is zero for some reason. And then I'll multiply that by row, and that's going to be the amount by which I update all my weights. So, so in the error trace, and then I plot, I create some testing test data and evaluate it using the forward pass just in a single line so here. Remember, so X test one. That's our inputs testing inputs multiplied by the that gives me Z, or, let's just say a apply 10 h that to get Z, add ones to Z. So this is going to be the same as the one above multiple that by W that gives me why. And then finally, just plotting as before. So let's go. So what do you notice, comparing this with Adam compared to SGD, but the law's curve yeah. Yeah. Yeah, I didn't really they they sort of, I mean, they individually converge right this and they sort of become a plateaued a value, but it's maybe not quite as neatly aligned right we see here. The test armacy is actually lower first in the train, and then around 10,000 epochs they tend to switch. What else do you notice. Yeah, we've already. Yeah, the train here so I mean we we may we might be overfitting to the training data a little bit. But what, what did you just notice about the shape of the curves. The Adam one is much more. Give me word abstract, or just I was going to say just like bouncy or something like that. See, it jumps around a whole lot. In particular, we see that it's not. Usually with SGD you see a nice curve from a high value to a low value and it just sort of reaches some minimum value and tends to stay there. Right. With Adam huge drop, slight drop, bigger drop. Oh now we're going back up again, go down again okay now we start climbing. It sort of looks like, you know, when I'm running a marathon I like to look at like the elevation map beforehand and this sort of right here looks like that part that was put like mile 29 or mile 19. That's just like hell because I'm two hours into a run and maybe climb the hill. So basically what's going on here is that this is skipping out of the gradient. So there's some minimum that it's encountered here, and it might be a might be the global minimum I feel local minimum. And it's moment estimations are such that you're multiplying that by the step that you're taking across the gradient. We say move down the gradient because the goal is to reach this minimum, but the gradient is just some some surface in multiple dimensions. And I just sort of keep moving in the steepest direction. If I move in the steepest direction from where I am now, if there is sort of, if I'm moving in this direction there's suddenly a big hill, or maybe just a little hill. So let's say my slope here is very steep so I'm moving down it very fast. And then there's a slight upward hill in this direction. So my step size is sort of from the tips of my fingers, then, and I'm starting here where my left hand is, then I'm going to be taking a big enough step that basically lands me over here, even though the minimum is somewhere here around where my belly button is. So effectively by taking a step that big I have kind of skipped over that minimum. And what Adam is hoping to do effectively is that it's hoping that maybe this is a local minimum. And so by moving past it, I can get on a trajectory where somewhere further along I will find another minimum that maybe will lead to the global minimum, if one exists. I wonder what seems to be happening with this data and if I were to run this again, even with different numbers. This might well change is that there's a minimum in the gradient of the training data that we are able to find, like we get practically down to zero here, and the fuzz here at the end suggests like, maybe we're taking really small steps back and forth across some global minimum. For the testing data remember that every the gradient defined by every data set is going to be different, even if they resemble each other. And so the minimum for the training data that we've calculated is maybe not the best one for the testing data. But perhaps the minimum, the best minimum for the testing data is back here somewhere and we actually kind of found it, but it, the training data was still somewhere else to go. So perhaps it was not actually able to find that. So, Adam has some advantages and disadvantages. But you know in this case, if you look at where our error curve is relative to the training data it's doing really really well. So questions. Yes. So if you wanted to hit assuming that that did. Yeah, the global. Yeah, to kind of change this to better hit that dip and not bounce out of it. Yeah, so you can you can change the betas you can change that exponential decay rate you can change a number of things right you can change the betas here you could change the learning rate itself, right how much am I scaling the whole thing by maybe I just this data is that I should be taking smaller steps overall. I can even potentially just train the number change the number of training epochs. So like, I only I can only see the step to the fact but it sort of seems like maybe I hit that minimum at 10,000 maybe I should stop training there. You can use some techniques like early stopping your patients, where I can see if I use say my validation accuracy as my criterion, I can sort of say, if I don't see an improvement in my or validation law or error in this case. If I don't see an improvement my validation error for like 10 epochs or something. I'm going to say, I'll stop now because I probably not going to get any better than this. So the number of techniques you can use the problem with this is that this dip here is in the the gradient for the test. And I'm just plotting this for comparison when I'm training I do not have any notion of this. So I'm kind of trying blindly I'm hoping that training against this data, I'm going to be encountering test data that resembles this closely enough that this will be a good model and here, even though we kind of generated from similar formulas. It seems to, you know, be a decent feeling it's not like the RBC values are huge or anything, but maybe not as good as it could be. Other questions. Yeah. Yeah. Yeah, so basically here, we're looking at this. This is in case the hat ends up being zero so that is your movement, this is the variance of your, your second moment estimates. And so when would this be zero, this would be zero if the past and moment estimates that I'm sampling are all the same. Right. So this would indicate a couple of problems one you're not converging your sort of you get you've gotten the exact same error value for the past and epochs. But that might be the case you may sort of plateau for a bit and then, and then sort of find your way out. So in the case that like, you're going to be plus epsilon and going to be dividing by this but let's let's assume this is zero. So this was zero mean that I am kind of getting I've been getting the same error for the past and epochs. So what I'll do is I'll add some epsilon and then divide by that to kind of hope that I'll the first moment estimate can just be enough to sort of shake it out of this this run. This can allow this can cause us to kind of bounce out of that gradient. So for example, if this value is too small, you might end up sort of taking a big leap. It's like I'm not moving. I'm just going to like take a leap of faith, and I'm just going to like jump way out there, and hope I land on a favorable part of the gradient that was a kid you know, so you don't want your epsilon here to be too too small in this case, because you might have situations like that. You don't have to be too too large of course because then you know you, you, you want don't want to just you cut your your moving estimate down to nothing, because that also would stop training. But then you also don't want your, your step to be you know too small to be effective. Again, this is sort of one of those hyper parameters in the Adam paper. They found kind of what they found to be best values for the betas. And I don't recall if they found like a best value for the epsilon if I if they did I should have written it down to get to the part of the notebook. Other questions. Okay. So, finally, let's take a look at this if you remember the model for SGD was sort of just a straight line. Right. It's the green line is just kind of straight. So it sort of optimized. It didn't really make use of the non linearities available to it, to be quite honest just sort of optimize straight line through the data that approximated the correct slope. Here we can see that the blue line is the training data. The green line is the model that follows it very closely, which is also witnessed by this very low RMSE value at the end of training. So it's doing a really good job of fitting to the training data. And it's, it is getting the general shape of the curve of the testing data but not as well. Right so you mentioned it we're fitting and here this might be a case where we are fitting very very closely to the training data and not fitting as well. It's not like it's a bad model. This is a pretty simple case. It's still getting the overall shape pretty well, but if I'm looking at training versus testing, then it's definitely doing a much better job fitting to the training data than this to the testing data. And then finally here are the, the actual hidden outputs for this. For the sample so. Okay, questions on on Adam. What other people are like my friends, but really, um, yeah, so you. Well, I guess the obvious one is like it's simpler to implement so just getting started. I would suggest mastering SGD before moving to Adam. Also, it does have SGD is a little bit more deterministic. And then assignment to, for example, I believe you were asked to compare and contrast SGD and Adam and you'll see like the Adam loss curves you get these kind of more stochastic things where they bounce around a bit more, meaning that you may have these kind of spikes, where you are popping out of some minimum, and there is a small chance that you sort of get off on a wrong track, and your training actually, you know, will sort of collapse after that point. So, you know, you get somewhat more reliable, assuming all other conditions with regard to the quality of your data or true. A bit more deterministic, either to implement has fewer moving parts. So, you know, one of these things, these are things that you can all try, like when you have when you have a different different types of data you can try you know different types of optimizers. And the various variants are extraordinarily popular. And so basically, the, everything is built on an SGD backbone, but SGD is kind of, it's a bit pedestrian right everything goes really slow, you're just taking very slow deliberate movements along this gradient. And so it's not good for really big tasks. Adam will allow you to converge faster at the cost of maybe a little less predictability and some risk of kind of going off into the woods and your training failing once in a while. Yeah, other questions. So, you know, Adam variants are used everywhere. So most of the large language models are trained using something called Adam W says Adam with weight decay so you actually specify a value where a weight has been updated in the long time it's value it's kind of assumed not to be important. So then it's value will kind of attenuate. So I guess the corollary of that is basically the larger model you have for a given task, the higher probability there is that some subset of those weights are just irrelevant. So if I have a trillion parameters in my modern model to do, you know, diffusion or language generation or something like that. I probably don't need all those billion parameters some 10 million of them might be incredibly optimized or could just as easily be zero and really wouldn't change the performance. So this allows us to that property sort of allows us to do things like fine tuning where I can assume that for a different task, maybe I can use those weights that aren't really being used and better optimize them for task performance. So, these different optimization techniques allow you to kind of leverage different properties neural networks. So, it's now 237. So I will start the next notebook which is going to be on finding good parameters. So controls go to number seven. All right. Let's start this. So optimizers data partitioning and finding good parameters. This is notebook on a couple of different topics, all of which are going to be important for doing assignment to which I'm currently planning on assigning Thursday. But basically optimizers are these fun, these operations like SGD and Adam, right, how am I actually managing my movement along with Brady in order to try and better fit to my data. So when I talk about the optimization function, this is going to be, you know, I've used SGD or I use Adam or I used RMS proc or use Adam w etc etc. So, I guess when discussing Adam, we just discussed how to create the single vectors of weight values. And so we can also view parts of that vector and find the weight matrices for every layer of a neural network so I can say, I got my entire weight vector. So I'm going to have my, my update vector, and then just by performing a single operation I'll be able to update all my ways at once. It's not going to be much more computationally efficient. So, here is a function that will do though, create the views on a weight matrix automatically so let me just create a random sample. So, I'm going to edit it and I'm just going to create this, this just this object. So, in actually view the values I have to slice it. So, now I can see instead of a three by three sample I just have nine values. So now define this make weights and views function. So what this does is it'll take the shapes of the different weight matrices let's say I've got you know, weights V and weights w and maybe other hidden layer weights. So, I'll have pre specified shapes. And so then I will take, I'll take all of those and then stack them into a single factor. Right, so this gives me something like this the ray. So just create this all weights factor. And then I will, in this case, I'm just initializing it with some uniformly distributed values. And then I'm going to build this list of views. Remember review in Python is just a shallow copy right so if I change the value in the view I actually change the value in the original as opposed to a deep copy which creates an actual separate place in memory. So, here what I'm going to do is by creating the views I can then change the value in the sort of view shallow copy, and that's actually going to change the original weights. So, what I'll do is I'll reshape the corresponding elements from the vector of the all weights into the correct shape reach layer. And so, for every shape in this list that I passed in. I'll create basically a view onto the all weights vector in that appropriate shape. So now this allows me to treat the in w as separate objects but if I modify my put V and w objects actually modify my all weight vector. What it does that is that when I actually get to my training step. I can have my, my update vector, and then just apply everything every operation to every element of the weight vector at at one time. So now this allows me to keep things organized in that I can just see like what the values of say V or w are, if that's all I'm interested in, while still maintaining the computational efficiency of a single operation at training time that. So, you know, let's take a look what the shapes of the weight matrices would be if I had a neural network with two inputs to him layers with 10 and 20 and 10 units respectively in a single output. So I'll build that. So what I'll do is I'll specify the number of inputs and in number of units per layer, the number of outputs, and then I will initialize an empty list that will store this so for every, for every hidden layer. What I'll do is I'll append one plus the number of inputs, right, the one because of that bias column. And so it's going to be one plus one plus number of inputs by the number of units in this layer. Right. So if I have two inputs, this is going to be one plus two so three by the number of units in the layer. First one was was 20 so we get three by 20. So then the next thing I'm going to do is all of so then for each one right by three by 20, and then one plus 20 so 20 is what comes out of that first layer one plus that is 21. And then we just have a single output. And so then what I will do is I will append you to one and in plus one by an H, and then I'll set and into an H because then H is the number of inputs to the next layer. Right. So I can just do this recursively. All right, so then I'll make weights and views. And so now these are like all of the weights right so you can see that this is, these are what's 20 by 20 by 20 and then 21 by 10 maybe and then 11 by one. All right, just one more thing at the end. So let's make some data with two inputs for sample and a single target output. So what we'll do now is we'll have our target values, the X and Y coordinates, and then we'll make basically a kind of a train map. And then I just have, I just have like a square area, and I have an X and Y coordinate and then I want to create some hills by specifying the Z coordinate as the output. So what I'm going to do is I'm just going to create like the surface where my inputs will be say latitude and longitude, and my output would be like altitude or something like that. So what I'll do here is I will specify some centers. So this is like in my 2d plane, this is where I want the centers of my hills to be, and then the heights for each hill. So 2d coordinates so we can, we can look at this as like my input would be two by two, and my output will be five. So these are coordinates and then this is the height, whereas for an input of five and four, my output will be four. So you can see here just by looking at these, you can see that this is like a highly nonlinear in that I have the same output for, or five for two different entirely different values like two and two, and eight and two have the same output, and then five and four and three and seven also have the same output. So very nonlinear function. So this should be something that I wouldn't require a neural net to actually predict. I'll define this calc heights. So why am I doing this because I don't want to just have like this really sharp and I don't have a completely flat plane and then like one point in the air, but I want to actually have a surface. So what I'll do then is like for every point, I want to be able to take in a value that's not one of these inputs and calculate the appropriate height based on will assume you know a circular hill with an even drop off from the from the peak. All right, so now I'm going to plot these using this mesh mesh grid function. So what this will do is it's going to take, you know, take coordinate vectors and return coordinate matrices is going to create an end coordinate arrays for vectorized evaluations of end the scalar vector fields over and grids. So that is basically taking these numbers here and turning them into a nice even surface. So what I'll do is now I'll create my surface. So I'm going to have two evenly spaced lines. So I'll just have an x axis and my other x axis in this case just by two horizontal surfaces. And then I'll make a grid out of these two arrays. And then I will allow now printing x will allow me using the mesh grid function, allow me to show the coordinates of every point in the two degree grid. So, if we take my 20 points I have appointed zero zero and also point to like point five zero, and so on and so on until I get 10 right so I've got you know, zero you know the distributed numbers until 10, and then like the next row zero through 10 next row zero through 10, and then a corresponding column at each of those evenly spaced values. So now, for each of these and I have like 400 points for each of these I now want to apply the calc heights function that's going to give me according to the hills and centers that I specified previously, what the what the values for each of these points would be. So I need to take now not just like five points, but 400 points and create a relatively smooth surface. Any questions about what I, what I'm going to be doing here. So now running the calc heights function, given the centers, right this is going to be the height of each point in the grid. So, if I have this should be 400 by one. So remember this is H right H is 400 by one X was the 400 by two array here. So if I ran h stack, if I stacked x and H together, what would that give me. How many columns that it has. If I stacked x and H so X has. So, so eight, this is this is a trade here. Right, this is X. So if I horizontal if I stacked each and X side by side, how many columns do I have three. Right, so if I now have three columns what do you think say a row of this stacked array represents. Yeah, so this should basically be the 2d coordinates on the ground, and then the height of the hill at that point. Okay. So, now you kind of get a sense of what probably this is going to look like that actually visualize this. So I'll be used, you know, just some some visualization tool kits, just using axes 3d I'll be able to plot these in three dimensions. And that gives me something that looks like this. Right, so I should have 400 points. Basically, these are my two horizontal axes, and then these, these peaks, these are those five hills that I specified, and it's kind of smoothly interpolated the surface between those. So now you can see the highly non linear nature of this function. So, um, right now we're just going to play with the lighting for a little bit so we can make this look a little bit cooler using light source. So this is a library that basically creates a light source from a specified point and it'll render it, render the surface in sort of a really nice way. So now if I put a if I kind of take a point like like over here and shine it on my on my surface. It looks like that. So it starts a little more like a landscape. All right, so now we can make some data. Right, so the whole point of this. You can use these cool visualization tools and it'll make your your projects and your assignments nice. But the whole point of this was to actually try to fit to this to this service right. So let's make some data so the access is going to be these points on our base plane. These are going to be the inputs. And then the target values for T is going to be the height of those points. And for every point in this on the surface I should be able to calculate a height for that, because I've already created a smooth surface I've got this calc heights function that should be able to take in a number of a tree number and give me the height of that point on this surface. So, what I'll then do is I've already calculated Z, right, these are those are the heights for each of these. So just make those into my target values. Now this is the thing I'm trying to predict. So, if you look at the shape of this data I now have 1600 points in my inputs so these are going to be 1600 by two right this is 1600 x y coordinates, you can call them, and then 1600 associated z coordinates so now if I just have these stacked together their x y z coordinates I can just slice off that last column and make this the thing I want to predict. I can set this up really nicely as input to a neural network that has two inputs, some miracle occurs in the middle and then there's an output. Right. That's what we're after. So, we observed in the previous notebook that there may be a tendency towards some overfitting. So, who can define overfitting for me we've kind of alluded to this term and I'm sure many of you know what is overfitting. Yeah. When your neural network creates a model with it, like, perfectly fits your training data, but one flight to any other data, don't consume it as well. Right. Yeah, so we have overfitting is where the model is really good at optimizing whatever patterns it finds in the training data, and it becomes so good at that that it's not good at anything else. Right, it's sort of like you train for years for user running metaphor again right you you say in bold you like train your whole life to run the 100 meter dash, but you fall apart when it comes to the marathon, because although there's like a superficial resemblance. It's really not at all like the thing that you've been training for so you say in both overfit to the marathon and Elliott keep choking the guy who like one the has the world record marathon over fits to marathoning right and I neither of them is going to be good at the other sport. There might be better than the average person at either but they're not going to be particularly good at that other sport. So it's also like, you pick your metaphor it's like taking a football player and hope and assuming he's going to be like an Olympic League swimmer or something like that. So, how do we avoid this yes. So I said it again. I think I'm going to very said that just to repeat the question if you have a neural network that's like was that you designed to do one thing that ends up being better at something else is a question. That would be a very weird case. Yeah, so I, you could argue that that is maybe a kind of overfitting, but it's like, it's sort of something must have gone longer in training and that point. So, I guess over overfitting is not necessarily, you try to use a model of the design for one thing to do something completely different. Like, yeah, sure it overfit to the data compared to this other thing that doesn't resemble the training data at all. But you can't assume. So that would be like trying to use a hammer in place of a saw just because they're both tools. Right, it's like a, you have different tools in your toolkit. I do not expect to be able to chop down a tree with a hammer. It's going to be very, very difficult. Those are design choices that you can make and like if you assume if you design your network assuming that you're going to do one thing. It's just it's not fair to apply it to a different thing. So you see like, you know, that kind of bad faith critiques of some some some papers like well your network doesn't do this well it was never designed to do that. So, I did this is not the problem I was trying to solve. You're really not making a fair criticism of this right there for the other things you criticize it for but that's not it. That being said, generally the goal for neural networks if neural networks are universal function approximators that is if I'm assuming that there's a function that maps for my input to my output, and my job is just to find that function. The neural network is a universal function approximator meaning that with the right combinations of non linearities and layers you can approximate any function and principle that just might be a really gnarly function that takes forever to approach. Nonetheless, you can do it. So the goal with neural networks is that always some level of generalizability. So what we do we don't want to have and we don't spend all this blood sweat and tears in training this neural network that does just one thing on one data set once. I want to be able to reuse this, at least somewhat. So one thing that I can do is make sure that the data that I'm going to evaluate on it's never been exposed to before. Right, so this was something that we kind of have slipped under the radar with assignment one and that you don't have to do the train valve test split. Because the data is friendly enough, especially cyclical if you look at its temperature over a year so like the last date is very similar to the first date. So works nicely there. But generally speaking you cannot assume that's going to be true. So what do I do there. I want to create these train validation and test sets. So what's the role of each one of these obviously we know what the training data is for this is what you actually fit your model to. And the test data is some unseen data that I've never that the model is not exposed to. And this is what I want to actually perform well on this is where I'm going to basically prove that my model is good enough approximator. And it's not seen before that's what makes the argument that I've actually approximated that function that I'm searching for. Well, these things involve a significant amount of computational power the examples that we're going to be using or not really all that big but we're all familiar, you know, with, you know, the large language models and the things that we're going to be using is those take forever and neural network training has taken forever for a long time one of the reasons that it didn't take off initially was that it took a mainframe the size of this room to do simple digit classification. Right. And so people like okay this is a great party trick. I don't see what it's actually useful for. Well now we have the technology and the tools to speed it up, such that we can actually use them for real things, but it still often takes a while so. Again, I don't want to sink all this effort into training this neural network and then run it on my test data and find out that it completely collapses. I want to have some reasonable some reasonable assurance that I'm going to perform well on my test data. So what do I do for that. This is where the validation set comes in. The validation set is an extra set carved off of your training data that during training you continually test against. You're not training on this data. So you should not overfit to this data, but you can check your model against that to see on this other unseen data set that is not the test set, but I can use to tune the model. Am I performing reasonably well. And so I can use this to find things like good hyper parameters like in kind of see like is my learning rate too high am I skipping over my my optimum the gradient is is my neural network the right size do I have the right appropriate number of non linearities in it so these are the sorts of things I can I want to be able to do. I still want that nice property of the training validation and test set to resemble each other. Right so if I'm training on this hill data I need to get like a roughly uniform sample of points on this mesh. Right I don't want to validate only on this corner. This is not going to give me an accurate picture of whether or not my model is fitting to the rest of the data. So, effectively what I'm going to do is I'm going to train on some points on this mesh. I'm going to validate and other points on this mesh. I'm going to test on further points on this. So that's what I'm that's what I'm looking for. I'm going to shovel the samples into a random order. Right, we did this before. I'm going to partition the data into n fold so maybe say I want to create n sub partitions of this data. And then I will assign the first fold to the validation set. The second folder the testing set and then the remaining folds into the training set. Generally we want to have both degree data and the training data, which will have a substantial enough sample in the validation and test that you can be reasonably assured that your model is going to perform okay on that data. So what I'll do here. This is just going to do the, the, the proceeding in code. And then I will pull the row indices we did this already in one of the previous notebooks, I'll just specify the number of folds here I'm going to do five. And then I will divide my number of samples by my number of folds round down to make sure that I always have an integer. And then I will accumulate those different folds into the different samples so like I'm going to in this case, just by convention I'll take that first forward make it the vowel set second hold make the test set, and then the remainder, make it the training set. And then I will print out the number of folds, five, the number of the number of the first fold, and then how many samples in each one, right so 320 by two, two inputs, and 320 by one that's that output. Okay. So now what I can do is I can specify which one of these I want to use for which fold so the. So, the X validate and t validate is going to be fold zero right there are two elements here. The first one of which is going to be the, the training the training and the second which is going to be the targets. Okay. So, then I'll do the same thing for the first fold is going to be the test or the second fold be the test set, and then I'll just stack the rest together into the training data. So, now if I look at the shapes of each of these. The training data is 960 samples, each of which has two inputs and then targets, and then I have 320 samples each in the validation the test. So, this is a pretty generously sized validation and test set in that validation and test set or like one third the size of the training data. In this case that's okay, because there's like, not that much noise in my in my data that I'm trying to predict. But the size of your validation and test set is another one of these things that you want to be judicious about choosing when you're when you're performing training. Okay, so now I'm going to basically run a solution to assignment to that you won't see because I've saved it off previously, but we'll see how we can actually use a neural network to fit to that that hill data. So, what you can do, you're not required to do for assignment to but once you complete the neural network class definition you could save it off into a file called neural network dot py. And you can use it right so assuming that you perform well in assignment to you now have a implementation of a neural network that you can then just reuse indefinitely. Okay. And later, when we are through with assignment to I will actually like just give you a neural network implementation to use another things, just in case you know you're not so you're not relying on a potentially buggy implementation of a to. All right, so I've got this neural network to py file import that. So now let me look at just the size of my dimension I'll be my inputs right to samples, and then a single output for each sample. So the final define is the actual, the hyper parameters of my neural network the actual architecture. So, the inputs, the input layer is always going to be the dimensionality of the thing that you're measuring. Right, so if I've got two samples or two inputs for each sample, there should be two things, right, two nodes, two inputs to the network. And then I need to specify the hidden layers. So, 10 units and five units. This could be as long as you want so basically the way that this, this neural network classes written is I can just add numbers to this list and it will automatically create a new layer of that size. Okay. And then finally I need to specify the outputs and of course the output is going to be how many things am I trying to measure. Right so in this case I've just got one thing that I'm trying to measure the height of the land at that point. So it's going to be single output so. And then the default here, the is the 10 inch activation function, because we have not talked about others just yet. All right, so now if I just print the neural network. This is sort of in my my non pi neural network implementation. This is my architecture. So I've got two inputs, one layer of 10 units one layer of five units one output of the 10 inch activation function. So when you're using libraries like TensorFlow or pytorch the they do come with a handy print function where you can actually print out the network architecture and see you know the sizes of the different layers, and all all the fancy things that you can do there like different activation functions and residuals and whatnot. I define the train function that is much like what we have done previously. So again, a generic one that takes in the inputs the targets number of epochs going to train for learning rate and then method is just the the optimizers are using SGD. So let me run it. So you can see it's working. It's pretty fast. This is not a very simple not this is a pretty simple problem. So my error after 10,000 20 epochs ends at 1.19. So I can experiment with a couple of other things, right so I can try and see what happens if I use Adam instead. Right so in my implementation I I've got a way to basically just pass in which type of automizer I want to use it will use that method. So let's explore using Adam instead. And we can see, you know, first of all here we can see first hand one of the benefits that I'm so in both cases I've trained for 10,000 epochs. But I've even in the first 1000 epochs of training with Adam I got lower error than the whole training for 10,000 epochs with SGD. Right. So this is one of kind of a tangible demonstration of the benefits that I'm training takes about as long as case it takes about 2.61 seconds. And these little spikes here right that's the sort of skipping across the bottom of the op. So it's getting there and maybe trying to find its way out finding that that's not a good way out going back down and eventually to sort of settle there and maybe is a little bit of movement one thing you will see. When you do assignment to is that Adam optimization is a little more approximate. Basically there's a kind of a toy test function you can use just approximate the minimum of a parabola and SGD will get you there exactly by with the value that we know that that that is the minimum of that parabola and Adam will get you close, but not quite close enough to be useful, but not exactly. I guess close enough to be useful but not exact is like a pretty good motto for most of machine learning. Okay, last thing we'll define the RMSE function, you all are probably familiar with this already. And then I will have to find my use function and I'll just apply that over my training data, and then just print the shape so basically here this is giving me the actual output so this is being stored, and then I put the shape, and that's 960 by one. So over the X train, right 960 samples each of two inputs through the neural network, and then it gives me an output for each of those. So now let me print some, some values. So what I'll do here is I will print the RMSE for the for the for the training data, the validation data and the testing data so basically what this is doing is I'm just computing the RMSE for my target my train targets, and then my predictions and end dot use of X training that's going to give me all my predictions. So now we can see that I've done a pretty good job at splitting my data such that I'm training in a way that is allowing me to get reasonably close testing or validation and testing error. Yeah. So you, you can do that. This is this is something I guess I, I guess I omitted that here so this is something that you can set as a as a criterion in kind of TensorFlow or PyTorch. So here we're not really doing this is sort of homebrew implementation. You can set it up so that you're, you can perform continuous checks against the validation, remember the validation data never hits the actual neural network itself, except for you just sort of passing the use function passing through the use function at sort of every given step to see how am I doing. So you can set it up to do that and print out you know every and epochs, what's my validation accuracy or for error or whatever metric I'm using, based on the state of my neural network right now. So you can think of what we're doing here at the end is I just doing a check on the validation data. Before testing. So what I can do here is, if I am not sure, like, so you're not supposed to touch the testing data until you're ready to apply. So what, what often happens is, you know, evaluators will keep like a hidden testing set. And it's just like, you don't get to see this, you have to write your neural network the best you can. And then you send it to me and then I will apply it until you did. Here, we're not doing that, but you can imagine that we did. So, testing data is often this. I want to know how I'm doing. So I will check against the validation data and if this value is lousy. So if I just print these two, and it's like okay, instead of point one on three, it's like 10. Okay, this neural network was wildly bad. Something's wrong. I need to like add more hidden layers I need to add use use a different activation function whatever it is. And so then I can try and change the hyper parameters of my neural network so I get a lower value. Okay. Yeah. We have like our assignment. So it varies in the so if I remember correctly in assignment two you're given it's like a time when you're given template code. And you have to fill out like train and use function and something and you have to change the optimizer to change from SGD to Adam. And then in assignment three, you have to do some sort of grid search. All right, let me get through this in the remainder of the time I think we're almost there. Okay, so we trained, right, it seems like we're doing a pretty good job. Let's actually try to visualize this so now what I want is I want to be able to take my, my surface, show all my training points, show all my validation points and show all my testing points. So, let's take a look at that. And this is what we get. So the blue points are trained the yellow points of our val and then the green points are test. And so we can see that we seem to be doing a decent job. Right, so the training points. It's kind of it's a little bit difficult. I don't think I have this now it's not set up to rotate. But you can see the training points are like very closely fit to the surface, and the validation points and testing points are mostly to there's a few like here's a testing point that's kind of maybe not so close. But it seems to be doing a pretty good job of being able to predict the height of the surface from the XY coordinates. All right, and then finally, let's see if we can visualize what the hidden units that actually learned. So previously, we had these lines saying okay for this x value, my hidden unit is outputting this value. Let's see if we can do something for for this data. One other thing to notice like as we get close to the edge here you'll see how we kind of see these these dips here. So, this suggests that basically it's not very good at optimizing for this local neighborhood probably because of a lack of data off the off the surface right we have no points here. So, this is kind of continuing transit may have observed from this direction, leading it to be a little bit lower than the actual value. Right. This might be a reasonable prediction of like what might happen if we send to the terrain in this direction but we don't know. All right so visualization. So, this is I'm not going to go through this code or in any real real depth this is just for visualizing the outputs of the hidden layers three dimensions. And so you can see now how this is not necessarily very interpretable per se, but we will tell you for each input what the unit and every hidden layer is putting out. And this might be useful for you know if you want to trace the path of like a single point, right we want to figure out like why is this testing point down here. Then you could actually calculate what each hidden layer is outputting with a network of this size this is like reasonably tractable like you could probably do this math if you were motivated enough. The black box nature of neural networks just comes from having it happen at scale. Right if I have 10 or more, you know, dozens of hidden layers each with thousands of units. It becomes really hard to trace you know what's going on with a single input. Alright last thing. We need to examine the effects of various hyper parameters so we can vary different things right we can try different lengths of training different hidden layer structures. Different, you know, different optimizers etc so you can try all these things. And each of these are called hyper parameters, right the parameters are the weights. Right so my neural network is a function or combination of functions parameterized by weights. So what I'm trying to solve for are those parameters. So those are the coefficients, the hyper parameters are the things I actually have direct control over. I don't go in my neural network and tune all one billion of my weights. What I want is a function is going to let me do that automatically and order in order to achieve best performance. I'm going to be looking at those things that actually have control over such as the learning rate training time model architecture optimizer etc etc. So the hyper parameters are just the property of the architecture that you actually have direct control over. So what we can do now is grid search. I can say that for different combinations of learning rates training durations layer sizes and optimizers and whatever else you chair you care to examine. I'm going to instantiate a version of the neural network with those hyper parameters train it, see how it's doing against my validation or my test data, and then from that I can decide which one I want to use so the role of the validation data is really is really key and I'm trying to find the right hyper parameter to test it. So if you assume the test data is not to be seen until testing time. I don't want to tune my model on my test data that's cheating. So instead I'm going to tune on the validation data that I'm just assuming is reasonably resembling of the test data. So in this case, this is similar to what you're going to be doing I think in assignment three, except that's classification, I think, maybe. So what you're going to do is you're going to try a bunch of different type parameter combinations and then try to observe trends like what happens as the learning rates decreases or what happens as I train for longer, or what is SGD doing, you know, worse than Adam and by how much. So due to time I've already run this I'm not going to go through this again. But what I want to look at our which parameter values are best. So, I can plot them, right, I can just plot the armacy, but that's not really helpful but I want to look at is every plot with respect to hyper parameter values. So, oh, what happened there. Well, a bug happened. But if we go to one on the calendar should give us a version that we can look at. So, so, two minutes. So, all right, so now we can check out the different hyper parameter combinations so for example, if I want to look at what happens that to the training validation and testing when I use the SGD optimizer, we can clearly see that this is a larger architecture and that seems to work best right where as for Adam. I can see that I still get that with the architecture size seems to matter more right so here I have 1000 epochs and 5000 epochs with the same architecture, and I don't get that much improvement when training for 5000 epochs so this suggests that if like I'm trying to optimize for compute time maybe Adam is going to give me the best bang for my buck which we observed already in that previous notebook. And then another way to look at is just put it on a pandas data frame, and then we can actually compare each one so I can see you know, where do I see the low RSE number as well. I see them with Adam more than SGD, and I see them with larger architecture so it seems like the things that are most important are going to be the architecture size and the choice of optimizer. So once I've kind of once I've got that I don't get much benefit from training for much longer. Right, so I get a minimal benefit from training for 5000 epochs versus 1000 epochs. Overall, yes, the best thing to do is just to train the hell out of it 5000 epochs with Adam on this big architecture. Yes, this is true right yeah and so that that is true what's what's the reason for that so if you look what's on the y axis RMSE right. So we also see that these two, like the best SGD performance is not at all comparable to the best Adam performance, the best Adam performance is like way way better than the best SGD performance so if you were to plot both of these on top of each other we basically see like SGD kind of up here, and then Adam is really kind of showing how strong it is. So, this is sort of a method of deciding like what are the best type of parameters for me to use for this data. Alright, that is all for today. So, assignment one, do on Thursday if you need an extension reminder reminder to get that in by your request in by tomorrow for my consideration. Alright, we'll see you Thursday.