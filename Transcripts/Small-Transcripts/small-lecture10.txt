 Yeah. Yeah. Yeah. Yeah. It's like the number three bed hospital in the nation, or maybe it's even the number one now and yeah, I get a 20% discount as an employee, which is like it's a lot right when you consider how much how much treatments cost. Okay. So we connect works toники it's just like, I would have enjoyed the question I Make is how does anybody use. That's what we... Okay. Let's get started, guys. So, let me begin with a note mostly to the people who are not here. So, I'm just going to tell you guys and those of you who are on Zoom. Around this time of semester is when I start to notice that people are not coming to class. And this class is roughly 75 in person and maybe another dozen or so online. So, like, I count the number of faces and it's like a lot less than 90. So, those of you who are watching this recording, I do appreciate it when you let me know, right? And there are some people I can see who have already requested clearance to attend remotely. And I appreciate that. If you're not one of those people and you're just, like, skipping class, I do start to notice, especially when it's, like, not negative 15 degrees outside. There are certain days where it's, like, a lot more forgivable to skip class. It is now currently 31 degrees. So, today is not one of those days. So, just consider this a warning that I'm addressing to the folks in person who don't need the warning. Not coming to class without a reason is going to affect your participation grant at the end of the semester and I assume that most of you don't want that. Of course, just keep in touch with me. You know, if you are sick for a long period of time, I understand, you know, just all the usual things, but if you don't let me know, I'm going to assume you're just skipping class. So, and then what am I supposed to do? Okay, so you guy who was talking to me about the ethics cloak. What's your name, by the way, Hunter. So I'm trying to learn folks names and it's hard to do in a class. So Hunter was asking me about the ethics colloquium today. So if you're interested in hearing me and some other folks talk about chat GPT that's going to be happening in the LSE theater. So I'm going to be out with LSE and like, kind of as you're going in turn away from the food court down that like green line hallway and just try to keep going. And you'll get there eventually. It will also be recorded I'm told. And so I'm going to get the link from them and I'll post it. You know, if you weren't able to attend. If you're interested in watching some of that discussion so it should be pretty interesting. So as a result, I have to be there for so I will have to have to close office hours at 345 at the latest, in case any of you are going to come by so eventually office hours will begin like the moment classes over if you need to talk to me I recommend just like catching me on the way back to my office and when we get there we can sit down and chat for a bit. But I will need to leave my office at 345 or so to head over to LSE for the event. Okay, we are, but I think we are almost done with grading a one I say we a when it's sorry we're doing all the grading. But I think probably the weekend right. Okay, so yes, so show give me the grades by about the weekend and I'll just spend about a day or so reviewing them so hopefully we'll be able to get back to you by the end of the weekend maybe Monday at the latest, which should be in time for you to take any feedback from PA one and then we can go back to the incorporated into your assignment to submissions. So, if there are no other questions I will start the lecture on classification. You guys got anything you want to discuss. So discussion today all like your side. It's me I forget who all I think it's like five people it's from all around the university so it's basically like, I'm the only person in the university who does an LP as a primary research area as far as I understand so like, now I'm getting like lots of requests to talk about how CSU is handling or how I'm handling I'm not sure the university policy there as far as I understand there is no distinct policy yet. My policy is don't. But it's going to be me, and there's like some folks from I'm going to say like economics or the business school, maybe English department. I don't recall exactly but there's like four, four to six people total. So basically a panel discussion for about two hours. So you know me and other people are just riffing on the topic I've literally no prepared remarks aside from what I rehearsed in my head so I would assume so I will see we'll see what happens right I mean but I think it should be an interesting discussion I went to another one I did I was a panelist and another one last semester. And then they decided to do chat GPT so they asked me back. But it's a good discussion. Usually it's pretty lively in this a lot of interesting questions and things we discussed so if you can't make it in person I definitely recommend watching the recording and it just should be pretty informative and I'll probably touch back on some of the material we talk about when I do the last lecture on on ethics and and the process of machine learning. Okay. So, if there are, would you a question. I, I just wanted to know that it's. I am told it starts at four. So I, that commences at four. So, yeah, I'll probably be there at least 10 minutes ahead. Yeah. So, let me share my screen. And let's start talking about classification. Okay. So, so far we've talked about regression problems, which is basically taking scalar mapping them to scale or output so my input is some set of numbers and my output is also a continuous number or set of continuous numbers. But many of the AI tasks that you might be interested in. So, we're not necessarily just predicting numerical values. Right, so many people actually want to assign a meaning to things. So that is to say, how can I represent meaning numerically, right, one is an abstract quantity that kind of resides in our brain and we have interpretations of, and the other is a quantifiable metric that we can actually you know perform numerical measurements on right how do we get these two things to align doesn't seem very intuitive. So, one regression using linear models, and then also nonlinear models aka neural net so now that we're starting to use classification in similar form I'll be basically showing you linear examples of classification, and then nonlinear classifications using neural networks and we can see what is most useful for in which circumstances so let's just take a look and see how linear models are not suitable for classification. So let's we have some data like this, say we got 13 students are taking a test. And yes, you could plot the grade as an actual numerical value and then maybe it could be solved for using a linear function but let's say that instead we have this data plotted as x being the number of hours at every student studies for the test, and then the y axis is at the letter rate, let's just say ABC. So instead of a numerical value between they say 70 and 100 is just one of three categories. Right so it doesn't matter so much if this person gets like an 83 and this person gets an 87. This is a be this is what we care about right now. We could still try to do linear least squares on this by using the integers one, two and three for the different classes right so now I could have x being the hours of study, and then just plot one, two and three almost as if this was worth or something. And you could do that, and then you could linear function to the data. Right, and so you could get a line that maybe looks like this given this data. This is not a great fit. Right if I were to say, trying to calculate the r squared value of this or something. It would not necessarily be very high because if you look at whatever value this is, you know, four hours. It, you would expect the value to fall here somewhere around 1.5 right when in fact the value is two. Even though this kind of fits the data sort of relatively well. It's not a great idea because you have to convert these continuous y axis values into these discrete integers just one, two and three 1.5 is not allowed right 1.5 is not one of my classes. So, without adding more parameters I need to figure out like where I have to split these, and I could use this general solution of like splitting at 1.5 and 2.5 and if it's like above 2.5 is like I'm just going to round up and say it's a three. Right, which would be an a. But if I do this, then rats. Chuck Anderson wrote this part of the notebook obviously because when things don't work for me I use more colorful language. So, unless racks, the boundaries are not where we want them to be, because we can see here that if I'm just if I have my 13 samples and trying to arrive at an even distribution. I'm basically grouping one sample that I know is a be with the seas, and another sample that I also know is a be with the A's. Right so already, there's noise in my output set. So I can already tell that by doing this, I'm going to have a very suboptimal model. So for example, if I have this, if I say x equals four. It's going to say, okay, well the predicted output is 1.4 which is below the 1.5 boundary and therefore it's a one or a seed. So already, there's going to be at least two samples out of my 13. I can do a lot better than this. So one solution would be to represent the the class labels, not as numerical values, but to rather decouple the modeling of those values from the actual boundaries. So using one value to represent all the classes doesn't allow me to just not have any kind of flexibility. So instead I want to use three different values that are all orthogonal. So now, there's no clear linear dependency between say class B and class C right so even though the data may be broadly monotonically increasing, if you plot you know the grade versus the hours of study. So this could just as easily be three completely different classes of, you know, subjects, maybe that maybe if you're in a study that or if you're a field that requires like, on average fewer hours of study per night, for whatever reason, just because the nature of the field, you could use that to predict, you know, this predict what what field someone is in based on their hours of study per night. So class modeling basically what you want is all your classes to kind of be orthogonal to each other. So one way to represent these are orthogonal values are just to think of them as say vectors. Right, so if I have three classes, this could be a three dimensional vector where every every class is just perpendicular to all the other classes. Right. So one way to do this would be just to represent these what we call indicator variables. So if I have three classes of indices 01 and two, and I'll just put a one where it is a member of that class, and zeros elsewhere. So, indicator variables for one hot vectors, right, are basically the term that we use here. So, we can model class one as one zero zero class two zero one zero class three zero zero one. If you think of how you might plot these coordinates and 3D space you basically end up with something that looks a bit like this. Right, so if my my thumb is the x axis my index fingers the y axis middle fingers the z axis, you can see how, if we assume that all my fingers were unit vectors, all these vectors would be the same. Okay, so now I can just let the output be this this triplet this that has you know for however many value however many classes I have it has that many values. And I want to maximize the predictions that I get close closest to the indicator value that represents the class that is the true the actual true value. So then I'll need to convert to do is take these values and convert them to a class by picking the max. So, for example, if the class is zero zero one, then this should why one should be one or close to it why two should be zero why three should be zero. If I take the arg max it's going to say okay index zero a k class one is the most probable class. Okay. Yes, do you say why one should be close to one or it should be one. It should be one, but we're approximating so eventually we're going to get it as close to one as possible. I mean a little ahead of myself with the with how we calculate error classification, and we won't really get to it so much until the next lecture. So for the moment you can just assume they want class one to be one. Okay. So now, if I represent my outputs this way, then I can just have two values on the y axis zero and one, and wherever that the class in question is one if it's one else right zero. So, if I plot the data like this now we can see that for example all my a's are ones in class three and everything else is zero, and these would be all my bees, and these would be all my seeds. I can still fit a linear function to this data. Right again it's not going to be great fit, but it would look something like the following. So then what it can do is I can overlay them to see which one is the maximum values so for example, the blue one is basically the line fit to the a data, the red line is the line fit to the C data, and the green line is the line fit to the be data. Now if these are the boundaries between my actual classes I just look and see for each of these lines in which class, which, which model has the highest value at that point. Right so the red line is my C's we can see that for these samples, the value on the red line is higher than the other two. So therefore these would be in numbers of class one, the green line is the highest for for this set so then these would be members of class one, and the blue line is the highest for members of class three. But what if the green line were slightly too low. Right so now if we look at this sample here, right this one, which line is the highest at x equals this value for the mouse is the red one right and the red one is the sort of the class zero line if you will. But for this sample that we know is a be it's actually predicting a C because the C line is highest. So, why might this be the case. One reason that we could that could be the cause of this would be a master problem right we have too few samples of class two. So if my class if only have three samples of class two my data looks like this, and the line that is fit to this data is going to be lower than this green line. So that's actually going to be much more like this. And so, if I have too few samples of a particular class then novel data that belongs to that class is kind of not going to be modeled as being a member of that class. So there might be no values of x which the second output y to is larger than the other two given a linear model. So this class two has been masked by the other classes. So if I have say, five samples of class zero and five samples of class three on only three samples of class two. It becomes harder to pick out new samples of class two from new data. This is the same issue, as we would encounter with an unbalanced data set, right it's more probable that something is going to be a member of class zero or class one or class three. And I think that a lower error if I predict those more often. Right so this is going to, this is going to make me under predict instances of class two. So, let's think of what other shape of function would work better for data like this. So you can chew on that if you have any answers, you know you can you can spin them out later. And then we'll just try an example, real quick. Well, we'll mostly try an example for the rest of the lecture so not really all that quickly but we'll go off to an example and come back to this later. So we'll be using this other data set from the UCI ML archive and particular a data set of Parkinson's disease. So what this data set contains is the font size bit. So what this data set contains is 147 samples from subjects with Parkinson's disease, and then 48 samples from subjects who do not have it. For these, each of these samples, they extracted 22 numerical features from voice recording so basically they recorded the voice, and then they run you know a bunch of like, you know, spectrograms and things over and for a transforms to extract different features. So feature engineering is kind of already done. These, these features are assumed to be relevant features from vocal analysis to predict Parkinson's, and then they're labeled. So we just have zero for healthy subjects, and one for subjects with Parkinson's disease. So we have a lot of data from a collaboration with the University of Oxford and the National Center for Voice and Speech in Denver. So let's download the data, read it in, and then just do some statistics over the data. So, if you look at this, we can see that we have 195 samples that is 147 plus 48, and then 24 indices. So we have the 22 numerical features, plus one for the label, and then the other one is just taken index. So if you look at all the columns we see here. Okay, so we have name right this is the index, and then we have all these different features. So there is. There's status in here somewhere so there's status. This, for some reason is not the end but we know that this is the actual value. So we're going to extract that status value. And this is going to be our targets. Right so now we now we've taken the 24 features sliced out one of them. This is actually the feature trying to predict. And there are 195 by one values in that array. Okay, remember that other. There's that name column that's just an index this is not going to be a predictive feature right you can't predict whether someone has Parkinson's if you know their name, or if you just have like some numerical index representing where they are to study that's correlated. So we're going to drop that and then we're going to drop the status of course we don't want to have the thing we're trying to predict in the training data, because then what would it learn. Yeah, or whatever it would just ignore it could just learn to ignore everything but the status right the status column and the input is obviously a perfect predictor for this is for the output so it's like well, here's the answer. So we're going to drop everything else and I don't learn anything. Right so one thing that we do in in analysis is just sort of you look like is is a model. Does the training data effectively answer in it obviously. And if it does then that's contaminated training data, but also are the features indicative of the answer right you you want to give the model enough information to figure out the answer, without just giving it the answer. So now these are the features that we actually actually use so I don't really know what all of these things necessarily mean maybe some of them mean something to some of you but they're they're just you know features like frequency and volume and vocal quality and things extraction So let's print the values for each of these so we can see that the ranges are very different right we have some that have means that in the hundred and some that have means close to zero and some that have negative means, and then there's a wide distribution as given by the standard deviations as well. So, let's look at the occurrences of the individual values so we can see here that as as was mentioned in the description of the data. We have 48 samples of zero, that is, people without Parkinson's disease, and 147 samples of one that is people with. So this is basically just a binary classification negative positive for for this trait. And you can see that the sample is is unbalanced, right we have way more samples who have Parkinson's than without. So, this is going to be one challenge with this. Okay, so for we have the small sample size overall under 200 samples so not huge even though we do have some pretty good data for each one. And then it's very unbalanced that we have like 100 more samples of one class than the other. So, what we want to do is we'll just force equal sampling from the proportions of the two classes, and use that when building our training test partitions so we'll use 80% for training and 20% for testing this is a very typical split that you will see. So we can specify the training fraction so you can run this notebook and mess with this value to your liking, and then we can extract the class for each of the subjects, and then permute all the, all the data, take the training fraction, and this should keep roughly the same proportion in the training and test sets as occurs in the actual data. So, we can see now. So for the training set we've got 156 samples. And for the testing set we have 39, and this should be roughly the same proportion I can verify this by taking the number of class zero divided by the number of class one, and we can see that it's roughly one third. Right, this is not going to be exact. But it's close enough. And so 32% of healthy subjects versus 34% of healthy sub or three to one ratio I should say between healthy subjects and Parkinson's is, it's decent, right. And then this compares to the original data set. These numbers are very comfortable. This is what we want to see. So the least square solution will be, first, we'll standardize the inputs. We don't need to standardize the outputs because now they indicate the class. Also, the outputs are zero and one. So if you standardize them. I'm going to end up with something very similar. So what you do what you don't want to do is, when you're doing classification as compared to regression, you don't standardize the outputs because they're basically numerically indicators and actual class value, and then there's just some mapping that you use or just convert the numbers to say a string label or something like that. So, then you just calculate the linearly square solution. So the training function for this would look something that looks pretty familiar. So we have, we calculate our means standard deviations we standardize that we then insert our bias column, and then instead of doing iterative training using SGD right now, we can just do linearly squares, so I can use this the linear algebra library from NumPy and just do that. And then I'll have a use function that basically will do the same thing. We just, we have the model, we standardize it, insert the column of ones and then multiply by the weights that were in this case we're calculated using squares. So let's take a look at the data real quick right these are those 22 columns that are supposed to be interesting features. And so then I will just take this feed that into my train function. And then I'll insert the bias. And then, because this is a linear model what I can do is after training I can actually see what weights are assigned to each of those, each of those values. So, here we go. So, of this, right just take a look at this list real quick, and see you know which ones do you think appear to be the most important. Right, remember how we interpret those weight values. So, what do you think is the most important feature shimmer. Yeah shimmer right so this is basically a highly. Well, so this one is highly positive correlated and then there's other one that is highly negatively correlated. What else. Jitter is pretty high. So those three. Probably and then this MDVP, or AP, I don't know what that means but it also has a fairly high negative value. So like those four features are probably getting us most of the value for a model, right, however well this model does. It's doing mostly on the strength of those four features. The rest of those values are like pretty close to zero. So now it's Tesla linear model right so to compare the target values of zero and one, you need to convert the continuous output to zero or one whichever one is closest. Okay, and this would be something that you can do with a number of different functions but we'll have this one function convert to zero one that's basically just computing the distance from the target that is either zero or one, and then just figuring out using the argument whichever one has less distance. So, if I convert some of these samples where you can see this function at work. So whichever one of these guys is closest to it will then assign to be that value and you know that is not bounded at zero and one, right we can go above. So that's an output that's 1.1 that is much closer to one than it is zero so it must be an instance of class one, we've also got negative values in there. And then there's things like 0.56 right which is very very slightly closer to one, but I must make a choice. And so to choose one. So classification algorithms just to keep in mind is basically, I'm giving you a fixed set of possible classes and you must choose one of these things. Unless you explicitly have, I don't know as an option. It's never going to say I don't know, it will say there is an infinite infinitesimally small, a greater chance that it is a member of class one. So it must be a member of class one. Right, so this is the difference between like 0.5001 and 0.4999. Yes. Yes, yeah, so we'll talk about that when we do classification with neural networks, and it's pretty easy to do that. So you can actually quantify, you know, sort of your confidence interval and say this is, you know, I believe it's one and I'm 90% confident that it will, you know, it's going to fall in the range that's going to be classified as one. You can get it to do that. Yeah, so you can usually most models will not do this by default, but you can write some code it will do for you. Okay, so let's use our, let's use our model. So I'm just going to, I'm using the model over the training data right now, just to sort of test how it's doing over the data that's been exposed to receive it's at least a good fit to this model. And then I will do the same thing for the test data. Right. So what do you expect might happen you expect to see a difference between, say the percent correct train and the present correct test and how much. You can speculate. You can guess I mean, 10, yeah, or you know, I was just looking for like a lot or not much. Okay, but here we go so let's let's run this and we'll see we just convert all of these to zero and one, and then convert that to percentage. So in this case, actually, they're the same. And this might be, you know, you run this a couple of times with different training splits and you'll see some difference. It's usually not a whole lot of difference in this case because the features are quite predictive. And the test data is just, you know, there's so many features in there. The test data is very, very resemblance of the training data so it's, unless you get like a really unlucky split and there's like some peculiarities in the training in the test data. But you know, random splits are kind of this double edge sword because you can, could you could get a lucky split, right you could I could do this. I could say, Oh, wow, I got the same testing and training accuracy and therefore this is as good a model as I'm ever going to get. Right, but if I run it again, and it has a different random split and may not see that. And so, when you're reading a paper or evaluating model. And to look out for whether they mentioned you know is this like an average of 10 runs or how are my splits done is like the best of and possible evaluations. And when you're writing a paper, you know, it's good practice to report that as well. And also think about how you're, how you're presenting your results, you know, are you averaging over multiple runs. Are you using random splits or are you doing like participant life splits yes. And that are like, I guess, we want to use that to alleviate the random splitting. Yeah, yeah, so you basically for something like this exactly when it's a small test set right there's a higher chance of maybe getting some slightly odd samples in either training your test set and so if you are unlucky enough to get that, then you either have an overvalued evaluation or like a suppressed evaluation. So yeah, you can use like different different models and ensemble them you can run multiple times and average them. You can do, you know, voting, there are multiple ways of handling this. But for small, for small test size or for small test sizes and things like this it's generally good practice to maybe like get a second opinion. Like, this one model is doing really well. How do I know it's not just something weird that this particular model picked up about my training data. So it's good practice to evaluate it multiple times multiple models, etc. So, let's do some visualization, right so what kind of visualization could I use to check the results so what I can do is I can plot the true class of the output, and then each training samples you basically have to two series, what the actual class is and then what my model predicted that. So that may look something like this. And so now we can see that the blue line underneath is the actual class we can see that these are like my 30 odd is zero samples and these are the rest of the one samples. And then those things where the orange dot doesn't hit the blue line those are the misclassifications. Right. And so we can see, for example, what do you notice like which class has more misclassifications. So the which which are the which are the classes zero or one have more misclassifications so this is zero right and that does make sense. Right if you think about the imbalance in the data set, because you're going to minimize your error. And then you're going to get a greater minimization of the error by predicting more things as, as ones on balance and predicting more things is zero. So it's basically losing more or losing less by misclassifying zeros and it is my misclassifying ones. If this set were more balanced may not be the case. So we can see kind of the same phenomenon, right there's fewer samples of course, we can see there's only one misclassified one class, and three misclassified zero classes. Okay. So, remember what we did for convert to one is basically saying, I'm just going to have a decision boundary at point five. And doesn't matter how far it is from that decision boundary, if it's above it it's a one if it's a blow it's zero. So for these samples do we have any that were just like real borderline in cases, right there could be one where it's like, well I classified as a zero, but just because my output value was like point 49, you know, whatever. It's like, okay. You easily could have gone the other way if there's like a slight slightly different split or something like that. So it's like really borderline maybe don't want to take that as like absolute truth. So here's the actual continuously predicted output. Right. And so remember, it's not bounded at zero and ones we have values that are below zero and values that are above one, but we can see for example if you look at some of these misclassifications right these samples here. These are the ones that are like just barely below point five. So classified as zeros, they're actually ones but it's like these actual values are probably you know, point 45 to point 49 somewhere in there. So you can, you can think of this excluding the ones that are above one or below zero just think of it as like a percent almost. It's 49% likely to be class one, which means it's 51% likely to be class zero so it's like if you're when it's when it's election season, I like we'd 538 like religiously. So if you're saying that the predictions like saying well x is like 51% likely to win this race like yeah, or no. And then they lose a what happened well it was a coin flip right it was not you did not have a good prediction for that. So like you're saying it's 51% likely does not mean it's a sure thing. Right so this is a cognitive bias that people have similar things we see in the testing data right so here's a couple of samples. So one thing we can observe actually is that if you look at the samples that are misclassified as one that are actually zero. Those output values are much higher, right these are falling in like this is like point eight point six point seven something like that. This is also an artifact of that sample in balance, these output values because there are fewer zero samples, kind of drift arbitrarily toward that that one boundary. Okay. So what is the shape of the boundary. So imagine that you've got just two variable attributes x one and x two within our least squares model will make a prediction for some sample. And it's just going to be our linear model right w zero plus w one x one plus w two x two. For the Parkinson's problem will just have this decision boundary at point five. And if it's less than point five this output values less than point five, then that is zero and otherwise it's great. It's, it's one. So the boundary is given by the equation, you know, the sum of all weights times the times the inputs equals point five. So what shape is this actually. So the above methods what we call discriminative that is they're trying to they're picking up the features and trying to say, how do I wait these features so that given a set of input features I can discriminate one class from another right it's not taking really into account. Things like how many classes there are, or how many instances of each class there are just like in in the data set or in the wild. So another alternative approach is to basically create this probabilistic model from each class. So that is, this is a generative model so that is, there is a underlying base rate for each class. And that's going to influence your prediction. Right so if, if I'm looking at you have to know something about your data I'm looking at classifying Parkinson's. One of the things I want to know is like what's the overall base rate of people have Parkinson's in general. I might also be more interested in like what is what's the base rate of people have Parkinson's who come into this clinic, because this is where my data came from. Right so if I'm if I'm looking at data from a particular source. I may be more interested in the class relative to that source as opposed to just like the class in the wild so this is one of those. So things we just have to be aware of like your environment so like a lot of studies that are done at universities of course, there's a bias towards the sample population right so there's this acronym weird Western educated industrial rich democratic. And so you know, most people who come to a university you're going to fulfill like at least three of those five categories. And so there's a bias toward people who come from those societies and against people who do not come from the societies because they're left out of the data. Right so maybe more interested in like, what is the prevalence of this class in my capital weird population rather than what is the the prevalence of this class in the entire world because your sample is not of the entire world your samples of a particular group or set of roots. Okay, now before jumping into simple generative models. Let's just do some review of the probability theory that we talked about in lecture two. So, if you want to review this many of these concepts were raised in the entropy section of that lecture. Let's take some boxes of fruit. And we have four boxes. We jars, this red one is a blue one, and they all contain each contains some number of number of apples, oranges and strawberries. So, if we count the number of fruit in each jar, we can see in the blue jar there's two apples, six oranges and four strawberries and the red jar. There are three apples green ones. And then the orange and then two strawberries. Right. So, there's 12 fruit in the blue jar and six fruits in the red jar. So then the probabilities of a fruit from a given jar that is in the blue jar. There's two out of the fruits to the 12 fruits or apples, six out of 12 oranges, 412 or strawberry so there's like a given the blue jar. There's a 50% chance that if I pick a fruit out of that it's going to be an orange. Right. So, if I have the red jar, there's a one in six chance and I'm going to draw an orange out of there because there's only six fruit and one of them is an orange. So, now, let's say that first I choose a jar, and then I choose a fruit from that jar. Right so now instead of a single event that is, here's a jar pick a fruit out of it. There's two events. So, first you pick a jar, then you pick a fruit out of that jar. So now we need to know the probabilities of picking the jar in the first place. So let's just say for argument's sake, that the probability of choosing the blue jar is 60% choosing the red jar is 40%. These are just numbers and no real reason behind this you could say, these are equal probable, because there's only two jars and you could choose them randomly, but that that wouldn't change the probabilities, which is what I'm interested in showing here so let's just say that these are. So then the probability of choosing the blue jar and drawing an apple out of the blue jar is that joint probability or the product of these two choices so 0.6 times 0.167 or 0.1. So if I do all the multiplications, we can see that the joint probabilities of these events are, you know, these different values here. And you'll notice that these don't sum to one. Right. Why would they sum to one. They wouldn't because they, you're basically picking the jar first, and then you're picking the fruit from within that jar so whatever about whatever value you finally get is predicated on the probability of those two events, and you will notice that the sons here are not due to rounding, but they're really really close to 0.6 and 0.4. So basically what I'm saying here is that the probability of picking the blue jar and then any other event relevant to that happening is the same as the probability of picking the blue jar. So there are a limited number of things that can happen. After I picked the blue jar, the probability of any of those things happening after picking blue jars the same as probably just picking the blue jar. So it's literally just, what's the probability of event x, and the probability of literally anything else happening well the probability of literally anything else happening is always one. So, just multiply by one. So combine these into a two dimensional table and show the joint probability of the different events so it looks something like this I'll have the jars on the rose, and then the fruit on the column, and they look something like this. And so you can see that here are my numbers that are just the sum of the probabilities of the of the jars. And these should be the probabilities of just like picking a fruit right and these together, right at all these together they should some to one. So there's where I get my, my someone. So now, just symbolically, let's just represent these as variables to let J be a random variable for a jar and F be a random variable for a fruit. And so you can represent them like this so here are the joint probabilities, just given as you know p of j comma f. So you can usually written, you can usually just eliminate like the, the equal sign here so you just maybe write this as like probability of blue comma Apple, and the assumption is like you know which one of these refers to a jar between these reverse to the fruit. So, these can be used in Bayes rule. And so, what we just saw as an example of the product rule. So that is this so if I take the joint probability is going to be equal to the probability of some event, given another event times the probability of that event so I have some event f and some event J, then the probability of F comma J is going to be the same as the probability of F, given J right vertical bar means given times the probability of J. And since these two, these are joint probabilities so I can just swap them, right the probability of f and J, the same as the probability of J and F and just looking at core currents is no conditional dependency. So, we also now know that if I swapped these, then I can rewrite this equation as probability of blue given orange times the probability of orange. And so now we know that this equation here, and this equation here are the same. So now I have P of blue given orange times P of orange equals P of orange given blue times P of blue. So now I can divide both sides by P of orange. So what I do is I end up with P of blue given orange equals this side of the equation P of orange given blue times P of blue, divided by P of orange. The general form is this this is Bayes rule, P of a given B equals P of B given a times P of a divided by P of B. So if you want to remember the pattern, it's a b b a a b, and you just need to remember where you put your lines. So, in other words, the posterior probability, don't laugh, is this thing here so this term. This will equal the prior probability. The latter term in the numerator. That is the initial degree of belief in a or the base rate of a, and then times the evidence that is P of B given a divided by P of B, so that the support that be provides for a. So, on the right hand side of Bayes rule, all the terms are given in the fruit example, except for the probability of orange. Right so I know I can get all of these terms these initial probabilities and the probability of the jar from the tables. So, I have to calculate the P of orange but I can do that using the sum rule. And so this would just be the probability of the fruit, the joint probability of the fruit, getting orange, and the jar being some jar J, and just sum that overall jars J. So in this case, if the jar is blue the probability is point three and if the jar is read the probabilities point zero six seven. So we end up as point three six seven. So the Bayes rule can be rewritten by having in the denominator, the sum for all events. B, or sorry events a, the probability of the joint probability of B and a. Okay. So given that we can then put back the, the equivalent to joint probability we calculated earlier in terms of conditional probability. And so for all J some the conditional probability of F given J times the probability of J. So, these are all multiple multiple different ways of representing Bayes rule, depending on what values actually have accessible to you. And this is known as marginalizing out and can be done doing using the joint probability or the initial probability depending on what you actually know. So now we can do this in Python, we can actually represent a conditional probability table as a two dimensional array. And I kind of doubt going to get through this notebook today but I believe I planned on that I'm already ahead anyway. So let's include the row and column names as list and then write a function to print the table so I have jar names and fruit names, and I write my print table function, and it prints out a nice little display, like I had earlier, including the sums, right we got like 18 total and the bottom right here means 12 fruits in the blue jar six fruits in the red jar, five total oranges six total or five total apples six total seven total oranges and six total strawberries and here's the distribution in every jar. So now I can just like some across the axes to get those those some values, I can calculate the sums of the fruits in each jar by doing this. So now I can calculate the probability of drawing each type of fruit given that we've already chosen a jar. Right so I get take the jar sums, and then it's divide the counts by that. And so now I can create this conditional probability table that gives me those values that I saw above. And you'll note that when we do this. We actually see sums to one at the ends of the rose. Right, because what's the probability of drawing a fruit, a given fruit or a known fruit, given a known jar, and then you sum all of them right that should equal to one what's the probability of drawing a fruit, whatever it is out of a jar. It's one right if you're not concerned with the actual characteristic of the fruit is. So we can do it more if we code the probability of selecting a jar as an array of point six and point four. And so now I can use this to calculate the joint probabilities just by multiplying those conditional probabilities by the jar probabilities. And so now here's my joint probability table, and now we can see that some to one only occurs in the bottom right as it did before. So this is this table is all possible results and so this better some to one now we're back and like, what's the probability of an event. If something happens, anything I don't care about a specifics there for its one. So how do we get the probability of a fruit from this table. So we just marginalized out to remove the jars by something over the jars. And this can be accomplished just using NP sum over the axis that represents the jars. And here we see those product those fruit probabilities that we saw before. So we know the probability of a jar, given that you know which fruit was drawn. And so now we can calculate that as p of jar given fruit equals p of fruit and jar divided by p of fruit. And so now we get this. So, just we can represent all these as NumPy arrays and with some pretty simple operations, just keeping track of your axes, you can get all these joints or conditional probability values out of it. And so if you don't know which of these values you need you can use these operations to get it. So now let's use Bayes rule for actual classification. So instead of our fruits. Let's look at hand drawn images. Right. So now I'm trying to say, I pick a image out of a quote jar. And I want to see, okay, what digit does this represent zero through nine. So let's let I be a particular image. And then to classify I some digit like for, we need to know the probability of a digit for given an image I right so it's assumed there's some a bunch of samples of hand drawn for is another digits. Right. And so those are individual instances, and they all belong under a class, right that is the digit. I just want to be able to classify the image as the digit so I need to know the thing that I have is the image. What's the probability of each of these 10 classes given this image. Now we probably only know P of I given digit. Right. But if we assume that I is one over the number of images. Right. So what's the probability of drawing this specific image. It's one out of however many images I've got right. And then how many of what's the probability of the digit being for if we assume it's evenly distributed. Then this is going to be one out of 10. Right there are 10 digits and like I'm just choosing them randomly should be 10% chance of getting any single one. So then we can use base rule by plugging in all those values. I'm sorry. In this data set now. So we'll assume that these, these are all unique samples. So I'm here that look like really similar. But they're all unique. So, base rule will give us the following so P of four, given I is going to eventually simplify to P of I given four times point one, right that's the P of digit being for divided by one over the number of images. So, for a simple generative model above we had used your function as a discriminant functions there's a boundary. And if it's on one side of the boundary it's one class of us on go side of the boundaries and other class. Now if we had three classes, you would end up with three discriminant functions, and then you compare the values to find the maximum value to make the class prediction. So a discriminant function is this describes a curve that separates points in the data that describe different classes. So if you remember from the neural network lecture I had the one example of like a curve of blue points and a curve of orange points. Right so if those represented different classes and just trying to draw a line between them. Right so instead of trying to draw a line to fit data, and now trying to draw a line to actually separate individual points. So for n number for any classes the number of discriminant functions is going to be either n minus one, or P, where P is the number of predictors that is important features that are strongly correlated with each each class, whichever one of these is smaller. So a different way to develop a similar comparison is to define a probability distribution over each of the possible values, which are generative models so basically I have for every class I have a different model. And this is going to incorporate things like the base rate of that class in general. And so then I just run all of my models. And I'll see whichever which one produces the highest results. So these all eventually should generate some sort of probability. I'm going to say like, if I have three classes one two and three it's like okay it's 50% likely that it's a one 40% likely that it's a two and 60% likely that it's three. Right, it's going to say three, and those classes don't have the probabilities don't have to sum to one. Right because each of them is a separate model it doesn't know anything about the other models. So how would you like to model this probability distribution or a typical cluster. So the data samples from a class have actually does it tend to be close to a particular value. So that is that the samples cluster around a point. So if I have some n dimensional representation, all my samples from one class kind of cluster close to each other, right, or at least a two other than they are to other things. And the cluster could be very loose or could be very tight. But the point of a cluster is just that things in that cluster are closer to each other than they are to other things. And we can do clustering algorithms and we will later. And you can also do like a cany or a neighbor to assign new values to one of n clusters. But we want to kind of cluster over some central point of the sample space you want to pick a probabilistic model is going to have a peak over that point. So whatever point this is in n dimensions. If I describe the distribution that distribution should peak close to that point. So that is things that are closer to that will have a higher value according to this model. So that is the value of the model is close to zero as you move away from that point. So to construct such a model we want a couple of different characteristics so that is the value of that model will decrease as we move away from that central point. So I have a sample that is far from the central point for a given class, it should have a low probability of being a member of that class. And then the value will always be greater than zero. So the least probability that you can have as being a member of any given class is going to be zero. So if x is the sample and mu is some central points you can achieve this by taking one over the distance between x and you. So it's magnitude of the vector. So let's take me to be 5.5 and make a plot. And we can get something like this. Right if I use the function that I defined previously, then this will give me something on the order of this. So it meets our criteria of clustering around a point, but it goes to infinity at the center, and you can't control the width of the, the decay, which central samples may appear. So first of all we can take get rid of that infinity issue by taking the distance to be an exponent. So that when it's zero the result is going to be one. And the result is if I take two to the one that is sorry two to the zero, right if these two are exactly the same, right this is just going to be one over one. So let's use base two for this to make things simple. So now we can want to see how we do a calculation with a scalar base and a vector exponent. So let's try to do this. This is not going to work, right because I cannot exponentiate a an into a list. It's not compatible. But if you use an umpire a you actually can do that. Right, so this is a lie to do it element wise. So neat trick there you don't need to stick into a full loop. All right, so if we do that. Okay, so this is better right it doesn't go to infinity as we approach this point. It is capped at one. So the whole falls off way too fast. Right, so I want things that are like really close to this but not quite at this I probably want that that probability to be like point nine something, not. I don't want that probably to fall the way down to like point eight. And so we don't want to be so exacting that you must hit this, this central value exactly in order to actually get a reasonable probability of being in that class. So we want to change the distance to this function that's going to change more slowly at first. But when you're close to the center but then continue to fall off. So what if we square this right now I have two to the distance squared. This looks better right so it starts to fall off less slowly. So this is a much nicer shape. And so now we can scale the square distance to vary the width. Right so if we scale it by point one this means that the probability of a sample mu being in cluster x is going to fall off 10 times as fast. So this looks better right this is a pretty nice nicely shaped function. So we could just pick a center and scale factor that best matches the sample distributions. But let's make a single one more change that won't affect the shape of the model is going to make the calculations simpler. And so that is we're going to change the base of the logarithm. Right. So, you know, I assume you're all familiar with some of the properties of logarithms and we'll see how they come into play to fit a model to a bunch of samples. So we've got the logarithm of say two to the point one times the square distance, or that is what is rewrite this as Z. So what's the logarithm of this, we know we can we can convert between logs so if we're talking base 10 logs, then log of two to the Z would be Z of log two or Z times log two. So this means we can pick the base of whatever we want. So, there is a number as you're all aware that has some really nice properties when it comes to logarithms which is E. So the logarithm of the so we use the natural log so natural log of E is one. And so now, if I take natural log of the Z, this will be Z times the natural log of E, which is equal to Z. So this makes things a lot simpler. So now our model is going to be the probability of X is one over E to the race of the scale factor times great distance, or each the negative scale factor times it's great. So this is just going to be some constant. If I plot this again, this is really not going to change the distribution. So the scale factor is a bit counterintuitive. So that is the smaller the value the more spread out the model is. So let's divide by the scale factor instead of multiplying it and we'll just call it some some variable Sigma so we can tune that value. Let's also put it inside the square function. So that's going to directly scale the distance rather than the square distance, which also makes the calculation simpler. So now we end up with something like this. We're going to end up taking derivatives should not be surprising. So we'll be taking derivatives of this function with respect to parameters like mu. So then let's multiply by one half so that we bring the exponent down. It looks just going to cancel with that one half. Again, this is all just to make the math simpler. We actually go to do the computations. So now the one remaining problem is that this is not a true probability distribution. So this probability distribution must have values between zero and one, and then it must have values that sum to one over the range of possible values. So we satisfied the first requirement but not the second, and we can fix this by calculating the value of the integral and dividing by that value called the normalizing constant. So this is called the Gaussian integral which ends up being the square root of two pi Sigma squared. So if you want to go into more about why that is check the article. Now we finally have this definition, right so we scale a by one over the Gaussian constant. And then we take a time raised to the negative one half times the square distance over Sigma squared. So now we've arrived at the normal or Gaussian probability distribution or 10 technically the density function there of assuming mean mu standard deviation Sigma and thus varying Sigma squared. So now you know a bit about why we use the normal distribution, and it's so prevalent because it has these really nice properties and desirable characteristics that are useful for for constructing probabilistic models. So now you just listen more about you know probability theory from various authors. Read more about that at your leisure if you so desire. So now before getting into Python, we need to define the multi variant normal distribution. So we should go to multiple dimensions because we don't know how many classes are going to be dealing with, and we need to have a normal distribution to allow me to do that. So we're going to handle multi dimensional data and not just scalars will go up to multiple dimensions. And so now if we say have two dimensions that hill even drawing is going to be a mound, right, look at looking like those hills that we drew in the neural network lectures like lecture seven or so. So basically we're just going to have a two dimensional base plan and define some coordinate. And then we want to have this distribution fall off appropriately in all directions. So we'll define now x and you to be these two dimensional column vectors, and we're doing two dimensions here so we can visualize it well, but once you've established the convention in two dimensions, it can be easily scaled up to over many dimensions that you need. So what should sigma be. So you need scale factors for both dimensions. This will allow us to stretch or string, shrink the mound in both directions right because you may not want to fall off equally in both directions. So it might be more clustered around in a particular dimension or along a particular axis. So in two dimensions, the difference vector is going to be basically x minus mu. And you end up with these two values d one and d two. And so then the square distance is going to be d one squared, plus two times d one d two, plus d two squared. And so now you can see where the three scale factors go. Right, so we have s one here, and then s two here and then s three here. And so this can be written in matrix form. If we collect the scale factors like so, we have two instances of s two, because we multiply it by two. And now I can put s one here in the top left d s three here in the bottom right and s two here along the opposite tackle. So now you can think about, yes. So think about, if you have multiple dimensions. So, think about how we expand a polynomial. So if you have the, if you have just a binomial, you end up with basically a squared plus two AB plus B squared right so we have a, you know, a cubic function right it's going to be a really bad function of the top of my head like a cubed plus two a or three a squared B plus yes. Then the coefficients get arranged in the matrix. You're going to have like three instances of s or two instances of s two like three instances of say whatever s three would be and then two instances of s four, and then that's five. So now what we can do is we can now have d transpose times sigma times D. Okay. And so now if I do this if it's the inverse the identity matrix is just cancel out to be sigma. Okay. So, if I have d transpose times sigma times D, then I have D one D two times sigma times the original D. And so this will eventually expand out to this where I have every instance of the scale factor, the right number of times. Okay. So it's more intuitive to use scale factors that divide these distance components, rather than multiply them so in the multi dimensional world we can achieve this just by taking the inverse of sigma. Right, so now we're coming back to that in this matrix. So now the normalizing constant is a little bit more complicated. This is this will involve the determinant of sigma. So this is going to be the sum of the eigenvalues and basically a generalized scale vector. So what's an eigenvalue you can go to that here, right so eigenvalues and eigenvectors characteristic vectors of linear transformation. So this is going to be this non zero vector that changes at most by scalar factor if you apply some linear transformation to x. So you could you take a vector and you stretch it you rotate it you scale it. And then you can see that there's going to be a value in there that will let most change by the scale. And that's going to be the, the eigenvalue associated vector would be the eigenvector. So you can skim through the Wikipedia entry on determinants or whatever source you you wish. But basically the multivariate D dimensional normal distribution is going to be given like this. So now you can see that this bears certain resemblances to the function that we had previously. So we're basically trying to figure out what the value of sigma is. And so we can see all these terms in terms of D and sigma, big sigma that are basically defining what little sigma is. So if I have a multivariate distribution, there's going to be a standard deviation and basically all of these dimensions. Right so I need to figure out what that value is going to what those values are going to be. And so I can see that these values might not fall off evenly in all dimensions so I could have different values in the different dimensions, and I can collect them all into a matrix, and then represent this function in terms of that matrix and those. Okay, so definitely not going to get through this all today. So the Gaussian distribution is a nice choice. It's integral sums to one its values always non negative. It has a derivative of a natural logarithm which is very nice and very convenient. So we can divide P by this kind of nasty function but it contains all the terms that we need right x minus mu is the distance from the sample to the cluster center. And these are distance constants right so now I can multiply them by the covariance matrix, and then I reuse that covariance matrix in the calculation of the normalizing constant. So now if mean mu is some D dimensional column vector, and sigma is a D by D symmetric matrix. In addition to the above reasons for this this has a number of interesting properties so that one is the central limit theorem. So that is the sum of many choices of n random variables or 10 to a normal distribution variables, you know, trends towards infinity. So, let's play, just go through like much can we reasonably get through. Okay, maybe I'll get as far as QDA in the next 15 minutes. Let's go for it. Let's play with this theorem a little bit in Python, and we can use this interact feature. So, what I can do is I can plot a uniform distribution, and then the sum of all the samples and I'll then be able to mess with this value a little bit. And then I can see that as if I start with n equals one they're basically just almost evenly distributed. Right, not entirely but close. And then as I increase the number of samples, it very rapidly starts to approach this curve. Right, so as they get closer and closer to to infinity, right, I get a nicer and nicer Gaussian curve. So I'm trying to approach, and I could increase this value is just take the notebook with my crash I'm not going to do that. But you can play around with it on your own you can see that as we approach greater and greater values this curve starts to get smoother and smoother. So, there's that. So now how would we check the definition of probability, according to what we just calculated. So first you need a function to calculate p of x given mu and sigma. So that is p of x vertical bar, mu and sigma. Now if I put in my normal distribution function with inputs x mu and sigma where x contains those samples is an n by d matrix, mu is the mean vector and sigma is the covariance matrix. So, what I can do here, let me just run this code. If I look at normal D this now reproduces the the dots, doc string for that. Let me check out the shapes the matrices and last calculation. So, if V is going to be x minus mu, right, this is that distance. So, all my distances divided by that or minus that cluster center. So these should be all those d values. And then the normalizing constant is just one by one. And so this is going to multiply by something that is n by D. So, I'm going to take the dot part of n by D times D by D so this should come out again to an n by D. So one by one times an n by D. Then I take an n by D. This also much like comes out to an n by D some across all of the axes or some across all the axis one. This is just n, and then I can reshape it into an n by one matrix. So, if I take this and then I got an n answer is one for each sample. Right, this is what I was after. So I have a bunch of inputs, and I transform them into a single column vector, or column matrix that has an answer for each sample. Okay, so let's look at it. Do just an example with some some dummy numbers. So if I create an x a mu and a sigma and print them all out here those values. So now I want to see if it's, if I run normal distribution, this will say if you know for given this, this mean and this covariance matrix, if my input samples are 1235 and then 2.1 1.9, my just two dimensional coordinates. So, you can see the probabilities of falling into each of those classes. Right, so we can see here that given this, this mean and this covariance. This model is not a great fit for this sample, whatever it is because it has a low probability falling into all those classes. It will predict that it's an instance of class three, because that has the highest probability, right, although it's, again, if we're talking about like say confidence, it might not be very confident that this might be slightly more confident that than anything else. So, I'm just working with plot these. So nearly a surface plot in three dimensions to show the value of normal D. So we can do the 3D plotting that we did before. So I'll use actually access 3D, I'll plot create my, my mean, and my covariance matrix and I'll just create the Z mesh. So this looks something like that. So, given those that mean that covariance value, we basically have a probability distribution for this class that looks like this so just imagine for however many classes I have I just get a bunch of hills like this in different locations, right and then maybe wider or narrower, right depending on how rapid the fall off is. Okay, so finally back to that masking problem and is it through this and I will, I will review it next time. If you were thinking of the radial basis function to fix the masking problem you're right. I don't know if any of you were actually thinking of that. But if you were congratulations. But, remember what a radial basis function resembles or if you don't know what a radio function basis function resembles, you can know that it resembles a normal distribution. So let's say we come up with some gender distribution like the normal distribution for some class K. So I want to call this the distribution of p of x given class K. So how do we use it to classify. So for each of those classes, we run that model and take the highest value. Right, so we can do actually do better than this. Think Bayes rule. So we want to know the probability of the class, given the sample not necessarily probability of the sample given the class. So how do we get this from the probability of the sample given the class we can apply Bayes rule. So, given this let's just jump to the second line here. So if P of K given x equals P of x given K times P of K divided by P of x. We can get the joint probability we can get the value of P of x by marginalizing out over all the joint probabilities. So we can get the probability of P of x and C. And in other words, because this is equivalent to the conditional probability times the probability of the class, we just now need to sum over probability of x given K times the probability of K. So for two classes one and two, we can then classify a sample as class two if the probability of C equals two is greater than the probability of C equals one. I just run this, I can rewrite both of these in terms of Bayes rule. So now just the probability of X given C times the probability of C is greater than the probability of the other class. So now all I need to know is the probability of the class and then the probability of the sample given the class which I should have. I can factor out these because this is just some constant. Right, so this is a constant in both sides is the same sample. And I can just say that I don't actually need to know what this value is, because as long as it's constant these two will be the proportion will be relative. Okay. So using the assumption that our gender distribution for each class is a normal distribution. Now all I need to do is take this and are the function times the probability of the class in question and this evaluated for both classes. So, running out of time here. So if I do, okay if I do this, then this will, this will simplify to the version below. So I can make that a little bit simpler by using logarithms, right if I do the logarithm, it's going to bring the exponential down in front case in point so by just take the logarithm of the covariance matrix for the second class. This allows me to bring the one half down in front as a as a coefficient. I can get rid of the E bring the one half down. Now I'm back in terms of the square distance and the inverse covariance matrix. So all I need to do is calculate this, add the log of the log problem of that class and do the same thing for the other class, and then compare which one is greater. So now we can define the last, the each side of this of this lasting equality as some discriminant function is called that delta for class K, and then the new sample this the class of a new sample X is going to be the arg max for all those discriminant functions. So then the boundary class between the boundary between class one and class two is going to be the set of points for which the discriminant function is equal for the two classes, right if I'm right along that boundary, it should be 5050 which one I mean, and this equation has to be quadratic and meaning that the boundary between class one and class two is quadratic. And so just to find something you may have heard of called quadratic discriminant analysis, which is a quote linear way of doing discriminative or doing classification using generative models. So a lot of terms in the mix here and kind of counterintuitive. It's, I'm looking for a discriminative function for probability distribution, which I'm using to define generative models of different classes that, and it is a quote linear way of doing classification, even though the function is quadratic. So I apologize for that I of course did not invent these terms and their usages. So it's just a lot to kind of keep track of. Okay, so we will do, we'll go through QDA code and do linear discriminative analysis on Tuesday. All right, thanks, and I hope to see you at the event this evening or watch it afterwards.