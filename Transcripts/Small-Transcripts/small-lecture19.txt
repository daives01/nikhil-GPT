 Okay. Can someone confirm you can see and hear me okay? I'm like, okay, you can. Okay, great. Yeah, sorry. The ironically, I've been going to wifi here is like terrible. So hopefully this will hold out. It seems to be okay up here in my room. All right. Give me a few minutes to let everybody in. We'll get started. Okay. Yeah, good enough. I'll get started. So yeah, thank you everybody for logging in this should be pretty short. I was just doing the next lecture, and it is pretty brief and one after that is relatively long so what I'll do is I will just do lecture 18 today, and then just take Thursday off and then tomorrow or next week will resume and we'll be back on the expected schedule. Should the wifi crash for some reason and you lose me, what I will do is I will just record this and I'll post it of course as normal. So I guess, only just a couple of announcements today. So we are working on assignment three grading and we will if we get that back to you by the end of the week or so. And I know some of you had submitted some questions on assignment for I will do my best to respond to those in a timely fashion. I'm traveling tomorrow I'm going from San Francisco to the east. Although I will have like I mentioned before, like the layover in Denver so maybe I can spend some time responding to your messages. So, one announcement that I want to just send out to to all the CS undergraduate through here as you might be, as you may have heard, we are in the process of recruiting a new chair of the department and our first candidate, Bruce Draper is going to be doing his campus interview. So, we're going to be in the department tomorrow and Thursday. And as part of the chair search we are, we want to basically get feedback from every constituent group on campus. So, our in the departments that includes, of course, faculty and staff and then people at the college and university undergraduate students but also undergraduates. So, if you are a computer science undergraduate I know there are a number of people from other departments and forgive me for just sending this out to the CS folks. If you are a computer science undergraduate and you have some time around lunchtime on Thursday, you will have an opportunity to meet with Bruce, and once you get free food. And I think it's a good chance to talk to him about his vision for the department how he sees it evolving the next five years during an insensible term as chair, and make your, any of your concerns, or known or get any questions answered. So, at the very least this is an opportunity for you to get some free food. This would be in room 305 of the computer science building. So, if you're interested in that, you have an opportunity to talk to our incoming chair candidate, and we'll have one more after that probably early next week, where the same opportunity would be extended to you. So, if you're interested in that I would encourage you to take part in that we would like to see I would say, maybe a dozen or so folks at least attend this lunch. If you have any more questions about that feel free to reach out to me I can give you some more information if you're, if you want. I believe that's all I had to to announce right now. Anybody got any questions about anything. Okay. Let me screen share up. Okay. All right. Share screen. Let me grab my water real quick. Okay. All right, so this will just take as long as it takes and after that I'm going to let you go. So, we hide the controls. So, basically what we're going to talk about today is going to be reinforcement learning in continuous spaces and this is going to be just a slight evolution of the quote one D maze example that we use last time so remember. One D maze I suppose is just a fancy way of saying a number line. So we're just we have some determined goal position on the number line and our goal is to try and move our agent or our objects whatever it is. And then I'm going to move to that position. So that is if my goal position is five and a number line from one to 10. Then what I want is if I'm to the left of index five, and then I'm going to move to the right, I'm to the right of it then I'm going to move to the left and if I am exactly at five I want to stay still. So the way we modeled this is basically we have a discrete action space, which is negative one zero one. And then we will take these steps in given directions. So I have basically a cost associated with taking a move. So that is, I get a reward of negative one for taking a move unless that move lands be in my goal state, in which case I get a reward of zero. So in this case we have a cost function that we are basically trying to, to maximize the reward by virtue of taking the fewest steps possible or the steps that are most likely to land me in my goal because I get a non negative reward for that. And that's kind of our deterministic version, or discrete version. Let's think about how we can model this in a continuous space. So that is, let's, I'm going to say we have a marble. So now I have basically a flat flat plane. And I'm going to move horizontally along this, and I'm going to do that by say picking up and moving my marble but not going to do it by exerting forces on that marble. And so we can think about how this is actually going to work in say a real physical space and you can imagine that if I have a rolling object which is Sam to constrain the motion from into a single dimension. And my, the state can actually be modeled using the position of the object and its velocity. And then my actions are going to be forces that exert upon that marble so I can actually push it to the left or I can push it to the right or I can do nothing. So you imagine this is going to be a slightly different scenario and I wish I'd brought a marble with me of course I didn't think about that. But if you imagine if the marble is already moving in a direction, right, if I, if I may take no action is going to continue moving in that direction will assume basically I'm not going to have things like friction. So I'm going to say it's a frictionless space. You could model this as a as an environment that has friction if you had a more realistic simulator. And in this case we're just going to be doing it kind of text wise in Python. But let's imagine that we have a marble, and I can push it in one direction or another, or I can do nothing. So of course if I'm already moving in a direction, and I do nothing it's going to continue moving in that direction. Whereas, if it's got a high velocity say moving toward the right. If I push it toward the left if I exert a force into what left it might not slow it down way it may not move it, start start moving toward the left. It might slow it down, and it may in fact slow it down enough that it will, it will basically stop the motion. So my goal now is that if I have a goal location. I'm going to have a slightly different policy, right so you imagine now that instead of taking these discrete steps left to right, I'm exerting forces, and those forces are going to have effects on the continuous motion of this object and so the policy is going to be quite different. So, unfortunately, I can't really see any of your reactions I hope that was a clear explanation if it's not what I recommend you do in this remote scenarios basically just like raise your hand, or put a question in the chat, or even just like speak up and interrupt so honestly speaking up and interrupting is probably the best way to get my attention in this in this scenario. So our goal is now to get the marble in a particular position on the track. And so we're going to be doing this using a similar method, basically just going to be using my neural network as a regressor, and we'll see how this is different from the discrete case of the exam last time. So import my neural network with regression so I do my standard imports with numpy and pandas and pi plot and then also I import my optimizers and my neural network class. Now we're going to define our reinforcement function initial state function and next state function, but they're going to be defined to basically model this dynamic marble problem. So if you compare the functions as you specify them here to the ones in the notebook from last time you'll be able to see the differences. So, what are our variables. So we'll have first X sub t, let's be position, and what's called the unit meters for now, it's gonna be whatever it's a big marble right it's going to be moving in terms of meters, but this might as well be centimeters. And then X prime is going to be the velocity in meters per second. Okay. So if you look at what is my position at time step t plus k that is X of t plus k. This should be my current position right at time step t, plus the integral over time current t to t plus k of the velocity. Right, so that makes sense if I'm moving a constant velocity over and seconds I need to calculate how far I moved over and seconds, and then add that to my current position that's just going to be the interval of those values. So, you know, we all should be familiar with with how this is done. You know, we've probably all done, I hope you've all done intervals and your calculus classes. And so we know that velocity is the derivative of position acceleration is the derivative of velocity, and what's the derivative of acceleration it's actually we call it jerk. And then beyond that I don't think they have term for that. So, we're trying to now calculate the integral the anti derivative of velocity with respect to time. So of course this is what this year. So I have, I'm going to be calculating with respect to t, I'm going to be taking the, the integral of X prime sub t. So, that is the case position from now is going to be the current position, plus the total of all velocities over k time steps. Of course this is a continuous, continuous calculation. So for any time step t calculating the change of position is basically going to be the change in t delta t is going to be the current position, plus the change in t however many time steps have elapsed, plus times the velocity for for those time steps. And then of course calculating velocity is going to be the same I'm just going to be adding the acceleration right so X double times can be the acceleration at time t and this of course would be in meters per second square or whatever your unit of distances per second is going to be in meters per second square. Now, this is a continuous problem is mentioned, but of course we have this issue that I'm sure you have encountered before whenever you try to calculate integrals and calculus class we have computers being discrete machines. So, we know that the exact integral curve is is unknown, but we know that it's starting point is a particular value. So, if we have my starting point of my integral curve as a sub zero here, we can use the order integration method to approximate it so that is I take a small time step, right some discrete time step, and then I approximate the value of this curve over those time steps and of course here, because the size of this time step is large enough that my approximations get to about time step four is somewhat higher than the actual value of the integral. And so this is going to be if you remember when if you study calculus like I did. One of the first things you probably did was break the curve in the area under the curve into basically boxes right discrete time steps so you might draw something like like a terrible drawing. Let's say I have a time step, you know, something like that and then the next time step I might approximate it like this much, and then so on and so on. And then basically the integral is taken to be the sum of all these and then of course, if I have some time step, you know, T, if I decrease this value of T I'm going to get a better and better approximation right so if I subdivide this suddenly, I have these these small boxes that are basically giving me a finer approximation of the area under the curve right. So that's the other integration method. And then of course we can show that as the limit of this, that subdivision approaches, you know, infinity, then this is going to be an accurate approximation of the actual area of the curve. So in this case, whatever Delta T is or we define that as is going to be the order time step so here might be fairly large value so maybe over a second, this is an imprecise approximation but over say 10th of a second it's going to be better. So how do we model this in actual code. So first let's define our functions. So our valid actions right remember these are forces now so I'm going to be exerting a force of negative one, or one or zero on my moving object. So now my, my functions have to be defined somewhat differently so for the reinforcement one ready I take an SNSN of course this is my current state on my next state. So my current state is state five. But of course, I know that the likelihood of landing at exactly state five is pretty small. So I want to basically optimize my policy to approach state five as close as possible and I want that the reward for that to be zero when I kind of get within an approximate integral of that value. So what I'll do here is I'll define this as basically if my, if my next state is within index of one within one unit of my goal, then I will get that zero reward otherwise it's going to be negative, negative one for every action that I take. Okay. So initial state is very similar, except I have, I'm going to be sampling continuous value instead of just some value between like zero and 10. So I will take you know, don't add some random noise to my to my starting position. So the next state is kind of the continuous analog of the previous version that we had. So element zero is the position element one is the velocity so now this is broken up my state down to two distinct values. So not not just the position but now this is the, and then my action is going to be one, either negative one, zero or one. So the difference here now is I'm going to define my order integration time steps so here I'm going to say it's a 10th of a second. And so now, given my velocity that is s one, I'll be able to add that value. So this is per seconds, we're going to multiply that by how many seconds are in my time steps in this case point one, and I add that to my position. So force is the action that I take. So I'm going to define the mass of the object so here I'll just define the mass to be a particular number. And then we can, here we have say, a little bit of friction here that we can so I'm sorry, I guess I was incorrect and saying that we're going to model this completely friction this environment we're going to have a small amount of friction. So for this, I'm going to take that delta, and then multiply it by force divided by the mass minus point zero five or to account for some friction, and then times my velocity value. So this will be my new velocity. So now I'm going to have some balance on it of course so I don't want to roll off the edge. So if I hit the, if I hit the boundary, then I'm going to set the velocity to zero I'm going to stop moving. Okay. So my initial state function above that's going to sample initial state will be a two element array. And the first element is the position to the marble. And second element is the starting velocity so starting velocity is always going to start at zero. So if I end up I started like at seven point something. Then it's going to be sitting there, and I want to figure out what action I have to take, and that might be exerting a force to the left or to the right. That's going to move me to a different place on my on my line. So, in this case, I sample my initial state it gives me 6.13 so we can say okay my goal is five I started 6.13 and kind of to the, to the right of this. So, given this, I'm going to randomly choose from our three valid actions. So here it says okay I'm going to take an action of one that is zero force to the right. And then I'm going to do this, you know, a number of times. And so they're going to sample these actions, and then calculate the reinforcements accordingly so here I'm going to do this for like 1000 time steps. What's going on here, well I'm going to make a random choice from my valid actions. I'm going to use that action to calculate what the next step state is. I'm going to use the value of those that state to calculate my reinforcements. And then I'm going to append that to a list and I'll plot it. So, run this and we can see what happens so after 1000 time steps, I started at, in this case it's starting at 5.1. And it is getting to 5.56. I'll try it again. So here, we start at 5.8, and we end up at 6.6. So try again. I started at nine I ended up just hitting the wall at 10. So briefly, let's try and gloss this graph a little bit hard to read, but we can see. This is actually like a bad, let me try one that's a little more indicative. So here. Okay, so we start at 8.8. And we can see that the, we can see how the position kind of to move towards the left and then it goes back toward the right then it takes kind of it's just toward the left pretty consistently. And then it starts to go back toward the right and eventually it ends up pretty much not far from where it started. The other thing we can see here is that where we look at the reward here is the red graph. You can see that it's usually negative one, except for when my position is kind of within one unit of being at five. In that case, the reward is zero. So, we can see the modeling of the of the position and the velocity is sort of hard to read if the orange line there you can see how you know when it's exceeding to, for example, moving very, very swiftly toward the right and so on. So, you might have noticed something kind of weird about this and we'll get to that in a moment. Let me plot my, my last few values. So if I look at the last 10 positions, and the last 10 velocities. Oh, sorry, right. I read that again I guess so in this sample. Now this one starts at 3.5 and ends at 4.5. So, the last 10 positions right 4.46 kind of moving toward 4.5 last 10 velocities is, you know, point zero two point one five when zero two again, and then the last actions you can see here to go from time step. Let's see. T minus eight to T minus seven. You can order and say maybe two minus seven to T minus six for example, we can see that it's got a no sorry I'm reading this backwards. My mistake. Sorry, I'm pretty tired. So this this compare zero and one. So here we have an action that is zero, and then the, it was moving in a given direction. And so then the velocity is like point zero or two. We have to take this action of one and the speed increases and take an action negative one speed decreases take an action of zero, the speed decreases just a little bit due to friction, take an action of one again it starts to increase. So, did the marvel ever get to the goal position. Well, not really. But we see some places where it approaches the goal position, and that those are where we see this reward as being zero. But it never stays there and it doesn't ever seem to learn anything about how it's supposed to approach that position and and stay there. So, what went wrong here. Well, there's a couple of things. One is basically we're just we were just replacing the state with a state plus action. And then just calculating the reinforcement actually just based on like two instances of the same state. Right. And so what we need is we actually need to have some way of approximating what the best action is according to the state action pair inputs. So that is we need to be using our no one has the Q function. So we need to define our epsilon greedy function. Remember what the epsilon greedy function is. This is going to be there is some probability that I'm going to take a random action. Why do I take that random action, most because I might find some sort of strategy that does actually get me to the goal that I want. But it does it in a very self optimal way. And so once I discover that strategy I do not want to keep repeating that strategy in case it's very secure to serve very cumbersome or something. I have to allow for the possibility that there is in fact a possibly better option there. And I might be able to find it by instead of exploiting my, my best current strategy. I'm going to just take a random action. Okay. So the epsilon greedy function is going to specify some value of epsilon, you can say point two. And so if I sample a random number there's less than that value I'm going to take a random action. Other than that I'm going to take the greedy move that is going to exploit my current best, my best strategy according to my, my Q function. So remember the neural network is approximating as basically a function approximator that's allowing us to learn what the current best move is according to the state action pairs that I have sampled from the environment previously. So this is the epsilon greedy function that's basically the same as it was before. So I have the only difference here is that the state instead of being a single value actually be two values. Right. So now it's going to be position and velocity, instead of just position, but the function is written the same way and all I need to do is just pass in that two element tuple or list or whatever into my state. So, now I have my, my q net. And that will allow me to just like use that, that function. So now I need to be able to train that function of course so here we can see just the epsilon greedy part is not actually training the function. This is just using it was already been optimized. So I'm going to generate some samples that represent my, my experience from conducting various trials in this environment. So I've defined my environment is now having a model of a position and velocity representing my state and then a set of actions that I can take that are these are discrete actions, but they're going to have a continuous response within this space. So in terms of, I'm not going to be picking up and moving my object and be exerting a force on it which means it's velocity will speed up or slow down in whichever direction. So make samples is written generally to accommodate for whatever, whatever my representation of my state is so here I define these function kind of placeholders where I can define what my initial state next state reinforcement functions are. So, here, you know, I just, I just call these functions so next state f is going to take in my state and my action, give me my, my next state, our end is my next reinforcement. So that this should give me the resulting reinforcement from the previous state and the next state. And then epsilon greedy will just use the q net to choose the next action. So, let's try this right so if I create an s is going to be the leftover s from my my sample kind of randomized run here. So you can see that I've got 4.57. That's my position 0.39 this is my my velocity so basically at the end of this that last kind of randomized 1000 trials. So this is our goal that's at position 4.57 and it's moving relatively slowly toward the right. So, this actually would eventually get to the goal, but it probably would keep going and overshooting it. So this s state consists of a position or velocity. So now this is a two element array as shown. And so this triple s a is actually going to be three elements. So, therefore, we're going to modify this make samples function, just to allow our state variables to contain multiple values. Here's that. So what I will do is I'm going to update the state SN from SN a. And so then here, the rest of it is pretty much the same. Okay, so this is a very small, a small adjustment. So if you compare this version here, there is no next state, there's no call to the next state f function here. So what this does is this is going to take my next date. I'm going to assume that my starting actions always zero. And so my next day when I take no action is going to be the same as the current state. Okay, so now I'm ready to train. So before that we do this, we're going to make this plot status function to show how we're doing, and we can look at the various different things that we're going to plot. So like last time, we're going to show a bunch of different graphs, and we can go through all of them. So what I want to plot is, you know, one, the probability of taking a random action. So that is my my exploration over the number of trials. So if I'm doing epsilon decay, then I would expect that as my model gets better and trains more than I'm going to have a lower probability of taking a random action because I'm fairly confident that my, my best, my best strategy so far is actually relatively close to optimal and will give me amount of data and training that I have. Second thing, starting position of every sample right so remember we're creating these samples. And each time I create new batches samples it's going to initialize my environment and start my marvel in a new position so we want to see how my training evolves. And that one time I might start my, my episode at, you know, 2.6 and then the next time I might start it at 7.3 and then one time I might start at 10 next time I'm going to start at like at 4.9. And so as my, as I train, I should have a better idea of how to get to the goal from my starting position. So here I'm going to mark in this graph when I mark the goal position at five so you can see how things kind of evolve. So the next thing I'm going to plot is the latest policy shown as the action for each state so we should have some graphs showing that, given my state. This would be the best action, right and this should be some sort of continuous manifold at this point. For the mean reinforcement versus the trial, so let's smooth this and we'll show the mean reinforcement every 20 trials, and then five is going to be velocity versus position, right so this should be well trained policy well to be intuitive. So if I'm close to five, I should be moving kind of slowly in the right direction so that I get as close to five as possible and and don't overshoot it. And you can check this fill between function is kind of cool you can we'll see what that what that does in a, in a moment, and then plots. I guess this actually be six through 10 I don't know why I'm numbered to seven through 11. So we'll see the max q value versus the state, and then versus the actions actions versus the position velocity, and then we're going to show some top down and some 3d plots. So we can see kind of how this looks and what what inferences we can make from the 2d versus the 3d plot. And then the rest is drawn. So we have a function to test it will pop the marble down at a number of positions and we'll try to control it and we'll pop the results. So, for N trials starting positions I'm going to run a bunch of simulations for a certain number of trials and then plot that. So now we set up the standardization. This is pretty much the same as before right we have our, our means for my inputs and my targets, I'm going to use that to standardize my my values when I'm using them to train the q net. And so then I'll plot, you know, my, my standardization. So I'm going to have a pattern here, but this is what this is basically my training and my plotting. So I'm going to have this is gammas my discount factor that is how much do I discount projected reinforcements like far in the future. Not very confident of those. So maybe don't want them to have a huge bearing on the actual training of my model, because, you know, they're there. They're so far in the future they probably don't matter all that much. So I'm going to have a number of repetitions of the, the key update loop, and then the number of steps between new random initial states. That's a learning rates specify my final epsilon so I can calculate my decay rate. And then I create the q net I'm going to use just use a fairly simple one that has 10 hidden units as a single layer. So here's my q net structure. So three inputs. So for the position of velocity in the action, and one output. And then I'm going to use just be the best predicted action. Where they started the best predicted the best predicted reward. So then I run set up standardization. And I'm going to specify some initial epsilon value. This is one, because when I start I don't have any best strategy. Right so by saying this to one. I'm just gonna say start by taking a random action and then I'll burn some that and then they can decay my epsilon value. And then I collect all my samples, run train, and the rest is for plotting. So, let's run this real quick. And there we go. So, we can see here, for example, this is the random action probability starts at one. And you can see it sort of rapidly decaying towards zero because this should be learning something. So, exactly, is it learning. Let's take a look at the mean reinforcement. So, it starts out like, kind of point eight five. I'm not sure what the initial, the very first initial state that it selected was might have been somewhat close to two five, but it pretty quickly drops towards negative one. And then so sort of just a bums along here for about 300 time steps. And then it starts to actually learn something right so we can see here that it's at this point right about 300 types steps. It sort of start to figure out what the best policy was. And so now here, if you look at the maximum of the, the Q values here and in 2D and 3D, we can see that, you know, for example, what's the Q value when the position is very close to 10 and then the velocity is negative, right is negative point three. And then kind of the converse is true when it's when it's like close to five and the velocity is zero. The actions. This is pretty interesting so you can see that when my position is five. There's sort of this sharp boundary, given the, the actions that I have available to me. So at the state trajectories for epsilon equals zero. So, this is my desired position, right this is five. And so we can see that if I have say a position of five the state trajectories for the lossy 10 towards zero. I have a lot of positions that are say closer to 10, then my state trajectories, you know, 10 toward the negative numbers where there's my positions are zero my state trajectories 10 toward more positive numbers. So this is the policy for zero velocity. So you can see the kind of how well this maybe has trained. So, this is maybe not quite as optimal as I would like so I would expect that, you know, my best policy would basically be our our our our, our zero LLLL. So, for, if I'm at zero velocities that is if I'm not moving I mean one of these, one of these states, where should I be moving. Well, what this has learned so far is that if I'm in states eight, nine or 10 I should move left, which is good. But it's also learned that if I'm in states zero through seven I should move right, which is not quite right. So you can try to train this again to see what happens. So, this is just a little bit of a change taken up like 15 seconds or so. You kind of see similar patterns starting to evolve right this this curve stress familiar. So does this one. And again, I think this is yeah. So, actually you got a little bit worse that time, right now it's basically learning that if I met 10 I should move left. I should not have anything else I should move right. So try one more time. And then we'll move on. We can kind of see things changing here over time right now it's basically like learning a strong policy just moving to the right. Okay now things are starting to happen. We're getting some, some indications we should start moving left there. This is done again. So, maybe this is not like a terribly successful policy and try one more time to see if I can get it to kind of demonstrate something nicer. One thing we're observing here is that reinforcement learning has a tendency to be unstable. And so often you start to learn something and maybe your ex your epsilon is decaying too fast and you kind of find some sub optimal strategy and stick with it. Or, and I'm liking what this is doing. So, you can see that you know, there's, it sort of finds some sort of degenerate strategy and it's kind of sticks there for a little bit. And so sometimes you need more complicated strategies maybe we need a bigger neural network that would take longer to train. Maybe we need more more samples per per episode. That would allow us to, to kind of get get more experience. Let's just let's just sort of accept that this is sort of learned some kind of sub optimal policy actually seems to be doing worse now than the previous like five times and I tried it. So let's plot the or let's print out the last last 10 rewards. So what this has learned here is that if I over time, the last 10 rewards of rewards that it got were negative one. So this was basically somewhat unsuccessful in training by, you know, a couple of different things like you can try maybe adding a adding more hidden hidden units. Try this again. So you can see this taking longer to train this time. But now where we at. Yeah. So we're a little bit more bit earlier. So, now, the policy for zero velocity in this case kind of seems to be just sort of bottom me out with always moving to the right. So what we're seeing here in this example is a strong tendency for the instability of reinforcement learning. And so, in this case for this for these continuous problems. Basically what we, what we find is that say Q learning is not necessarily always the best choice. And so often we encounter these problems with stability when trying to use a sort of DQ network so we have other policies that I alluded to earlier in class such as the actor critic methods. So we have say things like a DBG or a soft actor critic or an A to C. And what that does, we have these two neural networks where one is the actor that is to choose the action. One is the critic that tries to predict how good or bad the action is going to be. Right. And so then the actor tries to get better at predicting good actions and the critic tries to get better at predicting quality of the action. And so these two networks are updated in tandem. So that's one way of kind of solving some of these or addressing some of these instability problems that we encounter with continuous spaces. Okay, anybody have any questions. Let's again, just for fun. Now we're seeing something desirable here. So now we have a policy that's kind of approximating what we'd expect. So this was not another red done here. Let me show you a case that we've been moderately successful. So, if you look at my, my policy for zero velocity. This is where my mouse is that state five this is very want to end up. And so what this policy has learned is that when I'm less than five want to move to the right. When I'm greater than six, I want to move to the left. And when I'm at either five or six, I stay still. So this is actually this is a moderately successful policy and what we actually managed to get one in that it's approximating when I get close to five at least knows that I should probably try to slow my marble down. Whereas if I'm far off to the right I need to be accelerating towards left far off to the left and you'd be accelerating toward the right. And this is reflected in some of my my action policies we can see here that where I have this last sample actually appeared to be quite successful where my my marble position is within this red bar that has been one unit of five, and my velocity is very close to zero. So similarly, if you look at the actions here. It's learned that if my by position is say 10, and my velocity is like negative four, then the action, this is, we really probably don't want to do a whole lot here, for example, because I'm moving in the right direction, maybe I don't want to accelerate too much, whereas the opposite is true. If I am say close to zero and my action, and my velocity is four, right so I might want to accelerate a little bit, probably not too much, whereas if I am at 10, and my velocity or close to them and my velocity is four, I'm moving fast in the long direction, and then I need to kind of accelerate or stop my motion, start moving back in the direction. So now we look at the state trajectories and now this is the pattern that we would expect. So, as I am, if I am at zero, and my action is my velocity is zero. And then we can see that what I'm doing here is like as I'm moving in toward five, I kind of want to increase my velocity but then I also want to start by cross over five I need to be kind of moving back towards decrease my velocity moving back towards the left. Okay. Questions. So, in conclusion, I suppose, continuous spaces are challenging for reinforcement learning. And so you need to have sometimes more sophisticated techniques to try and solve these. We find these cases in, in scenarios like robotic manipulation, where you're trying to control some, say some movement of a joint and continuous space made a manipulate an object. So this is a problem that can be solved with a lot of continuous sampling and learning from experience, but it often takes some fairly sophisticated hardware and the ability to sample, you know, lots of lots of experience from my environment. Okay. So, next time, so basically Thursday, just going to take off because I'm going to be traveling, and then we will reconvene next Tuesday. So what we will do on Tuesday is going to be the reinforcement learning for two player games that is learning to play tic-tac-toe. That's going to be the subject of assignment six. And we see if I have the schedule up since schedule was here. And then I will also assign assignment five then. And so that's this is at present that's going to be the day that assignment four is due so assignment four we do. So I'll assign assignment five to have two weeks for assignment five. And then I'm going to assign assignment six before assignment five is due but you should have plenty of time for this. Also, I hope you're all working in your project proposals. That's going to be due next Thursday question in the chat. Very interesting. Okay. Thank you. Okay. So, I guess there are no other questions or comments, then I have no problem with calling this a short day. And I will see when I get back home. Do I have all I will not have office hours today I have to go back to the conference. But if you have questions, again, please feel free to email me and I will respond as quickly as I can. Thank you. Bye bye. Bye bye.