 I think we're going to have to try to make it all the way. I'm like, I'm lost. That's what Friday's. Yeah, I'll wrap up. Okay. Okay, let's go ahead and start. So here's where we're at. We are still the day ahead. So this is the 23rd be finished next year. I'm going to be doing my first one. So I'm going to do 17. And then 18 next week, remotely. And then hopefully we'll be able to take the 30th. Off. Great for all of us. So today I will do. I'm going to be doing this. This is going to be due. We have two weeks to do this. And, but I recommend that you get it in sooner. Because then I can, I'll be checking for those to come in. And the moment I sign off on it, you can start, even if it's not the 6th of April yet. So I'll talk more about this at the end. Before I start. I just want to like reinforce a couple of points about office hours, ethics. And I wish more people were here. Just a, just a note that. We are, we need to be able to resist every. Fairly. And so I would request that. If you come to office hours, my policy, these is like pretty much open door. And as you observe, you come in. If you, if I, you know, have you do something. You are welcome to stay and try it. But I recommend, you know, I would ask you that you like move to a different chair, or if it's going to take a while, go out in the hall and work on it. Cause there may be other people coming in. So just, you know, I don't, we can't, we really can't have people there like monopolizing our time. Other, you know, office hours for me. Start effectively right after class most days, because they can start talking to you guys if you need, but they ended 430. They end up coming in. So I would like to say that I recommend that you guys come after 430. I've moved on to other things. I'm not there for office hours. Okay. And I think the same is true for, for lab ops, right? There's a specified time I come there for class questions. After that time, we are not obligated to discuss class class content. We both have plenty of other things to do. Um, but, um, I don't know if that's enough. I think that's a good point. So I think that's, um, I think we're just going to go back to this early. Um, again, if there's a flood of people coming into either of us, you know, insisting on help at the last minute, very likely the answer at some point is going to be no. You should have planned your time better. Okay. Um, so just, it's not like a global problem. Um, but it's been prevalent enough that I need to make this clear again. So, um, if you. If you go to college, if you go into college, if you're on mental health, and you're in a mental health problem, you should be learning with time and very likely, you know, I might help you a little bit, but at some point, you know, it's on, it was on your own. You need to be responsible. Um, you all are adults as I said at the beginning of class. Part of, you know, part of being an adult is learning to manage your time. And this is the best time of your life to learn how to do that. Frankly thought of the habits you develop now are going to stick with your life. Um, so hopefully that is this clear. Um, so please be respectful of us. Please be respectful of your fellow students when it comes to, um, uh, making use of office hours resources. All right. Um, so any questions? Uh, very hopefully you have. If this hasn't scared you into starting your assignment, you have, you will start your assignment after this class. Um, because that is going to be due, uh, in two weeks from Tuesday. So, uh, please, uh, please bear that in mind. All right. Um, so I will go into the lecture if there are no further questions. Um, I had the controls. Okay. Oh, okay. Good. All right. This one, this is big. And sometimes when I reloaded and it takes like forever to render all of the, all of the formulas, um, of course he has already loaded. Okay. So if you recall, uh, last time we introduced reinforcement learning with this, uh, thing called a Q function with Q function, um, is basically the function that you're trying to predict given a state and an action. Um, how good is this going to be for, for my objective of achieving my goal? And also where am I going to end up? Uh, if I am in the state and I take this action, um, what state am I going to end up in and what actions can I take from that state? So previously we had filled this out as a Q table, right? So the Q table is going to basically have, uh, a two-dimensional state. Um, and then the action, which is one of four things move up, down, left, right. Um, so we were looking for the sequence of action that either maximizes or minimizes the sum of reinforcements along the sequence. So depending on whether you're, um, you're trying to minimize your cost or maximize your reward. Um, what we're going to do is we're going to be, we'll be looking at lots of examples and looking at the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the cost. So we're going to be, we'll be looking at lots of examples and looking at those reinforcements and returns. So that is if I'm in an action, if I'm in the state and I take an action, um, how good is that according to some, some measure of goodness, right? So again, I would give you the example of if I'm trying to solve a maze and I'm in state s and the, uh, the goal that's right in front of me, I'm going to take a sequence because there's like six unnecessary steps in there. When I may have discovered that from this point, if I take one step forward, I reached the goal. Right. So I don't want to just find one sequence that works and continually exploited. Um, so we thought the queue table as this function that takes the state and a possible action that returns some value for that pair. Um, but we, uh, this, this table is actually a function. So if I have a function, we have certain types of mechanisms that I can use to find the value of that function. So now let me give you, uh, something we've talked about before. If I have some inputs, uh, and some outputs, and I assume that there is a function that maps from those inputs to the output. My job is now to find the values that parameterize that function. So I need something that can say approximate that function. What type of mechanism might I be able to use in the title? Yes, a neural network, neural network universal function approximators. Right. So if it is a function, I can use a neural network to approximate it. I can, for example, if my function is y equals x, I can approximate that function with a neural network. I don't know why you would do that, but you can do it. So I can do with a function of arbitrary simplicity. Of course, I can do it for a function of arbitrary complexity. So, um, if Q is just a function, let me just optimize for the value of that function. So the objective of a reinforcement learning problem is going to be reduced to the objective of acquiring this Q function that's going to predict the expected sum of future reinforcements. So now I'm trying to find this big R value, right? This is going to be the thing that I'm going to try to predict with a neural network. So we're now back in a kind of a pretty standard neural network paradigm of I'm going to take in some inputs and predict some outputs. My output now is going to be the sum of future reinforcements for the total return, and my input would be some representation of the state in the action. Okay. And so this correct the correct Q function once it's optimized will be best determined the optimal next action. So now my objective is to take this approximation and make it as accurate as possible. So here are the components of this. So for, for every step, I want to compute the return for all the summer the summer of the reinforcements from here to the end. So the lower this is basically I'm trying to predict what big R is from this time step to the end. I'm going to do that for all possible steps. And that's going to be the value of the Q function. So to make this sound a little bit more. This is usually formulated as a least squares objectives that is going to minimize the square the expected value so because the, the sum of reinforcements is some sort of scalar value. I can still use only a mean squared error metric to approximate this. So this is a no longer a classification problem so much as I'm trying to optimize the values in my in my network to predict the sum of a future reinforcements. So now I'm trying to, I'm back in the territory trying to minimize error. So I have the, I haven't established Q value, and I'm trying to minimize the error between that and the predicted sum, and I'm going to square that. So the expectation is going to be taken over a couple of sources of randomness so that as there's the action selection, right, we talked about how I can, I have a set of actions that I can take. So sometimes let's say I don't want to, I don't want to exploit a correct but suboptimal solution such as moving back and forth left and right before moving toward the goal. So I want to have some level of randomness of I'm going to take a random action. So that random action that that's that epsilon from the previous lecture. That's the thing that allows me to determine with some probability I'm going to take a random action instead of exploiting the strategy is right now. Right, so if I find a strategy that works with the best one, I'll keep exploiting that, where in fact there could be a much easier solution but I have no way of finding that, because I've kind of found this rut in the search space, and I just keep doing that over and over again. There could also be some randomness in the state transitions right it may not, it may not be deterministic, you know what, which state I enter, given that I take an action right so if I'm in a state and I have two possible actions, right, I take the left fork or the right fork. There's some probability distribution maybe they're not even the distributed. And then I'm going to take either of these actions. And then finally the reinforcement received right so this is a continuous continuous problem, I can approximate what I think my expected reinforcement would be but I might not be exactly that right due to other factors depending on the complexity of the environment. And then of course, the key problem is that we don't know, we don't know this. We don't, we don't know this term, because this is asking me to perform a sum over infinite elements and I can't do that. So what I'm going to use is I'm going to use, you know, the Markov property basically does anyone know what the Markov property is. I've never heard of this term before. Kind of any sense like in what context you're in this term. I've heard Markov chains. That's a good place to start what's the Markov chain. Yes, and does it use it so to repeat for those in zoom a Markov chain you take the last output and that is the input that determines the next output doesn't Markov chain use like the previous output before the last typically. No, I mean, if a Markov chain if the first order Markov chain which is by default a Markov chain it does not. Markov chain is kind of the something that we might call a drunk walk. Right so if I have a bunch of grids, right, and I am in one grid and I take a step forward. Well then the grid the square that I'm in now has influence on where I go I could go to any of the adjacent squares, but it doesn't know that it is came from the square behind me. So it could just as easily just take a step back. So now I do this a bunch of times and stagger around like a drunk person. Hence a drunk walk so you can, maybe you might stumble upon a path that you actually do start moving forward and then suddenly you were to the left, or you step backwards or something like that. So the Markov properties basically this memory this property that is where I go next depends only on where I am now. It does not really take into account anything previous such as where I came from. So it may be very useful for solving a sequential problem, but in fact it actually is because we can now approximate the sum for all K of our sub t plus k plus one, as the sum of our the next return so our sub t plus one, plus the sum of all returns from that point. I'll just kick the can down the road by by one by one step but if I can use this highlighted up here to approximate. Let me draw this out so easy. So if I can use this part here to approximate this part. Now I have to approximate this part. What can I do to approximate the part that's just in the box. I just shifted again right so now the sum from K equals one to infinity of r sub t plus k plus one can be approximated as r sub t plus two, plus the sum from k equals two to infinity of r sub t plus k plus one, and so on and so on. And so at some point, right this goes on to infinity because I don't know how many steps going to take me to reach the end, but at some point I will reach the end, and this no longer infinity. I'm just looking at the return that I said I just received. And so, if I approximated this way, then I should end up with a relatively relatively close approximation of the entire, the entire return. So recording okay. So basically, because we have realized the Q function as this sum, then I can approximate the entirety of the Q function as the next return, plus the Q value of the next state and the next action. Okay, everybody clear on kind of how this is working so far. And so now I'm back and trying to figure out what the actual value of this Q function is approximating that with a neural network so let's assume that the Q function is some function of an arbitrary level of nonlinearity so I just have to find kind of a good network size is going to allow me to capture that level of nonlinearity and approximate the values in that in those ways. So, the gradient of the objectives and our minimization problem is like this. So, Q is going to be some function parameterized by weights w. And so now I need to minimize the gradient of this function. So, if I have the error gradient, j sub w or j w is going to be equal to the error of the, the expected return minus the value of the Q function for this input parameterized by these weights. So, this is what we're trying to solve for w. So in other words, because we now can break. We can break this down into the steps above. This can be written as the, the return, plus the Q value at the next state minus the Q value at the current state squared. So if this function represents some kind of gradient. What kind of operation could we find to get the lowest point on the slope. We could do that gradient we could descend the gradient eventually was we descend the gradient successfully will find the lowest point in the gradient. Okay. So this is this is now hopefully back in some level familiar territory to date the gradient of j with respect to w and do gradient descent. And then we'll see here Q appears twice in the expression. So a pretty common approach is to take the respect, take the great respect to Q at the current state and then we treat the the next, the next value of Q as a constant. It isn't necessarily correct, but it works well and makes the algorithm a lot simpler. So it requires less compute so now I only have to compute the Q function once, given given its current values, rather than twice. So, in more recent methods, a second Q function called this the target Q function has been used to calculate the, the Q value at the next state given a set of different parameters. And at certain intervals, we're going to copy my source weights to my target weights, and then update the target Q function to basically keep these two roughly in line. Okay, so just some principles let's recall that the expectation operator is going to be a sum of possible states weighted by probabilities so if these the value on top of the fairy die. The expected value of D is actually three and a half, right which doesn't make a lot of sense because it's not a possible output. So the expected value is going to be the expected value of the gradients so the gradient of J. We can just take the gradient of the above formula. And so this is going to be to E times the return, plus the Q value at the next step, minus the Q value the current step, minus the current gradient of the Q value. The expectation operator requires knowledge of the probabilities of all those random effects. We don't have that. So what we'll do is sample from the world. So this is now, because I don't have this I need to still need to see for my states and my actions what kind of returns do I get if I take those actions in those states. So that is more probable events will occur more often. And then if we just add updates w for each sample you will get the expectation. So if I'm in a world and there's there's a distribution of events that can take place, regardless of the state and the action, the statement in the action that I take things that are more probable will occur more often. So, you know, you can think of like continuous problems when balancing my inverted pendulum. If I'm already over the marker here I don't use the eraser. My pendulum is like already tilted over here it doesn't really matter what I do I'm very likely to continue to drift to that one side. So given the state in this action, the state of the world, you can keep the markers okay. Given the state in this action, then it's very likely I'm going to end up in the same place regardless whereas if I'm more balanced then maybe there's a wider distribution of possibilities that could occur. So by sampling the world with the current state I can basically get a decent doing that repeatedly, I can get a decent survey of what types of things are likely to happen, given the current state of the world. Okay, any questions. Okay, so the first is look at the grading of the q function as a table, and then we will look how and see how we can update that using a neural network. So, when the gradient when q is a table. We first have to decide what the W parameters are for a table. So these are actually just the entries in the table. So if you think of what I'm, if I have like a state that is two dimensions and an action that is one dimension, and I have my three dimensional table, I basically just go k x y z at this cell in the table. So this is the return. This is what I what I expect. And so, what I just I just hear what I can do is I can take that input and multiply it by those weights instead to get the return. Right. So you're not going to get like the identical values basically just get an alternate way of saying hey, instead of just retrieving this value actually want to take what's in here and multiply this by your input and this is going to give you the expected value. So, since W also weights, you can formulate the selection of this correct parameter as a dot product so if we have x of t as a column vector. This will be equal to the number of table cells and values. And these are all zeros with a single one. So that one designates the cell corresponding to s and a. And so then q of that is going to be that that input times those weights. So this looks a lot like the prediction step in a neural network. And therefore, the gradient of the q function will basically just be the gradient of x sub t transpose times w. And that since this is taken with respect to w. This is just equal to x sub t. So I'm actually update the weights. I need to define the temple difference error. So in this case this is the formula we've seen before so delta sub t is going to be r sub t plus one, plus the q value at the next time step minus the current q value. And so if we substitute the gradient of J in from above like this. So now, this we can just replace with delta sub t. And so now the gradient in respect to w of J is going to be two times the error delta sub t minus the gradient of the q table at the current step. So if we just use the values we've established this is going to be except T, then this ends up just being negative two e of delta sub t times except T. So in other words, for because our input except he is represented as a one hot vector where one that one indicates the cell corresponding to s sub t a sub t, then this value right so this is delta sub t times one. So this value ends up just being negative two times the error of delta sub t, and then elsewhere is just zero. So, now we can replace the expectation with samples, and then we'll get the temple difference update for the q table so this is just the standard weight update I take my current weight, weight value and then I subtract the gradient and store that in the new weight value. So if the gradient is delta sub t times except, I just multiply that by some learning rate row, and this is the update. So, w times row times delta sub t times except. And so this is really the same as what we've seen before so I'm updating the q table q of s of t a sub t is going to be q of s of t a sub t plus row times the temple difference error. And this is the same as what we did in the previous lecture. Previously the updates to just the cell was implicit. So this is also the same as the weight update in neural network. That is I update a weight w based on an error, in this case delta a learning rate row, and an input x. So, same components as we've seen before, just written differently and kind of arrived at using a different type of formulation. Okay, questions about that. So now the neural network version. So we've shown how it corresponds to a table and that the tabular update is effectively interchangeable with doing a weight update. If we assume that those weights are effectively just entries in the table, but using a table for q has limited use reasons why maybe the size of the table might be too large to store in memory so imagine if I'm trying to solve some sort of complex environment. I might have a continuous function or even if I don't it might have too many possible state action pairs. So I might not be able to store the entire table in memory. Learning could be slow, right so I could learn something from if I have two similar situations in different locations in my environment. Right, I could learn something from doing something in one location and eventually find my way over to the other location, and I'm not able to solve it. So it's like, if you learn by reinforcement learning that red means stop and green means go, but you learn it at that traffic light like on the north of the parking lot, and then you go to another traffic light, and you have to learn that all over again. But that would be very inefficient and also very dangerous. So, instead, what if I have learned that red means stop and green means go, and now I can see, well I've gone gone somewhere else, and I see also read and green happening here. So I might have learned something previously that's relevant to this. So, instead of having to represent every single state action as a set of cells like every intersection for Collins being being a distinct cell. I actually can learn from features of the environment, and then be more likely to reproduce good actions when I encounter those features elsewhere even if I learned about it in a particular location first. So we can use this universal function approximator, or a neural network. So to do this we have to make a couple of changes in the derivation of the gradient, but they're going to be things that you've seen before. So we need to approximate this q value parameterized by w with a neural network. So we already know the values of w are the weights and all the layers. So if we have two layers then w is going to be composed of hidden layer weights v and big w. Looks like the neural network should have a single output and inputs for s of t and a sub t so to find the best action, I would I could input s of t, try all possible actions a and then calculate the, the output of the network for each one. Then I pick the action that produces the best neural network output this is not how you would actually want to do it, but if you think about it this way, then the math makes sense which we want to do is basically when we accumulate enough experience that you get a good distribution of the various different possible actions in s. Yes. I see w but the other hidden layer is the same as the other. The w w is all the weights, big big v and big w are the individual layer weights so little w is all of the weights in the neural network, v is the hidden layer weights, big w is the output layer weights. Other questions. Okay. Really what I would what I would want to do is sample enough experience to say okay I was in the same state s, a bunch of times and half the time I did action a half time it did action be. And those are really my two possible actions but then I know what the distribution of expected returns or reinforces would be. So, let's just remember the objective function of going to minimize so we have j w, given as before. So this is the approximation of the sum, and then the gradient, as we saw before. So, what we'll do is, instead of the expectation operator, we'll replace that with samples at time to be so now I can look and see. So instead of the sort of e. Sorry, I kept calling that error previously I'm going to say expectation I apologize for that. So, we have, we replace these with just a bunch of samples and now I have the return that I got it's time steps t plus one, plus the q value for this sample, right so now there's going to be a particular input sample and then the weights associated with that sample, and then minus the q value the current q value times the current q value, and then we'll get to those particular weights of the q value. So what remains then is to calculate this last term. But we've actually seen this before when calculating the gradient of the output of neural network respect weights. So remember w set of both of both layer weights. So the only change the previous algorithm for if you want to train now a neural network to do a non the new regression is that we're just going to be using the temporal difference error, instead of the target output error. So the output error is basically, I know my ground truth value is T, and make some prediction why I measure how wrong I am is the difference between T and why. So the temporal difference error is now as given above. So, this is delta this is basically going to be the predicted return, according to my approximation minus the actual q value at that at that point so what's happening here, the q value as it stands is sort of my best prediction. And this is what my, my neural network or my q table predicts is going to happen it may be very wrong and maybe close to correct. This is my approximation of T so that is this is my best estimate what's actually going to happen if I take this action, and it could be very close to what the q table or the q function actually says or very far. What I mean when I say in reinforcement learning you are using and training the network at the same time. So here, I'm going to get an imperfect approximation of what I think my full return is going to be. I'm going to use that as the stand in for my target value because it's the best thing that I have I cannot get a better approximation from this, because I don't have a bunch of samples I only have the experience I've accumulated so far. And then I'm going to use my prediction quote this is the analog for why that is my current q value. Okay, so where does what does this do. Yeah. No R is the row row is learning rate. Greek letter row is learning rate this this is the learning rate. But we've always used a row may sometimes you use alpha this we almost always use a row. The little R refers to the reward at some time step T so r sub t is the reward of time step T r sub t plus one is the word time step T plus one big R is going to be the sum of all those rewards. That is the overall return. Okay. So let's review the gradient descent updates that we had for non linear regression with neural networks so Britain layer weights V multiply those by my inputs. I apply some activation function H this gives you some intermediate scale or intermediate scalar value z. So I multiply that by my output layer weights w and this gives me my prediction why why is going to be more or less wrong when I compare it to T. So now T minus why this is my error term. And so these are the updates that I use to update V rights this incorporates the error and then also incorporates the derivative of the activation function. And then these are going to be the updates for my output layer so here's row. We have in this case different learning rates for the two layers that you need not necessarily have that. So the only thing you need to make is in the error. So as I alluded to just now, what when I calculate my error over is basically the best estimate I can make at the time, and what my neural network before updating currently says the expected reward should be. So basically my neural network I have two estimates of the reward I have whatever my neural network predicts, and then I have something that's probably slightly different from that based on a little bit more experience. Okay, so I have, I've predicted that my my Q function for state where I am now plus moving forward is slightly positive it gets me a little bit closer toward the goal, I then take a step forward and I fall into a giant pit. Like, oh, that was bad it's not going to be closer to the goal. So I have a little bit more experienced, and I can now use that to update my, my q table, my q function. So these key changes in the error. So the targets are now going to be the reinforcement plus the next step, and then the predicted q values so then we'll be assembling a bunch of these samples into matrices. So, these inputs will be the states in the action of these tuples be collected as rows and x. Those will then pass with an neural network will give me Z, and then why. So, this is the tuples q, because the prediction of my network is just the output of my q function. That is, its best estimate of the expected return, and those reinforcements will be collected as rows of our. So now I just need these q of s of t plus one a sub t plus one. So, you can think of these as this is just the q function, once I've taken my next action. So, this is just rows of y shifted by one, and maybe slightly slightly altered. So we'll do is we'll call this q n for q next. So now I can just rewrite the grading descent updates except by using our q and q. And then x is the same so I have x r q and q. All these except x have one column where x has however many dimensions I have in the input. So now, T is equal to r plus q n right so this is that approximation of best return. And then Z is 10 h or some activation function of x times V is before. And now the update functions are pretty identical except you'll notice that I replaced y with q. I replaced y with q, because the prediction of my neural network is the output of my q function. So to make this even more obvious we'll just replace q with y and you can see it here. So now T I have a way of calculating that as my return, plus the next row in y, and everything else is pretty is pretty close to being identical to the standard neural line. The only thing that's missing is we don't have this one over K that we had there. But because that's a constant, we can effectively just factor that out. Questions. All right. So dealing with infinite sums. So for these tasks that have no final state. So let's say, if I'm trying to balance the pendulum and my goal is just to keep it balanced right there's no, it doesn't end if it's perfectly upright or something my goal is just to not let it fall over as long as possible. So for these the desired sum of reinforcements is going to be over an infinite future I might say it's going to time out after a certain time my goal is actually to keep it balanced like 100 time steps something like that. So this is going to grow to infinity unless I force it to be some sort of finite value somehow. So what we'll do here is basically if I'm just trying to keep something going as long as possible. I can't accumulate all of my experience from one to infinity because eventually I'm going to blow up. So instead, what I'm going to do is I'm going to discount reinforcements on actually discount ones that occur farther in the future. So at this point, I'm going to say, well, I predict that this action is going to keep me in a good place for the next 200 time steps, and I can't really predict much beyond that. So I'm not going to worry about that right now in 100 time steps. I'll start worrying about that when my time horizon actually reaches that. So what we'll do is we'll add this factor gamma. And so this is going to be some discount factor between zero and one. So as the further I get into the future, I'm going to kind of scale down how much I take this this reinforcement into account. So you get into the objective function. So you saw that we put that inside the sum. And so when I rewrite my sum is the approximation of the next reinforcement and the sum of future reinforcements. I will also use that gamma terms is going to be some value of gamma sub zero, and then I'll use gamma sub k in the sum. And then this is just, of course, we can rewrite this sum as the sum of the t plus one reinforcement plus all futures. And finally, remember, this is just the same as the queue, key function for the next state, the next time step. And so gamma will also be out in front of this. Okay. So, what I'm just going to do is kind of simply add the multiplication by gamma to the next state action q value in the temporal difference error. So if I'm using a bunch of updates, these are badges. And so what I'm doing non linear regression, we had input and target samples and x and t. And so now one set of samples are collected for one set of weight values that is for one q function. So after the updates we're going to have a new q function, which is going to produce a different actions. So what I do then I need to generate more samples. So I need to do these in smallish batches so that I don't train too many iterations for each time. Otherwise, if I do training convergence every time, then the neural network can forget everything that might have learned from previous samples. So if I want to say, train for like 200 at a time and it doesn't give me a perfect function, but I'm getting closer. So I train for another 200 samples. It's like, okay, this is this is getting me closer so I'm iteratively improving right I don't fully train to convergence, with every batch. Okay, so any questions probably do the example. Okay, so we'll do an example. It's called a 1d maze. What's a 1d maze it's a number line. Don't let it for you. It's a number line. My goal is to land on some desirable place in the number line that I would just specify this is the cartoonish the simple example but it does illustrate. So let's take a chain of states numbered one through 10. And so I can be in any state I can move left I can stay put or I can move right. I can move off the end. So if I'm in one I can only move right if I'm in 10 I can only move left. And so I want to get to state five, for example, so let's model this as a cost function. So for every move it's going to be negative one or a negative reward. And then if I end up in state five I get a reinforcement of zero. So at this point I'm just trying to minimize, or I'm trying to maximize my reward is the same as minimizing my cost. So modeling the reward in this way will drive the agent to get to state five as quickly as possible because once it gets to state five the first time. It'll see oh, I didn't get dinged for this so I want to do this more often. So the state is going to be an integer from one through 10. So let's approximate the q function using a neural network. So I have two inputs for the integer. So that is the state in the action, six hidden units and one output. So the, these are the states and actions on this one through 10, negative one for action left, zero for action stay one for action right. And then the state is going to be bounded between one and 10 is just taken to be s of t plus a sub t so if I'm in state four, and I move left I should end up in state three. So I'm going to move right forward. I'm basically just trying to find a desirable place in the number line. So here's my neural network class this should look pretty familiar to all of you at this point. So I will store this. Okay actions available, stay left or step left, stay put, stay right. So represented as changes in positions will model them like this so it's having an array of valid actions negative one zero and one. So I'm going to take an exploration by taking random actions, but because this is a neural network. As it learns I want to decrease the amount of exploration. In this case we may not always want to do that depends on how well your neural network actually fits to the environment this is very simple. So, if I've got a really good policy for arriving at state five I don't know like risk disrupting that by taking a random action at that point. So we use epsilon to indicate the probability of taking a random action bounded zero and one. So given a q net called q net, the current state and the set of out actions in epsilon we can define this function. This will return a random choice from valid actions, or the best action determined by the q net. So that is, if I have a 20% chance of taking a random action, then once presented with my, my, my action policy, I will either take the best thing predicted by that according to the state, or I will to about to the 8% of the time, the other 20% of the time I will take the random action, as if this is called the epsilon greedy policy. So to find a function epsilon greedy that does that so basically this is the important part. If I choose a random number there's less than epsilon, then I'll make a random choice out of our actions. Otherwise I will take the greedy move. So that is, I will run all my samples through my q net and predict the best action for my for for example. So now I need a function. What this will do this is actually do the batching. So this makes samples function will collect a bunch of samples of state action reinforcement next state next action. I can make this a generic function by passing in functions actually to create the initial state the next day and the reinforcement value. So to find those functions first. So the initial state, next state and reinforcement. So basically the initial stage just going to choose randomly from one to 10. So bounded at 11, it's exclusive. And then new state is going to take in the current state and the action and then add state the action to give me the state bounded at one and 10. And then the reinforcement will return negative one if I'm not in five and zero if I am in five very simple reinforcement policy. So, give me syntax warning there but it doesn't matter. So here's my next function, make samples so I pass in the q net and then I have a passing these functions for initial state next state and the reinforcement function, as well as my valid samples and the number of samples and my value of epsilon so now I'm creating my X, R and q n matrices so these are just initialized samples and then I will generate my first state, and then my action is going to be the epsilon greedy policy over the, as dictated by the q net, given the initial state s, and then about action the epsilon. And then for every, for every step in the samples, I'm going to sample the next state computer reinforcement at that state, and then choose the next action using the epsilon really policy, and then advanced one time step at a time. So, this is plotting. So we'll just basically draw you a bunch of plots, showing, you know, a bunch of ways of visualizing the output. So now for the main loop will create 20 interactions, we'll call them steps for each trial then update our q net at the end of each trial. So, we have 5000 total trials. Now what I'm going to do is when updating the neural network q function just going to run Adam for 20 epochs basically perform a little bit of training to try and update my, my, my function. And then I will put in the next set of steps. Right so I will click 20 interactions update Adam for 20 epochs, like 20 new interactions update Adam again. And then it allows me to basically perform these incremental iterated iterated updates, not necessarily trained to convergence every time. Okay, so, and then I will create my neural network architecture here, we have 50 hidden units to think about them. So, because it's doing 50 now I must have changed this description above it says six. Now, gamma this is my discount. So I'm going to scale every future reinforcement by point eight. Right so the reinforcement at t plus two account for 80% as much as the reinforcement at t plus one. So, I'm going to start with the epsilon decay. Basically, what this is is I have my epsilon value. I want to reach this final epsilon. Right when I, when I'm done training, I want the probability of taking a random action to be point 0% So I need to start from 20% decay to point 0% 1% over the number of total trials. And then I'm going to calculate how much I decay every, every trial. So the epsilon decay in this case is point 99. Okay, so now all this. Here's my initialization of my neural network I need to set the standardization parameters now. So that the unit can be called to get the first set of samples. Before it has been trained the first time so I'll create this function this is going to create my standardizations now. I'm going to start my standardization with my inputs. So this is going to be 110. And then I will use the following means 2.5 and 0.5 and then zero and one for the means standard deviations. And then the rest of this is is this is just plotting. And now we call make samples and actually train my network. Okay. So this is actually performing the value of epsilon. This is actually performing the epsilon decay. And then the rest of the loop is for plots. So let's run this and watch it go for a little bit. So this will continue the update. So basically what we can see here is, I have the queue for each action on every set of trials, right. So this is getting a bunch of different randomly initialized samples. So for this one this is going to be queue for each function. You can sort of start to see it start to take some some kind of shape. So you can see that I'm getting some sort of peaks around around five for the action stay, and maybe getting some peaks that are more correlated with like moving left to right for the other states. So this is going to be the value of X for the last 200 samples. This is like not easy to see is basically the sum of total returns is just keeps accumulating so it becomes a big kind of block. Let me see if I can go through some of these other ones, not terribly easy. So this one is the action. Okay, so sorry, sorry for the sorry for the jumps might make you some some of you kind of seasick. Let's focus on the action for a little bit so you can see at the state when the state is five, the best action. This is plotted just as live with state at the best action between five at five is around zero, whereas the best action. That's now it's done the best action for those states less than zero is one that is action for the states less greater than zero is negative one. This is the value of the epsilon you can see it decaying. This is the smooth value of the total total return. So you can see that it does train and it does start to rise and kind of approach zero. And this is this is just plotting what each hidden unit is doing so this is kind of hard to interpret. And here's the temporal difference error. So ultimately, this is actually what happened here is it basically plummeted for a bit and then after more exploration, the TD errors actually kind of starts to to even out. And so now if I hit define run, I will not run this because it says don't run live so it takes about six minutes. So what this is doing, this is going through a grid search trying to find what type of hyper parameters are actually best for this task. So, we'll try a bunch of different numbers of trials numbers and stuff for trials epochs architectures values gamma learning rate, etc. So, let me skip to the bottom of this. I'll just plotted it in a pandas data frames when we go through each of these. And then we'll talk about the results so this is the number of trials I sorted them by our last two I'll get that in a moment. So this is the number of trials steps for trial number epochs network architecture value of gamma, etc. So the last two columns here are basically this is the total return over the entire training process. And then this is the last two returns so this is over those last two batches, what the returns were. That's the value sorted, sorted by. So take a look at it is there. Do you have any sense of what network architectures look like they might perform well. Or network architecture in conjunction with other other factors perhaps. The goal is always the same the goal is always the land at five. Yeah, I could. Yeah. So, this network would over this this network is is overfit to cases where the goal is five. Okay, so if I if I if I if I change the goal and retrain I'll find it. Right. This, this network has been trained to arrive at a goal that is five. If I change the goal to six or something this train network isn't going to work, but I could retrain the network I might even be able to like take this network and like update it or something and tune those weights to update for for goal six. Let's talk about generalization. So, this is an example from last time. So, what you can do is, if I train it to stack two blocks, and then I just change the environment to allow it to stack three blocks, because all is doing is choosing an action on top of the top most block. I'm going to do an okay job of stacking multiple blocks. Right, as long as I because what it's predicting is basically here's an action is relative to something very specified in the world. It's very localized. And then I can basically update what actually what was actually executed in the world like what my agent actually physically does. And, and they can probably, you can learn miss x re blocks or I could maybe take that train number can tune it slightly to be to be better at that task. Do you know like all things. How do you do things that you have to do the action. Because the state here is basically learning that if I'm in state for the best action is to take one step to the right. If I'm in state six, the best actions take one step to the left. If I'm in state five, the best actions to stay still. So if I change my goal to six, and it's a change the environment. If it gets to state five is going to say the best actions to stay still, and it will never reach the goal at six. Okay, so the environment of course is going to be is critical. Does anybody see kind of what might be something of a discriminating factor in terms of success of this type of network. Is the number of hidden so we got 1010 here at the top. We also have 1010 and 1010 here at the bottom. Yeah. So the steps for trial looks like it probably does. So like some of the worst performing ones all of 100 steps per trial, whereas the best performing ones usually have fewer. So one reason for that is that this is actually training less per trial, right so supporting a very small iterative update, so just training for like you know, 10 epochs or something, updating the optimal the optimizer. Whereas here, it actually might be training closer to convergence. And then the next trial comes along and it has to forget everything that it learned in order to optimize again. Okay. All right, questions about neural network reinforcement learning. I'm sure there are many I can talk about this some length if you want. So if you imagine a more complicated environment like solving a game level or something. Yeah, question. No, go ahead. I have to go like choosing activation function. Okay, yeah. More than we can consider now that we're doing reinforcement learning. Yeah, so choosing activation functions. Let me think about that. So you have a couple of you do want to be kind of choose the about what activation functions you're using depending on the nature of the problem. So generally speaking like if you use a re lu activation, it's possible you may lose some important information about say bad actions, for example. But also that would depend on how you formulate your problem, right because if your reinforcements are negative. Maybe Ray Lou is a worse choice there because it's pushing on negative information but reinforcement for merely less positive is basically the worst thing you can get is zero. Ray Lou might not be a bad choice. Right. But if you're modeling it like we did here. Maybe you maybe really would be a not not as optimal choice. Other questions comments thoughts. So if you're trying to solve kind of a more complicated environment where there were say multiple recurring similar circumstances are trying to solve some sort of game level or something like that. With a neural network what you get is you can increase the size of the network to accommodate different types of conditions yeah question. Yeah, yeah, so we did we didn't talk about that a whole lot but there's a brief sidebar at the end of I think of the Ray Lou notebook. There are there are a bunch so the, the L you part of Ray Lou is linear unit there are a bunch of other linear units. So like you know exponential unit Gaussian error linear unit parameterized Ray Lou so I'll talk about some of those like briefly just to refresh your memory so I'll talk about the leaky Ray Lou is one for example. So Ray Lou zero until zero and then y equals x well, maybe I want to squish out most of the negative information but not all of it so leaky Ray Lou kind of let's a little bit of it through so like point one times x if x is negative otherwise y equals x parameterized Ray Lou is similar to that it lets through some of the, the negative information, but it's not a constant. It's usually a tuned value. So I can kind of decide or I can try to learn the value of like how much of the negative information I don't want to let through maybe I want to let through like more of the less negative information closer to zero and I really want to disregard most of the really negative information. Gaussian error linear unit is basically imagine what Ray Lou would look like if it were a nicely differentiable function. So basically we're trying to put the smooth curve in at zero. The Gaussian error linear unit is the one that's the that is used in most transformers and most of the like the chatbots natural language applications. Those are using chewy units. Yeah. Other questions. I think that's a message for loop strategy that generally best find. Oh, for like, for like grid search. I mean, if you're doing a grid search like yeah this is like the way you would do it on if you're just doing your own. There are a number of libraries out there that you can use now. So, hyperopt, I'm not sure it doesn't do neural networks very well but basically you know there's a lot of parameter parameters search libraries out there that they will. So the best ways to do it is basically, I have a bunch of parallel processors. I will put one instance on each processor and run them all at once. And so that way I can try a bunch of things at the same time get all my returns back and figure out which one of these was best. So, but that of course, you know, it's going to be difficult to do just like on your on your laptop. Yep. Okay. Any other questions before I start on the project. Okay. Let me open that up. See, where is solutions. These are project proposal. And then here is a an example. This is the report example actually. Okay, so the project proposal. So basically, you can work in teams of up to four as I've mentioned, the, the scope of your project needs to reflect the size of your team. So, you know, it needs to be more ambitious the more people you have. So, the project as I mentioned to some of you before is really you think of this as an opportunity to maximize your own success. So that is, if there is a technique that you're particularly interested in, or an application that you're particularly interested in, or like you have some outside area of expertise which is something to basically what keeps you interested is what you probably would want to do for your project if you have an existing research project, and you want to put some sort of machine learning spin on it. That is fine. I do not object to that. So, there are a couple of options. This is not necessarily exhaustive, but this is what I want you to do is like be inventive and pursue some topic of your interest. So, for example, if you need some ideas, take a neural algorithm that we've covered in class and apply it to some different data set that's of interest and do some analysis and draw some conclusions. The analysis has to be more in depth than what we've done in class more than just like a bigger network work better or whatever. So, the next thing that we do is like, yeah, do the analysis of network strategies and different hyper parameters but also looking for error analysis. What are the things that your implementation fails on and why hypothesize. You can use other outside tools if you want, if it helps with the analysis. So, I will now load code from the internet that implements now going we didn't cover. So, we talked a lot about neural networks I will briefly talk about like can and SVM and stuff at the end. But by then you will be pretty deep in your projects if there's some other machine learning algorithm could be neural could be non neural that you want to explore, and maybe compare that to something that we did do in class that that makes that makes sense. So, if you have a bunch of coding that you're already doing you're like I can't bite off another coding project. You have the option to write a research report. Or if you just want to write a research report regardless of the amount of coding you're doing else or you can do that. What that means is you need to study at least five research articles of a topic of interest to you again a machine learning topic. You need to present a report that summarizes each article in detail, describing similarities and differences between the papers and then you also need to provide a conclusion section to basically summarizes your takeaways in on that topic so like you know, if you look at, if you're to look at chat box or something you study five papers on chat box and then you would write a report summarizing each approach and then talking about like what the, what are common approaches with the current state of that field is. What you need to put in your proposal is basically just a confirmation that it is appropriate be scoped so remember you start this now. You could probably get approval if you're really fast as early as like middle of next week, and you could potentially start then but let's assume that the sort of the starting on fire is on April 6 that gives you roughly a month and about a week to get everything done so you need to don't bite off too much. Make sure this is something you can do within about a month to five weeks. Okay. In the project proposal, you need to show that they're appropriate scope for time period and team size, and make sure you put effort into both the implementation and the analysis. Talk about what questions are seeking to answer, and then what hypotheses you can make about the data that you'll be exploring using whatever method you use. You need to explain why you want to do this project and the steps you will use to complete the project describes the method you will use I need to see the sources the data. Are you going to define new algorithms from implementations, you're going to use them from an online source where you're going to get them from basically do your due diligence show that you have cited your sources and that I can go there and see what you're going to be using. If you are working in a team, you also need to provide some definition of like how the work is going to be divided among the team members so who's going to be primarily responsible for what this is not like intended to be like a hard like great wall between the team members of course I expect you will be collaborating, you're going to help each other out but kind of need a sense that like one person is not doing all the work and the rest of people just like free writing. Okay. So, possible results. So, you can speculate on possible answers to the questions you provide me introduction this is going to be a little bit different between the coding projects and the research projects right the results the research projects are basically what you, what you're going to be looking at in your discussion. So, what types of contrast you think you might find between different approaches. What do you think you this might tell you about the particular topic that you're researching for example, possible results for the coding project or of course more on the lines of what do you think the likely outcomes are going to be provided timeline. So I want to see like for entries with some dates and describe what every team member will accomplish by these dates approximately. Your grade will not depend on meeting these deadlines. But this is basically for your use so that you know you can come to me and if you have problems for example and I can help maybe try to get you back on track. Okay, so you don't. This is pretty short right doesn't need to be more than like two pages long. submit this in canvas, and there's going to be a dropbox. So going to be last name proposal or you know last name dash last name proposal. And then, don't email this because it's going to get lost. So, grading of this grading is basically complete or incomplete. If these are not satisfied I will do is I will send it back for revisions. Once you revised it to satisfaction you'll get the full credit. So this is basically 15% of the project grade, and you will eventually get that 15% as long as you've heard it in and complete the revisions. So don't worry, you know about like getting dinged on like if your proposal isn't is isn't clear or something I will send back to you for revision describing what needs to be made. So, then also you make sure you're not like under committing or over committing. So like I said, 15 points. So, do this, and you know don't give these points up. So basically the grading on the project is going to 15% for this 15% for the lightning presentation at the on the last two days of class. So what I will do is once I've gotten all the proposals in. I will count how many there are. And then that will determine how long those presentations are usually ends up being about two and a half minutes per each one. So, I will go into more detail what you want to do for that but really for those last two days. So, every team is going to submit a slide deck that has probably about three slides maybe more if you have a lot of teams me have longer presentations, but basically say you know what's your problem. Not in an aggressive way what is the problem you're trying to solve. I'm trying to solve what progress, have you made it towards it and what approaches you're using and then like what, what is left to do or what have you learned so far something like that. Okay. So I will do one lecture on the project deliverables. Yeah. We will. Yes, I will do. Do at least one lecture on transformers toward the end. You can yeah you could yeah so um and then oh yeah I gotta do this course. You may not use chat GPT to write your proposal or your project. You can use any line, any chat but I can't use bar and you can't use all the other ones that came out you can't use like the the one that the Chinese government put out the other day that didn't do so well apparently. So yeah, cannot use a chat bot to write this I want you to write your own words so basically here's my attitude toward the chat bots right now right now I'm actually literally, you know, in and out of meetings with people that till trying to help come up with some sort of coherent positioning of chat bots so eventually I do feel we will come to some sort of coexistence and understanding what is and is not an appropriate use of chat bots in the university. My attitude toward writing is if you're using a chat bot, you're not using your own words. Okay. If you may possibly use like a chat bot to maybe help with like the mechanics or something that ultimately it needs to be rewritten. So if you submit your own writing, and I will be checking against, you know, your written report and say your previous homeworks. You know, I'm going to be looking for things to see like, has the writing style drastically changed is, were you making grammatical errors and suddenly it's like perfect right I'd rather see, I'd rather see the grammatical error version, but I'd not going to be grading for grammar, because it's like so pervasive that I can understand. So if you're, if you spell things wrong occasionally you make the case for grammar where I don't really care that much, because one is telling me that you're actually doing your own work, and two, this is not an English class. Okay. So, I think at also the other thing if you want to study, if you want to use like the outputs of chat bots in your project. That's perfectly okay. So, using a chat bot as an object of study is a okay I think that would actually be really cool to see. So, I think I'll cover some guidelines on like what my positioning is toward the use of these tools right now. Okay, other questions. But yes, we will be talking about transformers at least at least once. Yeah, yeah, what. Yes, I will go back I'll start off sour as a 330. Okay. Okay, bye thank you.