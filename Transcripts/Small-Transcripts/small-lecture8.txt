 Okay, let's get going. Okay, let's get going. Very, very good chance this class is going to be short today, because if you look at the schedule today is the ninth and we actually finished this notebook on Tuesday. I'm going to do that like a day ahead and Ray Lou is a pretty short notebook so I'm going to do that and then I will assign the second assignment, and I didn't bring my GPU machine so I can't run half of the next notebook. So, I'll just call it a day after assignment to and then I will set up my GPU laptop over the weekend. Alright, so assignment one due tonight. If you don't have an extension of course, some of you requested one, some of you have arranged one. So, but for the rest of you. If you hear me say this and you're like, oh shit, I did I forgot to request an extension and I really need one. I think the best thing for you to do is to get something turned in today. Okay, therefore, various reasons about how I run this class, it is much, much more to your advantage to turn things in on time even if incomplete, then to try to, you know, spend a couple of days getting wrapped up and then submitting it late, because you will get a 10% late penalty, well 10 points not not not percent of the remaining grade basically like minus 10 for every day that it's late so it is definitely in your best interest to get something turned in today. Even if you feel it's not complete. Just trust me on that one. Okay. Are there any questions about. Yes. I will have office hours. Yeah. So I will have office hours at the usual time at starting at 330. I will try to give you feedback. So what I have a rubric about different points that I will send you. And about like and use what I what we try to do is basically say you know, count it off x points for not doing why if you do perfectly will say like great job or something. So if you get 100 don't expect a lot of feedback. If you do something like super cool in your notebook that I can't resist commenting on on do it but if you just checked all the boxes you know, just take your 100%. If you do get counted off, we'll try to indicate what that is for other questions. All right, cool. So what we are going to talk about today is re lu who knows what re lu is. Few of you. Okay. So, we talked about activation functions right activation functions are these non linear functions that we apply to the output of a hidden layer, and meaning that when we back propagate the error through the network, the weights in that hidden layer are effectively optimized with the assumption that a certain non linear function will be applied to that output. So, that is, if I'm applying the 10 age function to something. And I want a specific target result. And then there's some transformation that needs to be applied before applying the 10 age, the particularities of that transformation are going to be different, assuming the 10 h is there versus when it's not right so that's the activation function. The activation function is constant. It's usually not parameterized very much, except in maybe some very specific ways. But basically, it's what you typically do is you apply an activation function to an entire layer. Right. So, you may have heard of this thing called re lu, which is this weird term that people all the time, and no one really knows what it means. So, my goal here is to basically demystify machine learning in this class and so the one of the goals is to tell you what all these weird incantations actually mean. And we, if we're going to address non linear problems and assignment to is going to be non linear regression using neural networks. We need some way to include non linearity, we do that with activation functions, and any sort of non linear function can in principle be used to introduce non linearity. And so, if I'm assuming that my activation function after my transformation of my inputs by my way is the 10 h function, then my opt my optimization is going to be performed in a certain way, so that my weights end up at certain values. If I change that function. Of course when I perform the same optimization operation, the weights are going to end up different. Right. The reason for that is because when we are performing back propagation into hidden layers. So one of the things we have to incorporate is the derivative of whatever non linearity that we apply on on the at the output of that layer. So if we had what you what you can, something that you can do and it's not really clear why you would do this but you can do it is if you have a neural network where you have no activation function on any layer, and it's just a linear pass through. Right, you still end up with a just a kind of concluded way of solving a linear. What that means is that when you're applying these non linearities when you're applying. If you had no non linearity. If you remember how we have how we perform back propagation at the output layer. It's just the error term times the input to that layer right. Then, if I had no non linearity on my hidden layer when I'm trying to apply back propagation to that hidden layer, and the derivative of a linear function is just one right x equals x. Then I'm not applying any extra term to my back propagation operation. And so in if that were the case, then the backdrop for the hidden layer would be the same as the backprop for the output layer. So what that means that when I change the activation function I just change that one term the derivative whatever that activation function is. Given that we of course need our activation functions to have some desirable properties so that by when you take the derivative values don't vanish they don't grow out of control. They allow you to keep optimizing and taking the appropriate size step down the gradient toward that global minimum. So, as you probably read those kind of rambling. Right we have the desirable properties include computation be simple, right, because we want these to be faster going to be applying them at a high number at a large scale you don't want to be computing some arbitrarily complex function for multiple units and multiple layers. We also want, you know, for initial small weight values that function should be close to linear, because as the weight magnitudes grow into function to be increasingly nonlinear, such that when you get when you reach magnitudes of extremes you're not having those values greatly affect the input, much more than input values that are maybe closer to zero. The derivative function should be computationally simple for the same reasons as I have number one, I'm going to be taking the derivative during back prop again at exactly the same number of activation operations that I've learned during the forward pass. And then most also be computationally simple. And then the magnitude of the derivative should decrease as the weight magnitudes grow, and this should perhaps approach some sort of asymptote, so that if I have an extremely high value. It's not your disproportionately affecting the, the adjustment in weights during that propagation. So the maximum value of the magnitude of the derivative is limited. And as you will see, some of these properties are more desirable than others and some of them we can actually let slide a bit when certain other conditions are met. So, given these properties that we identified previously, we also identified two functions, and their derivatives that have these properties one is the sigmoid function, given by this. And this derivative is sigmoid of x times one minus sigmoid of x, and the 10 h function, given by this where it's derivatives one minus 10 h squared, both of these obey these properties. Sigmoid of course is this s shaped curve bounded between which value and which value zero and one, right, and the 10 h functions bounded between negative one and one. So, the, these two functions effectively have the same shape, just with different bounds. And so then the derivatives also have kind of they have basically a hump in the center. It's just that the maximum value of that of that hump is different for the two functions. All right, what do these functions actually do right let's take a look at them so we'll plot them with code that you've seen before so I will define sigmoid function 10 h function and their respective derivatives and plot them using different types of lines. So here is the plot, right this is what we see. So the blue lines that's the sigmoid as derivative red line that's the 10 h and its derivative. And so we can see how they obey these nice properties where when I'm close to zero. This is more or less a linear function both of these. The maximum values as the, as the inputs go to extremes are bounded, same as true for the derivative has a limited maximum value for sigmoid is point two five for 10 h it's one, and then also that the derivative decreases towards zero as the values go to extremes. Okay, so we see the derivative of the sigmoid and the 10 h function decreases as x gets further away from zero. Now remember the three things that we need to actually compute a weight update over some predicted output. So, one, the learning rate, right generally speaking, what's the size of the step that I'm taking along the gradient, the error between the output and some the gradient, right, how wrong am I if I'm very wrong. The total step that I want to take would be bigger, right scale by learning rate, and then the gradient at C, right, how steep is the is the slope. I'm trying to approach the bottom, I'm trying to approach a place where the slope is effectively zero. And I'm trying if I'm on a steeper portion of that gradient, then that's going to affect how far I moved down. Okay, so absent fancy things like Adam, which also further further adjust the total size of that step based on things like previous weighted averages and variances. These are the things that you need. Right, so you take some combination of all of these, and this determines how far and in which direction I need to be moving along my gradient. So that if the gradient is small, then the weight update is likely to be small as well. Generally speaking, this is what we want, right, we don't want weight updates to be too huge and shoot us off into into the abyss or actually trying to approach the abyss the bottom of the, the gradient. We're going to go off into into space up the up the curve of the gradient. So we like these derivative functions that don't explode as the magnitude of x gets larger. And we don't want that weight up to be too large such that we miss this local here I should change that to global. We don't miss the global optimum entirely. Okay. If the down if the activation functions are too small. There's some downsides as well. So if you have some function H that's our activation function. And if the output of that function is too close to zero, then the outputs in that layer with that function will be close to zero, and sigmoid actually has this problem, which is one of the reasons why you prefer 10 h right sigmoid, we have really a set of values, which may often be the case, you have values close to zero. And so, as opposed to 10 h where the set of values close to zero is pretty small 10 h of zero is zero. And as I move away from zero it linearly also moves away from zero until I start to reach more extreme values. And also if the derivative of that function is too close to zero, then you can't communicate useful information to back propagate the weights, right because I'm going to be multiplying the change in weights by among other things the derivative of the activation function, and if that's really close to zero or is equal to zero, there will be no change. This last point in multiple applications is generally known as the vanishing gradient problem. And it shows up in a number of different circumstances most particularly in RNNs we'll talk about that briefly later in the course. And among other things what this can do is it can cause the training of the neural network to just slow to a crawl or stop. And so if you end up with a place where all of your, your gradient derivatives are zero or a bunch of them are suddenly individual neurons that might be useful for the problem. Those weights are no longer updating. So I could end up with my weights at a place where they're sort of arbitrarily far away from the best solution, given that neuron in that in that layer, but it has no useful information about which way to go. So it just stops. And so both the sigmoid function and the tanh function can suffer from this problem. Everybody clear on the motivation so far. Okay, so we use this thing called relu. Relu stands for rectified re linear unit. So the unit here refers to the neuron. So we have a neuron that this relu function is applied to it and we'll call it a relu unit. And so you can also have relu layers, right where the entire layer uses relu activation. And we also talk about relu networks, where all the layers all the neurons use the relu activation. And it turns out that these relu networks actually has some very interesting properties that we'll discuss briefly at the end. So, in the example so far we've been using the tanh function for everything. It's like this is our default non linearity. This is the only one of the explored so far we're just going to apply the tanh function to everything. We don't really talk about tanh network so much but you could right that would just be a neural network where you use nothing but the tanh function. And relu networks because they have some interesting properties tend to be considered a single group because those properties apply or appear to apply to all relu networks. Okay, the function itself looks like this. So, already you can see that unlike tanh. This is a what kind of function. So, it's zero if the value of the input is greater than or equal to zero. Otherwise, it's x, if x is greater than zero. In other words, this is basically taking the max of zero and x. So, there are multiple ways to write this function. And we will use the for implementation will just use np dot maximum. So, if I have some input a, and I take np dot max of a I get this. So, all it's doing is just taking the positive part of x. So, if I am zero or less the output is zero. If I am greater than zero the output is x so there are some small debates about whether you need to bound it at zero. If it's less than zero it's going to be zero or if it's greater than equal to zero it's going to be x, we end up mathematically end up in the same place. Although, I think people who study like real deeply the mathematics of machine learning may actually have some strong opinions on this, because based on where you make that boundary. Sometimes the properties the network may actually change. For our purposes and I think you really need to worry about that. So here's our real function. And it simply takes the positive portion of x. So, what's really derivative. Right, so we'll describe it here and you can tell me what you what you think what you think it looks like. So, if x is greater than or let or if x is less than or equal to zero, then Rayleigh of x is a constant zero. So, the derivative of any constant is also zero so for this portion of the function where it's less than or equal to zero, the derivative that should also be zero. So, which makes sense. Now if Rayleigh of x equals x. So, the derivative of y equals x is one. So, we should all know that. And so therefore, if x is greater than zero, then the Rayleigh do x is equal to one. So, the derivative of Rayleigh also known as the rectifier function is called the heavy side step function so what do you think the derivative of this function looks like. When I show it next. Yes, a line and then another line right, what do we think. Okay. So, we define it D Ray Lou as the head of NP provides a heavy side step function, right, we get this. And this is this is what we said so we, of course, are assuming it's it's, we're drawing it like the continuous function so it does draw, does its best to draw a vertical line here. Maybe it's drawing it between like point negative point zero or between zero and like point zero zero five or something, because we're sampling 100 points. But yeah we're seeing is it's zero from negative infinity to zero and then suddenly it's one. So let's see what actually happens when we apply this to some weights. First of all, I'll define this term as being a weight sum. And then I will just take, you know, create some random numbers so let's assume that these are inputs to a Rayleigh function. So first of all let's see what happens if I take, take the tan h function to this. Right so I get this. So these are my, my original inputs. This is the tan h of that so 10 h of zero is zero, the 10 h of the negative numbers are increasingly negative as the negative number the magnitude the negative number increases, and then also increasingly positive bounded one as the magnitude increases So, basically what we have is I have a bunch of a bunch of ways and sort of squishing them into a distribution between zero and one. So Rayleigh, the, I can also basically take a function like this for some input s for those elements of s that are less than or zero, less than zero, I will set them equal to zero. And then that will give me the output so Rayleigh that looks like this. Right so this is 10 h of s of everything is squished into that s shaped curve, and this is Rayleigh that's where all the negative values are simply set to zero. If I take this s again. So now what about the gradient of s. So, well, I guess point here, just this is one of those peculiarities of memory so here I put in s and then I modified s in the function. So I can turn s, and I can take Rayleigh of s and it's going to give me this and then if I print s again, because I modified s there, it's going to give me s. Okay, so just to be careful in this implementation will do is make a copy of s. So that I actually have a deep copy of as I'll store that is why and then why will be the Rayleigh version of s. Okay, so I can return y and then if I print. So, why. Oh, what's happening. Yes. And why. There we go. All right, now what about the gradient of s. So I will just define again, D Rehlu. I'm sorry. The gradient of s. Well, so s, this s is modified in line. Right, so this is modified in sort of this version, this non memory safe version of the function that I wrote. So really s what we want to be doing is returning s to the original value here. Right, so if I were to do this again. So, it's a I'll start from the top here. Alright, so I'm going to define s. Okay, now s is this. I'll take its tan age. Okay, who let me define Rehlu using s inside the function I take the Rehlu of s. Okay, now I get s with all the values negative values at zero. Oops, I changed s in line in the function so actually change the value. Of course, if I apply Rehlu to this I will get the same thing because all it's doing is taking values that are zero, or less than seven to zero and so the only thing that gets changed here are things that are already zero so nothing actually changes. So, a better version of Rehlu make a deep copy of s. Of course this takes more memory. So let me define that function. Let me redefine s to what it was previously. Right, so now I have the original s, y equals Rehlu of s, y, and then s again. Right, so there's the original s. So now it's worth these. So now what about the gradient of s. So let me define my D Rehlu function. So s will be just n samples by n units. First I'll just create a deep copy. And so now I will just define the heavy side step function in in line. So here we have d y, where those elements of of it are less than zero those become zero, where they're greater than zero they've become one. So I still have to deal with zero. So here, because every value in the function has to be given has to be given an output value. I don't want to be, I don't want to define I don't want to end the function here. Right. Because then if I put in a zero it'll give me some, some error probably. So I will just decide that effectively the same thing is true, where if I just set the sequel to zero I'm doing the same thing as if I did that. So now D Rehlu of s gives me this. So all the negative values are zeroes, all the positive values become one and there was one zero in there that remains. Okay. So, there's that again. And there's just multiple ways of doing this right so I can take the, I can just copy Rehlu. And another way of doing this would be because Rehlu is already setting those negative values to zero. I just copied the Rehlu output, which were the negative values were already set to zero and then derivative of those would also be zero, and then all I have to mess with is now zero or the positive values. So that's one way gives me the same thing. Another way would just be to make a copy of s. And then I'm just going to return basically a Boolean, where if dy is greater than zero. This would be true, which if I do as type int that then turns it into a one. Okay, so this is basically just saying all of those, all those elements of this input that are greater than zero will become true or one, and everything else will become false or zero. And so these are, you know, more or less efficient ways of performing the same operation. Okay. So now the weird part right why does this actually work, because it doesn't really seem very intuitive that this piecewise linear function would be able to introduce non linearity into a neural network. So why would this, why would this work. It's not totally linear in that it's got these two pieces, but it lacks the clean curves of the signal or the 10 h, and just sort of looks like I lopped off the bottom half of my linear function for no reason. So, why does this work. So let's remember that the sort of modeling curves like the following adapted from notebook five just I'm using the 10 h function. So again what I'm going to do here is I'm going to create some, some inputs and then apply a no non linear function to them. So in this case is just you know, this is this is like basically linear function with some random noise, and then we're going to train a model to fit that. This is the same. This is just doing linear regression, except I include a 10 h function here so I'm sort of trying to optimize my weights with the 10 h in the mix to model on a linear operation. This gives me this these are my actual weight value weights and bias. And here's the model. Here's what model predicts the orange line right so you can see that it gives me a very slightly curved line. This is effectively trying to do its best to straighten the 10 h function. So it's applying weights that are really small such that when I apply the 10 h function to it I get something that's like really close to the original input. Okay, so there's a bit of a curve here if I were to train for longer, probably this curve would might flatten out a little bit. So, what these functions do is they're going to bend the otherwise linear regression to better fit the data so you may observe that by adding this noise. There is, I'm actually, because of the random sampling, there actually is a bit of a curve to the data. Right so if I were to model this purely with a linear function, maybe it wouldn't be quite as good a fit as I did with the 10 h function. So, ReLU does effectively the same thing, except instead of having a smooth differentiable curve all the way through, it's picking one point in the line and making one bend. Okay. So this is a lot more computationally efficient obviously, because it's just, it's really just a linear function with a bit of a twist. But it's also less precise. So let's say that I have this data below, and they have these blue points and these orange points. And let's say that I wanted to somehow draw a line to separate them as cleanly as possible. Right, so what I'm trying to do is I'm trying to draw a line that will be about here, and then here, you know, going to try and fit as closely as possible may not quite get it because these points really closely overlapped, and then go down again. Okay. So, we could probably do this with a 10 h layer or two. So if you remember from notebook five, we had two distributions. So these random noise samples, where one kind of curved up, and then one kind of curve down, and we can just apply a 10 h function to a linear function, and kind of fit that if there's a single curve, then we had one that sort of look more like a parabola. And so now I want a function that's going to go down, and then go back up again. And you can do that with a single 10 h, but I could do that with, say, two 10 h functions, one that would put the curve for the negative values and then become flat, and then one that starts out flat and then it's zero starts to climb. And so then if my weights, if I have two hidden units, each of which has a 10 h function on it, and the weights in one are effectively taking the negative values and making them close to zero. That's going to give me that curve that starts flat and rises, then have another one that takes a very positive values and makes them close to zero that's going to give me the one that starts to fall and then ends up flattening. And then I add those two together, and I'll get that nice curve. So that's how a 10 h layer would do it. So this will basically draw this like smooth envelope around the orange points. So Rayleigh with a single band would have a much harder time, of course, right so it might, you might be able to infer that like okay, I get one line and one bend. So I'm going to put my Ben here, where my cursor is, and I'm going to have straight lines in their side. Okay, probably not too bad of a fit, although you do risk having certain points like maybe this one that fall on the wrong side of the line. So this is going to be less precise. What if you have a bunch of different Rayleigh units, and each of them is going to be optimized to put that bend in the right place. This could approximate the same curve. So it's sort of like, instead of drawing a circle, you draw like a 36 sided polygon. So you can look a lot like circle. Okay, or 100 sided polygon or your pick your number the more you the more you get better it's going to look like a smooth curve. So by, by these Rayleigh units working together this would allow us to approximate the same curve. So, outside this nonlinear function, all the neural network operation is doing for a unit is y equals WX. So you're just moving the line around, and then, and then stretching it, you know, rotating it, basically choosing the slope. And so then you you moved around by some bias B to figure out where to place it. And so the right bias vector, it will allow you to choose smart places to place each of those bends and then the remaining ways will sort of tell you how I'm supposed to, how I was supposed to rotate the entire, the entire curve. And each of these let's say what I could do is I could optimize for like a small piece of the curve is kind of a shallow been like that. That's going to handle some part of maybe from all the values like up to negative four or something like that, or up to negative two. And then the other one that is optimized to place the band in a slightly different region, and then so on and so on, and then I add all those together, and it's going to do a pretty good job of creating something that looks like that curve that I want to separate these regions. And that means that Rayl is real strength is in numbers. And so we can have a really large number of these rectifier units that actually approximate the nonlinear behavior of functions like 10 h and they do it, even though you use them in greater numbers because it's so much faster to calculate the Rayl function, they actually do it with greater speed and greater efficiency. So if you want more on like how does this work this is explainer here that you can check out. And gives me the pretty nice examples of trying to separate you know sort of curved regions and data. Alright, questions. Okay. All right, so now back prop is relatively straightforward, because as I've kind of foreshadowed already, the only thing that you really need to change is the derivative of that activation function. And think of it as if there were no derivative back prop and the hidden layers the same as back prop in the outer layer. So all the only thing that I'm adding is the at the derivative of the activation function so we have remember for our neural network operation. If V is some hidden layer, I'm going to multiply x by that fly some activation function h gives me some scalar value z. I add a bias to those, and then multiply those by output layer weights w and that using my prediction. So here is back prop. Let's just look at the w line first this is back prop in the output layer. So this is going to be T minus why this is my error term how wrong I am. I'm going to multiply that by z, which is the input to that layer. I'm going to do this for all of my samples for the entire dimensionality of the output. Right. And then there's some learning rate in there. So the same thing all I'm doing is all I'm changing is basically this part of the equation. When I go from the output layer to the hidden layer. So now instead of just the error term and taking the error term times the output weights times the derivative of z in this case the input to that layer. So what is what is the was he is actually going to be a function applied a function h applied over x v. So that has to be incorporated into this operation. And so I need to know what the derivative of activation function is, if it just 10 h, then the back prop operation will look like this. So I'm going to have minus z squared, because the derivative of 10 h is one minus the 10 h square. If h is relu, then the back prop through v is going to look like this. So I'm going to have everything up until this point the activation function is the same. And there it is multiplied by the derivative of the activation function, which because if the activation function is relu is this step function. So now it's going to be dependent upon what the precise values of each element in Z are. So I'm going to take in some weight matrix Z, and those elements of it that are negative or zero becomes zeros, and those elements of it that are positive, get left to love. Okay. So, let us view how those weights actually change. So what I'll do is I will create some data represent sample inputs and targets like you've been doing. And then I'll create some randomly initialized weights. And then I'm just going to simulate one iteration of training with the two different activation functions 10 h and relu. So I'll have basically versions of Z and the output as assuming that the activation function is 10 h and then one assuming that it's relu and I'll store the error using each one, perform back prop, and then calculate the change in each way. So these will be the delta between the previous value of those weights and the current values. All right, so we do that. Now let's view how the weights change depending on the layer and which activation function. So just view this in a pandas data frame, and just reshape the data so we can, we can view it. And here we go. So if these are the initial weight values, just for say the first nine here. This is what happens after I apply the 10 h function. Okay, so we can see that for example, for those values that are very close to zero, they remain very close to zero for those values and all them pretty close to zero so you don't this effect is not really very pronounced, but those things that are are very far away from zero, you know, get a little closer to negative one or one. And then we can look at the amount of change, right, how much each each one has has changed. Now, relu, we can see that after the relu, the relu function, there are certain values that have some change applied to them, and certain others that are completely unchanged. So this is all dependent on the whether they were, you know, positive or negative after you take that and then multiply them by the inputs. Okay. So now we can look to the same thing for the output layer, right and we see similar phenomena. The only real difference that here we don't have that many, we don't see any values where the value has not changed at all. And this is mostly because we have this additional computation in the way that hidden layer that's providing other information. So finally, relu is not the only linear unit type activation function you can use. There are plenty of others really is the simplest. So you can use J Lou this Gaussian error linear unit this is the one that has been used in most state of the art language models. So, I'm doing exactly know what GPT uses for certain but I'm pretty sure that it's some sort of J Lou function. And then there's the sigmoid linear unit, which is basically just x times sigmoid of x. And then there's the end result of being another smoothed version of relu. So it's sort of, it's the looks, you typically like the relu function, except it doesn't have it doesn't have like a sharp bend at zero it's got more of a smooth curve. So, all of these will not not necessarily all those many of them. They try to smooth out that bend in relu, because it makes for a nicely differentiable function all the way through, whereas, yeah. So, let me let me do let me do leaky relu first. So leaky relu is basically, instead of being instead of clamping everything at zero before. So, for all negative values, and then just letting x pass all the way through what we do is we let a little bit of that negative value through so there's there's some constant value usually like point one or point oh one. And what it'll do is it'll take for negative values it'll do you know, k whatever that is times x. So it's going to a little bit of the negative value through but not that much. So, maybe take one 1% of that negative value, and then the, and then the positive values just get passed through. So parametric rate relu is the same thing and that we're going to try and learn, or we're going to try and let through some of those negative values. I don't know how much right so I could just leaky relu everything, but maybe that constant point zero one is like not the best value. Right, maybe, but I don't know what it is I actually want to learn that from the data. So we basically have a tunable parameter that allows us to figure out what the best value of this quote negative leakage is. And so this is that this can actually be learned during training as well. And exponential unit, what this will do is in order to keep mean activations close to zero actually exponentiate the negative values. So all of these are effectively methods of leaving the positive values alone, and trying to do things the negative values that are meaningful usually in combination, and also don't cause your collapse. And all of them are usually more computationally efficient than 10 h. Some of them like like parametric relu, you have to you actually have to tune a parameter in there. But they all have different uses. And they can be very useful for different types of problems you will find the convolutional neural nets, usually use combinations of Ray Lou layers. Okay, questions about relu. Okay, so I guess there's there's two answers to this. Yes, you can as in it would it's definitely possible to write a function that does that. Does it satisfy the nice properties that we want with neural networks that's less clear. Because pretty much everything gets standardized and so you're assuming that everything is going to get clustered around some mean, which then gets standardized to zero. And so zero tends to be the best place to start making differentiations about your data. So yeah, you probably could write a function that's like, I'm going to maybe learn where the best place to start. Where the best place to start letting positive values through is. But there's a good chance it's probably going to be like really overfit to a particular data set, because you're, there might be some data set like where you you standardize everything. And then there's like a weird secondary cluster sort of somewhere else that's not zero, and it might learn okay this is like a really good place to start my linear part of my relu unit for this data set. But once you train on that data set and you try something else, even like different testing data. So you're going to have that same property in the testing data so it's just, you know, back of the envelope guess really sort of seems like this would not be something that would be very productive although maybe people have tried. Okay. Other questions. All right. So, let me go to assignment two, and go through that real quick. So, general shape of this assignment is very similar to assignment one in that we give you some starter code, you to fill out parts of that starter code, apply, apply that to some dummy data to make sure that your functions are apparently written correctly. And then you actually have to perform some experiments on it and discuss your experiments. So, you will give you in this case a neural network implementation. If you use this assignment correctly you will at least have a complete neural network implementation that you can use yourself. Do you want to do that well that's questionable because it's all written in NumPy doesn't have GP acceleration so you can use it. And if you're a talented hacker you can probably, you know, translate into pie torch or something. We'll also use a pie torch neural network later so you don't need to do that. But again, if you succeeded to assignment you at least have in principle neural network implementation that you can use an experiment with and does give you a lower level of control than the pie torch or TensorFlow implementation so we do have that that advantage. So you're going to need to do that to complete the neural network, define a function that partitions the data into training validation and testing sets, like we showed in the previous lecture, apply to a data set, and then define a function that's going to run experiments with different hyper parameter values. And then you're going to describe your observations. So first thing you're going to need to do is you need to complete the optimizer class. So, we give you a mostly complete optimizer class that has complete implementation of SGD. And what you need to do is refer to notebook six and turn that SGD implementation into an atom implementation. So this cell will implement SGD, and then there will be some implementation of Adam that you need to complete. Right. So, we give you for example, we already give you like, you know, MTV T beta one beta two values, what you need to do is you need to finish the implementation by updating these values and the values of self dot always. Okay. So then you can test that if you do view correctly you should get the same results as shown below. What do we see here so I alluded to this property last time. So, your, the test here is basically finding the minimum of parabola SGD finds it exactly right we know the minimum of the parabola is at five SGD finds it at five, Adam gets very close, but doesn't quite find exactly five. But this is supposed to this is the expected value, right you're supposed to get 5.0390403. Okay. And we can also see here that for example, both of these are training for 100 epochs and SGD starts with an error that's very close. And in this case because it's a very simple problem arrives at zero pretty quickly. And then starts with an error that's like way off 16, but it also gets, it closes that gap within that hundred epochs. So if you look at the change in the level of convergence from the start to 100 epochs, and basically eliminating this error about 16, where SGD starts very close and only has a little way to go. Okay, so you can also see kind of some of the properties of the two functions there. So what you are going to implement the neural network classes is going to call the optimizer is functions. So we already give you that the, the call to optimizes is done. So first you need to complete the use function. So you can make use of the forward pass function in your use function I'm not going to give you much more information that you have to figure out exactly how you use the forward pass function and the use function. It might not be too difficult, I think, if you've been paying attention to what the forward pass is and what backprop is and what it means when you see straining. Okay, so do that the rest of the classes pretty much given to you. So complete the use function. You can assume that exa standardize we have to return the unstandardized prediction from the use function. So then you test that this test neural network function, your results should be the same as these below. So you can test that there. So you can see these graphs and basically if your, your output matches these graphs, done it correctly. Again, as always, I recommend the other this notebook create a copy, just in case something gets messed up you can always revert to this one and you also compare your results, visually to this one, and numerically. Now what you're going to do is you are going to cut and paste basically create a copy of your own network class cell here, and then modify it to allow the use of the real activation function. Why do I want you to cut and paste well mostly because I don't want you to risk screwing something up that would throw off these results that you're trying to test. Okay. Yes, it does. It does seem kind of extraneous to copy and paste this large chunk of code. But I think it will help organize your, your stream of thought, and also prevent you from maybe creating on desired results. Okay, so what you're going to need to do is you're going to modify the neural network to allow the use of the activation function. So right now it assumes that the activation function is 10 h. So if we go to in the train function, for example, we're going to see somewhere. For pass gradient F. Find this 10 h. There it is there's the 10 h. So yeah, in the forward pass. Here's the 10 h function, right. What you're going to do is you're going to need to modify this to take an argument activation function that will specify either the string 10 h or the string Ray Lou and depending on that value, apply the right function. There is no Ray Lou function in NumPy, so you have to define your own. So you have to matrix of weighted sums and return the Ray Lou value the implementation of how to do that is basically given to you in the notebook which you need to do is modify that for use with the neural network class and make sure that your inputs are all correct. You also need to define at Ray Lou. So if you add some if statements to the forward pass and the gradient F function that depending on the value of this activation function argument will apply the 10 h function or the Ray Lou function correctly. So this should be pretty straightforward if you just have a new class variable that has one of the soft dot variables in the neural network constructor that will then accept the value of the activation function. So the experiments so you're going to use the auto to MPG data that we've used in some lectures in lecture two. So let's apply these neural networks to predict miles per gallon using different neural network architectures. So, kind of doing the same thing that we did in lecture three, only this time, we're doing it non linearly. The data processing is going to be more or less similar to that you should end up 392 samples after you remove all the samples that contain missing values. Then you're going to need to partition the data into five folds as we should as we showed in the lecture. So this should return train validation and test partitions for the inputs and the targets. And then you're going to need to write this function run experiments that will have three nested for loops that basically do a grid search for you. So you can write different parameters of the number of epochs number of hidden units per layer and the activation function so these two can be whichever you want activation function is just 10 h a railing. Okay, now you that allows you to compare the training time network architecture and activation function and see which combinations work best. So I'm going to try zero for one of the end and hidden units per layers, because that will be a linear model. So right if I have no hidden units in my neural network is just a linear model, the way we constructed this lives just passes zero and and get that. So then for each set of parameter values, create and train a network using Adam, and you should have implemented, and then evaluate that neural network. So you can write the number of values, RMSE. And then when you're done constructed data frame that looks something like this. So, then, when you're before you start this nested for loop you need to call your partition function to form the train validation and test sets inside run experiments, use the same partitions for all of your experiments. So for example called the functions would look something like this what you need to do is then change which values of these should all be constant right x t and fold since day five, and then you need to specify which ones you want to use there. So these, this is just a list of numbers. This is just a list of lists. And this is just 10 h and Rayleigh, then find the lowest validation error and the lowest test error and report on the parameters that produce this, and then discuss how good your prediction is. So you can describe at least three different observations that you make plot the RMSE train valentests sets versus the combined parameter values we did a version of this at the end of the previous lecture. So you can model it off of that. You can work with a partner on this. If you choose you're not required to. This is the only assignment that you were allowed to work in pairs until the final project. So here like before should be uploaded on canvas. Same procedure as before. And so I recommend you know keeping the notebook in a, in a folder with just the greater. So then you will get 70% for the code and 30% for the discussion and the experiments. So you get some extra credits by look up the the switch activation function or start this article. And then you can implement that as a third choice of activation function. And if you do that successfully, you'll get five points extra credit. And you can just do my experiments just say, comparing just comparing the three activation functions, say, pick your best performing set of other hyper parameters, and then try the three activation functions and discuss the results. Okay, any questions. Thank you.