 Okay. Let's go ahead and get started. Sorry, we just found out that we have a paper to submit tomorrow and our co-author is flying home from India and I thought he was the one who's going to submit. So, slide emergency. Okay. It's just making it's coordinated with my student. Okay, so let's go ahead and get started. Let me share my screen. I guess I want to one point that I think I wanted to make real quick. Where is it. So, right, so I'm not going to post. Remember the part about don't put, you know, Christ for help in your code comments. But I don't want this to be taken as like, don't ask for help. I'm just saying don't, you know, put something that could be taken this way in the code comments. If something is wrong, the best thing to do is to come and tell me, because there are there are resources that you can be pointed towards. There are also options to manage the coursework. You know, issues with like course load or something, you know, just talk to me, don't, you know, put a don't put a cry for helping your own comments to be found, you know, when it's when it's too late. So just wanted to make that clear. But today's topic is the windows shade of this year. So, which one. Oh, God, I have so many things. Of course overview. This information theory. Okay, so today, I'm going to do probably the first 30 minutes or so. Just talking about information theory and evaluation so basically how especially when we come to classification, you will be performing evaluation of your models and why we do it this way, because there are some very solid mathematical principles for performing evaluation in the ways that we do. And then following that, I will begin the first real technical lecture on linear regression, which we will almost definitely not finish today because it's very long and so we'll finish that up on Tuesday. So, if there are no questions, I will go ahead and start. Yes. Yes, so they will be posted on the public facing page. So this one I think still says 2021 running the contents of the slides are the same as well just throughout the version after class. Yeah, so just just as a reminder all the information should be available on the public facing page yes. Oh, the first notebook. Oh yeah so I'll fix that. So the issue with that is that you look at the link it says like nb viewer dot ipython dot org change the ipython to dot Jupiter. jupy ter work. So that's just because I Python Jupyter notebooks you call ipython notebooks, and because this course has existed for a while I think the URL is just inherited from a previous version so if that first one's not working, change ipython and the URL to Jupiter that will work this afternoon I will fix the link so you don't have to do that. Okay, anything else. All right, great. So, what is information theory and why is it important. So first let's just do an introduction to what it is. So information theory is the study of quantifying and communicating information. And information were kind of colloquially raised to think of that as this sort of amorphous quantity or quality so really, that is not something that is easily quantified or measured. So, you know, you don't think of information as having, you know, mass or dimensional properties, when in fact it actually has all of those things, and we'll get into why that is now. So in the field information theory was first established in the 1920s by the two gentlemen on the left so Harry Nyquist and Ralph Hartley, and then it was codified in the 1940s really became a field, due to the person on the right named Claude Shannon so Claude Shannon is kind of referred to as the quote father of information theory. And so, just generally as a field this lies as the as the intersection of probability theory statistics and computer science, and really is just about what is what are the properties information that allow it to be measured numerically. And what does that entail. What does that allow us to do. The future in information theory is the concept of entropy. And you've probably heard of entropy in, say physics class. And so basically, the greater the uncertainty involved in the system, the greater the entropy. Right so we clean up my well my wife cleans up my daughter's playpen in the evenings. And then my daughter goes in there when she's not a daycare and throws everything around the system trends toward disorder. And so the entropy if I take the playpen to be a closed system. At the beginning of the day has lower entropy the end of the day it has higher entropy one day one way to think about this is, my daughter has a whole bunch of toys that when they're put away properly or a little basket. Right. And so at the beginning of the day, they're in the basket. And at the end of the day they're everywhere. And so, if I want to know where particular toy is when the system has less entropy that is the toys are cleaned up. I least know that's more than likely to be in the basket. And I'm usually correct when the system has greater entropy when the toys are strewn everywhere if I want to find a particular toy. I don't know where it is. So this greater uncertainty about a particular property of the system. And so it has greater entropy. So in general terms that's a specific example. How much uncertainty is involved in the value of a random variable, or in the outcome of a random process so the outcome of random value the random variable is like position of the toy in the playpen outcome of the random process is, what's the way much with my daughter is throwing her toys everywhere what's the process by which that happens and so she doesn't throw her toys everywhere in a different place every day. So in some places every day is actually in a different place so what's the uncertainty involved in that outcome. One, another way to think of this is you have a fair coin flip. There are two possible outcomes. Whereas if you have a fair die there are six possible outcomes. So they're both fair, you know they're not they're not unbalanced anyway, but the coin flip has lower entropy because there are fewer possible outcomes. Okay. Why is this important. So we begin with this quote by Shannon the fundamental problem of communication is that of reproducing at one point, exactly or approximately a message selected at another point. And he was thinking about this kind of in terms of telephone lines so remember this is the 1940s telephone technology was present but wasn't very sophisticated. And so you had basically static lines, or what he called the noisy channel. So that is, there's a person on one end who wants to say something, and there's a person on the other end is receiving the message, and there's some level of interference in the middle. And so person be who's the recipient has to decode that message and they might hear the wrong word or something. Right they might hear, you know, the, the, I don't know. I think of a word that's like, common, you know, hop, when the person meant to say cup, right. And they might be able to disambiguate that when they hear the following words being of water. And they're like, well, a cup of water makes a lot more sense than a cup of water and so therefore I'm going to interpret this as being cup. Okay. And so this has a lot of implications for a number of different, a number of different concepts like the entropy and redundancy of a source mutual information, the channel capacity of the Gaussian channel and the notion of the bit, the two in blue or the two in blue that we're going to focus on in this lecture and for this class. So, if you've heard of entropy in physics class. There is actually a close correspondence between physical entropy and information entropy so in particular we talked about Boltzmann entropy gives entropy and Shannon entropy. Information entropy is Shannon entropy. So we're going to look here through the denoted H. And so it's basically going to be the sum for all possible states for the times of the probability of that state times the base two log of the probability of that state. And so, each is going to be the bits per symbol of an information source. So let's say an alphabet of symbols, or a vocabulary and piece of eyes the probability of occurrence of the ice possible symbol so in our fair die example, the probability of every possible probability of an equation in six. In an evenly distributed set of six classes, the probability should roughly approach one out of six. Okay. If you have an unbalanced set then whichever set is whichever classes over represented is going to have a greater problem base rate probability of occurring. Now, the other two types of entropy Boltzmann entropy is written with the following formula s equals case of the times natural log of w, where s is the entropy of an ideal gas so little bit Bolton was studying gases, and the composition there of case of the constant, known as the Boltzmann constant, and w is varsheng lechkite which is a German for probability. In this case the probability is the probability of a specific molecular configuration of a gas so I've got a vessel full of air. And I'm like what are the what's the probability of the molecules being arranged in a particular way. Well, you can't actually count the individual number of possible arrangements of molecules, but you can come up with a probability distribution that described that right, what are the probability that they're all clustered over on the right side, vanishingly small, what is the probability that they're relatively evenly distributed throughout the vessel, probably somewhere in the fat part of the dog curve. Gibbs entropy is a generalization of Bolton entropy, and it uses a similar formula. So negative case of the times the sum for all I of the probability of the state times the natural log of piece of I. The probability of the system case of B is the Boltzmann constant, and piece of I is the probability that microstate I occurs as a system fluctuates by gases in a vessel. The molecules are moving around and I microstate I is just like some, you know, choose some configuration what's the probability that the get the molecules are arranged exactly that way. So, let's take a look at these formulas. The Gibbs entropy formula and set Shen entropy formula. Do you see similarities. What do you see. Sorry, you mumbled probability, right. What else do you see logarithms. What else do you see constants, right. So let's take a look at how these things. What happened. I used to have like some. Anyway, so we have probabilities we have peace of I, what's W stand for the German word for probability right so this could just as well be written P. We have natural logs, we also base two logs but it's pretty trivial to convert between bases and logarithms that's not a big issue. And then we have a constant, right in some cases the constant has a negative but that's just a constant times negative one. So if it's a constant I can factor it out. And then in this case we have a sum. And then in both of an entropy basically I'm taking the sum for all possible states. Right so if I sum this and W is just considered to be probability that I'm taking all possible in my probabilities of microstating summing them. So, if actually these formulas are equivalent. So, how does this apply to to machine learning what are the information theoretic concepts that will be learning in this class and why do you need to know them. So, if we take entropy that is h. So h sub x would be the entropy of a discrete random variable x. So that is if I know the distribution of x, what is the measure of uncertainty in the value. So, if I take this curve, which is going to be the entropy of a bitterly, Brumley trial which is binomial as a function of success. So if I have two possible outcomes, x equals zero and x equals one, then the entropy is maximized when the two outcomes are equally probable. What is an example of a system that has two possible outcomes, a coin flip, right. I just assume that success is one possible outcome I, you know success is arbitrarily defined as being landing heads up. Right so, if the coin is heads up on the ground, then the probability of heads is one 100% right if I if this is if this is the event that I'm observing the probability of that event must be 100%. So if I'm observing my penny being heads up, then the probability of tails is zero. If the coin is in mid flip. Right, what's the probability of success in this case heads being 5050 assuming that it is a fair coin so the probability is maximized when the two outcomes are equal probable in the case of the coin that's in mid flip. So if I don't know anything about the outcome of the system, if my possible outcomes are evenly distributed, then the, every individual outcome is going to be equally probable and I will basically fall in this part of the n dimensional curve, brand equals the number of classes. The concept is expected value. So this is going to be written with the bar to E. This is just a generalization of the weighted average. So this can be the arithmetic mean of a large number of independent realizations of x. So if I have a fair die possible outcomes one through six, and I throw out a bunch of times, right the expected value of that is what if I have a fair die, right, I throw it a bunch of times. So if it's a if it's a fair die the weighted averages every possible outcome is one out of his busy 16.66% likely. So say I throw it 100 times, and then the average of all those possible outcomes, the value of that number should approach what 3.5 right so that's a little counterintuitive right the expected value of a fair die 3.5 but we can objectively say that a fair die will never show a fair die 3.5, right, because it can't it's not one of the possible outcomes. However, this is that this is the expected value of all the of the possible outcomes. And so that's just because the formula is the generalization of the weighted average. So now I sub x or sorry I have x is called the self information or quote, surprising of x. And that is going to be the negative base to log of the probability of some, some possible outcome. So if you think of, say, scrabble letters, right every letter is assigned different point value. So Q is 10 points, he is one point and you know x and Z are also 10 points and rare letters or more points. What this is the those point values are effectively a realization numerical realization of the surprise of each of those of each of those outcomes So there are very few cues in the bag of scrabble letters and so you're surprised and maybe a little bit excited when you take out a queue because you get a lot of points. So, another way of thinking about this is that he is the most common letter in the English language. It's unsurprising, and it contributes pretty pretty little information. So if I think about where he can fall in a bunch of words, right, if we count the number of ease in the header for the slide theoretic concepts in machine learning 12345 right this five occurrences of E. And it doesn't tell you a whole lot about like what comes next to it. Right, but I think about where, where did the ease fall there next to H O C P and and then L and a whereas Q, for example, usually falls before you in English. And so, if I'm looking at words like quit or quake or common English words or sequence. If I see a queue, there's a very high probability that there's going to be a you. Now it's not always true. Right, for example, you can have loan version like Arabic that have Q in them. It's not followed by you. But those are comparatively rare. Okay. And so that's why, among other things, Q is worth 10 points in scrabble, because not only is it rare. I also in order to make real English words you usually have to have a you. So, now the entropy of X will equal the expected value of the self information of X for X in the set of all symbols produced by the information source. So now the entropy of X can be given by this formula which is going to be equivalent to the expected value of the formula on the previous slide. Okay. This can also be written as the negative sum for all X of the probability of every possible outcome times the log to the base two log of the probability of that outcome. So this is a powerful formula because you can combine it with when you have multiple variables. Okay, so the joint entropy of two variables X and Y is going to be equal to the same formula, except I'm just going to plug in X and Y. And then I'm going to plug in X and Y at the same time. So this is pretty straightforward in that if I want to take the joint entropy of two variables X and Y, I can then take the negative of the sum for all X and all Y of the joint probability of both of those times the base two log of the joint probability. In other words, if X comma Y is the position of a chess piece on a board, then H of XY of the row and the column is the entropy of the position of the piece. Similarly, I can do almost the same thing with conditional entropy the only difference is that I have to instead I can't just do the expected value of X bar why that doesn't really make sense. I still have to use X comma Y, but effectively, it's going to be very similar to the formula on the previous slide, except now I'm going to take the conditional probability of X given Y. So the bar here means given that is, if I know something about the thing on the right side of the bar, how much do I know about the thing on the left side of the bar. So in other words, if X is today's weather and why is the season, then the entropy of X given Y is how well season Y predicts whether X, right so it is winter. And so there's probably a high probability that it's going to be snowy or court Collins gets 237 sunny days per year actually when I interviewed this job they said it was 300 and I looked it up and that was why. But 237 is still pretty good. But these those days are not evenly distributed right we're more likely to have sunny days in the summer. Yes. But this year. So if I take the if I sum for all values of X and all values of why they take the joint probability of, so let's say my four values of why some spring summer fall and winter. I'm going to have, you know, let's just say some of your cloudy for X, right. So then if I take the joint probability that it is a cloudy day, and it is summer, or it is a cloudy day and it is winter or it's a sunny day and it is fall all possible combinations, right. Those two, there's going to be some joint probability associated with both of those things occurring at the same time. So it's not going to be the same as the probability of the weather conditioned on the season, because it's more likely to be cloudy and winter than it is in the summer. Okay. And so I'm going to compute the for all possible combinations, the joint probability of that combination of values, and then I'm going to compute the conditional probability for the same combination of values which is not necessarily going to be the same. So I'm going to base two log of that and then they multiply them together. I compute all of this as a product, and then for all possible values of X and y I take the sum of that and then I take the multiply that by negative one, and that will give me the conditional entropy. Other questions. So now the cross entropy which is the key term here. So we're talking about cross entropy loss. For now cross entropy is basically, if I take two distributions. So let's say I have a distribution P and a distribution Q, and then X is a single event, but the value, the probability of that then is going to be different depending on which distribution I sample from. So then the, the negative, or the negative of the sum for all X of the probability of X according to P times the base two log of the probability of X according to Q. That is called the cross entropy. So what this means intuitively is that this will be the average number of bits needed to identify if an event, identify an event, if the label set is optimized for Q, rather than the true distribution P. So let's think about, let's say I'm trying to classify, you know, the season, given a bunch of factors about the weather or something. And I have the true distribution of outcomes. So I had say I take you know, hours of sun. That's right. That's like almost per predictor. So I take like inches of precipitation and cloud cover and wind speed or something like that and those are my three inputs. And I use that to try to optimize a model that will predict the season based on those three inputs. There's a true distribution that is like this is the weather in Fort Collins for the year 2022 and that's going to be my true distribution. And I try to optimize a model to predict that it will become arbitrarily close to the true distribution depending on how well trained my model is, we'll get into the training parameters later in the class. But it's probably not going to be exactly right. And so how off is it. Right, is it way off. Is it, you know, does it predict that the weather is winter, you know, a few times out of say 100 or 65 times out of 100. So that optimized distribution is Q, the true distribution is P. And so this is going to be how much more information do I need to identify that event, if I'm sampling from Q, rather than P. And so this is important in cross entropy loss because those labels that are incorrect, those they're drawn from say a poorly optimized Q will signal some difference in Q from the truth P, and the amount of that difference will tell you how much further you have to optimize your model to get close to the truth. Right. So far, everybody. Yes. So would you use that to like compare for distribution. I certainly can. Yeah, so, you know, one way, for example, in the process of optimizing a model and I'll go into model optimization like in the latter half. But let's say I have a model I'm just trying to find a function that maps from an input to some output, and there's going to be it's going to be parameterized by some weights. My job is to solve for those weights and that's the optimization processes. And let's say halfway through optimizing and I haven't quite optimized to conversions yet. My model maybe kind of correct but not very correct. And so this is a measure of like how wrong am I, given the current state of the model, and this will tell me you know do I need to kind of shift my distribution further in one direction or another, and that's just going to be in multiple dimensions so but for now you can just think of it as like, do I move it left or right forward back etc. Any other questions. Yeah, you first. What is what is a label. Okay, so in this case, you can think of it as basically being a distribution of clusters. So if I have let's take the example of the weather again. If I have three inputs of wind speed cloud cover and precipitation, and I plot them in three dimensions let's assume for a moment that these are good predictors of the season. That means that you should have a bunch of points that are like winter weather, they're clustered close together, and a bunch of points that are summer weather that are cluster close to you in spring weather and then autumn weather. And if these are good predictors, then those clusters will be closely defined. And so in the optimization of a model the label is the output that I'm trying to use to map the inputs to. Yes. So basically the cross entropy of Q and P of P and Q remember these are distributions. Yeah, yeah so it ends up being a number. Right and we'll see you know how we calculate the number and how we use it. So when I when I say how wrong am I, I mean I'm saying that you can, you can actually assign a number to that value which sounds counterintuitive, but it's always going to be relative to the information that the how you represented the information that you're doing with. Okay, so it's not like, you know it's it's not always like between zero and one you can normalize it between zero and one, but sometimes how wrong am I is like 3.5. And that doesn't in and of itself mean anything, but it does relative to the data that I'm using to train my. Alright, mutual information. So we talked about self information. That was I have x mutual information. I'm not entirely sure why the use the semi colon, but basically we have I semi colon y. And so this is, you know, how much, what do I know about why if I know x. Now, as they can be calculated using the following formula so again for all values of x and y, I take the probability the joint probability. And then I divide that by the by the base two log or multiple by the base two log of the joint probability, divided by the marginal probabilities multiplied together. So, if I have two events that are conditionally independent. Who knows what that means by say two events are conditionally independent what do I mean by that. Yes, they have no relation to each other so effectively it's saying if I don't if I have x, and I have why I don't know anything about x if I know why. Right. So another way to represent this is if I just take the overall marginal probability of x p of x, and the overall marginal probability of why then if I multiply them together if they are truly conditionally independent, the joint probability will be exactly the same, because there's no difference. There's there's no difference in the probability of these events occurring together, then there is of them occurring together separately. So, you know, it's, it's not so much of a sense. Yes. If they if they were truly correlated if you had a vector representations of these things yeah then then you would get adopted. So maybe something very close to one. Okay. That's assuming you're representing them as a vector which we'll get to in a moment. But for probability distributions. It's sort of like, yeah. I'm going to check the math and that and get back to you to be sure but I think they think that that mean intuitively that seems like a good way to think about it. They are generally they're generally inversely correlated. Okay. One way of thinking about this would be like, if it's dark at 7am. And what's the probability of being dark at 7am and what's the probability of being cloudy. Right. Those two things are probably related somehow, right, because if it's cloudy may raise the probability of it being dark at 7am. Whereas, if I'm asking two completely unrelated questions like, what's the probability of there being a car crash on college. And what's the probability of there being a hurricane in Florida. And it usually has no bearing on the other except maybe with an extreme interpretation of the butterfly effect. So if I have the probability these two things they have individual probabilities and the joint probability them occurring together is just the multiple of both of those. Okay, so this can also be represented as the expected value of the point wise mutual information PMI of the events x and y. So, point wise mutual information refers to single events. So again if I have car crash of college and hurricane in Florida. It's like how much information do I does that car crash give me about the world versus how much information is that hurricane give me about the world. Those single events, whereas mutual information in general refers to the average of all possible events hence the expected value. In other words, the, the mutual information of x and y is going to be the information entropy of x minus the information entropy of x given y. So now we get to the next bullet. So that is if what do I know about why if I know x. So this is telling me, what's the entropy of x given why so why has some conditional bearing on x. So if I take the overall information entropy of x, subtract that from it. This will tell me what do I know about why if I know x, because I'm assuming that x has some bearing on or why has been bearing on x. So, in English, this would be like, if I see a queue with is a higher probability that you comes next. And so these two things have a relatively high level of mutual information in English. Yes. Mm hmm. Sorry, so you said again. I'm just cutting out a difference. Usually so the vertical bar means given. Okay. And so if I have x given y, then why falls on the right side of the bar. And if it's why given x you just flip them around. Yeah. So the last of the, the conceptual definitions is KL divergence. So this is also known as information gain. And so it's written as D sub KL, and it's usually denoted with the double bars. So this can be realized as the sum for all x of the probability of x times the base two log of p of x divided by q of x. Once again, p and cure a different distributions. Right, so if I have some distribution p, that's the truth, and some other distribution q that is arbitrarily closer far from the truth. So if I perform data compression, assuming q. Well p is the truth, then D sub KL is the number of extra bits I actually I need to actually compress the information according to P. Okay, so in other in other words if I have two people Alice and Bob, and they're drawing colored balls out of a bowl. Alice knows the true distribution. So like the number of balls in the bowl that are black or red on. And Bob has a different distribution so let's just take very simple cases. Alice knows that all of the balls in the bowl are black. Bob thinks they're all red. So Alice's distribution is P the truth, Bob's distribution is q, the erroneous assumption. So when Bob takes a black ball out of the bowl, D sub KL will measure in bits how surprised he is to see that, assuming his distribution me this is subject to like how many do we know how many balls are in the bowl total. And do we know that only black and red are the options. Right so if we throw other possible options there the number of change, but given other parameters that we know, then you can use KL divergence to measure in bits basically information So another way of thinking this is information gain. So if Bob takes the black ball out. He has gained some information about the state of the world. Right so what how much has he gained well that's debatable, but we know a couple of things. If you thought they were all red, and he pulls a black one out. At least he knows that one of them is black. He doesn't necessarily know that all of them are black but he knows now they're not already. Right so what bearing does this have on how we actually evaluate things in machine learning models so we think about the probability densities. So, now P and q again P is the density of the true labels that's the distribution over all the classes we have. What's the distribution of samples into those classes, and then q is the density of the model label set. And so we assume that the same number of classes right out of distribution is a different problem. It's a topic of open research let's assume that we specify the number of classes in our model. And then they, the only difference is in just like the density of the distribution of classes in the ground for versus the model. If I draw predictions from q, that's going to give me some prediction of the underlying data, and my goal effectively is to get q as close to P as possible. And so I need some way to evaluate how incorrect the model is quantitatively. So classifiers make errors but they make different kinds of errors right. So for example, if I have. Well I guess in the paper we're talking I was just talking about with my student we have this case where this is severely up on balance sample, we basically have a very few positive samples that we're trying to extract and a whole bunch of negative samples. Right now, I could have a model that just says if I have like 99% negative samples and 1% positive samples, I'm just going to say everything is no. Right, my accuracy be very high. But that's not a very good way to evaluate my model because I'm probably not going to get much higher than 99% accuracy, but I will miss all the relevant information that I'm trying to extract. So we need a better way of classifying errors than just plain old accuracy, accuracy is great, but only useful in certain circumstances. So, no model is perfect. So we think about spam filters search engines cobit tests I'm trying to retrieve a certain set of relevant samples, while minimizing the number of irrelevant samples that I retrieve. So if I have a cobit test that is quote 9095% accurate, and I gather 100 samples. How many tests are wrong. Well, how many tests wrong. By the tests are wrong. But how are they wrong. Right, that could be important. So for example if I test 100 people for cobit 19 and I, and let's assume that 10 of them are actually infected that's the true distribution. So if I have test A and B, and then a finds five out of those 10 positives, it gets 95 samples correct and five samples incorrect 95% accurate. If I have test B that finds 15 out of 10 positives. It still also gets 95 samples correct and five samples incorrect it also is 95% accurate. Are these two tests the same. So accuracy is how many do I get right divide by the total number of samples so that is the true positives plus the true negatives divided by the total population. So, if cobit test a finds five out of 10 positives there are five false negatives. There are five instances five cases of cobit that this test missed. If I have cobit test B that finds 15 out of 10 positives, then it's got all of the true positives and it also retreat five false positives. Right. Now which of these is better. Which is better be in this case, it depends on the use case though. In this case is the slides from 2021 so like written early pandemic I guess now we just like don't care anymore. But you know, we didn't we really didn't want to miss positive code infections. And so in this case and you know and many people still do not want to miss positive code infections so in this case cobit test B that runs the risk of retrieving some false positives but as a far lower probability of leaving a negative on the table is the one that you want to use. So, there are two measures that we use here precision also knows the positive predictive value, which is the number of true positives divided by the number of true positives plus false positives. So in this case, the precision of a, we have five true positives divided by five true positives and no false positives, five out of five is one that has a precision of one precision of B has 10 true positives divided by 10 true positives plus five false positives. So, I have a precision of one out of 15. And that's precision of point 666 continuing. Right. So this is basically precision is I have my dart board. And if I throw one dart and it hits the center, then I have 100% precision. Right. But if I throw 100 darts, and one of them hits the center, and a bunch of them go wide. And that's going to lower my precision recall is kind of the inverse of that. It's also known as sensitivity and I'm not sure why I got cut off there at the bottom this is 10 and that zero. That's the number of true positives divided by true positives plus false negatives. So in this case the recall of test a would be five false positive five true positives divided by five true positives and five false positives so a recall of point five. And recall this point five again this is those five positive cases that it didn't find right. The recall of be because it got all of the relevant samples all the positive cases of coven has no false positives. So it has a recall of 10 out of 10 or one. So recall is you know if I have my my dart board. And I throw just I my success is defined as like having 10 darts at the board. If I throw 100 darts and 90 of them go wide but 10 hit the board, then I have achieved my goal where. So, the trick is, if I test 100 people for coven. You know, at least early pandemic and really still, we don't know how many people are actually infected and you do the test, and even then the test is not perfect it's just kind of you have to take multiple tests. So we don't actually know this. So, we don't actually know how many people in a population actually have the disease. And so the consequence in this case we're in 2021 of a positive was a two week quarantine, which sucked. And you don't want to have to do it but if that's it and you know you're not going to be locked down forever was a relatively low stakes consequence. And so in this case, should we prioritize precision recall recall, right, we don't want to miss those positive tests, but it depends on your use case. So we will prioritize recall in this case. But there are other cases where you may want to prioritize precision so there's a quote from this British jurist called William Blackstone, it is better that better that 10 guilty persons escape then that one innocent suffer right so if for example, as in 18th century Britain the penalty for a lot of crimes of death. And so going back for that. And so you don't want to accidentally execute an innocent person. And so it is perhaps better to let someone go maybe they committed like a minor crime, then give them a very permanent, very severe punishment. Right so again, precision recall, what metric do you want to use it depends on exactly what you're trying to measure and how you're going to use the results. But in most cases we actually have to balance these somehow. So, accuracy is one way of doing this in that you can take the number of true causes plus true negatives divided by the total population. But when you have more than two categories, this often is not meaningful, especially if they're not balanced even if you have two categories and it's unbalanced also accuracy may not be a meaningful metric. So I have a sample of an image classifier, and I have three classes cat dog and rabbit and I'm trying to classify, you know, images as one of these three animals. So, the precision of cats in this case would be the number of true positives that is things that are cats that are classified as cats, divided by the number of true positives, plus the number of things that are not cats that are classified as cats so basically in this case, true positive would be read as members of the class that are correctly classified versus numbers not of the class that are classified as this class, and then true negatives would be number of things that are not of this class that are not classified as this class false negatives are things that are classified as this class. Okay, so positive and negative just you know in this case are going to be always interpreted relative to the class. So in this case, if I if I write it out like this using what we call a confusion matrix, I can just calculate the precision by summing across all the rows and dividing take that as the denominator and then dividing the number in along the diagonal by that right so I take 13 divided by the total in this row that's 25. So, 15 divided by the total this row this 2313 or 10 divided by the total in this row, which is 2013. So, has a relatively low precision of cats. 52, but higher precision for class dog and class rabbit. And so what this tells me is what types of classes are tend to be more problematic for this model. And then you can tell me where to optimize recalls the inverse. So I'm just in this case I'm just going to sum down the rose, right, but the, it's just the number of true positives so that number along the diagonal, divided by the sum of all the numbers in the rose. When reading confusion matrices, it is important to note that this actual verse predicted is not universal. Many people will flip the axes. So it's not always as simple as just some across the rows or some down the columns, you need to pay attention to which axis is which. And in the assignments, you usually use like a preset function that just like doesn't in a certain way. I don't remember if it's exact if it's this way or the opposite, but just use that function and just you can remember which axis is fetch. So now what do we do what's another way besides accuracy to balance between precision recall. So we have another metric called f score. And it will be most usual cases f one score. This is a harmonic mean between precision and recall. The formula is two times the quantity precision times recall divided by the quantity precision plus recall. In this case, what I would do is I'm going to compute the precision for all the three classes. So here I take the three numbers for each of the classes divide by three, and then do the same for recall. Multiply those two numbers together in the numerator, add them in the denominator, divide it, and then multiply by two. And here I get an overall f one score of point six six. And so this is a typical measure that you often find of assessing the quality of a classification model. Now this is macro average, that is we are assuming our sample is relatively balanced and so I'm not going to wait these numbers by the number of samples in the class. So you can do micro averaging, which does do that so if I have an overbalance sample it's over balanced in one direction, then I will wait that accordingly when computing the f one. Yes. When the waiting and the macro average. Yeah, good question. So, generally macro averaging will work for balance samples are things that are close to balanced. It's kind of, it's a bit of a kind of a qualitative game. So, you'll notice that our samples here are not completely balanced, but they're close enough. Right, it's like it's pretty close as I can probably get away with just doing macro averaging. If I did micro averaging I wouldn't get like a hugely different number. On the other hand, if I had like a lot more rabbits than cats or dogs or something, you know, if I put all my animals in a barn and come back a year later I'm going to have a lot more animals, a lot more rabbits, different barns I guess, dogs even. Then I will then I need to wait my sample according. This is f one is a special case of the F measure, which is usually written F sub beta this is just a tunable parameter. That's basically how many more times do I value recall then precision. So if I have a case like my cobit tests that I still value precision, but I maybe value recall more right don't throw precision out completely, but I want to value recall and times as more than I can set this value and so I'm going to have a set of two is just me one plus B squared. And then in the denominator I'm going to have the square times precision. And so of course, if beta set to one it comes after this formula. Okay, finally, this is just like a nice little schematic is actually from Wikipedia if you go to precision and recall. So let's say we have just some samples. So, we have relevant samples here on the left side. So, of those relevant samples the things that are retrieved those are true positives. And then on the, on the right side those things that are also retrieved are the false positives so precision, how many items are relevant. Recall how many relevant items. And so this is just like a little schematic you can use to remember that. Okay, I think that is all for this slide deck to have any questions about these metrics. All right. So, now let me proceed to the Jupyter notebook that will get started. So, the first topic is linear regression with SGD SGD is to cast a gradient descent. This is our common optimization model, or algorithm. So when I talk about optimizing a model, I want to bring the weights that parameterize my model closer to those weights that will predict the true distribution. So that's what we will be doing for pretty much all the units as we're going to do for whichever unit is going to be the linear version first, and then the nonlinear version. So, the first unit is on regression, and regression, as you were probably aware is fitting lines to points. Right so most machine learning, let's say all machine learning in some regard, is effectively glorified curve fitting. So, just about, what am I trying to do with that curve am I trying to fit, you know, fit to the data, am I trying to separate regions in the data, you know, am I trying to predict a trajectory. All of these things are done with lines. And so it's just a matter of how do I draw that line. So, if you're really enthusiastic about fitting occurs to points, you will love this class. So, if I have observations. So if I have an observations from one to n, and then a set of target values. What's the simplest model will call a g of x that you can think of. Well, the simplest model is probably just g of x equals zero or a constant but that's not going to be very useful. And that's all my data is laid out in a horizontal line. So, I think that's not it. The next simplest model might be something like this so if I have a model g that is a function of inputs x parameterized by weights w. What are those values of w so here you see bold. This can be assumed to represent a sequence of values so bold x is going to be a bunch of different x's bold w will be a bunch of different w's. So, weights w may be some weight zero plus the first weight times the first input plus the second weight times the second input and so on until I go through all the inputs. In other words, I can rewrite it as the sum from for I from one to D of w sub i times X sub i. And so, if x of zero equals one, then this can be written as just the sum from zero to D of w i times X sub i assuming that X sub zero will equal one and therefore factor out. So, this can be written also alternatively as this vector of w's times the vector of x. Yes. What do you want to do with the font size increased. Yeah, okay, sure. So, we have this thing here w tx. What is the T for anybody know transpose right so what does what does that mean. Let's just write it in code real quick. So let me define a vector w and x, and this will just be 0123 followed by 123 zero. So if I just do element wise multiplication, right so if I print w print x and then I print w times x. This gives me the result 0260 y zero times one is zero one times two is two and so on. So, this isn't what I want, right I want G of x paradise but w to give me a single number. How can I get a single number out of this. Well, in the last lecture we had this Matt mall operator. So the at sign is the matrix multiplication operator in NumPy. And so if I do w at x, I get eight. And so what if I just write out the matrix multiplication operation by hand using summations and multiplications we can verify that is correct. And so that is, in fact, correct. Okay, great. But you haven't really explained what he is. So let me try it again with two by four matrices. Right. So, if I have these two matrices, w and x that are now two by four dimensions. So, I will print w dot shape and x dot shape. This will show me exactly what the layout of my different matrices are. So, I'll print w. I will print x, and then I will print w again element wise times x, I get this, and then I'll print w Matt mall x. This works so far until this point but now it's throwing an error when I try to get it to print w times x. Yes. I remember why because you can have the same number of columns in one array and the rows that come to that two by four. Right, it's got to be so if I look at two by four. Remember how we do matrix multiplication you basically take every element of a row, and then multiply that by the first element of the column, right that becomes the first value. And so what that means is that if I look at the shapes of my matrices, these two values here in those inner values have to be the same in order for it to multiply together. So I can't multiply these two by four matrices together using those shapes. Right. And so you will see this error frequently. Probably when you're doing your homework and so you will find you mismatch and score dimension zero with g u funk signature blah blah blah, and people don't really know what this means often. What it means is that the shapes of your matrices are wrong. Right and is either because you set up the data you did so there's some bug in setting up the data, or you need to reshape the matrix. So, now let me transpose this so I can run NP transpose over w, and then print these print the shape. So now we have four by two, and two by four. So, my sense is this should probably multiply. So, now if I transpose w and then multiply that by x. This actually works. So if I wanted to get a two by two, instead of a four by four, what would I do. I transpose x instead of transposing w right. And so now you, you know what, if you know what shape you want to get out, you should be able to identify how you need to reshape your matrices. So, an N by K matrix times a K by M matrix will always equal an N by M matrix. And so, if n equals one, then the transpose of w is actually equal to w internally in num pi. Right so if I just take the transpose of 0123 and print it. It's just going to give me 0123. So, W transpose x is nice because linear in the parameters of w. And so we can do optimizations based on derivatives and we can solve this analytically. It's not so nice because it's also linear in the inputs. And this greatly limits the complexity of the model so one infamous example is that linear model can't solve the x or problem which we use to motivate neural networks, but a model that's linear in the inputs might be the best you can do. Let's say you have a sparsely sample distribution, then maybe you only have a linear model, you know that would actually be able to fit to this data. So, fitting data samples to linear model. Let's assume the following situation. If I have a force f exerted on a spring. This is going to be functional to the length of the spring. This is hooks law. The potential energy stored in the spring is proportional to the square of its length. So let's say that we want this rod here so let's say I have, you know, some springs, and there's a rod suspended between them. And I want that rod to settle at the point that minimizes the potential energy and set of springs. Right this is basically equilibrium for this system. So we're going to construct a series of measurements of the potential energy with the rod in different positions, and we'll store these lengths as a vector w. So, given by this formula here, T sub n will represent the end experimental measurement. So that is in this case potential energy. And then, if G is an affine transformation. So that is it's linear plus the constant. So these are the lengths of the springs, and then G is some affine function over that. This can be written as we did above. Right this is basically going to be the same thing that ultimately resolves to x transpose w, or does or W transpose x. And so this will have the parameters. So we have this function G of x semicolon w, this w those are those weights that parameterize the function and so if I have if I know the value of these weights, and then I can put in my inputs x, and then just perform this linear operation on it. This will give me the output for any new value of x, according to those those weights. So my goal is I need to find these the best value of these parameters for weights. So which ones give the best fit. That would be the one that you take the argument of w for all possible for all the sum of all values for all our experiments right so T sub n is going to be the the actual experimental measurement, and then G is going to be the output of the function. So this should just give me the least squares distance. Right. So here what I can do is I can set the derivative also known as the gradient. Yes. I'm sorry say that again. The argument argument is the input to a function. So, the argument is the argument, the input that gives the minimum value so basically, I'm trying I'm looking for the values of w that will minimize the output of this function. And so if I if my very simple version would be like, what's the, what's the argument of x squared. I'm looking for that value of x that produces the lowest value of x squared. And this case it would be zero, right, because that's the, because it's parabola. And so you know if I go negative the x squared value goes up. Okay. So what I can do here is, I'll set the derivative, which is the gradient is the key term we use but for now it is used derivative with respect to w to zero, and then we'll solve for w. So, you know, hopefully, you're all, you're familiar with like linear algebra, and, and what a derivative is, and what it means to set a derivative with respect to a parameter. Now we can do this with matrices. So the formula is get a bit simpler. If we assume a couple of things. One if we assume that the first weight w zero is multiplied by the constant one, and then x, I zero, that's the first component of the sample I is that constant one. And what we can do then is we can collect all of our observations into matrices, and effectively allow us to stack up these matrices such that there are correlations between the inputs and the weights and the desired outputs and then solve for the value of weights. So, T is going to be the observations I want to fit to. So in this case this would be my actual experiments when I'm trying to find a model that fits my experimental data. So I'm going to collect these into a matrix T, given by this, and I'll collect the examples into matrix X, right, and is the number of samples. So this will be the rose. And then D is the sample dimensionality that is the number of observations you made the number the number of things that you measure each time so in this case if I like four springs. I would have four measurements for each of these. Right. And so now w. These are the weights that I'm trying to find. So, in code, let's set some dimensions. So n is three so that is will be the number of observations this is a toy example. So, so let's say n is the number of measurements. D is the number of variable values per observation so number of things that I measured. So I'm going to initialize these values to random values of the desired dimensionality, and then I'll print them. So now you can see that I have one, one three dimensional array, or, I should say, one, one by three array, and then I have a three by four array or four matrix, and then a one by four. So the collection of all the differences is going to be T the targets minus X times w. And this should be an n by one matrix to form the square all values and add them up I just do a dot product. And this only works if the value is a scalar which means that he has to be a column matrix. If you want to predict more than one value for each sample, T will have to have more than one column, which we'll get into later. So let's continue assuming that he just has k columns, meaning you should want to linear model with k outputs. So let's compute this T minus X w. And this will give me according to these random values here. So this this output this is basically assuming these weights w. And given these inputs x, how wrong are these weights w when coming when it comes to predicting the output. So I can do the, I can use this to get a complete scalar value so this might be one way of measuring the distance from the predicted value given these in this case randomly initialize weights from the true value. So if I want a simple scalar value, I can just take that product. Also, just in point of fact, dot T is the same as NP transpose so I recommend using this just because it's shorter and requires less typing. So now to find the best value for w, get through this part. We'll take the derivative of the sum of the squared error objective, set it equal to zero and then solve for w. So here's all the math what you will find is that in this class, we're probably not going to, I'm not going to dwell a whole lot on the details of the written math, because what we will find is that we'll end up doing the same thing in code. So for those of you who are interested in how the mathematical operations work. The math will be presented for you. But effectively what we find is that I want to, you know, I want to take the derivative of the square error objective with respect to the weights w. And so this would be if I take the derivative I can bring the two down in front. And so now I can take the sum of the differences times DG with respect to the w. So this all works out in the end to negative two times what we see here. So T sub n minus XT w times X. So basically look at look at this this is the target minus the predictions right X times w times those inputs. So this gives me this this again also uses all three of those values. So here's where we get the benefit of expressing the inputs and the target samples as matrices, because the song can just be performed at the dot problem. Right so these are all expressed as matrices, then I can just take that entire matrix X transpose it, and then multiply it by matrix T time minus X times w which itself would be matrix of the same size. So, if we just check the shapes and the size sizes of each matrix in the last equation above I take X dot shape. It's three by four X transpose dot shape is four by three T dot shape is three w shape is four. So now we can set this equal to zero and then the solve for w. So if I take the above equation set it equal to zero. Well, I can just divide by negative two is a constant. So if I can just divide by X transpose times T minus X times w. I can distribute XT through right and then I can move. Then I end up with X transpose T minus X transpose X times w which means that I can just add X times XT times X times w to both sides. And then now I can just divide by X transpose times X and that's going to give me w in Python it looks like this. So here if I take, I'll just use the NP dot linage inverse function, and then I can compute those values. There are a couple of different ways to do this in basic numpy you can use the solve function. This will assume that X transpose times X is full rank that it has no linearly dependent columns. So that is you can't have a column that's can be represented in terms of other columns so I can't have a column that's like 123 and then another column is 246 right because 246 is just a constant times 123. Solve function assumes that it is full rank. So basically this assumes there's no linear dependence or not a constant times a column not a constant plus a column, not two different columns added together. Assuming that is true, then I can, I can use this function, or better yet, use the least squares function this will not make that assumption. So least squares actually produces a number of different things. So, what are those things. Well, one trick in Jupyter notebooks is you can use the dot string so just put a question mark after the function. And it will pop up this little box or sometimes not so little box that gives you the doc string for that function so you know what the inputs are, and what it actually does. So this returns least square solution to a linear matrix equation. It assumes the following parameters, and it will return the following things. So what we see here is this will return the least squares weights, the residuals, the rank and value s which it said was the singular values. But hold on a second, these solutions do not appear to be the same. So, if I look at the all closed function. Maybe I use the all closed function before. Okay, so if I have two values that are equal to each other, like zero equals zero which return true right. So, what if I have zero equals point 00001. That's not going to return true. But remember in machine learning everything is estimation and so it is entirely possible to let's say write a neural network where you have an activation function that's a plus sign and inputs of two and three and it tells you that the output of that is 4.999999997. You have to get comfortable with approximation. So the all closed function will basically say are these values close enough within a distance. So, it takes a very small epsilon value, and you can take any two arrays, it can be individual, just single numbers or huge matrices, and we'll tell you, are these things you know close enough to be equal to within a rounding error. Okay. So if I run this, it will actually say these two things are true. Close enough. So all of these, even though they may not on the surface appear to be identical, they are actually close enough to be the same. Now, there may be multiple solutions to a system of equations right many times, you know, a times B equals a time C, or a dot B equals a dot a dot C and B and C are not the same thing. So I can simplify them because I can actually, you know, multiply them by a different matrix a and get different values. So the least squares and solve functions can be written with simpler arguments, because they're designed to find the value of w that minimizes the least squared error. Using this matrix product as an argument will simplify the regression implementation so linear regression. Right now we'll use the simpler version. So let me just get w out of this. So now I can just use least squares that just takes in x and t and I'll pass our cond equals none. And then I'll get back w which is what I'm interested in. So, according to our random values that we initialized above your this. These are the best weights to solve to basically map from those randomized inputs to those randomized outputs. What if I have thousands or millions of samples. In this case, x and t can be pretty large. So to avoid dealing with matrix operations on huge matrices and we just see. Okay, I'll get down to example of SGD in action. These can be quite large so to avoid dealing with matrix operations on these huge matrices. You want to derive a sequential algorithm that finds w. And what this does it uses the fact that the derivative of a sum is the sum of the derivatives. And so now you can express this derivative as a gradient. So that is a derivative in high dimensions. So I can now represent my single derivatives in every dimension as just a vector of derivatives. So if you imagine, we're used to thinking in two dimensions where I have like some curve, and the derivative is this is the slope of a line at a given point right. So now imagine you have a three dimensional surface. So, what's the slope of the curve. Well it depends which way you're looking, right if I'm standing on the side of a hill. The slope is going to be different. If I'm looking at say east versus I'm looking south. I'm looking at the north side of the curve because it may have the surface is going to have different slopes in different dimensions and so if I am you know standing on a if I'm like leaving my, my house, and my driveway faces north. Then the northward slope is going to have like some negative value because it slopes down, but then if I turn and face east it's a flat surface so like the eastward slope would be close to zero. And so each of these dimensions is going to have its own derivative that can be represented as a vector, right can be negative and and zero or something like that. And that's the gradient. And so grading is just a high dimensional derivative usually written with this upside down triangle symbol pronounced Dell, and it can be as big as you want. Okay. So here's the math for that so recall that what I'm trying to do here is I have my function of x parameterized by w is just a linear function. I now have going to I'm going to take the, the error here so easy error with inputs xt and w. This is represented as just the sum of all targets minus the predicted value squared. So if I take the gradient of this, then what ends up going on here is now I have the gradient. I can now bring the squared down in front. And so now I can take two times T of n, or T sub n minus g of x, w times the gradient with respect to all W's of the error. Right, so now this is the error gradient. Eventually this simplifies to this thing at the bottom. So I can bring the negative two out in front. And so now this is going to be the sum of all the errors times the inputs. So now instead of summing over all of the samples. What if I just take a sample of the gradient, you know, update the associated weight for each sample based on the gradient for that sample. So if I'm on my driveway and I'm facing north and it's, you know, sloped in a certain a certain direction that might tell me that I might need to update that value more than the grading the other direction which is zero. So another way to think of this is, you know, some last four minutes. If you are standing on the lip of the Grand Canyon. What's the fastest way to get to the bottom. Someone someone said, someone said, no, you're going to die. What's the fastest way to get to the bottom without dying. Well, what you might do is you might look in the direction of the steepest slope and move that way. Right. may not be the wisest choice is like a lot of cacti and stuff. But that would be one way to do it and so if I'm trying to get to the bottom and inefficient strategy would be to walk along the lip or something like that right because it's going to maybe it's going to slope up and down but it's not going to get me. It's going to really get me toward the bottom very fast. And so I want to assess in which dimension. Am I going to have fast find the fastest slope, and I'm going to move in that direction. And so that's the gradient is this high dimensional slope. And what we're going to do is we're going to descend the gradient. And that's gradient descent. I believe that's kind of all the time that I've got today. I will field questions in the last three minutes if there are any. Okay, so we will continue with this on on Tuesday.