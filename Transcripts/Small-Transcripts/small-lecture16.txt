 You know what I'm asking? I'm not going to get out of here. I'm going to get out of here. I think you guys are the best. I want to get something for you guys. I think it's going to be a great job. Okay. Thank you, Brad. Thank you. First of all, we're going to start with this trade. I'm not going to give you anything. I'm not going to give you anything. I'm just going to give you a little bit. I'm not going to give you anything. I'm not going to give you anything. I'm not going to give you anything. I'm not going to give you anything. All right. Okay. I'm going to start. All right. So messages again to people who aren't here. If I excuse you from one day doesn't necessarily mean your excuse from the next day. Or if you're still sick or whatever, you got to let me know. I understand the weather is kind of crazy right now. But I'm going to start with the weather change. So just please make sure that if you remain sick, you let me know that you remain sick. So if you are not here one day and you're still not there the next day, I'm going to start wondering why. So it's just where are we right now? What I'm going to do today is I will do a convolutional neural network training with PyTorch. So we're getting pretty far ahead in the content of like what, what we're doing is we're going to do a convolutional network training with PyTorch. So if you're not here, you're going to be doing a cross validation. And then we're going to do MNISP. It's going to be the fully connected that you won't do convolutional mess for an assignment or assignment five. So now sort of. You apply versus what is what your. So I'm going to divert a little bit. We are back on schedule. What I'm going to do today is I'll go through this notebook 15, which should be on the shorter side. And so then after spring break, what you have is kind of like a review session. So I'll take any extra time today and just, you know, answer questions about CNN softmax, some of the various components that we've had to put together, you know, in the last couple of weeks. So I will have office hours. If we end early for some reason, I'll just go over there. And so now would be the time to ask me questions about assignment three. And also if you started thinking about project ideas, I want to discuss that. You can also do that during office hours today. So after the break, I will formally announce the project as a proposal. You have to write down what you propose to do and I'll either approve it or ask you for revisions. Then you have about six weeks or so to complete the actual project. Yeah. Are we working in groups? You are allowed to. You don't have to. You can work into groups up to four. The catch is that if you work in a group project, I expect to see a commensurate amount of effort. So like if you work in a group before it better look like it took four people to do your project. Other questions. I'm going to be in terms of the amount of work there would be comparatively a lower bar to clear because it's one person working for six weeks as opposed to four people. I do expect sort of see quality work, but it's going to be, you know, I'm going to qualify that based on like how many people are on the project is not going to be like, you know, you have to run every exhaustive evaluation on your own. If you have four people, you can probably run a lot more evaluations. That's the sort of thing I'm looking for. Anything else? Okay. So I guess the point of that is that it would behoove you to start thinking seriously about your projects. And I'll go over this again after the break, but if you already have something you're working on, maybe start thinking about how you might be able to apply that to like put a machine learning spin on that. That would be an acceptable thing to do for this class. So my goal is not to give you extra work. If you already have some active line of research and you want to make it so it applies to this class, that's perfectly acceptable. You just have to convince me that that's what you're going to do. And you're not just like completely piggybacking on something else. You have to do some dedicated effort for this class, but you're allowed to use existing resources and things, you know, your, the goals for you to use your expertise in, in a way that's relevant to the course material. Okay. So to share screen. The. All right. Can you, can everybody on zoom, you can see my, my Jupyter notebook window. I hope. Sorry, can you, can you see the Jupyter book in zoom. All right. So, the dawn intro to convolutional neural nets. Basically the core mechanic of a convolutional neural net is this filter. This is just a weight matrix that's effectively needs to be optimized to output a higher value when it encounters things that quote match it better. And the system of course can be multiple different combinations of features. We did our handcrafted filters just to illustrate how the concept works in terms of what type of activation and scalar value are going to get out of performance linear sum operation from a filter to a patch of the image. Of course, you want to be able to actually learn these filters from real data. So pretty much the training process of a CNN is trying to optimize the weights in these filters. You have to specify things like what the size of your filters are, how you patch up your images. So that's going to learn weights that are optimized for whatever types of data and counters assuming those hyper parameters. We did the NumPy version. And now of course you want to make use of as much speed as you can because as your network start to grow, they start to take more compute. So let's go through effectively similar operations except using PyTorch this time. So just a point, if you were going to use PyTorch on the workstations at the CS department, you'll want to execute this command. So basically what this does is it's going to add the site packages library where PyTorch should live to your Python patch. Sorry, Python path. I can't speak today. So you can run this every time that you log in, but a better solution would be to add it to the start of script in .bashrc. So that way it's always there when you log in and you don't have to worry about this. But if you have things like, if you want to import Torch and you can't find it, very likely it's missing from your path. So just a point of fact. So this is taken from this example of implementing a CNN and PyTorch. So this is kind of stripped down version of that. You can look and. Okay. So we're going to do our usual sets of imports. We're going to import numpy. Of course, we still want to use numpy to do things like processing and plotting our outputs. Because PyTorch operates as a tensor and you can't do certain operations for things like nice visualization directly on tensors. Nonetheless, the main thing we're going to be using is Torch. And from Torch we're going to import the autograd module. And then we, of course, are going to keep track of time and Jesus and Pickle are to open our data. So in our numpy version of a CNN, this looked, our constructor looks something like this. Hold on. There we go. So we specify our patch size and then we initialize an instance of this neural network classifier CNN. So 28 by 20, that's going to be our input size. So remember, fixed size inputs. And we're going to assume square images. And then this two and two, this is going to be our list of the defines the hidden layer architecture and output size would be the number of unique labels you have. So for MNIST, this should be 10. But unique values in T are going to be whatever your class labels are. I set my patch size. Remember that this is a square. So I say patch size five. This is effectively a five by five. And then stride two. Similarly, that's also a square. I'm going to take two, step two pixels over and then two pixels down and I reach the end of the row. And then I'll just go ahead and make my own. And then I can see the two. The two pixels are the same in both directions in the pytorch version. You're going to see a lot of the same things, but it's slightly, slightly different. So we're going to make use of some of the pytorch functionality to more automatically define things like our network architecture. So. First of all, just like take a look at these two definitions. And what do you notice that is similar about them? What do you notice that is different? Sort of the easy one. What's the same about these definitions? Yeah, I'm going to go ahead and make a parameter. Right. Both 20 by 28. That's the input size. What else? Yeah. Yeah. So the output size too. So it's the same data. Right. So it's going to have this is MNIST. They're going to be 10 classes still. And our inputs are going to be 28 by 28 images. Okay. So now what's, what looks like it's different about this. Yeah. Yeah. Yeah. Yeah. The full connected layers. Right. So I'm separating out, you know, the convolutional layers and the fully connected layers. So here. Remember when we did the version last time, we basically said, okay, I'm going to just decide there's a single convolutional layer. Everything after that is a fully connected layer. Right. So I could write the NumPy version so that it is more like this. But pytorch basically does this. We'll allow you to do this automatically. As we'll see. So right now. So I'm going to be here that I'm highlighting. This is not a pytorch command specifically. This is just a Python call that instantiates an instance of this concept class. The net, the conduct class will be written using pytorch. And so now I can take these, these parameters and use that to basically, you know, construct the individual pytorch layers one by one. Exactly as I want them. What else do you notice it is different about the pytorch definition. Yeah, so this is the stride. It's, it's, it's two. What's the name of the argument that's there. Per layer, right. So I can do, I can do something I can have a little more control here in that I can specify different size patches per convolutional layer and also different size strides per convolutional layer. Let's think a little bit about why you might want to do that. So remember, you know, this is a single convolutional layer example. We have, let's say a 28 by 28 image, and I might specify a seven by seven patch. And then I stride over it, say two pixels at a time, this is going to give me, you know, some number of it being like 11, I think 11 by 11 patches using cropping. So this is taking a 28 by 28 image turning it into an, so the 11 by 11 little sub images, each of them gets multiplied by a filter that should be the same size as the patch. So also seven by seven. From that, I get a linear sum, right, or I get some sort of value. So the easiest one is just get a linear sum. So a single value. I can also do pooling, right, which is going to take those initially seven by seven values or whatever the size of the patch times the filter is. And then I can down sample that. So I did like a two by two examples. I take the seven by seven down sample it to a two by two, or just take a linear sum and down sample it to basically just a single value. So let's take the pooling example, because that makes it friendlier to more convolutional layers. Let's say I take my seven by seven feature map down sample it to a bunch of two by two feature maps. All of those, let's say there's also 11 of them. So 11 by 11 of them, 121. All of those. Now we have 121, but effectively like four pixel images. That way. Those go into the next, the next layer. So if I have a bunch of input, there's actually like pre patched already, they're two by two. And I have a patch size or a stride length that is three, or let's say even two, what's going to happen. I have a bunch of two by two images. I have like 11 by 11, and I stride over them with a stride length of two. Am I going to get any overlap between my patches. No, let's say my stride length is three. No, I'm also going to miss things. So effectively, the stride length is going to be dependent upon what's the resolution of the thing that's actually going into the layer. So if I have a patch size that is five and a stride length of two, this is going to down sample my five by five and I pool my down seven my five by five feature maps into some lower resolution. This makes it easier to, to, to compute right it's faster. This is less like less information. Also, if I do my pooling right, I might get a pretty nice representation of what's really important about those feature maps at with less computational cost and less memory cost. But I'm also going to be feeding in a smaller quote image into the next layer. And so if my stride length is too big, I won't be able to patch my images effectively. So it's going to basically just throw an error. So it can be, it's usually desirable in fact to have different patch size per convolutional layers because you want to get important information from maybe a very complex image down sample. Something that's like you think is truly important about that image. And then see that in the next layer so we can do a similar operation over those, what we'll call just did feature maps in order to get what it then thinks is really important using filters that are probably optimized for somewhat more complicated features in the end. So when you're using your convolutional neural nets, you want to be careful that you're not feeding in an image that's of too small a resolution to be useful in the next layer. So for a task like MNIST, one convolutional neural net, one convolutional layer is probably enough because it's very simple task. For more complicated things, real images, multiple convolution layers, but you've got to be careful about these things like stride length. Okay. So that all clear so far. So first, just let's look at backcrop from our previously existing neural net lectures. So if I'm trying to optimize the weight or the value of the single weight, so remember we're going to have some weights that just sort of reside inside these hidden units all apply some activation to them. And we showed how we can optimize the weights one at a time. So if I have this is layer V and I have an individual weight I'm trying to optimize, the value that this gets optimized to is going to depend on among other things, the error that results from every unit that is connected to this. So just consider there's like one neuron right here, and it's connected to in this case three other neurons. So whatever value comes out of this first hidden neuron is going to have some bearing on the output that comes out of those three other neurons. If those are my predictions, then it's going to be some level of wrong compared to what the actual truth is. And so I want to optimize this weight in this quote parent neuron, remember these are all graphs in a way that's going to take into account how how wrong all the children are. So it's like, you know, if you if your children misbehave you punish the parent. I guess that's what people do. My children are very well behaved. Not an issue. One of those even born yesterday has an eye chance to misbehave. Anyway, what we are trying to do is like if this is slightly wrong, right, if Y1 minus T1 is like a little bit wrong, Y2 minus T2 is like really wrong. And then Y3 minus T3 is, you know, also really wrong. It might be more beneficial to perform a greater optimization on this weight because two out of the three things that it connects to have very large errors. Right, so we can we can consider all of these. So basically in plain English, what this will show is that we've got K output nodes in this case three. And so this should output, you know, K number of values for any input sample. So for this regression example, it might be continuous values. If I want to predict properties of a car from other properties of a car, you know, classify this is going to output probabilities. So again, the error term is really the same. It's just that I have to make sure that my quote, not units, but kind of units are the same right. You'd be subtracting MPG from MPG horsepower from horsepower, probability from probability, and just modulo whatever standardization you have to do. So all of these are there's, they should all be apples to apples comparisons at this output layer. So then the weight update will follow these web lines. So if depending on how wrong these things are, this is going to be updated based on values that are in part derived from these from these values. So I see parsley dry here just in the colloquial sense, not in terms of the partial derivative, although the partial derivative is kind of the key component of the actual mathematical operation of propagation. In this case, I'm just saying a portion of this error is going to inform how much the connected way it gets updated. So weight updates in the hidden layer are going to depend upon the weight updates, to some extent in the output layer or any other layers that are connected subsequently to that hidden layer. So let's assume that we've got like say two units in W and the output layer and just a single hidden unit in B so we'll just have you know one hidden unit and the two output units connected to it. So these are the derivatives that we use that you may remember from from the first neural network lecture. So the partial derivative of the error with respect to weight one is going to be negative two or in this case to Constance you can factor it out times the error times the input disease going to be the input that that that output layer. And then the partial derivative of the error with respect to the hidden layer weight is going to be similar except because there are two output weights, I have to sum the the error times the value of that weight. Right and then, because I'm also going to be applying this activation function before the output of layer V goes into layer W. I have to account for that so my activation function is 10 h this is going to be one minus z squared. And then the last thing I need to do is I need to multiply it by the input to the layer and in the hidden layer that it will be X. So, the update on w one is going to be proportional to the product of the error and Z, which is the input that layer, and the update to V will depend on the sum of both of those errors in output layer w times the derivative of the activation function with respect to Z, and times the input to the hidden layer. So, same thing is happening in my classifier network, and my my regression network so what was the point of all this you know if you've been paying attention you understand how weight updates work. So let's actually see what happens when we apply it in convolutional net so if you just remember from Tuesday. I have the same filter that's going to get multiplied by a bunch of different patches. And all these are going to contribute to that final prediction. And so, I need to best optimize this particular filter to account for it to make good predictions for everything that it might encounter. Right and good prediction in this case might mean that has a high activation for for certain things and a low activation for other things is just trying to best optimize what those things are that when this filter encounters it it's going to best produce the most predictive values. And remember all these working conjunction there are a bunch of different hidden units, just trying to do this all at the same time. So, then in order to out to perform the actual back propagation operation in a hidden layer, what I need to do is I need to collect all those delta values from all applications of all filters to all patches. And then just turn this into a big weight matrix, as you can do do my multiplication operation all at once, just to make sure that reshape the inputs into a compatible shape. Okay, so we'll do our kind of square and diamonds over the task again. So here I'll define my diamond. So, there's my diamond. And so now what I will do is I will define a patch size and a stride length and I'll patch it appropriately. And then this is going to give me something like this. And there are 64 patches, according to this patch size and stride length that I specified, and this patches look like this. So now, this filter would be applied to each of these patches. So as you're going to generate a feature map that is the product of the filter and the patch where in this case see is going to be the weights of that of that filter. And this is the sum of all those values. So, if prod here is basically just a list of the values that resolve when you apply this function patch, a sum of all some all those, this becomes the scalar value representing how responsive this weight is to the values in the patch. And then this value thing it's propagated through the network so right now I'm taking a single value. I could you know do pulling and just down sample to a lower resolution image. So what our standard neural network operation looks like now is very similar so all the only thing here is that is the equals h of x v, v are now our convolutional units, and there's an application of each one of these to every patch. So remember my patches are now just considered them to be input samples. I just flatten them, and I feed them into into my wealth item, as I flatten the, I flatten the image. And then what we do is we just reshape everything so it's a nice square that multiplies together to get our value. And why is my prediction this is just going to be z with my bias times my output layer weights. And then we have the softmax operation right so what I do is I exponentiate wise wise just some scalar value. I need to turn this into something that can be turned into a probability distribution. So let's be an inch eight that other than some over all values of f for for each of each class. So this is basically something across the columns. And so then I will take that and then take the f for each individual value class divided by the sum. This is going to give me a probability distribution. So now the, but I'll do is I'll take the the sum of all my indicator variables, right so now my indicator variables are just these, again, one hot vectors or think of them as a probability that is all zero except for one case where it's 100. And then times the log of the actual probability that I predicted, which is G. So if I do that, then what I can do is I can take the gradient in V. And then what I'll do is I'll take my inputs x input in layer, and then multiply them by T minus why, except now these are as these are as probabilities so I have the indicator variable version, minus the probability of the variable. And then multiply that by the weights and then multiply that by the derivative of the activation function. So these parts are pretty much the same is just that I've got the indicator variables minus the predicted probabilities rather than a scalar minus a scalar. So now what I need to do is for the convolutions I need to sum all of the partial derivatives of the error for every delta value in the gradient so then for every convolutional unit, we need to find the delta for every scalar value and that's going to come out from every feature map when that filters applied to each patch. So let's let's review the back propagation of weights in a CNN so we'll assume this is kind of recap of the one from last time. So assume this only has a single convolutional layer and that's the first one. So what I'll do then is I'll take, I'll reshape this backed up delta matrix into the right form, and then I'll reshape convolutional input matrix to something that's compatible. Now I can take the that input times the delta. So this is just doing, you know this part here. And then what I will do is I'll just calculate the derivative of the error with respect to the weights for the convolutional layer with simple matrix multiplication, and then in the fully connected layer it's the same as we've been doing. Okay, so the trick is basically just collecting all of my values for every application of every filtered every patch into a big matrix. This allows me to effectively perform the same operation as long as I can ensure that my matrices the right shape. So this sounds like a lot of work. Right, so you got to remember, okay, I got to collect all of my, my dumps of values into a matrix, I have to make sure that my inputs are appropriately shaped. Then I can do a matrix multiplication. But let's check the pytorch definition again, right, this is a single convolutional layer in the pytorch definition that we had, we can have one multiple convolutional layers. Each one has a different patch size, and each layer has a different as potentially different stride length here they're the same but I could very easily change them to be different. So, you can see that this is rapidly going to grow out of control I'm trying to do this all manually using numpy operations there is a very good reason why we limited the discussion yes or on Tuesday to having a single convolutional layer. It seems like a pain. If only there was like something we could use to automatically calculate the gradients. So if you're thinking, boy, I wish I had something like that I have good news for you let's remember lecture eight. We got this thing called autograd and pytorch so recall what autograd does in brief. So we construct our neural network as a computation graph, where everything can calculate its own value and the partial derivative of its own value, and you connected those things into a graphical structure, such that operations outputs from a single from one node can then be fed into as arguments into a single line, and then you can use a small neural network to make sure that child know. And so then I can calculate everything. They say the parcel of it is at every leaf, and then back propagate them with a simple with a single one line call. So this definition of ConvNet using. This is basically what we're going to execute that Python call and instantiate. So first you observe we're inheriting from torch on nn.module this allows me to make use of all of the pytorch functionality for doing things like creating neural networks with simple commands. So now I can specify those things that I want right the input size number of convolutional layers and new stride length patch sizes etc. I'm going to do some things to make sure that we can use the GPU, but I spent I specify the activation function. Now that I've done this I can now create all of the convolutional layers. So here I'm going to create this thing called module list, and then I'll add things to that list that represent those individual layers and their properties that I want. So first, this argument is going to be the number of channels for each pixel this first argument here. So the input height and width is going to be the square root of the number of inputs. So again if it's 784 its height is 28 with this 28 so the square is And so now I can create this module list for for the convolutional layers, and I will just kind of zip up my number to the number of hidden layers, patch size for that layer stride length for that layer. And then every trip to the loop I'll add something to the convolutional layers list that contains those properties. What that is is, this is what you're probably going to see when you read other people's pytorch code is you you instantiate these items it's the port dot nn dot layer type. And this could be dot dense, or dot coms 2D or dot com 1D or dot LSTM or whatever type of network you're trying to write. And then you specify exactly those things that we just we just talked about the input size, number of units in this layer. And then other in this case for convolutional nets we have you know the kernel size, I have stride length for other types of for other types of layers you these would be slightly different properties. So you'll see, you know, I'll create, you know, and that equals torch dot sequential, and then I'll add these things one by one. So what I'm doing here is I'm feeding in a list that automatically specifies what I want properties as a layer to be, and then because I'm assuming this is all convolutional except where I, for the things that I specify like number of hidden and convolutional layers. I'll create the convolutional layers that way, and then I'll also create, you know, these linear layers for the fully connected. Okay. So, this is just your standard way of just creating your your pytorch neural network is effectively just a list of what layers I want. And then when I do like you know model of input, it'll run my input through all those layers in sequence like I can specify very cleanly. I want a convolutional layer here. I want another convolutional layer I want to do some pooling. Then I want a linear layer that I want to do like drop out and everything that I that I could possibly want to try. So forward all outputs is going to be basically just like one pass through my, my network with inputs x. And so this forward, the forward pass will basically call this. So you can see that it's handling the convolutional layer and the fully connected layer. So, what I can do is I can just say for con layer in self dot con layer so for every element in that list, which is an object that represents the layer itself. I can actually just use that object as a function over the input. And so now I can just say that, okay, I specified the convolutional layer it's got some number of hidden units, each of them have x weights in them. If I just call con layer of inputs, it'll run that convolutional layer of over the ad. So, this is what I specified because we've done before so I specify what method I want to use all this should look fairly familiar so I just remember set requires grad. I specify the type of loss function that I want to use in this case because we're doing categorization I'll use across entropy loss. And then the rest of this stuff is pretty much just for plotting and training so to calculate the loss, I'll actually run my CE loss function that I just specified over why which is my predictions and the values for this batch, and then loss dot backward will actually perform back propagation and then call it optimizer dot step will actually perform the update of the weights. And then lastly remember that we have to zero out the gradient every time otherwise it's going to be a cumulative gradients because that's just how I torch works. Now we have the softmax function in in pytorch this is the same as we've seen before I'm just using pytorch tensors. So here, this is a trick here to avoid, avoid overflow or actually calculate the max. But then I exponentiate and then I take the denominator, I divide divide the expenditure in version by the denominator. That's our softmax operation. And then use function is use function. Right, this is pretty much the same I'm just using like the torch version instead of the numpy version. Last thing, especially if you encounter problems with the GPU recommend you refer to this notebook here. So, you need to detach things from the computation graph, move them back on to the CPU and then optionally convert them back in the numpy to do processing with them. So if you leave things on the GPU, you will encounter difficulties. Alright, so I will set the device that I want to use. In this case I'm running on my lab machine so I have access to the GPU I will hit yes. I'm using the GPU in CUDA. I can check Nvidia.smi I forgot to tell my lab that I'm using the machine for class. However, it does not appear that anyone's really using it. It's fine. So you can actually see with this Nvidia smi command the usage of a given machine. And so here I can see like, someone's suit this is probably me running Python 38 no one else really appears beyond this right now. So now that I specified my network, I can actually try to run it. So I'm going to open up MNIST. Right so this this should look pretty familiar. So what I'm doing I'm just splitting it into training test. We must reshape the the input matrices for convolutional networks in PyTorch. So what I'm going to do is here I'm just going to take the number of samples, one dimensional sample, 28 by 28. This actually requires a two dimensional input to be fed into the convolutional layer. It will do whatever flattening is required. Yes. What is the parameter of the Torch module here? The parameters of torch.module. So did you see that somewhere? Like these? Yeah. Yeah. So basically this is going to say what what what needs to be updated. So remember self is an instance of a neural network so it inherits from torch.module. Dot parameters is going to be one of the actual weights of this. So this is w. So what am I updating? This is my collection of like w and v and v prime and whatever. It's basically you remember how we would collect all their weights into like a single array to do, to do say out of optimization back in lecture six. This is just a version of that. So I'm going to have a neural network that is a convolutional net of a certain size with a certain number of layers and a certain number of units per layer. This is going to in turn mean that I have n weights that have to optimize. This self.parameters stores like basically the address of those actual weights and memory. So when I call torch.optim.whateveroptimizer, I pass in, look at what are the things, what's the address and memory of the things I actually am going to update whenever I make this call. And where are you saying that like make the, yeah, so this is never explicitly set because this is a member of torch.nn.module. So, because my convent class inherits from torch.nn.module, it has access to this. So when I call self.parameters, because self is that instance of convent, which then inherits from module, it has access to that member variable of module. And so when I create the convolutional net using that single line call that in my case it's stuck inside of a for loop, it's doing things like assigning, you know, it's initializing values in the self.parameters. Okay. Any other questions. All right. So, I'm going to go to the MNIST, we can see that it's pretty much the same. So you can see that I've got by default, the data, the numerical representation of MNIST is flattened. So it has just single 50,000 rows, 784 values. So PyTorch requires that we actually reshape it to that two dimensional shape to feed it into a 2D conflayer. So you'll see that before reshaping we've got 50,000, 10,000, etc. by 784. And now I have 50,000 by 1 by 28 by 28. So I've got 50,000 samples. And then each of them is basically a single dimension. And then each of that is 28 by 28. So now I can have things like I can specify a batch size that I might want for that second argument. So I can say, okay, I want to feed in 100 of these samples at a time, because it's slightly faster than just feeding them in one by one hundred times faster than feeding them in one by one, and optimizing the weights for every individual sample. Okay. So then what I will do is I'll instantiate my instance of my convolutional net. So now I call ConvNet, and it is instantiated using 10 hidden units and 20 hidden units and then a final fully connected layer of five units, output size of 10, because there's seven classes. So I have a five pixel stride, or five pixel patch in my first layer, a seven pixel patch in my second layer, and I stride two for each one. And then we train and we can see that we achieve, you know, these are lost, you know, over after just 10 epochs is going down significantly. And finally, one thing you can do is you actually just print the network structure so it actually will print out this nice little display of what your network looks like so you can say that I've got okay ConvLayer one and two, and then it takes in say a, you know, one sample, and then it's going to have 10 hidden units with a kernel size of five by five and stride length of two by two the second one is going to basically map from 10 to 20. And then I have a kernel size of seven by seven, this is two by two. Fully connected layers. So you're wondering, okay, I have to flatten the output of my convolutional layer to feed into the fully connected layer how many actually dimensions is that so we can just print this out and see exactly it's 180. So this fully connected layer will take, take in 180 inputs, and then map them down to basically five dimensions. Right so in features, number of dimensions on the input out features number of dimensions of the output. So that's five. And then this last thing this is the softmax layer. This is going to take basically a five dimensional representation of every sample, and then distribute it into probability over those 10 classes. It's going to say okay you got you me five numbers, and I will tell you what the probability that it falls into any one of these 10 classes is. All right so we trained looks like it went pretty well. So then I will use my use function. So I have my test set, and what it will print out is going to give me the classes and then I'll use that to calculate the percentage that's correct. And it's 90.58% correct. So we can try a few things like we can, we can train for for longer for example by training for twice as long, then then we get 95% accuracy, right. And so you can see that just by just by increasing the training size, the training length we get a significant boost you can also try, you know, nothing around the batch size, adding more hidden layers. So be careful when making sure that your your patch size and your stride lengths are appropriate for the layer. But you can see what kind of accuracy we get. So now let's examine the effects of one of the layers weights over on an input. So first let's get the hyper parameters of that first layer. So that first convolutional layer. Let's just take a look at what those values mean. Right so that's got 10 hidden units. This is the kernel size five by five and the stride length, two and two. So that's the outputs of each convolutional layer. So I'll take one sample in this case I'll just take the 20th sample from my test set. I will then turn that into a torch tensor. And now I'll just run that it directly through the forward all outputs so I have a single sample. What I want to do is I want to grab every, the output of every layer and accumulate them so I can see what's happening to the sample in the first layer the second layer etc etc. So now I'll have like layer one weights. This will give me the weights of data for this list or for this for this layer. So if you look at like CNN, CNN net dot children, this is going to say, in this graph structure, I can find the children so right these are like the first child to be this con glares set second child to be the FC layer set and then inside con glares I can get like okay the first convolution layer. That's what the 00 is. And then in that I can do weight and actually get dot data which is going to give me the actual weight values. So, now what I can do is I can plot this. So I'll take the layer one weights, and then I will multiply this input by those weights and we'll see what each layer is actually, or what each unit is actually predicting or actually outputting. So, this is the output. So take a look at these. So what you'll see here that's the original image is nine. On the left, you will see these are just the plotted values of the weights, right to these are the filters. And then this on the right this is going to be what happens when I apply this filter to every patch of this, this nine so take a look at this and what do you what do you see what are these different filters what do they appear to be doing to this image. Yeah, I mean ultimately yes but let's take, let's like this look like one at a time let's take this. Maybe compare like this one, and this one. Yeah. Yeah. Right so and they get remember what these numbers with these colors mean right so if it's, they were using the negative version here so if it's darker, it's more positive, and if it's lighter, it's less positive. This filter, for example this first one is probably activating more on like the outer edges of the nine so maybe it's like responding more to like lower pixel values or something. Whereas this one. It seems like it's mostly not it's sort of, it's gray on the outside sort of ambivalent, right, maybe it doesn't have very strong, either positive or negative correlation either way. So inside the interior of the nine it seems to be mostly negative or low values but then all those edges, it seems to be having higher activation so this one, this filter, at least for this sample appears to be kind of optimized to detect this type of type of feature. So this is sort of one case where this actually might be reflected in the plotting of the weights itself so you notice like, take a look at where you see the darker values in this filter and compare that to where we're seeing the lighter values in in the feature map. Right. So sometimes it's not so obvious this one kind of seems like there might be some sort of correlation there. So, what can we say kind of generalize generalize about this. We have different filters that are optimized to kind of pick out different features of an image. Right so some of these are maybe optimized for things more like certain types of edges maybe these two kind of seem to result in a similar feature map. So maybe these two, even though they look quite different are maybe optimized to pick out similar features, and these are all normalized so there's no guaranteed these these values are actually the same. But within that normalized range they appear to be similar and so does that one. Then if you look at like this these two here. And they also, you know, again the filters themselves have pretty different weights. When you apply them to different patches, you'll find that they're maybe selecting for like the outer portion of the image, as opposed to like the inside. All right, questions. Right. So, what do we observe here we just did that. So basically what we can take away from this is that these different feature if you have 10 different filters, and each of them is optimized to pick out a slightly different part of the image. So you put all that together and you can imagine that for the different things that occur in M this like okay yeah these filters can pick out, like the edges that I've interested in the curves that I'm interested in, maybe like the outline of certain shapes. So, these are all useful things to identifying hand drawn digits. So, this 95 90 95% performance we get on on m this to see what makes sense. So now let's take a look at the second convolution layer. Right so again I do index at one. I'll get the second layer so this then takes in 10 inputs has 20 units colonel size of seven by seven strong like the two by two. So this is going to be the weights from this from the second layer. And so now I'll see what the outputs are when actually input some samples. So I'll take the same x, that same nine image, and then plot the outputs of this. And this looks like that. So what's going on here. This is not very interpretable at all. Is it taking their one as an input and then. It's saying it's saying those feature maps that come out of layer ones basically the input to this layer to is all this stuff. Right. So, okay, we can see that this sort of, you know, this is reminiscent of a nine, especially if you know that a nine is what you're looking for. So you can make these values, and those are a quote image, right this has already been down sampled this is no longer 28 by 28 this is, you know, five five by seven by seven or something whatever. So, this is already lower resolution. And so then I put that in the second layer, it gets chopped up into patches. And then a bunch of a bunch of these weight matrices that you know if you plot them look something kind of like these as well, some random, random looking patterns, and then you multiply that by those patched chopped up patched low res versions of the nine that have already been kind of processed by some of these to such for some of these features. And it comes out looking like pretty incomprehensible. This you can't really impute any real meaning to what these things are actually detecting. It's more useful to see what those outputs look like numerically. So, this is going to look something like this. And you can see that right maybe this first row here is a kind of all the same for this that one output. So it's hard to say which one that is. So, basically every seven by seven filter is going to apply to a 12 by 12 input, and that results in a three by three output. So this is, these are the actual These are the inputs for layer one, and then these are the outputs for layer two, right so this is three by three. In this case, I think maybe there's one fully connected layer so this case this is this would be flattened into a single array, and then fed into effectively just like a non linear classifier. But then then you get your final but then you softmax then you get your final up. And that fully connected layers like not required it just often is used. But you know sometimes it just like adds extra compute for no real reason. So we're just doing it here to show the difference between convolutional layers and fully connected layers. Okay, so we've got this. Yeah, 12 is a 12 by 12 input not 11 by 11. That gets fed into convolutional layer it gets chopped up and patched that send my seven filters applied to each one. And then that and this resulting in a three by three output. So, what this will do is I will then look at the indices that 12 by 12 image. And so what I'll see you like how many intervals of size seven can fit an interval or 12. And we'll see that it's three. Right, we've prop one pixel, we run off the edge. But this is how we get from seven by seven applied to a 12 by 12 input to get a three by three output. So just think about like, how many times can I apply a filter of size and to an input of size and I'll tell you. And so then you use to buy their proper grip padding to make sure they're going to pass. Alright, so now what I'll do is I'll grab the first 10 samples from some random place in the test set. And I will plot them. And so this looks like what this will show is this will show the sample and this was going to show the probability that it is a member of any given class. So for example, here's a one, and it's got a pretty high probability probably close to 100 that it is a member of class one. There's also kind of a low probability that's a member of class seven. Right, here's a seven, and we see the reverse. Does that make sense. Right, because sometimes depending on how you write a one it can look a bit like a seven. If this if this had like a little line there you could it could be somewhat ambiguous so yeah. So I have to select for one it shows almost close to one. 1.0 for the first one. Yeah. There's a bit higher than 1.0 for seven. Does it have to match with the seven as well. Not necessarily I mean in this case it does just because we think about think about the number the digits you know, zero zero through nine. The one that looks most like a one is seven out of all the other choices that you've got. So like, it's likely that like the second place choice for a very obvious one like this is obviously a one no reasonable person even, you know, we can use this for a seven. So we're looking at the data. Yeah, we see that. Okay, the data what is obviously a one. Yeah, it also has some similarities to seven. Yeah, not not that high. Yeah, when we look at seven. Does it also have to relate but in an opposite way that some similarities to one. It's likely it's not necessarily globally true so there's probably some samples in here where it has like the European seven with us with a stroke across. And in that case, it might be really, really evident that's like this is seven and like nothing else even comes close but it's just check something out real quick so. Okay, high probability seven non zero probability of one. What's that. So, nine, right. So, this network, even though this doesn't really look like a nine and any real when you know you don't really confuse sevens and nines. Our network has determined that there is maybe a slight probability that this is actually a nine. Right so I could rank all the upper probabilities in order. This one, there's even like a very small probability that things this one is a five. So a lot of these really small values come out from just doing the optimization usually from randomly initialized weights. It's like, it does pretty good and predicting like the top one accuracy because the choices are pretty obvious if you go a little bit further down it's like yeah the third place choices thing is making is like, really make a lot of sense. At that point it's just almost like residual distribution of the probabilities. So we can find examples maybe that are somewhat more ambiguous here's another. So here's like one of those f sevens right we actually sees a similar thing right we see almost an identical plot, actually. So in this case the network has seem to have optimized you know for this type of distribution, maybe it makes sense with a nine because like you could you know if you close this gap and my book like a nine. So let's see if you can find some other examples that are maybe more ambiguous makes a couple of times. Let me try a different set. So. Okay, here's, here's one. Right so here's a nine. And it's pretty obviously a nine but the actual value here might be like it's 50% likely to be a nine and that just happens to be the highest of all of my choices. But you know it also thinks is probably a 20% chance that it's a four, and also a slightly less than 20% chance that it's a seven. So our network right if you were to plot the distribution of our test samples, we probably see our sevens and our nine is kind of close, because for whatever reason, it seems to have noticed that like one of the nearest neighbors to a nine is probably some instances of class seven. But like this case this is one where maybe it's not quite so confident that this that this is a nine, even though that is the highest probability. So the individual samples they, the probabilities have to all sum to one obviously, and you usually see similar distributions like you look at these two threes, right there's roughly similar probabilities of it being an eight and maybe even a zero. And so you're going to see like kind of similar distributions across classes, but there are cases like if we look at this nine and this nine, you know, the distribution is roughly the same in terms of like you can sort of like linearly scale one to the other. But this one this particular nine seems to have a much lower confidence on the probability of being nine. And if you compare it to like this other one, like this nine looks a whole lot more like this last one. Alright, other observations other questions. Okay, so just briefly review. This is going to be important for assignment for so good to start thinking about this now. What's the key difference between calculating your error for a regression problem versus a classification problem. Yeah. Almost the probabilities is the key part I mean not necessarily the problem that you're incorrect you're calculating. What, yeah. That's what you output that's what the softmax is happening so we talked about the error so if I have my prediction minus my, or my target minus my prediction. So I would say in assignment one for regression problems. What's different about classification problems, the representatives probabilities and therefore everything's between zero and one right and so the targets are represented as like the ground truth values are going to be represented as what one of them is going to be one. So it's a what it's a what kind of variable teams of the night. So we have the indicator variables right indicator variables are a bunch of zeros, except in the crash class it's a one. Right. So if that if these if you think of these as probabilities instead of numbers. What does that one represent. So this is the ground truth. I'm saying, I know that this sample is a member of class nine. Okay, in other words there's 100% probability that this, this sample is a member of class nine. So, if these are probably they all sum to one. What is my network outputting in terms of probabilities for classification problem. So it's a sum to one right probabilities. And so, if my network is is predicting that for some sample. It is 30% likely that it is a member of class nine, and my target value saying it's 100% likely this is a member of class nine. Then, what am I subtracting. Yeah, so you're subtracting the different the probabilities right so, and then you're doing that at a scale that includes all the classes. Right so if I have a free class problem. And see if people can see on the more trying to draw this. Okay, so basically if I have a three class problem, and my sample is a member of class two and this work doesn't work all the time. This would be better. Okay, so if my indicator variable is basically 010 saying that my sample is a member of class two and I can really read that. Then, my network will be output like okay point one point seven and then okay point two. So, this is my target T. Wow. We try the red one. That's even worse. Whatever. This one is T. This one is why. Okay, put a minus sign there. We're going to end up with is zero minus point one, one minus point seven zero minus point two. This gives me an error for every probability I can then use that to optimize my weights. So what I want to get it I want to approach this thing. And I'm going to get this thing I want to minimize the distance between this thing and this thing. The only difference here is that these are probabilities whereas in regression as predicting scalar values. So the quote units here are just like percents, whereas in a regression problem they actually represent things like whatever the units are the values you're trying to predict. Other questions, comments, thoughts. So what do you need to have more fun with? Yeah, so as a general rule of thumb, yes, more congressional layers will allow you to get better accuracy or get a better performance, but also a bigger network. You may be more likely to overfit. So, and this does not, it's not a difficult problem. So, you know, throwing four convolutional layers at it. So I'm not going to get a whole lot of extra, like extra mileage out of that for more for harder problems. You know, image net action recognition problems. So, you know, video classification, whatever the more complicated your problem is, the more likely you are to benefit from, you know, additional convolutional layers as long as you're not just kind of throwing them at the wall, you know, randomly. I mean, it's going to have an entirely on the data set. So there they will, there will come a point, if you know your data set there will come a point at which adding more convolutional layers is just not going to give you anything besides taking longer to compute. And usually that point is pretty apparent. You have to empirically try and verify it. So you know you try like three layers and like, okay, I get 97% train accuracy and 80% val accuracy or whatever you add another convolutional is like I'm not getting much better than this. And it's just taking me a lot longer to train. So you're going to try and find that sweet spot between where you can actually train it in reasonable time versus getting accuracy. And as you add more, you're you basically approach the log diminishing returns. Right. Start you get to a point where it's like I'm just not getting any more from adding this extra compute power. And one more. When we get training test results accuracy and train has higher accuracy in the test. What does I mean, you might have explained with but what does that tell us about it. Does that mean that I train went well but when we actually tested it. It depends on like what the gap is, if it's like my train accuracy was 97 my test accuracy was 94. That's fine you're actually doing very well. If your train accuracy is 97 your test accuracy is 79. That's much more of a problem. And I think that's really like that would suggest that there are probably peculiarities of the training data that it might be overfitting to. And that might be because like you have too many convolutional layers or something. And so it's like it's able to, if you got a train set that contains a disproportionate number of nines that sort of look like this one. And it might be learning something about this like weird hook at the end, or other features that are kind of spurious correlations to something in the training data that doesn't appear as much in the testing data it's like well I see this and it doesn't necessarily match anything that I train on very well so I don't know. Is it because it can be also because probably separate the whole set. It can be yes so that this remember this is also why if you do is it's important to like do stratified cross validation so you can say like, I'm not just getting a lucky split or something and my test data just happens to be very nice. And perform like my training data my test split or such that they train very well and they test very well, whereas if you try a different 20% all of a sudden it falls apart. Right so what you really want to do is you want to try these averages and see like, if I sort of hold out a different and then train my data, and then train on the remainder and validate and test on this portion, and then rotate that portion I can see how well is my network. Can can that be expected to perform on a random new sample. And so that can take more time obviously is like it's hard to, you know, train a big network like this. So often, most of like the modern tasks will have a curated train validation and test set. And so that's what I've evaluated on like this, the test set of this data set and everyone knows that they can go and get the same test set. Presumably the test set is assumed to be kind of curated such that it's friendly to never but not too friendly, if you train on that training data. Right. If you else. So I'll call it a day there I will head back to my office. And you know you guys can just stop by anytime between now and and 430, and I will. I guess have a good break. And I will you in a week.