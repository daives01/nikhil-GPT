 you you I mean, I'm looking forward to the day when it's like, you know, passable as real meat and kind of. And what. Yeah. I mean, like I, I don't mind, you know, it's just sort of waiting for the price point to come down to what becomes. Yeah. It's fine. I mean, it's like, it's obviously not me. Never going to pass for like, actual me, but you know, not huge, but you know, my, you guys are talking about Radizia my mom's vegetarian and like for, but for like weird reasons basically, my mom, for some reason, like the post office thinks my mom lives at my house, even if she lives in Canada. So Radizia steakhouse keeps sending her like coupons. For some reason they're like, my mother and not me. I probably could it's not like it's in her name or anything I don't need beef though. So it's like, you know, the steakhouse, they probably have some stuff like that. Yeah, I'm looking for it actually looking for like a good restaurant to take like visiting faculty candidates to so I for I like rare. Yeah, so 2020 so like two and a half years ago. Yeah, that's good to yeah, recall. Yeah, okay. Yes, okay so you're naming all the places that I've been taken like once. So Yeah. Nice. All right, I should probably stop talking about restaurants. Let's go ahead and get started. As I, as I mentioned, like, if you have good Fort Collins restaurant recommendations I am looking for them for professional reasons and also because haven't gone out a whole lot since we moved here because pandemic and then baby so you know, tried like the same you know five restaurants and Fort Collins so send me recommendations if you got them. Okay, so I did post the link to that chat GPT event. If you're interested in watching that, not an assignment or anything, an assignment in my seminar. But if you are interested, you have two hours to spare, because I'm sure you all do. You know, I recommend watching that we will probably talk about some of those same points when we do like you know ethics and machine learning at the very end so it could be interesting for many of you. So where we at right now is do a quick share screen and let's take a peek at the schedule kind of get you going on where we're headed next so we are now. We should be back on track, assuming I get through the end of notebook 10 today so I'm going to finish up QDA and linear discriminant analysis and then we'll do linear logistic regression so quite a bit of content although we got through most of that on a last Thursday. So thank you all for getting your assignment one submissions I believe everybody submitted on time which is great. There are a few people didn't submit although I assume that those folks will probably end up talking the class is usually what happens. So keep that up. Also, so, as I mentioned, it is very much to your advantage to get things submitted on time. So the reason for that is as follows so I'm going to extend a regrade opportunity for assignment one for those of you who choose to take advantage of it here are the terms. I will write them out and send an announcement later. So, any late panel you may you first of all you may resubmit your assignment ones for regrade is going to be due a week from today at midnight. No extensions, no card deadline of February 28. So if you're required to do this, there will be a five point deduction. So that is, if you do perfectly you will still get a 95. This is, you know, I don't want people chasing those last few points if you got like a 99 stick with your 99 if you resubmit and get 100 it's still going to get a 95. And if for some reason you do worse on the regrade will keep your original grade. But I know that there are some of you, not too many but some of you who probably would want the opportunity to rectify some mistakes. So this is going to be your opportunity to do that. So I will reopen the box this evening. So any late penalty will still apply so that is if you turn it in a day late. In addition to the five points will get an additional 10 points off this is why it is very much to your advantage to get things submitted on time. It's also in future assignments in case I decide to do this again, right so the same policy will apply. So, automatic five points of the maximum you can get on this is a 95, although that may be very good for for some of you. You can rectify any mistakes will run it through the greater again and will grade your your discussion. It is due at the end of the day, one week from today so midnight on the 28. Absolutely no extensions, no exceptions. You're not required to do this so if you're perfectly happy with the grade I recommend you keep it. This is intended to be an opportunity to rectify mistakes that is both low overhead for you and for us. Any questions on this. This is not necessarily going to apply to all assignments I will decide on a per assignment basis depending on on how things turn out that assignment so for this one. This is this is extended for this assignment. Any questions. Okay, I'll post an announcement with the same details today. So let's get back into the material so where we are now. So, first, so I'll finish up notebook nine, hopefully get through notebook 10. If we don't finish that then we should be able to get through 10 and 11 Thursday. Next assignment will be rolled out a week from today. So just keep an eye out for that and then a to is due by default on Thursday extensions for those of you have blanket extensions should already be factored in. Okay, so I my meeting controls. Let's continue with notebook nine. On size okay for everybody. Good. Okay. Alright so just to recap. We were looking at introduction to classification. What we ended up doing was we're basically looking for, we want to find distributions in the sample where things items that are closest to an exemplar of a class would basically fall into a certain point that is represented here by a peak of a peak where the further you get from that sort of ideal points in the search space, the more the probability of falling into that class will taper off right. So, the idea being that there are distributions where there are clusters in your search space. You want to find the cluster the probability where a point falls close to that that peak in the distribution as closely as possible and you want to find that you want to have that distribution represent the data as as good as possible. And so what we do is we end up drawing these normal distributions and saying I'm going to assume that there are these points where if my sample falls right in this point is basically 100% likely to be a member of this class as we get further from that point we get a lower probability of being in that class and we can subsequently a higher slightly higher probability of being a member of other classes. So we model our distribution somewhat like this. What we want is that distribution is really multi dimensional, and it can taper off at different rates and in different directions. And so we're looking at the types of functions we can use to model that we ended up modeling this as a property of Bayes theorem. Right. So this is Bayes theorem here I have you know P of a given b equals P of b given a times P of a divided by P of b. So these basically the sum of the joint probabilities of B and all possible classes. In this case, we're calling a. So we went through all the math. And so using the assumption that each class is a normal distribution. We can then try to define the boundary between two classes so this is going to be basically online or hyperplane where if I fall on one side of that class and that boundary member of some class I follow the other side of the boundary and I'm up another class. So we went through, you know, all the mathematics is basically a lot of exponents we use natural logarithms to bring make this function slightly more tractable. Good reasons for using logarithms include that we can now do addition instead of multiplication so I'm now not multiplying a bunch of small numbers together so my probability of ending in underrun is far far lower. What we ended up with is basically have this discriminant function that we'll call Delta so basically this is the discriminant function of a class K for example X. And we just want to find that boundary where the discriminant of four class say class a of x is equal to the discriminant of class B for x that's going to define where it's equal probable that that sample is a member of those two classes. So this is just the two class problem. This equation is quadratic and x we call this quadratic discriminant analysis, right because the discriminant function is now quadratic function. So, all the math probably makes some of your eyes water. So we'll now go through it in Python. So, first, let's just make what we do usually do will make some some dummy data will define in terms of D dimensions so we can arbitrarily vary the dimensionality of the data as desired. So in this case we'll define our samples x and T, where now there's just a single, a single component to each sample, but this can be you know any size you want, and then the 10 samples for each class. So now we define x one and T one is these are samples in class one, right we're just going to arbitrary classes one and two. So we'll just say, there are some samples that are members of class one there's some samples that are members of class two. We define the normal distribution and then the shape that we want to sample that from the normal distribution into just one thing to note here is just for this data we've defined there to be a wider variance in the samples belonging to class two. So we put all this data together. So right you can see here the way we stack this is we basically take the x one samples and the x two samples, vertically stack them. And then we take the the T one samples and the T two samples these are the targets and work we stack them so now we basically have all our x ones are inputs x ones the next two then all our targets T one and T two. And now we've stacked them together. So we have a standard way of representing our classes where we have the target label says we've done before. So our data setup and pretty much the same ways we've done. So now imagine that instead of the the actual class labels we just have data right so basically don't know T one and T two. Right, we just have data we don't know how it's generated. We don't know the mean the covariance of the two classes so data might look something like this. So, if I just have these are my samples and I got some class labels I don't know how I generated these, these classes. So I don't know these means and covariance co variances. So let me just try and and try and compute the discriminates function for this so let's start as before. I'll just separate the inputs, separated the input column of the target column. So, similarly, right so ID dimensional data I'm going to take columns zero or three D. And then I'll just take the last column to meet the target. I will define my means and my standard deviations over my data X and standardize it. So that's not done before. Right we do not standardize T of course because this is now class labels. And so now we're trying to predict basically a discrete value, rather than some continuous value that might have a specific me and standard deviation. So when you define this QD discriminant function. Here's the mathematics of that again. So, one, the one key term here. Let's break down what all these things are so discriminant function for class K at X is going to be negative one half times the logarithm of sigma of K minus the one half times remember X is the sample you sub K is the mean of that of the class in questions that's the distance. This is a set of samples. So each of them is going to be a different X minus the mu for that for that sample should be the same sort of transpose that times the inverse covariance matrix. So we can define sigma as the distance plus the natural log of the class probably the class. Okay, so C equals case just whichever class we're interested in. So we know to calculate all of these things except for that covariance matrix, sigma. So let's consider a couple ways of doing that so we can define sigma, you know, as some arbitrary values, and then we can say take sigma times the inverse and then we can use the identity matrix. So now I can use this other function pseudo inverse so P inverse. So we should get something that if I use the pseudo inverse function that is very close to the inverse and I'll define this in a moment. I should get something that is very close to the identity function in fact I do. Right so we have one and then some very very small numbers here. So, effectively zero. So, we can define sigma slightly differently. So instead of this, you know, 1221 it's not 1212. If I try taking the inverse of this is going to throw an error. Right this is a singular matrix. So what is a singular matrix we look at a singular matrix is basically a non invertible matrix so we have typically two matrices, a and B, right if B is the inverse of a, then a times B will equal B times a which is going to be the identity matrix for whatever the properties of a and B are. So a singular matrix is going to be one that's not invertible, right for for whatever reason so we'll assume it's a, you know, a square matrix, but it's not invertible for some reason. So, what can we do in that case well what we can do is you can use the pseudo inverse function so this can can be so we call the more penrose pseudo inverse of a matrix. So this is going to be the generalized inverse of the matrix using its senior value decomposition, including all the large singular values. So what this does is this now allows me to basically compute something that's close to the inverse of any of any matrix. So it doesn't necessarily, if the matrix is non invertible for some reason, I can still use the pseudo inverse to to get something that is arbitrarily close to the inverse for that. So, if I now define sigma times the pseudo inverse of Sigma remember remember what's what Sigma look like it's one two one two. This is going to give me this is going to give me an output. Right, so this is not the identity matrix of course, but it will allow me to to to compute this. And now, now I can define a function that allows me to use the, the QDA equation so same similar inputs that we have all of the, all the pieces that we need to put together but we've got the inputs. We've got the means of the district different classes, the standard deviations are sorry, the means different this is mu, we have the means of the data the standard deviations the data mu being the, the mean points of the different classes. So this is the covariance matrix, and then prior is going to be that prior probability of the class. Right, so remember this is a generative model. There is a prior probability there is basically how, how common is my class overall than my data. Right, so I need to know the base for. So, you can see that we're going to take X minus the means time to standard deviations these are my standardized X values. And now I'm going to take that and then subtract mu right this is going to give me the distance of the standardized input from me, the mean of the class. And then what I can do is I can then compute the determinant of Sigma. So now, this is commented out here is it's not going to apply in this particular implementation, if the terminate is zero, then we will raise this error for the same recurrence matrix but instead what we're going to do is we're going to actually use the pseudo inverse to allow us to at least get an answer for for all cases even if it happens to be singular. Now we can start putting the pieces together right so here by negative one half right so negative one half times the NP log is the natural log by default. So this is the determinant divided by a negative one half times the sum of the product of X times the inverse matrix times X, and then it was going to reshape that so that all my samples are organized in the right order. And then so this this here should give me a single single dimensional matrix. I want that to be two dimensional. And so then I'm going to just add the log problem of the of the class to every element. So to use this we need to calculate the mean the covariance and the prior probability so we got that right you mean Sigma covariance prior probability. So, what about P of C equals k, this is going to be the prior probability distribution of class k. So if I have no prior belief that one class is more likely than the other, then it's just going to be the number of samples in that class divided by my total number of samples. In this case, if you look at the data that we use before it's 50% because we had five or like 10 members of class one and 10 members of class two. We don't we're going to pretend we don't know that though. Right so we don't know how this data is generated. So we'll just sort of try and figure it try and figure out if we can recover the right answer. So, let me see what all my classes are so I'm just looking at instances of T where their class label is number one so I can just do this by basically doing T equals equals one. So, apply the equals one Boolean function over my entire over my entire array. And then I can just reshape it to list all my samples out so now you can see that those first 10 are all true, because T equals one and the rest are all false. So now I can represent my class one and class two is those indicator variables, right, where it's either zero or one depending on the note which which class is in. So I'm going to have k minus one classes that are all zero, and then for that kth class it's going to be one. So I can define the mean and the covariance for all of those for all those different classes. So I basically just pull out which samples in my which rows of my data are members of class one which ones are in class two. And then for each of those, I can just take the standardized inputs for that class, and then compute the mean for each of each of my classes and same for the covariance. So now what we can do then is I will compute the prior probability right so this is going to be, and one is going to be the sum of the class one rose so that is just how many there are, but same for n two. And so and we'll just be land of T. How many samples do I do I have. So we can compute the prior probabilities for class one and class two just by dividing those respective ends by the total number of samples. So we can see that you know in this case we already know that our data is evenly split, but the same code would work. If I have an in balance data set this will allow me to do that. And this will allow me to then factor in that prior probability. So in case I had a data set where I had 80% samples in class one and 20% samples in class two right the overall probability is some sample. Falling into class two should be lower and I want to factor that in being a gender model. Okay. So let's look at the covariance for Sigma one. Right this is going to be this value. So now we can apply our discriminant function to some new data. So I've been able to define a function that given this information will give me the discriminant function. And then I can actually just compute the output probability for that forcing new data. So now create 100 new samples. And so this is going to be kind of representative of similar data. So then what I'll, what I'll define is to two models right so this is a generative model you always run K models for K classes. So if I like two classes I need to run two models. If I've got 10 classes I need to run 10 models. So I'll define two instances this QDA function and all I need to do is put in the distribution that I'm interested in right so the distribution and the prior probability so for the first instance, I'll put in those values for the first class and the second instance will put in those values for the second class. And so now what I do is after I run this, it's going to put output values into D one and D two. And you can see that it should for each of these 100 samples it should give me a label, basically, you know, true or false or a probability that when rounded would give me zero or one. That tells me how likely it is it falls into for the first set into class one and for the second set into class two. So we'll look at it. If you were to run this notebook and increase the dimensionality of the data you would still work right because it's written to accommodate arbitrary dimensions. But if the data is more than one dimensional we'll just plot with respect to the first component. In this case, there is only one component. And so to obtain the value of the normal distribution from the sample we've got two choices. So we can either start with the discriminant function value and then transform it into the full normal distribution value, or we can just use our normal distribution implementation directly. So in this case, we will just define this normal D function that has the inputs and then you in Sigma. So print out those values right so I'm going to have different you in Sigma for each class. And you can see already that since this is just a single dimensional sample, the mean value for class one is around center that point one to whereas the mean value for class two is centered just shy of point point three. Oh, sorry, my mistake. That was the covariance. The first class is centered at a negative point nine and the second classes centered at positive point nine and the covariances are given here and as we mentioned when we're generating the class to data, it's got a wider variance. If you remember that, hence this covariance being being larger. So what's the, what's the normal D function so x contains samples one pro and by D mu is that mean vector, so just D by one, and then signals the covariance matrix. So then what I can do is basically try to recover the normal distribution by using the discriminant function basically reverse engineering. So if you look at this, this code, you'll observe that it's kind of very similar to the the QDA code. So I'm just, I'm kind of doing everything in reverse right and try to get back to the normal distribution from the discriminant function where previously we define the normal distribution, and then we drive the QDA function from that. So let me define that. There's our new one in you to data again. So now I can plot what the discriminant functions look like for this data so all I'll do is obviously exponentiate the probabilities, and then I will apply the discriminant function to that. This will give me a nice, nice quadratic curve, and it looks something like this. Okay, so for this data. So we see where that peak is for the blue curve being class one for the orange curve being class two, and you can see that you know the peaks are different right about negative point. This is standardized. So, this is the standardized like negative point nine positive point nine. So this is probably about negative point nine still but then the peak for class two is probably about 5.5 something like that. So we can see that wider variance in the second class. Same thing as we see here. So this is this this first chart shows the discriminant function, you know, the actual value plotted as a function of the input feature. And then QDA here this is going to be the probability from directly from the discriminant function so here you can see more clearly. So these are those peaks that we were after, and then that kind of attenuation in the probability and so again here that wider variance in in class two is also evident. And so that here this one is just the QDA using the normal distribution, and you can see that we get a very similar output. Right. So this is what we want we were able to successfully recover that that normal distribution from the discriminant function. So, you know, 10 training samples per class you can expect the results to change a little bit, because it's quite a bit of noise. But what if we have like more dimensions than samples. So for example, I could set D equal to 20 right now I have 20 components for each sample, but I still have 10 results, and I can run the exact same code. And what happens. What happened here, we're not getting results. And sometimes I can run this again, and it may plot something for like one class but not the other. And even when it plots something will often see if it if it plots something for both classes will basically get a flat line here for the probability using the normal distribution. So, something goes wrong each time sometimes a different thing goes wrong. But clearly there's something not right with this shoe right with this distribution. I need to accidentally exited. Okay, here we are. Here we are again. So that stigma is very close to singular, which means that the columns of x are close to collinear. So, the determinant of the singular matrix of course is zero, and it can be inverted. So we'll discuss some ways of handling this in the future. But we assume a single normal distribution as the model of the data from each class. And this does not seem to lead to a huge complex model. But let's say you have how many parameters there are if the in the mean covariance matrix, if the data is D dimensional for some value D. So that means that the mean is going to have D components right so I'm just going to take the samples that have D dimensions and I'll take the mean for each dimension. And so I'm going to have like this D dimensional vector that represents the sort of the expected mean of the distribution of that class. Now the covariance matrix would have D squared components. So it needs to be D by D. So if it's one, if it's just like a single dimensional sample, we saw that our our sigmas just had a single value, right, one value by one value is one value. If we had 100 components then the covariance matrix would have 10,000 parameters. So in reality, the covariance matrix is symmetric. So it just has basically D over two plus D, sorry D squared over two plus D over two, or D times D plus one over two. These unique values, we can compute the remaining values just to pre know those values, but that's still quite a lot, right and it grows, you know, not quite exponentially I guess but polynomially. And so we have one for each class. So the total number of parameters including mean is going to be K for the number of classes D plus D times D plus one over two. Right so a lot of a lot of samples. So if the data distribution is then under sample that class boundaries going to have a lot of errors in it, right because I'm using. If I have 10 samples with 100 components. I'm not going to necessarily be able to find a very good mean for each of the classes that I have that I'm interested in. So, we're going to basically overfit to those few samples and if things, you know, there's basically a lot of variance the more, the more components you have that could end up on either side of that class boundary because you didn't do a good job of finding that. So we need to remove some of that flexibility from that normal distribution model. So, when we can, what we can do is we can restrict all the covariance matrices to be diagonal. And so, then we'd have these basically ellipses that you can draw it in the data. These will be parallel to the axes will come back to this point when we do dimensionality reduction. And this wouldn't really work well if our features are well correlated to each other. So now we can force all the classes to have the same covariance matrix by taking the covariance matrices for all classes and averaging them element wise. But this is doesn't seem like a great idea on the surface. Why, why not. I think single covariance matrix for all classes seems like it might. So no. What's the answer. So why is why is averaging all my covariance matrices a bad idea to get a single covariance matrix for all classes. Yes. Yeah, basically that's it right we we look at the variance in different classes could be radically different. And also the more components you add the more dimensions you add the variance in individual dimensions could also be different so even if I have like a three dimensional sample. So in one class in dimension three could be way less than the variance dimension three for another class. Well the variance and dimension one is a lot bigger. Right so averaging that you're basically losing a lot of information about in how much when I move along a certain dimension, my distribution starts to fall off. Right so this is not like a great idea. So what we can do then is we can use the average covariance for each class, and then wait it by the fraction of samples for each class. Yeah, similar. Okay, you're off those again. So everyone around thinking as soon as because he skipped class last week. So yeah basically we, we can assume that if my classes are like uneven unevenly distributed then if I'm averaging all the co variances, it might not be so detrimental this one class that like has a different sort of a different variance in it in some dimensions but it's it's under sampled in the overall data set right so if I've got one class of which I have 10 samples in a data set that has 10,000 samples like if I neglect this somewhat it might not be the end of the world. Right because it's so rare. I don't know if fitting model to the means and standard deviations defined by these data that I have for that class is actually representative of what other members in the class would actually be. Right so it doesn't makes doesn't make a whole lot of sense to overweight, you know or lens too much credence to just a few samples that might be really noisy or whatever. Right. So, we can actually do this and see a better result than than using a unique covariance matrices. So now to remember our, our discriminant function. This is the discriminant function. And so we're basically just trying to find values for where a D of some k is greater than or delta some k is greater than some other k, and then use the same covariance matrix for every class. So now instead of Sigma sub k we just have a Sigma. Right so now we can, we can use the discriminant function, plus the natural log of the probability for each of those classes. So this can be simplified as follows. So you're not going to go through the math basically just make multiplying out all the terms, so that I am able to have a simpler functions and my discriminant function is now becomes something like this right so D delta sub k of x is equal to x transpose times the inverse singular or covariance matrix times mu sub k minus one half of mu transpose times the inverse covariance matrix times mu sub k plus the log prob of k. And so now you can see that unlike the previous function might not be obvious because I'm not sure you don't you probably don't remember the whole previous function would go back and look at it. This is now linear in x, as opposed to taking a bunch of square roots and then trying to bring the exponents down in in in front. So now this can be rewritten as delta sub k of x is equal to x transpose w sub k plus constant. I'm sorry. It looks a whole lot like that linear function right. So remember, we did linear regression we did neural networks we basically have some inputs x multiply them by some weights w. And you add a bias B which is usually just sort of subsumed into the weights and you train you train the way for that bias. It's basically the same thing, right if we started with y equals mx plus be where m is a slope, and B is a is a y intercept. B is a bias right this is just is going to be some constant value, and then the slope in this case is multi dimensional, just defined by weights is the coefficients that I'm going to wait each each each input by. So using the normal distributions as a generative models and restricting the covariance matrix. This gives me a linear boundary. So this is now called linear discriminant analysis. So both QDA and LDA are based on these normal distributions by modeling the data samples in each class. So I can say for some sample. What would this look like if I try my best to model using a normal distribution and trying to find that boundary between my classes. So QDA has this flexibility, but LDA is actually often better in practice, in particular cases where we have under sample data or high dimensional data, right for reasons that we saw before, because we don't necessarily want to have that full flexibility of trying to define a covariance matrix for every class, when in reality I can model the data in a more flexible way, or in a more practical way with fewer computations using linear boundary. All right, questions. Do the example. Yes. So yeah, if you have a data set and it's not very comprehensive, I'm going to put words in your mouth and I'll say like, yeah, sort of just you have a sparse, you have a sparse data set. So, yes, very likely you'd want to use LDA, because you can. So, the definition is basically, you don't really have enough information to be very confident in the colloquial sense about what your covariance is going to be. So, trying to trying to fit a covariance matrix to like every data set you're going to probably overfit to any peculiarities of those samples. And so instead I can define one's a little more general. Yes. So, you certainly could. Yeah, so let's say you had like some high dimensional samples, but you can figure out that like most of that variance is actually captured in say the first two or three principal components, right. And then in that case you might be able to reasonably fit like a QDA model to that, where it's where it's it performs a little bit better, because you can basically infer that the risk of having that large variance in those higher dimensions is actually really low. Right, because they're not actually capturing all that mentioned information so could just be you have some sample that for some reason, you've captured 100 components of, but just the first few components are actually where what's really important for making some sort of classification distinction. And so those others say 97 components just adding noise. Right, so you could get rid of those and yeah you definitely could so just keep in mind just all these machine learning techniques are individual tools in your toolkit. And so, most of your job, you know if you apply this in your careers or in your research is going to be trying to figure out like what's the right combination of tools that I want to use for my data. Right, so do I want to do PCA on the data itself or do I want to do PCA for like visualization or something. You know, it may be helpful for one but not for the other. And do I can I do some dimensionality reduction technique that would allow me to use a technique like QDA because it's faster than say, trying to fit a neural network to it because these extra 90 plus parameters are just like not really that relevant right you can figure that out. They probably solved half the problem right. Okay. Other questions, comments. Cool. Yeah, so remember we had that Parkinson's data as we had samples to class to class problem. We basically have features extracted from the voice. And then each of the samples is labeled as has Parkinson's or does not. So we're going to go back to that and then classify using QDA. So first let's calculate the means and the covariance matrices so I'm just going to use the same splits that I had before so if you remember, we had like 170 or 195 samples I guess, or something like that. And then we split them into a train test split using 8020. So what I'll do is I'll fit these generative models to each class, right through two classes just zero and one. And so then I will standardize my trains, and then I will compute my means and my covariances for those two classes. And then same as before I'm going to run these two discriminative models over the samples from class one and samples from class two and see, given what I know the labels are how correct am I, then we do the same thing over the test set. Right so again, I'm going to be using X test, I'm going to be using the same, the same computed means and standard deviations for the data and then also the muse for the two, two classes and the covariances right. So now, then I'm the last term here is going to be the prior probability of the class of my class zero is healthy, and my class one is Parkinson's I just going to take the total number is each of those classes, divided by the total number of samples, and then I can return that into some percent correct. And so, you can see where we're going already but in this case, using qda right the train percentage percentage is like 98% correct. And the test percentage is about 87% on this split if I run again it might get slightly different numbers. But you can see that there's a significant underperformance of the test accuracy compared to the training accuracy. So, what we can do now is we can write this function is going to do it multiple times we're going to try different splits and run multiple times so again as I think I mentioned, we want you want to typically you might want to try to average like over a bunch of different cases, you got a lucky split once, right and you don't want to report those results, because perhaps someone trying to reproduce the work wouldn't be able to and they're going to be like well, I ran your exact code and I got a different results so what gives, but what gives is that there's a different random seed or something. And you got to use having a lucky split of the data that no one else can actually ever reproduce. So, basically this function is going to do what we just did, just a bunch of different times. And so we can see that I will make a, you know, a split of the data run my two discriminant functions over the train and the test data print out the percent correct. So, now what I can do here they can basically do this run park, I put in the data file and my training fraction. And then, give me that number, or do it again, right I get slightly different numbers you can see like now we're getting 92% test accuracy. So, the training numbers pretty much stay the same, right this is about as good as I'm going to get using qda on on this data, but for different splits, you know I may get sometimes radically different percentages so we have range here from about 84.6 to 90.92.3. And we can see here, we compare these two in this case, the test accuracy is identical. Right and this is not necessarily because we have the exact same split just happens to be that you identify the same number of samples is incorrect or correct. So, let's just you know for for your review you can just consider how would you get the values for these for these different things using base theorem, if you need some practice so you can just look at these. Look at these points, and then go up in the notebook and just see how we would get these these different values. So, now what do we need to change for rude is doing this with qda so what do we need to change for all this to run it with LDA. So, let's write this LDA function and see if the same classifier, or the LDA class for which assumes all the classes have the same covariance matrix just better than qda and the Parkinson's data. So we show that if we assume the same covariance matrix by waiting it and find the number of classes. Then our discriminant function becomes as follows. So then what I can do is I can write disk LDA that's implementing this function instead of the quadratic function over the same data. And then I can redefine run park to use this function disk LDA, instead of the, the QDA function so here we're going to run the QDA function that are on the LDA function, and then we can see how they compare. So, if I run this right now we can see, here's my QDA result and there's my LDA result and we run it five times. So, let's see, you all take a look at this and see, tell me, you know what you observe. Right, how does, what's QDA doing versus LDA. But the values for the test are fairly similar. Right, yeah, so a lot of this is based on that's on the split, right so based based on which 20% we're holding out for test can have a significant effect. And you can see that you know the, the QDA percentages are like routinely north of like, at least 95 often up into 9899, where the LDA numbers are lower 89% 93% the test fraction can sometimes be significantly lower actually for for LDA but also sometimes a bit higher. So, for example, here's, here's one case where basically the LDA is beating the test LDA is actually even beating the LDA train accuracy, and it's also significantly beating the QDA test percentage so we can see you're the QDA probably over fits and LDA is a little bit more generalizable often, but if you run this again for example, you know run it a few more times, you'll get some different data. Right, so here's, here's cases where for most of these numbers are pretty, pretty identical for the test so he's like say this sample. Same test accuracy, even though the LDA train accuracy was quite a bit lower. So, then what I can do is I'll just like write this out into a file. I'll call it QDA LDA. And so then if I just run this, then we can actually see the probabilities. So, now if I look at this. Here's a sample that's class one was predicted as class one, and then the probability is actually, you know, 20% or something. But the second class is just point zero zero one percent. So remember these classes don't sum to one is just looking at which one is more probable. Right so even if this is only 20% likely to be a member of class one 20% is still a lot more than close to 0%. So that's going to be the answer. Yes. Yes, so the one above. So, I just want to ask, how could you tell that this was over here. So you can basically see, there's no way to definitively tell that it's overfitting but it's very likely for seeing my train numbers being 98% and my test numbers falling significantly below that. So, it could fit really closely to this data and there's enough resemblance between the test data and the train data that it's kind of lifting it up and it might actually get be higher than the LDA model. So, usually it's going to produce a lower result but it shouldn't produce a result that's like so much lower than the train. Okay, so it's really, it's much more about that discrepancy between the train accuracy and the test accuracy if it's really overfitted to the train data is like your train data is going to be like close to 100% it's really good at fitting with the training data. If your model were good, you'd expect to see a similar number on the test. Right so if I'm seeing 98% train accuracy and 82% test accuracy for a problem that is this simple. You're probably seeing overfitting. There are more complicated problems, you know, some like very complicated like objects or action recognition problems that the state of the art on the test set is like 40% or something. Just because the problem itself is so hard. Yes. So if we assume that, I got you saying so we would be covering the data. Yes. So, you're going to look at the discrepancy between the train data and the test data. So here, for example, this one, right for LDA, the train and test accuracy is a really close. This model is trained well enough to get 89% accuracy in the train data. It's also generalizable enough to get similar accuracy in the test data. Okay. So if I look at this one, for example, my train data is like 98%. Great number I love to see that the test accuracy is a lot less than that. Right so there's something in in this way this is fitting to the data that it's like not so flexible and is able to generalize quite as well. So your ideal model is one where you just get like really good train and test accuracy. But in most cases, you can't really expect to do that. What you don't want to end up with is a model that is somehow fit to some sort of peculiarity in the training data such that when I give it new data. It just kind of falls apart or just doesn't do as well. Yeah, I mean, it does it does depend of course. It depends what the use case is right do I want to. I'm just interested in like just fitting to the park instance data. Maybe my QDS is great, right, is actually in some cases it's doing better so like, I could stick with that. But if I wanted a model that's like okay I just want to be able to handle an arbitrary to class problem, and I don't know where the data was generated from and I don't know those, you know, I can just calculate the means and covariances but I don't know kind of the distribution the data was sampled from, I might want to err on the side of some of it's more flexible. So it sort of depends on like what am I trying to use it for. If I were running a test that's like I just really want to fit to Parkinson's data of this form specifically. Then you probably just want to do whatever is going to give you the best tie the best test accuracy but it's like I just, I mean we're running a bunch of different samples maybe working a lab where you're trying to do to classification like a bunch of different diseases or something. And if you're like a filtering to send people to you know a specialist or something like that, you probably want something that's more generalizable. Whether or not, no, just keep in mind like, just because you can doesn't mean you should right machine learning may not in fact be the best tool for this if you think of like a medical environment that states are pretty high. So you may or may not want to use a technique like this. This is just a demonstration of like you know if we have data set up in this way this is how you can model the problem. RMSC. Well, I mean this case we're not measuring RMSC because it's a classification problem so RMSC is error on us on scalar values. Right so if you're having if you're predicting continuous values then the RMSC will indicate you like how how close are you to predicting the correct value. But for classification, right, your metrics are very different. Right, so here we're talking about accuracy. You might also do like precision or recall or F1 like we talked about in the second lecture or like you know area under the receiver operator curve. So you have a bunch of different metrics that you can use and part of your task is to pick the right metric for the task at hand but like, if you're dealing with a classification problem like RMSC would not be the one to do because you have to it's basically squared error over some units well what's the units of classification there aren't any labels. Yes. Yeah. Yes, yeah, yeah. Basically what I'm saying is I if I look at like this class right this is a misclassification so that's the set member sample one or class one is predicted to be a member of class two. So the model when factoring in things like the prior probability of the classes in this case it's equal probable but then also like the features. It's saying okay there's an 8% chance that this is a member of class one is a 9% chance or 10% chances the member of class two. Neither of these is objectively good. Right, wouldn't put money on this result. But these are the only two things these are only two models that have got. So, I must choose one. So you can handle the problem in a way that's like, if I don't get a value that's like above 50% or something I'm just going to say I don't know. Right, I'm not, I'm not, I'm going to, I'm not going to use this result. But if I handle my problem in this way if I set up the formulation this way, whichever ones higher wins it doesn't matter if that higher value is actually objectively low. Yes. So that makes the last misclassification. Yeah, yeah. I mean, we have to look at the exact inputs for that sample. Yeah, it happened to be that way but there might be you know, there may be something that's like a typical about that sample. You know, we're talking about like voice features for Parkinson's so like maybe that person had a particular timber to their voice already. Right, maybe they had a naturally shaky voice or something like that, or actually maybe this is I think class class zero. Maybe they just have like a really solid voice that even the shakiness that comes with Parkinson's doesn't really change that part of the vocal signature all that much. So just for like an outlier. Yeah, yeah, so it really could be I mean this is this is not a severe outlier but if I had a case where it's like, this is 95% likely to be a member of the wrong class might be an indicator that is an outlier. All right. Okay. So, what I will do now I will start and run this notebook on your own. I'll start the next notebook here which is classification with linear logistic regression so we're continuing with linear class. We're continuing with linear classification, and we'll just follow up kind of with an alternate method for a discriminative model rather than a generic model so just as a point of point of fact so a discriminative model and basically looking for for the individual features. What is most relevant to this class. I don't know about the prior distribution so much, I don't factor that into my model. So this set can have repercussions for how you fit your model, because there are fewer samples to me pull your decision boundary in a particular direction. But I'm not going to be using like prior class probabilities and making this decision. So this linear model use for classification we can have this masking problem where if I have to to few samples of one class. This can result in masking so we had these different membership functions that other other than linear functions. So first we use these generative models to model data from each class, then converted that to probabilities using base theorem and then drive those quadratic and linear discriminant functions. So now we're going to instead of doing that, instead of having like two model that's going to give me a probability for being in one class and being in other classes. So let's just exactly predict the probability. What is the consequence of this if I'm doing this, then I should end up with a probability that it is a member of one class that if it's greater than 50% I'll classify it as a member of that class because there's no other comparison to make. It's 50% likely to be member of class a but 30% likely to be a member of class be there for class a wins. I'm basically just saying, is it a member of class a well it's 50% likely or more so therefore yes, or it's less than 50% and therefore no. So, in this picture the problem was that this line for class to the green line this is too low. And in fact we'll see that in the middle of the range, all the lines are too low. Right so if I'm looking here. Yes, the green line is the highest for the set of samples, but none of them are particularly high, whereas the ones on the edges, we see values for these these these two functions of the red one red line the blue line are quite high and so you could probably reasonably confident of that. So, one thing we can do is reduce his masking effect by requiring the function values to be between zero and one, and then requiring them to some sum to one for every value that so these sound like probabilities these are properties of probability so we can actually represent the probability as basically there's some function some predictor function for x parameterized by weights w for some class and I'm just going to sum that, or divide that by the sum of the outputs for all functions, all such functions for all the classes. Right. So if I assume that f of x parameterized by w is rated in zero. We've not discussed exactly what f looks like yet, but we can see the w represents those parameters that you're going to be tuning to fit the training data. So now we're back in trying to optimize weights. So, we know that this expression will give me a value between zero and one for any x. So now we also have probability of C given x express directly, as opposed to modeling x given C for every class and then running all my models using base theorem. So, this is going to be an arbitrary function. So let's just give it another name was called G for now. Right so G is the probability of C given x, which is given by the output of f for function care class k divided by the sum, the sum values of f for all classes in M. So now we need to choose something for f and whatever that is we have to have some plan for optimizing its parameters. So, what's our plan. We need to try to maximize the likelihood of the data. So that is to say I've got some data. And I know that all my samples in these classes in my in this data belong to some set of classes. So we need to try to maximize the distribution of classes such that the likelihood of seeing this data is maximize. Okay, that makes sense. So I've got some data, and I want to see what classes do I need to define what distributions should I infer such that the likelihood of this data is the greatest that I can get it to be. Okay. So, if you have training data consisting of samples x one through n, and then these indicated variables for classes one through k. Remember, these are all going to be ones or zeros, where it's one where it's a member of that class and zero otherwise. So, each row of this matrix should contain a zero one, and a single one, and then we can also express my samples as an n by D matrix. But for the following samples will be using a single samples more often. So the likelihood is going to be the product of all probabilities for the class of the end sample, given that and sample for that and sample. So to express this using indicator variables would be this so here's my indicator variable raised T sub nk that's the indicator variable. So the probability raised to this value, right is either going to be is either going to be raised to zero, or raised to the one. So I have three classes, and the training sample and is from class two. So the product is going to look like this right so it's going to be the probability of one given x raised to the T sub n one times the probability of C given x raised to the T sub n two times probably C given x raised to the T sub n three. So of course if I raise anything to the zero becomes one. So now, only one of these terms is going to remain. So let's say it's a member of class two. This is going to this is going to reduce to the probability of C equals to give an X. So now this shows that if we use any kind of variables as exponents we can now select the correct terms to be included in the product because basically looking at what actually is relevant here. My class of interest is class two. I really only be looking at the probability that it's in class two. It's not I'm back to computing a single probability. So, if this is the data likelihood what we do to maximize the data likelihood. So again I'm going to be finding some weights w. So that maximizes the likelihood of actually encountering this data. So, if L of w, right so if this is previously likelihood of the data be. So now I want to find the w the like that maximizes that data so basically just looking and trying to solve likelihood of w is going to be for for all n and all came going to take the product of those probabilities of that sample falling into that class. So now I'm finding the derivative with respect to each component of w. That is because it's a derivative and high dimensions and not back to computing gradients. But there's a whole bunch of products in here and what happens you multiply a bunch of fractions together. It approaches zero right so the more multiplications I have the closer it's going to get to zero. So I'm going to make it easier by working with the log problem of the log likelihood. So I'll just call this L of w. And so now I can do things like convert all my products into sums, and then bring my exponents down in front. So now I take some for all and of the sum for all k of the indicator variable times the log problem of the class. And that becomes a lot more tractable. And I'm just trying to find this is going to be a negative number always, and that's like, well, it has to be a good number. I'm just trying to find the least negative number. So now unfortunately, still trying to solve the gradient, of course the gradient of the log likelihood with respect to the weights is not linear in X. So we can as before, simply set the gradient, the result equal to zero and solve w right so now, you're paying attention this sounds a lot like her neural network problem. Right, it's now no longer a linear function we've turned I my linear function into a nonlinear function. So we'll do a similar technique. So we'll call this gradient ascent. And if you're wondering why we're doing ascent and not descent. Just think about the properties of a logarithmic curve that make this appropriate to think about the shape of logarithmic curve. So what I'll do is I'll initialize w to some some value, then I'll make a small change in w in the direction of the gradient of log likelihood with respect to weights. So should be back in familiar territory this is starting to sound a lot like neural networks, or just, you know, linear regression even with SGD. So I'll repeat this, this step until I seem to get to some sort of maximum value in the log likelihood. So this is a form of convergence and it's just going to see my, my value does not seem to be increasing very much I've probably reached about the maximum on this gradient I'm ever going to get. So, what we see here is now what's the value of w I'm going to take the previous value of w, plus some some value alpha times the gradient of the log likelihood with respect to w. So, alpha is going to be that that constant that affects the step size which sounds like learning rate right so again you know also sometimes labeled alpha. So remember that w is going to be some matrix of parameters, let's say we'll have some columns that correspond to the values required for each F of which they're going to be k minus one. So, we can work on this update formula one column at a time so here we have this, this is for each column, and then I'll just combine them at the end. Right so this is this weight is going to be weight, the weight plus alpha times of the gradient for all of those individual weights those individual components. So now let's remember that we have some function was called H. So the delta, the derivative of the log of respect to x is going to be one over h of x. And then we have this function this probability function we've just labeled G. Right so now I'm trying to figure out what G is. I can now rewrite my log likelihood function just put G, G of x in place of the probability. So now my, my gradient with respect to weight J of the log likelihood is going to be some for all and for all k of T sub nk divided by G sub k of X of n times the gradient of W sub J times G sub k of X of So, if you're wondering why the above works just just remember what the derivative of the log of X is right so derivative that for log is one over X. So we can actually do that. So now it would be really nice if the gradient includes the factor G sub k of X of n so it's going to cancel with the other one in the denominator. So we can rewrite the function to get this. So if we define f of X of n parameterized by weights of k as e raised to W sub k transpose times X. Right so again we see this thing that should look familiar double weights times inputs. So if we rewrite this such that G of G of case of X sub and G sub k of X of n is equal to f of X of n parameterized by W sub k divided by the sum. So now we can work on simplifying this right so we take this function that I defined here. So we're just going to rewrite it in terms of this new definition of f. Right so we get this. And so now, by take the gradient of this end up simplifying to something that looks like this right so if I have radiant of W sub J times the sum for all k of e raised to this quantity. Over one over that times e raised to that same quantity. So now if I take a look at this. What's going on here so that I have the gradient of W sub J raised to e. Sorry, times e raised to the W times X. So now remember what are what are indicator variables were doing. Right so if it was a member of that class I get some value. That's not a member of that class it's always one. So taking the derivative. I can't get zero if it's not a member of the class of interest. Otherwise I get an actual quantity. So, therefore, what we can end up doing after all this math is basically saying for this value, what I can end up doing is I can just define this function where it's going to output some some value if it's a member of the class of interest and zero otherwise. So we take this substitute back into the log likelihood expression, we end up with something like this. So we basically have okay so the change in w sub J of the log likelihood is going to be the sum for all n for all k times indicator variable over our function g divided by or sorry times the gradient of W sub J times the times G. So now what I can end up doing is I'm just going to take this change in Delta, Delta sub J k minus the my function G times the input. So now this gives me this update rule. Right so we see what we had before so previous or previous value of W sub J plus alpha my learning rate times the sum for T sub and and sorry T sub n J minus G sub J of X sub n. Time X sub n. Let's focus on this term here, right because I went through all that very very fast. Let's focus on what's going on here so if we look at my indicator variable, either a zero or one. Right. This is going to now be some probability value. And so, if I have my samples that are set up basically saying, it's got a probability that's zero or one or zero. Okay, so I'm going to say it's number of class zero zero one zero. Now I want to predict something that's going to give me a meaningful error. So it's about three classes where my indicator variable is zero one zero. I want to subtract from this something that is going to be of the same dimensionalities that have to be three terms. Okay, I also want to be a meaningful error. What's a meaningful error in this case basically let's say that we've got three classes, whereby indicator variables are zero one zero. Think of those as probabilities instead 0% 100% 0%. So if I can have a predictive function is going to output probabilities that can be directly subtracted from a value between zero and 100. It's going to give me a meaningful error. So if you imagine that I have some function that says, okay, got three outputs and it is 0.3. So we have 0.3. We can now subtract that from being take that and subtract from zero one zero. So we'd have zero minus point three. One minus point six zero minus point one. So now we're back into the kind of traditional error formulation of how wrong am I. So this function is a 0% probability to member of these classes and 100% probability to member of these, this one class. I want my output values to approximate those values as close as possible, over all of my samples. So this function, whatever it is and you may be thinking of names for this is really just another way of representing error. It's just this time it's representing as an error in probability, instead of an error in some scalar value. So I'm going to be finished this I think so just to summarize what we've done. So I have my probability of my class given some sample and some data likelihood we want to maximize so what I did is I took my probability function, and I'm trying to find some function that's going to model and calling it J, or sorry, G, we want G to have the following properties. It should be bounded between zero and one, and it should sum to one for all possible outputs. So this value should be raised of this this value raised e raised to the w sub k times x of n remember this is just my input x times my weight t, my weights w. And so this should raise if or equals value of k is less than k. Otherwise, it'll be one if k is equal to k. So now what I can do here is then for the likelihood of w is going to be the product for all and then all case of these probabilities raised the indicator variables remember the probability function in terms of as this. The gradient of the log likelihood with respect to w is going to be something like this so I now take the gradient to give you the log likelihood. So I'm turning all my products into sums. Because I'm taking the derivative of the natural log I can now bring the indicator variable down and then also divide it by my function g. I multiply this by the gradient of my weight G or sorry my weight w times the function g of just k of x sub n. So now what I end up with is the simplifies to for the sum for all n of x sub n my input times my error. Right so I get some output here. So I get that from my indicator variable is the computer thought of as a probability. And then I multiply that by the input. So that's the gradient this now allows me to turn this into an update rule which tells me how much I need to move along that gradient in order to optimize those weights. So, um, last few minutes of questions about this will pick this up again on Thursday. Yeah. So the function of the school alphabet. Yeah, it says functional f of x sub n. So, okay, k is a number of class so big K is a number of classes. So K here is an individual class and basically it's going to be there's a class of interest. Maybe it's like 012. And so it's going to be the, the probability of falling into that individual class when for for all classes, right this should probably actually. This is a little confusing. This should probably be like a summation I think. So yeah, I think I'll fix that it's a bit of a typo. So, remember here we have the output of this is going to be what's the probability of it being in this class, and there's all this probability should sum to one. Okay. Alrighty. Yeah, so I will go back to my office. I'll be there until 430. And good luck on getting a to completed.