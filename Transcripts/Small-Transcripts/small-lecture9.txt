 Okay. Okay. Okay. Okay. All right, let's go ahead and start. So I am not as on today as I usually am probably feeling pretty tired. Huh? I feel I mean I like feel physically five is like have not been sleeping a whole lot recently so I'm like, uh, Who's water bottle is that? Apparently every time it comes in like a different water bottle right there, like someone keeps leaving a different person keeps leaving their water bottle there. Okay. If you forget stuff in this room, like, come back and get it because I'm not picking up that view. Okay, so, um, all right, so hope you all saw the announcement. I unfortunately am not going to be able to have office hours at least not very long if you come like right after class I probably talk to you for a little bit, but I will have to leave before the the 430 got a pet family emergency. I think I have to attend to so. But I think, you know, people have started PA your assignment to it's not due for a while so you know, hopefully you have time to kind of figure out what's going wrong. I hope things will be back to normal by Thursday. And so, I guess, um, that was only an announcement for today. Other than that, anybody have questions today might be kind of short, we're just going to do autograph basically. All right, no questions. Okay, so let me get my material up. So I'm sure it's green. And then only make sure they can still connect to the right. We need to. Yes. I brought my GPU laptop and then I forgot to charger at home and it was down like 13% battery life so back off to putting things on the lab machine. I answered the wrong password. No, what are you doing? Of course. All right, what if I just connect. There we go. Okay. But it's again. All right, yes. Okay. So, before we start this, I'm just going to do a quick tutorial that I realized I'm missing from this notebook on computation graphs just so you're aware of some of the terminology that we're that we're using so basically, we all kind of have this notion of what the schema of a neural network is. It's like right we have a basically a bunch of you know circles, and they're in rows, and you have edges between all the circles in consecutive rows. Right so those rows are of course those the layers, and the circles those the nodes in the end. So, every sort of neural network is basically it can be represented as graph, right. So, as we all know graphs have like the neural networks matter representation nodes and edges the trick is in a neural network. Each of those has basically representation that is realized mathematically either as a value or a function. So, we can say that if for an expression x, we can actually just graph it like the following. Right so you can actually have a graphical representation of any expression. And so the question is just what I represent as nodes and edges and how do I connect them together. So, if I have some expression x is just going to be represented as a node with an x in it. The edges represent function arguments. So for example, if I have some function where that's f of you, where you is the card network that the function is like the cardinality of you. Then what I can do is I can any argument that is connected via an edge to to this other node will basically represent the input to that function. Right so, for example, if I have another function f of you be that is you transpose times V. So, if I take some other values x and y so if this if this node represents the function, then the edges that go from these notes representing the expressions into this other node representing the operation over them will then take whichever values as I specify here, and use them as arguments to this function. Okay. A node with an incoming edge is basically just a function of the parent node so what does that mean for a neural network. What are the parents what are the children. So in this case you can just have the children be the, the nodes and subsequent layers. Right so now if we think of that lattice style representation. So the children are going to be connected by edges to their parents in the proceeding layer, which is in a fully connected ever would be all of the nodes in the proceeding layer. And then there's going to be some function that is applied to them. We've already seen what those functions are right is basically an activation function over a weighted song. That's the operation that that occurs in a network. So now, what this means that we can then represent these things as these neat computational structures that allow us to kind of get away from the from scratch building that you're doing in NumPy into something that has more efficiency that we will see. We do this by allowing a node to compute things. So of course this makes sense because the node is now a unit in the hidden layer, but there is an operation that's going to be that's going to go on in that, in that layer in that node. So, a node can compute two things right can compute its own values that would be the sum the weighted sum that comes out of that node using whatever input is coming to that node. And we can also compute its derivative with respect to each input. So, why is it important that we can compute both of these things, given what you now know about neural networks why is it important that we can compute that a node can compute its own value using its inputs. Any thoughts. So new here. All right, why. Yes. So it can be used in subsequent computation right so if I have a, if I have a node that is connected to some other nodes in a subsequent layer. Right. I don't need to know what the arguments were in the previous layer. All I need to do is take the output of this node, and then use it as an argument to subsequent subsequent layers. And then I can eventually just do kind of a dynamic programming style operation right where I can take the output of some computation that just use it in a subsequent computation. And then of course, why is it important to be able to be able to calculate the partial derivative of these values with respect to each input. What we use the partial derivative for. Yeah. Something back propagation right so the weight update that is the back propagation. We need to know the derivative of things that have happened in subsequent layers right so be being able to allow these nodes to compute both of these things now allows me to do both the forward pass and the backward pass. So, for example, in these examples the nodes they will compute you know, for now you X and then X transpose times why, and then in the left graph, this, this will compute D F D you may increase the font size. Just kind of read. So D F D you and then the right graph will know how to compute D F D you and D F D V right because that those are the arguments. Okay. So now the graphs, the entirety of the graph represent functions. And these functions could be expressed. You know, this one could be say nullary functions they have no arguments over example, this thing. Right this is an expression, also known as a nullary function. There are no inputs to this it just is its own value. And then unary right as one incoming at this would be a unary function. This would be a binary function up to basically just an arbitrary value to end so you know the a single node in a neural network represents an energy function for the size of the previous layer, right, but I've got and those in the previous layer they're going to be and inputs to every node in a subsequent layer assuming a fully connected network with none. We're not talking about like residuals or conclusions or anything just yet, but just assuming a standard feed forward fully connected network this is what you can expect. So basically just all the way up to some value then. So now we can express arbitrary functions as graphs. So let's say I have some expression, X transpose times a, where might we encounter like this for an arbitrary value of a what a represent in this case. Yeah. An image or just or just generically right you can write any input, but generically let's say in the interior of your neural network what might it represent. Let's say I've got a three layer neural network and I'm looking at like some function of a in that second layer, right second hidden layer. It's basically just going to be the output of the previous layer right so this can be a would be you know, h of X times V as came out of the previous layer. Right so this really can be anything so we can do here is that if I have some input X. It has some function applied over it in this case the function is just the transpose. And then that output can be used to as inputs to another function. Here we see for example I have an input X. This might be a raw input and I have some input a that might be another raw input or it could be a feature map or something from from a previous layer computation, and then I can perform operations over them. Now here, interestingly, what I can do is I can represent sort of arbitrary graphs that might have loops. So these don't necessarily need to be directed or a cyclic. So for example I can have an expression that is X transpose times a times X, right. I don't need to define X twice I simply needed to find the edge that tells me where X needs to go what it what X is an input to. So for example I could take the previous one right we've already established that X transpose times a can be represented by this. So anytime I see that I can just take that same graph right and clone it. And then I can just pop it in there. And so then I have another multiplication by X I just got to figure out where I put X, right already have X I can reference the same X, there simply just needs to be a function that's going to take the output of X transpose times a and multiply it by X that can be represented like this graph. And so, another way you can do this is you can also represent the same expression right expand transpose times a times X, with a simpler graph. Right so I could have a function of you and M that is actually the whole of this expression all I need to do is pop in those values that represent you and right so as long as I define the function that is being executed in the node, right there are many different ways that I could represent the same expression. So that means that is to say the competition graphs are not necessarily unique for a given function. So, if we take a more complicated one so xt times ax plus bt times x plus c. It may look something like this right so the first part is already given here, right, we've already seen that. And then there's going to be another node is basically the sum of these three, three expressions, and all I need to do is provide the inputs there. Right so I've already got the output of xt times ax, I need to provide some some graph that's going to give me a btx, and then something right so I've already got x I can reference that same x again the third time here when computing be transpose times x and sees just an expression. And so now all of these get fed into this node that is just a summation function, and it will give me that output. All right, so that is to say, what we can do with these things is now allows us to build complex operations from simple building parts. This is interesting for neural network purposes because we can write neural networks as computation graphs. Right. So the nodes are the quote neurons and the edges are the quote synapses. We don't really talk about synapses in computational neural network and just, I don't think I've had this event yet in this class so here we go. The term neural network, like, whoever came up with that is just an absolute genius at marketing, because like if I if this class were called you know introduction to nonlinear optimization or something, you know, two thirds you probably wouldn't be here. But machine learning and neural networks are just like incredible terms of art that just draw people to the field because it's like seems like something mystical is going on. So these neurons are sort of loosely inspired by cognitive architectures in that there is an input, you know, in the brain's like an electrical signal, it's transformed and then that transformed input goes elsewhere and becomes an input to something else so these were just doing functions right the transformation is some sort of nonlinearity times over some and we're just simply trying to figure out like what those right values are. So, in the sense that it's neural neural like machine learning doesn't really bear any known resemblance to actual learning, except perhaps coincidentally there are cases in the brain where we see processes that seem to be like like supervised learning like reinforcement learning or like unsupervised learning, then there's a whole lot of things that the brain does that we don't have neat machine learning metaphor. So, there's, there's, there's some way to go, but the term neural network seems to be here to stay. All right, given that we can write neural networks is computation graphs. We can also write loss functions as computation graphs. So that is you can write loss functions within the innermost SGD operation. And then also as you may have observed their plug and play right so if I have this expression, where I've already kind of determined some computation graph for say the first term of the sum. I can just clone that in whatever form and then just drop it in. Right. And then all I have to do is, if I need to reference a term that is referenced in this in this sub graph, I can't. It's really no big deal. And then finally, you know, we can, so we can basically construct this graph and just like use it in someone else's program, which is basically if you have used TensorFlow and pytorps that's what you're doing. You'll specify what you want your layers to look like. It's going to create a computation graph for them that allows you to do all these operations, and there's a whole bunch of like C++ or whatever that happens under the hood that you never have to touch. Right. So, part of this allows us to make efficient gradient computations because we're going to write the entire thing in say Python, the underlying C++ or C or, you know, assembly code or whatever that is so much faster is doing all of that for you. And so you get a significant speed up with these packages. All right, so. Finally, if we have some operation right h is going to be some non the near function will call the 10 h over W x plus B or my weighted sum. And then why my output would be some other weights V times h plus a would be like another that other bias. Right. This can be represented are two layer normal ever can be represented something like this. So I've got my weights, and I've got my input. And then I've got a bias is explicitly representing the bias as its own term, but as shown we can just consider this to be another matrix. And so now we add these, we take the 10 h function this gives me my output that goes in as the missing input that it's looking for for this other other function. And here's some other weights, the, and then I multiply them. And then I add some bias and there might be some other non the near function here that I want to apply. So plug and play neural networks, courtesy of computation graphs, very simple, allow us to do to do certain things so you know, like I mentioned, artificial neurons kind of loosely mimic some functions of biological neurons that they have the advantage of being able to find patterns and that's just from my NLP class I say language data but generally just like data, you know, in large, we don't have to do manual feature selection, like you don't have to try and figure out which power I need to raise some term to in order to fit my linear function. But they can take quite a bit of time to set up correctly as you may be observing. So the way that you represent the input data is very important. Everything's still got to be numeric. And what we want is we want to capture something about this is kind of again NLP specifically we just want to find a way to capture important features without trying to extract them manually to at least to the extent that we are, we do not have to. Okay, so why all that about computation graphs, because this goes into the topic of the main lecture, and we're actually still a day ahead, which is good. And I anticipate this will be able to finish this with no trouble today. So today I'm going to talk about pytorch particularly the autograd capability, and then also the, and I'm not sure if we cover that here in the next one. But either way, we'll talk about autograd and hopefully you'll see kind of how the construction of the computation graph allows us to use these features of pytorch, you know, with sort of a single line of code rather than having to write say a dozen as you may have had to do for for assignment to. All right, so, first of all, who here is used pytorch for some. All right, so we will get, you will get some practice in this in this class. So starting here and then in homework three, and then subsequently, you'll get the ability to kind of play on the pytorch see how it works differently from say the numpy version, or similarly and then we'll also see how we can get significant speed benefits so why use pytorch well obviously you probably wear some of the benefits right it's faster as I alluded to, it's more efficient as I also alluded to, it provides support for GPUs, right which speeds up everything as you saw on like that first lecture significant amount and see that again. So the code is usually easier to write right so if you see how I'm calculating the output of my prediction, what I'm going to do is I'm going to take, you know, why is if I'm using the form from the code in the assignment. Why negative one is that last element of the outputs for each layer right if you if you look at the code, you'll notice that it's kind of accumulating the outputs for each layer. So like the last output that I got, which will be the output from my previous layer times the weights, whatever it is, add the bias, and then apply my 10 h or my non linear function to and that gives me why. Okay, so, like a little bit hard to keep straight in pytorch. You basically just define the hidden layer, and that becomes the function, right as we saw in the computation graph, and then I just specify what the input that function is, and we'll do the computation for me. Okay, so the code is more opaque. And if you're like trying to figure out if you're trying to like infer what machine learning operations are from reading pytorch code is not going to be very easy. But having done some already you know what's going on in those hidden layers right now you see why equals hidden layer of why you should at least have some intuitive understanding of kind of what that's doing. Okay. So, the, the core of pytorch and also other packages like TensorFlow is this thing called a tensor. As we alluded to in lecture one, you know, tensors and numpy arrays are both n dimensional arrays but they are not equivalent. Right. So, I'm on the lab machine scroll is all fast. So tensors are a much more generalized form so remember that a tensor technically is basically a transformation, and the numerical array is sort of a form suitable to representing that transformation. So, the tensor being a transformation is a function. So that we have the tensor as say a way to ray living in a node. That's the function that gets applied over some input. So, the torch package contains these classes and functions that are mostly very similar to numpy. So, if you do like V stack and each stack and all the array manipulation you can do in numpy, you do a lot of the same stuff in pytorch. It also provides convenient functions to convert back and forth. You want to minimize the use of these functions because it does take time to convert the numpy data structure to the torch data structure so basically set up our data and numpy converted to tensors, then you can run pytorch. So, you can do a data analysis convert back to numpy arrays and you can do things like you know, make plots and all that stuff. Okay, so let's just view this in action right so I just create some data this is now just a list, right this is just a list of lists. As we know I can take a list and turn it into an array using the numpy dot array function. This is going to give me this. A two dimensional three by three array representing this data. If I look at the type, this is going to tell me okay it's made up of 64 bit floats, and it's a numpy and ERA. Now if I create torch dot tensor using the data it's going to give me something very similar. So, I can do the same function or the sort of the same function equivalent in torch, right so instead of creating an array and creating a tensor, but I can, I can do that operation over a list. I can also do that operation over a right give me the same thing. So how is, how is this represented now this is a torch tensor. And it is the, this one, and it is composed of 32 bit floats. So, a little bit different there at the numpy version of 64 bit floats the torch version 32 bit floats by default. Of course you can specify with the d type, which kind you actually want to use. So now what I can do is I do torch from numpy of a, this is going to give me the same numbers, but now it explicitly says that the d type is torch dot float 64 created from 64 bit floats from numpy. Right. And then running the type and the d type will confirm that. So now, I can also do torch as tensor. Right and this is going to give me pretty much the same thing. So again, same numbers, right I'm also taking this from a, and this is going to create 64 bit floats. So, and so now basically C, and D, contain the same information and contain the same data type. So the next one. So now what I can do is I can take D, right this thing that is created and turn it back into an umpire right. So now this is going to be an array. And if I take the type of this is going to say okay now this is an endy array again, and it is a, a float 64 data type. So what's the distinction, right distinction is torch dot tensor makes a copy of the data, whereas torch dot from numpy and torch dot as tensor do not copy the data. So, remember what happened when we were doing in the first part of the Adam notebook. What happens if you do a deep copy versus a shallow copy so they do, if I make a shallow copy. What happens by change something about the original. Okay, if I make a shallow copy and change something about the copy. It does change the original right if I make a deep copy. The two are completely separate they're you copy the data to a different place in memory, let me do whatever I want with this new copy and nothing about the original will change. Right so I ran into this. I did this yesterday when I was doing some manipulation for doing some, some data and like, I changed the class labels for something and all of a sudden all my plots like the colors are all wrong. It's like, I gotta rerun the model. So be careful. Okay, so here's a again. So now what I can do is, if I, oh God. So if I create this tensor from using the from numpy function right so this is be as a tensor. And now I set that first element to 42.42. Okay, here's a. And here's be. So this is that shallow copy and by changing a I've also changed be. Basically torch is referencing the same place in memory for the actual data. It's just got sort of a data structure that is of the right type to defeat it into into pytorch. If I tried to put a into a pytorch neural network you would give me some error saying you know was expecting tensor but got non pyray, but if I put be in it would be fine, although the numerical data is the same. Okay. So now I could do torch tensor, right which copy is the data as noted. And then, oh God, let's get this wrong. The scroll set to a different direction. Okay, so here's be right and then be zero zero is 42.42. Now I said a zero zero to 12,000 grand and 45 okay there is, and be is still the same. So torch tensor will create a deep copy. Now we can also use the at sign for major multiplication as we do in NumPy. So here let me create some data, right. And for a and B and I could do C equals a at be, and the shape of C is 10 by 20. Why is the shape of C 10 by 20 given what it was calculated from. So 10 by five and five by 20 those two inner terms are the same and so when you multiply you end up with the two outer terms as your shape. Okay, so don't need to do that about randomness I guess. So now if I do tensor versions. Right so if I do the dot random function just kind of the same as the NumPy random uniform. So give me the same thing right, but now when I put a CT dot shape it can be just torch dot size of the same dimensions. So again I can pull the same values out of this is just represented slightly differently. Okay, so now where we we get with what we get with autograd is basically the ability to calculate gradients automatically so I'm giving these two quotes to deal with hyperplanes and a 14 dimensional space visualize a 3D space and say 14 to yourself very loudly everyone does it. This is by Jeff Hinton the father of deep learning, and I do, I think this works I think my technique is slightly different. When I think of a high dimensional ray literally just imagine a really really long three dimensional array. So our tiny like human meat brains are not good at conceptualizing things in more than three maybe four dimensions if you think of time as a fourth dimension. Beyond that, they're just you see these long strings of numbers and like this, this looks two dimensional to me or one dimensional or something. It really what it's think what you're saying is that there's a value in every dimension that is orthogonal to all the previous ones. Once you get beyond three. This is not something that you can actually visualize very easily. And I guess what you need to know is that vectors, even in high dimensions preserve those similarities across this high dimension so things that are similar to each other. Numerically or semantically will point in a similar direction in high dimensional space, and that operation is equally true in three dimensions as it is in 1000 dimensions. So, as Abraham Lincoln said this is pretty cool. I can actually say this. So in the Cooper Union speech in the section addressed to the southern people. He actually says this that is cool. I believe he meant it in the sense of like that is kind of cold or cold hearted. Nonetheless, if anyone tells that Abraham Lincoln didn't say this is cool, you can, you can show them the reference. I didn't find my notebook anymore. Okay. So anyway, people are like pretty bad at calculus, just in general, in more than three dimensions it gets even worse. Right there's so many numbers you have to calculate the derivative for like every, every element of a high dimensional array with respect to all the by holding and holding all the other values constant. And so really part of the reason that machine learning sometimes feels like magical is just as like so much math going on that yes, you could sit down and actually calculate like what's going on inside of a neural network. In principle, if you had all the time in the world and the super powerful computer. But doing those operations, you know, by hand or like line by line would take greater than the lifetime of the universe to actually do. So of course no one actually does it. So we basically take the workout by by doing by using matrices. And then, as we mentioned, we have GPUs that are optimized for vector math and matrix multiplication. So we can get this big speed speed up with the the GPU hardware, but there's still explainable math that's happening at each step. If you were if you had the ability to zoom in on a single, you know, iteration a single gradient update, we will see the operations that we saw happening in lecture five or like updating a single way is just remember all that's happening at a massive scale. And we think about modern applications it is just truly mind boggling. So, that makes it impractical to calculate these gradients for these giant gigantic composite functions, especially in the high dimensional space. So for example, you know, what's the derivative of sine of x cosine of x right so we learned that in in trigonometry. And so you can see like how we can actually calculate this. So they create 100 evenly spaced points from negative to pi to to pi, and then plot the sign. So, why equals sine of x and then the derivative would be cosine of x. I can plot these right this is what we would expect. So, now what I can do is I can, I can do this using pi torching tensors. So, I can do this using x, right those numbers that I created. I'll create a tensor from that array. Here's the tensor. And so then, currently I look at this, this argue this parameter of this tensor requires grad. I'll get to what requires grad in a minute. So it says false. So right now requires it does not require grad or require gradient. So I can set that value to true, right just by you'll see the function version has an extra underscore there. So I said requires grad to true. Now I can see that it will give me this extra little bit of information at the end so set that set requires grad equal true and it'll tell me that whatever I print out the tensor. So now by setting requires grad equal to true. So, forming this computation graph, this backwards graph that is the history of every operation. So, when we think about how a node can calculate itself its own values from its input, and then also its derivative, and those values are going to be dependent upon the children, or the parents, the values of the children subsequent nodes are going to be dependent upon the values of the parents. So we can create this graph and so this backwards graph is going to be this history. Right, so if you think about the actual operations being performed. If we start from the input you perform an operation over you perform a few executed function over that operation, you then take that value and then the operation is performed over and so on and so on and so on. And so, each of those operations can also be part of the graph. And so setting requires graph through will allow me to basically automatically calculate on every component of that sub graph every every component of the sub graph within that graph, as I'm performing operations. All right, so this will aid in calculating these composite gradients. So I'll just look at this grad fn it currently says none will start assigning values to it soon. So now I can define the sign function right so of course I'm going to define why being sign of x. And so now here's why of T. So here's the sign you can see that we're basically starting from a value really close to zero, and we're getting closer and closer to one and then eventually end up with close to zero again. Okay. So, no doctrine for that one. So the shape of this is 100. So just a tensor that has 100, 100 elements in it. Okay, so now if I look at the backwards. So yt dot backward. So this is the gradient of the current tensor with respect to the graph leaves graph is differentiated using the chain rule we've seen that before. This function accumulates gradients in the leaves. So you may need to zero grad attributes that the none before calling it will see what happens if you don't zero out your gradient. So the backward will basically compute the derivative for every x in yt that has required requires grad set to true. So, yt is basically constructed by in this case performing the sign function over some, some other operative some other value in that case, x t. So this will basically compute the gradient for every value in that in that array or in that tensor. Okay. So, so now here if I just do dot backwards over an array of one 100 ones. So this argument represent the gradient of y sub t with respect to itself, hence it's going to be all ones. So now if I look at x t.grad. Oh, God. No. There it is. Okay, so now if I look at x t.grad right I did I did y t.backwards with the gradient with respect to itself. Not to look at x t.grad. Right we started one. We go down to zero. We end up back at one. And this is now actually the cosine of x. Right so I started with just some, some values, I computed the sign for those values. I took the gradient of the sign. So the initial input that x is now set to cosine. How did that happen. So let's look at how we compute, why T, right y t is generated by taking the sign function over XT. And since why T is created by performing this operation over XT, if I run y t dot backwards this will keep a record of updated gradients for XT because XT was one of those arguments in the computation graph that was used to create YT. And this only does it if XT dot requires grad is set to true, which we did earlier. So therefore, you just need to be alert to your in place calculations, right so if I'm doing operations over something that uses something else as an argument. If that other if that argument has requires grad set true to it I may end up calculating the gradient of why with respect to that in the original input. So you got to be careful, right you may have it you may not actually want to do that. So you have to you have to make sure that requires grad is set to true, only when you need it to be. So let's visualize some of these operations. So first you can see what's going to happen. I just want to plot XT versus YT, and then it's going to get mad at me. So why did it get mad at me, why do you think it got mad at me. Not the same dimensions. Let's see what happens. What it wanted so it didn't say anything about the dimensions. So XT and YT should both have 100 elements. The error is can't call numpy and tensor that requires grad use tensor dot detach dot numpy instead so remember we got those operations to convert between the values and tensors. Those only will work, assuming that basically the tensor is not attached to the computation graph. Okay. And so, if I tried to do that it's basically saying hey I'm not done with my computations over this, don't try to take it away from me. And of course, pi plot being pi plot, you, it needs to be an umpire array. I can't turn it back into an umpire array until I detach it from the computation graph. So there's this neat function dot detach that you can use to basically say, I've got this tensor, I don't want to do something that the analyzer use it as you know an input feature to another operation or something. I need to detach it from the current computation graphs to remember when that the doc string said that dot backwards accumulates gradients in the leaves. Right. This is one of those leaves. I need to take I need to detach the leaf from the tree that is the graph in order to use it anywhere else. We will use the detach numpy function and again when you're doing assignment, I want to say three. You may run into this issue. So tensor dot detach will return a new tensor will be detached from the current graph. And this this result will not require grad. And if I have something that requires grad and I detach it from the computation graph is not much point in having requires grad or what I'm going to use requires grad for if I'm detaching it from the graph is no graph to compute the gradient. So I can do whatever I want with it and not worry about changing that due to other operations having been performed on the graph. And then the numpy will convert the tensor to an umpire array. And now we're back in familiar territory. So now here we go. So here is xt and yt right that is the blue line. So we have xt being the original input and then yt being the design function and then xt dot detach right that will give me the same x values and then xt dot grad is going to give me the derivative of why right because of sort of that computation graph magic that we just discussed. And so now, instead of plotting, you know, two separate functions, sine of x and cosine of x. I'm really just using properties of the same to the same two variables xt and yt to get the values that I need. Okay, so questions. Be a little bit mind bending. So where we at now. I'm going to have to move faster. Okay, so now let me do yt equals the sine of xt and then I'll run yt dot gradient again, right to go backwards. And so now xt dot grad gives me something like this. So I started to go down close to zero in the back of two. But I do know that the derivative of sine of x is cosine of x right and that hasn't changed. So what happened. Let me plot it. Now we can see this. Right so now it's telling me that the blue line is still sine of x, and the orange line computed by xt dot grad. Now is like cosine of x except it's bounded at two and negative two. So I'm not going to actually change anything. So the magnitude of the derivative is twice what it should be. And this is because dot backwards is going to add the gradient values to the previous values. So I'd already, I'd already calculated one gradient that was eventually cosine of x, then it did it again, but cosine effects are still there so it's like okay I'm just going to take whatever this is and I'm just going to add it to this. So now my gradient is sitting there at two times the cosine of x. So we must explicitly zero out the gradient. If I do this, then the same addition is true, right whenever I can view the gradient is going to add it to whatever whatever's in the gradient. But if the gradient is zeroed out that's zero and so what I get is the actual gradient that I want. Okay, so use this dot grad zero function, and this will print out the, the, this will zero out the gradient. What I can do here is if I do you know for 10 times I'll just create, I'll take the gradient of the sine function, and I'll print out that first element, right so the first, the first term should be one for cosine. Now if I do this you'll see that I get you know one and then two and then three and then four and then five all the way to 10, because it's every time I run this, it's adding that gradient. I get that accumulation of values. So we add one to the existing value if I put the grad zero inside of the for loop. Now it's 1111111 so I keep getting the right value here. Okay, so always know you need to be zero without your ratings. Alright, so now, having done that we're going to do we'll run through some examples of training some simple models using SGD and pytorch, I'll compare the numpy version and then the pytorch version. This should look pretty familiar to you. So we see our X's and then my T is just not some linear function square of X. I'm just going to use my SGD implementation similar to what we did in in the assignment one, and that gives me this right so here is my T and this this function, why this is like the best linear function that I can fit to this right so still doing just linear operations. Now we do it in torch. So, what's different here right I'm creating these tensors from the numpy arrays, and then I create my weights as torch dot zeros. I'm not using auto grad yet I'm still just doing the calculation manually. So, gives me an error expected scalar type long but found float. So if you look at the X. It says it's 64 bit int. So this error is actually a little bit misleading in fact it suggests that some float should be an int. But X is already made of it. But w is now made of floats. So, rather than make w into ints, because integers do not make good weights typically will make X into floats. Okay, so let me run a version of this again. All I've done is I've just added dot float here. And now this works just fine. Very similar results. Okay, so why are we actually using torch if it looks identical to our numpy code what's the what's the point of this exercise. And just to get you thinking about computation graphs apparently. Where this really shines let's take advantage of automated gradients. Right so I've got this computation graph the reason that I do it is that I can automate the calculation of gradients. So why am I dealing around here with this for loop, when I can do everything much more than I can do it conveniently. Okay, so here we go. So, same up until this point here. Now instead of the update I'm causing just calling msc dot backwards right so msc in this case this is my loss function. So to update the gradient I just call whatever instance of my loss function I've created and just call dot backwards on that. Okay, because the dot backwards calculates the gradient so when my, when I was doing like yt dot backwards and yt was the sine function it's going to calculate the derivative of the sine function. But I'm always calculating the derivative of the loss function and so I just define what that is, and then call dot backwards on that. And then finally, I also do with torch, no grad so I can kind of not update the gradient I can sort of do a temporary, I don't really need to attach anything to the computation graph I just sort of pause the gradient calculations on that. And so then I can use that to update the actual weights here. So you can compare that to what we did before in the commented code. And this gives me, again, pretty much the same result. So one of the things you can do is you can use predefined optimizers. So here, you know I'm defining, you know what I've defined my optimization function, you know manually. So here I can do this. Using some some predefined function. So where previously I had defined my SGD operation. I can actually just pull that out of the library. So this is going to look like this. So if you instantiate my optimizer as an instance of torsion optin dot SGD, I can specify what weights, I'm going to be optimizing and what the learning rate will be. And then after calling loss dot backwards, I can just call optimizer step. This is basically this thing to perform one step of great optimization. And so now for every training epoch in 100 I'm going to do one per step, and each time I'm going to zero out the gradient. This also gives me the same result. So, you know, rather than define SGD or Adam or what have you by ourselves we can use this predefined optimizer class. And this source of optin is this package that implements a bunch of different optimization algorithms. All of them have like, you have to define different parameters for them right you have to find like the, of course the ways you're going to optimize but also for Adam you need to define say those beta values and things like that. And at this link you can see a list of all the implemented optimizers that you may want to use questions on that so far. The other thing we can do is we can use predefined loss functions. So, what was my last function to this point was I using. I'm giving you a regression problem. mean squared error right so I'm trying to minimize the error so here we go. This part this is the definition of the loss function right there. Okay, now I can optimize that. But maybe I don't feel like doing that maybe I forgot the function for means great error maybe I'm doing a more complicated function like you know categorical cross entropy loss something it's like I don't remember how to write this precisely and I don't want to mess it up. So, I'm going to put pie torch will give you the loss function. Like before so here, I can define this mse funk as torch dot nn dot mse loss. And so now I just calculate the mse as this function whatever it is over my targets and my predictions, and then I just call dot backwards and then I can optimize my weights. And I do get a problem so what is this issue right found d type long but expected expected float this is similar to the one above. So, we see this where we had x made of ints and it wanted floats, this is dropping off the backwards pass inputs are T and why. So we know why is made of floats, as it should be. So therefore the problem must be in T. And so now I can add dot float to you and I define it along with x. And so the last one is the same boom boom boom. Here we go. Okay, I actually will talk about torch dot and then module. Okay, and then sends if you haven't guessed already stands for neural net. So basically this allows me to create a lot of the infrastructure that I'm going to need for neural net and then define my specifics like how many layers and what types of optimizers and loss functions. It looks simpler for our linear model but if you try to apply to multi layered models it will. Right because I have to all have to do is define, but the specifics of my layer with, you know, a single line of code each. Okay, so number of inputs number of outputs looks similar. And then everything is going to be torch dot nn dot sequential and then I define the layers here so basically this is saying the only quote layer is just a linear function mapping from input to output, but if I had multiple fully connected layers I would define each of those in turn. So now if I just print the model, you'll get this nice print out of what the model looks like. And so you can see what all the layers are. This allows you like if you forget stay what the input size to a particular layer should be. You can print out the entire model and say okay hey layer for is expecting that things are going to be 256 elements and so that's where my error is my input somehow is not 256 elements. So I print model dot parameters, I get this, basically this list of the parameters that I've solved for. So, in this case they're just their random right so I've just got some random weight and some random bias weight. So now I can actually use this try and solve function. This looks similar, except all I've done is I've added an inputs, right so that in samples and inputs. And so now I've also added y equals model x. So this is just saying my model is a function I define it however, I'm just going to run that entire model over this input. So this is sort of the higher level version of running like hidden layer of why there's all the hidden layers are all layers in there. So I run all of them single forward past. And so by do that, then this gives me. Once again, I've solved the, I solved the problem, you'll notice if you have been paying attention notice that like, these are not exactly the same solutions in fact, some of them have like the orange lines like a little bit higher. So the slope is slightly less than the biases probably a little bit higher. So remember these are all estimations right you start initialize weights randomly and it optimizes based on your error as best as it can. And so you may get slightly different outputs each time it is in fact perfectly possible to write a neural network that has two inputs that are the number numbers two and three with a single unit is activation function is a plus sign, and it's going to tell you the answer is 4.999999. This is the thing that happens sometimes. So just be aware that this is all estimation. So by adding a hidden layer or two. Then what I can do is I can describe the structure of the network like this. Right so we see here, towards the end of sequential. So one linear layer that maps from size of inputs to that first layer size so in this case both in layers or 10, 10 h function, right define my activation function to what function gets applied after I perform this operation. That output then goes into another linear layer that's going to map it to whatever size and hidden one is, and then another 10 h function that's going to map it to the linear, the output, whatever, you know, size that is in this case just one. Everything else is pretty much the same. They run that. So now I'm actually get now instead of a linear function right it's able to optimize kind of the curve of the line. Right. It's not like it understands that there's a squared function here or anything, but it is able to match those values pretty closely. We do get a bit of a weird zigzag here at the end. So maybe Adam will do better. So what I can do here is I can in for my optimizer. I will just define torso often at him instead of SGD. And then I can change my learning rate appropriately. And then I run that. And there we go. Exactly where it needs to be. So basically optimized to this almost perfectly. Okay. So now, let's actually make use of the GPU. So it's trivial to move data and operations down to the GPU with pytorch. So all we need to do. Well, first of all, I'll run it without the GPU so here I'm just going to, you know, perform matrix multiplication for 1000 times over some random numbers. And we'll run this, and we'll see that it takes about a second. So pretty fast. This is running on my lab machine. The CPU itself is quite powerful. But we do have this thing on this machine called CUDA torch dot CUDA dot is available will tell you whether your machine has CUDA or not. Generally good practice to write your code to accommodate either in case you end up on a machine that has no GPU or they can't find CUDA at least you can still run, even though it probably be really slow. So, now what I can do is I can do. CUDA is going to say take this tensor and move it to the device. This is not done in line you have to reassign the variable right so at dot to CUDA is not going to change at it's not actually going to move it but I do at equals at dot to CUDA. Then every time I reference at it will get the person that's on the GPU. So here we go right C, CTE equals CTE dot to CPU using CUDA in the CPU is not much faster. It's actually know it's actually slightly slower. Right, because CUDA is a GPU acceleration library so what's why am I using it on the CPU. So what I can do here is I can define this function on use GPU. And in this case I have this code written so I can use the Linux machine or my other laptop, which I would have brought except you can't connect the internet anymore. So now let's compare the speech of the torch dot and model on more data using the GPU and comparing it to CPU. So first I'll just set use GPU equal to false, and then I'll run this like non linear model, move everything to the GPU, and this is similar as before. Moving data GPU training truth about a second again with GPU. And took 3.115 seconds without it. Okay. So now the torch dot and end at module forward function. We, we just saw how to implement this using this combination of sequential and then linear and teenage layers. So here's the forward calculation for the neural network is defined this way. So now we can just define a class that extends torch dot and end on module and define the forward function explicitly. So here I'll define an end net class that extends this class, and I can define the forward function that this is going to be very similar to the forward pass function in assignment to. So I start with, you know, I just set my why equal to my input and now this allows me for each hidden layer, I just recompute why is the output as the operation over the output of the last layer. And then this will give me the final output. So now I can set you know, larger network 100 notes each larger learning rate, and then I can move the data to the GPU and run. And we can see that it takes about one second. So, basically the GPU magic makes it such that the larger model you have the greater speed up you're going to get right the differences we see here with the simple models like a factor about three. But then, as your models growing larger becomes your factor of 303,000 and so on. And so you can define your neural networks, pretty simple syntax, and all the other code kind of remains the same and this allows you to very easily experiment with like different layer sizes and different hyper parameters and things like that. All right, I think we'll be it for today. So, sit down. We're almost done. Yeah, so we are, I think we're done for today so on Thursday I will start introduction to classification. Good luck with the homework. And I have like a few minutes if you want to talk but I'm going to have to leave early today.