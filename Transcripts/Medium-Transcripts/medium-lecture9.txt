 Okay. Okay. Okay. Okay. All right, let's go ahead and start. So I am not as on today as I usually am probably feeling pretty tired. Huh. I feel I mean I like feel physically five is like have not been sleeping a lot recently so I'm like, uh, with water bottles that Every time it comes in is like a different water bottle right there. Like, someone keeps leaving a different person keeps leaving their water bottle there. Okay. If you forget stuff in this room, like, come back and get it because I'm not picking up after you. Okay, so I'm alright so hope you all saw the announcement. I unfortunately am not going to be able to have office hours at least not very long if you come like right after class I can probably talk to you for a little bit, but I will have to leave. Before the the 430 got a pet family emergency. You have to attend to so, but I think you know people have started PA or assignment to it's not due for a while so you know, hopefully you have time to kind of figure out what's going wrong. I hope things will be back to normal by Thursday. And so I guess that was only announcement for today. Other than that, any anybody have questions. They might be kind of short, we're just going to do auto grad basically. All right, no questions. Okay, so let me get my material up. Sure screen. And then we make sure they can still connect to the. Okay. I brought my GPU laptop and then I forgot to charger at home and it was down like 13% battery life so have to back off to putting things on the lab machine. No, we're going to answer the wrong password. No, we're doing. Of course. All right, what if I disconnect. There we go. Okay. But again, Okay, so before we start this I'm just going to do a quick tutorial that I realized actually missing from this notebook on computation graphs just so you're aware of some of the terminology that we're that we're using so basically we all kind of have this notion of what what the schema of a neural network looks like right we have a basically a bunch of you know circles, and they're in rows, and you have edges between all the circles in consecutive rows. Right so those rows are of course those the layers and the circles those the nodes in the in. So, every so neural network is basically it can be represented as graph. Right. So, as we all know graphs have like the neural networks dramatic representation nodes and edges the trick is in a neural network. Each of those has basically representation that is realized mathematically either as a value or a function. So, we can say that if for an expression x, we can actually just graph it like the following. Right, so you can actually have a graphical representation of any expression. And so the question is just what do I represent as nodes and edges and how do I connect them together. So, if I have some expression x is just going to be represented as a node with an x in it. The edges represent function arguments. So for example, if I have some function where that's f of you, where you is the card network that the functions like the cardinality of you. Then what I can do is I can any argument that is connected via an edge to to this other node will basically represent the inputs to that function. Right. So, for example, if I have another function f of UV, that is you transpose times V, then I can take some other values So if this if this node represents the function, then the edges that go from these nodes representing the expressions into this other node representing the operation over them will then take whichever values I specify here and use them as arguments to this function. Okay. So, a node with an incoming edge is basically just a function of the parent node. So what does that mean for a neural network. What are the parents, what are the children. So in this case you can just have the children be the, the nodes and subsequent layers. Right, so now if we think of that lattice style representation, the children are going to be connected by edges to their parents in the preceding layer, which is in a fully connected network would be all of the nodes in the preceding layer. And then there's going to be some function that is applied to them. Right. We've already seen what those functions are right is basically an activation function over a weighted song. That's the operation that that occurs in our network. So now, what this means that we can then represent these things is these neat computational structures that allow us to kind of get away from the from scratch building that you're doing in NumPy into something that has more efficiency that we'll see. So, we do this by allowing a node to compute things. So of course this makes sense because the node is now a unit in the hidden layer. Right, there is an operation that's going to be that's going to go on in that in that layer in that node. So, a node can compute two things right can compute its own values that would be the song the weighted song that comes out of that node using whatever input is coming into that node. And then it can also compute its derivative with respect to each input. So, why is it important that we can compute both of these things given what you now know about neural networks, why is it important that we can compute that a node can compute its own value using its inputs. So new here. So new here. All right, why. Yes. So it can be it can be used in subsequent computation right so if I have a, if I have a node that is connected to some other nodes in a subsequent layer. Right. I don't need to know what the arguments were in the previous layer. All I need to do is take the output of this node, and then use it as an argument to subsequent subsequent layers. Right, so this means that I can eventually just do kind of dynamic programming style operation right where I can take the output of some computation and just use it in a subsequent computation. And then of course, why is it important that we able to be able to calculate the partial derivative of these values with respect to each input. What we use the partial derivative for. Yeah. Calculate weights I heard something back propagation. Right. So the weight update that is the back propagation. We need to know the derivative of things that have happened in subsequent layers right so be being able to allow these nodes to compute both of these things now allows me to do both the forward pass and the backward pass. So, for example, in these examples the nodes they will compute you know cardinal X and then X transpose times y. And then in the left graph. This, this will compute dfd you may increase the font size. So dfd you and then the right graph will know how to compute dfd you and dfd right because that those are the arguments. Okay. So now the graphs, the entirety of the graph represent functions. And these functions could be expressed. You know, these are these are could be, say, nullary functions they have no arguments over example, this thing. Right, this is an expression, also known as a nullary function. There are no inputs to this it just is its own value. And then unary right if there's one incoming edge this would be a unary function. This would be a binary function up to basically just an arbitrary value of an end so you know the, a single node in a neural network So this represents an n-ary function for the size of the previous layer. Right. If I've got n nodes in the previous layer there are going to be n inputs to every node in a subsequent layer assuming a fully connected network with none. We're not talking about like residuals or convolutions or anything just yet. But just assuming a standard feed forward fully connected network. This is what you can expect. So basically just all the way up to some value of n. So now we can express arbitrary functions as graphs. So let's say I have some expression, x transpose times a, where might we encounter like this for an arbitrary value of a, what a represent in this case. Yeah. An image or just or just generically any input, but generically let's say in the interior of a neural network what might it represent. Let's say I've got a three layer neural network and I'm looking at like some function of a in that second layer, right second hidden layer. So this is going to be the output of the previous layer. Right. So this can be a would be, you know, h of x times v, as came out of the previous layer. Right. So this really can be anything. So what we can do here is that if I have some input, x, it has some function applied over it. In this case the function is just the transpose. And then that output can be used to as inputs to another function. So here we see for example I have an input x. I have some input, then I have some input a that might be another raw input or it could be a feature map or something from from a previous layer computation, and then I can perform operations over them. Now here, interestingly, what I can do is I can represent sort of arbitrary graphs that might have loops, they don't, these don't necessarily need to be directed or a cyclic. So I can have an expression that is x transpose times a times x right. I don't need to define x twice I simply need to define the edge that tells me where x needs to go what it what x is an input to. So for example I could take the previous one, right, we've already established that x transpose times a can be represented by this. So anytime I see that I can just take that same graph, right, clone it, pop it in there. So now I have another multiplication by x I just got to figure out where I put x, right, I already have x I can reference the same x, there simply just needs to be a function that's going to take the output of x transpose times a and multiply it by x that can be represented like this graph. And so, another way you can do this is you can also represent the same expression right x transpose times a times x with a simpler graph. Right, so I could have a function of you and M. So that is actually the whole of this expression all I need to do is pop in those values that represent you and M. Right, so as long as I define the function that is being executed in the node, right, there are many different ways that I could represent the same expression. So that means that is to say the competition graphs are not necessarily unique for a given function. So, if we take a more complicated one so xt times ax plus bt times x plus c, it may look something like this right so the first part is already given here. Right. We've already seen that. And then there's going to be another node is basically the sum of these three, three expressions, and all I need to do is provide the inputs there. Right, so I've already got the output of xt times ax, I need to provide some some graph that's going to give me a BTX, and then something that's see right so I've already got x I can reference that same x again a third time here when computing be transpose times x and see is just an expression. And so now all of these get fed into this node that is just a summation function, and it will give me that output. Right, so that is to say, what we can do with these things is now allows us to build complex operations from simple building parts. This is interesting for neural network purposes because we can write neural networks as computation graphs. Right. So the nodes are the quote neurons and the edges are the quote synapses. We don't really talk about synapses in computational neural network and just, I don't think I've had this rant yet in this class so here we go. So the term neural network, like, whoever came up with that is just an absolute genius at marketing, because, like, if I if this class were called you know introduction to nonlinear optimization or something, you know, two thirds you probably wouldn't be here. But machine learning and neural networks are just like incredible terms of art that just draw people to the field because it's like seems like something mystical is going on. So, the neurons are sort of loosely inspired by cognitive architectures in that there is an input, you know, in the brain electrical signal, it's transformed and then the transformed input goes elsewhere and becomes an input to something else so these British doing functions right the transformation is some sort of non linearity times over some some way that some, and we're just simply trying to figure out like what those right values are. So, in the sense that it's neural neural like machine learning doesn't really bear any known resemblance to actual learning, except perhaps coincidentally there are cases in the brain where we see processes that seem to be like supervised learning like reinforcement learning or like unsupervised learning. There's a whole lot of things that the brain does that we don't have neat machine learning metaphor. There's, there's, there's some way to go. But the the term neural network seems to be here to stay. All right, given that we can write neural networks as computation graphs. We can also write loss functions as computation graphs. So that is you can write loss functions within the innermost SGD operation. And also you may have observed their plug and play. Right. So if I have this expression, right, I've already kind of determined some computation graph for say the first term of the song. I can just clone that in whatever form and then just drop it in. Right. And then all I have to do is, if I need to reference a term that is referenced in this in this sub graph, I can't, it's really no big deal. And then finally, you know, we can, so we can basically construct this graph and just like use it in someone else's program, which is basically if you have used TensorFlow and PyTorch, that's what you're doing. You'll specify what you want your layers to look like. It's going to create a computation graph for them that allows you to do all these operations. And there's a whole bunch of like C++ or whatever that happens under the hood that you never have to touch. Right. And therefore this allows us to make efficient gradient computations because we're going to write the entire thing in say Python, the underlying C++ or C or, you know, assembly code or whatever that is so much faster is doing all of that for you. And so you get a significant speed up with these packages. All right. So finally, if we have some operation, right, H is going to be some nonlinear function we'll call the tanh over Wx plus b or my weighted sum. And then why my output would be some other weights v times h plus a, a would be like another that other bias. This can be represented, our two-layered node can be represented something like this. So I've got my weights and I've got my input. And then I've got a bias. This is explicitly representing the bias as its own term, but as shown, we can just consider this to be another matrix. And so now we add these, we take the tanh function. This gives me my output that goes in as the missing input that it's looking for for this other other function. And here are some other weights v and then I multiply them and then I add some bias and there might be some other nonlinear function here that I want to apply. So plug and play neural networks, courtesy of computation graphs, very simple, allow us to do, to do certain things. And, you know, like I mentioned, artificial neurons kind of loosely mimic some functions of biological neurons, that they have the advantage of being able to find patterns in this is from my NLP class. So I say language data, but generally just like data in the large, we don't have to do manual feature selection. Like we don't have to try and figure out which power I need to raise some term to in order to fit my linear function. But they can take quite a bit of time to set up correctly as you may be observing. And also the way that you represent the input data is very important. Everything's still got to be numeric. And what we want is we want to capture something about this is kind of, again, NLP specific but we just want to find a way to capture important features without trying to extract them manually, to at least to the extent that we are, we do not have to. Okay, so why all that about computation graphs, because this goes into the topic of the main lecture, and we're actually still a day ahead, which is good. And I anticipate this will be able to finish this with no trouble today. So today I'm going to talk about pytorch particularly the auto grad capability, and then also the says and module I'm actually not sure if we cover that here in the next one. But either way, we'll talk about it auto grad and hopefully you'll see kind of how the construction of the computation graph allows us to use these features of pytorch, you know, with sort of a single line of code rather than having to write say a dozen as you may have had to do for for a second or two. Okay, so, first of all, who here has used pytorch before. Alright, so we will get you will get some practice in this in this class. So starting here and then in homework three. And then, subsequently, you'll get the ability to kind of play around with pytorch see how it works differently from say the NumPy version, or similarly and then also see how we can get significant speed benefit so why use pytorch well obviously you probably aware of some of the benefits of pytorch is that it's faster, as I alluded to, it's more efficient as I also alluded to provide support for GPUs right which speeds up everything as you saw on like that first lecture, significant amount and see that again. So, the code is usually easier to write right so if you see how I'm calculating the output of my prediction, what I'm going to do is I'm going to take, you know, why is, if I'm using the form from the code in the assignment. So, I'll take like the last output that I got, which would be the output from my previous layer times the weights, whatever it is, add the bias, and then apply my 10 h or my nonlinear function to and that gives me why. So, if you're having a little bit hard to keep straight in pytorch, you basically just define the hidden layer, and that becomes the function right as we saw in the computation graph, and then I just specify what the input to that function is, and we'll do the computation for me. So, the code is more opaque. And if you're like trying to figure out if you're trying to like infer what machine learning operations are from reading pytorch code is not going to be very easy, but having done some already you know what's going on in those hidden layers right now and you see why equals hidden layer of why you should at least have some intuitive understanding of kind of what that's doing. Okay. So, the, the core of pytorch and also other packages like tensor flow is this thing called a tensor. As we alluded to in lecture one, you know, tensors and NumPy arrays are both n dimensional arrays but they are not equivalent. Right. I'm on the lab machine is scroll is all fast. So tensors are a much more generalized form so remember that a tensor technically is basically a transformation, and the numerical array is sort of a form suitable to representing that transformation. So, the tensor being a transformation is a function. And so that we have the tensor as say, a weight array living in a node. That's the function that gets applied over some input. So, the torch package contains these classes and functions that are mostly very similar to NumPy. So, if you do like V stack and H stack and all the array manipulation you can do in NumPy, you can do a lot of the same stuff in pytorch. It also provides convenient functions to convert back and forth, right, you want to minimize the use of these functions because it does take time to convert the NumPy data structure to the torch data structure so basically set up our data and NumPy converted to tensors, then you can run pytorch, when you want to do data analysis convert it back to NumPy arrays and you can do things like, you know, make plots and all that stuff. Okay, so let's just view this in action right so if I just create some data this is now just a list, right, this is just a list of lists. As we know I can take a list and turn it into an array using the NumPy dot array function, this is going to give me this. So now I have a two dimensional three by three array, representing this data. If I look at the type, this is going to tell me okay it's made up of 64 bit floats, and it's a NumPy and D array. So if I create torch dot tensor using the data, it's going to give me something very similar. So I can use the same function or the sort of the same function equivalent in torch, right so instead of creating an array and creating a tensor, but I can I can do that operation over a list. I can also do that operation over a right give me the same thing. So now is this represented now this is a torch tensor, and it is this one, and it is composed of 32 bit floats. So, a little bit different there at the NumPy version is 64 bit floats the torch version 32 bit floats by default. Of course you can specify with the D type, which kind you actually want to use. So what I can do is I do torch from NumPy of a, this is going to give me the same numbers, but now it explicitly says that the D type is torch dot float 64 because it was created from 64 bit floats from NumPy. Right. And then running the, the type and the D type will confirm that. I can also do torch as tensor. Right. And this is going to give me pretty much the same thing. So again, same numbers, right mostly I'm taking this from a, and this is going to create 64 bit floats. So, and so now basically C and D contain the same information and contain the same data type. Okay, so the next one. So now what I can do is I can take D, right this thing that is created and turn it back into a NumPy array. So now this is going to be an array. And if I take the type of this it's going to say okay now this is an ND array again, and it is a float 64 data type. So, what's the distinction, right, distinction is torch dot tensor makes a copy of the data, whereas torch dot from NumPy and torch dot as tensor do not copy the data. So, remember what happened when we were doing in the first part of the Adam notebook. What happens if you do a deep copy versus a shallow copy so if I do, if I make a shallow copy. What happens if I change something about the original copy if I make a shallow copy and change something about the copy. It does change the original right if I make a deep copy. The two are completely separate there you copy the data to a different place in memory, let me do whatever I want with this new copy and nothing about the original will change. So, I ran into this. Like yesterday when I was doing some manipulation for doing some some data and like I changed the class labels for something and all of a sudden all my plots like the colors are all wrong. It's like, I got to rerun the model. So, be careful. Okay, so here's a again. So now what I can do is, if I, oh god. Okay, so if I create this tensor from using the from NumPy function right so this is B as a tensor. And now I set that first element to 42.42 Okay, here's a. And here's B. So this is that shallow copy and by changing a I've also changed B. So now I'm having basically torture is is referencing the same place in memory for the actual data. So now I've got sort of a data structure that is of the right type to to feed it into into pytorch. If I tried to put a into a pytorch neural network it would give me some errors saying you know was expecting tensor but got NumPy array, but if I put B and it would be fine, although the numerical data is the same. So now I could do torch dot tensor, right, which copy is the data, as noted. And then, oh god, get this wrong. The scroll set to a different direction. Okay, so here's B, right, and then B 00 is 42.42. Now I said a 00 to 12,345 okay there is, and B is still the same. So, torch dot tensor will create a deep copy. Now we can also use the at sign from matrix multiplication, as we do in NumPy. So here let me create some data. Right. And for a and B and I could do C equals a at B, and the shape of C is 10 by 20. Why is the shape of C 10 by 20 given what it was calculated from 10 by 5 and 5 by 20 those two inner terms are the same and so when you multiply you end up with the two outer terms as your shape. Okay, so don't need to do that about randomness I guess. So now if I do tensor versions. Right, so if I do the dot ran function just kind of the same as the NumPy random uniform. So it's going to give me the same thing right, but now when I put a CT dot shape it's going to be just torch dot size, same dimensions. So again I can pull the same values out of this is just represented slightly differently. So, now where we, we get with what we get with AutoGrad is basically the ability to calculate gradients automatically so I'll begin with these two quotes to deal with hyper planes and a 14 dimensional space visualize a 3D space and say 14 to yourself very loudly everyone does it. This is by Jeff Hinton, the father of deep learning, and I do, I think this works I think my technique is slightly different. So when I think of a high dimensional array literally just imagine a really really long three dimensional array. So our tiny like human meat brains are not good at conceptualizing things in more than three maybe four dimensions if you think of time as a fourth dimension. Beyond that, they're just you see these long strings of numbers and like this, this looks too dimensional to me or one dimensional or something. So I think what you're saying is that there's the value in every dimension that is orthogonal to all the previous ones. Once you get beyond three. This is not something that you can actually visualize very easily. What you need to I guess what you need to know is that vectors, even in high dimensions preserve those similarities across those high dimensions so things that are similar to each other, numerically or semantically will point in a similar direction and high dimensional And that operation is equally true in three dimensions as it is in 1000 dimensions. So, as Abraham Lincoln said this is pretty cool. He did actually say this. So in the Cooper Union speech in the section addressed to the southern people. He actually says this that is cool. I believe he meant it in the sense of like that is kind of cold or cold hearted. Nonetheless, if anyone tells that Abraham Lincoln didn't say this is cool, you can, you can show them the reference. Now I can't find my notebook anymore. Okay. So anyway, people are like pretty bad at calculus, just in general. And in more than three dimensions, it gets even worse. Right. There's so many numbers, you have to calculate the derivative for like every, every element of a high dimensional array with respect to all the by holding all the other values constant. And so really part of the reason that machine learning sometimes feels like magical is just as like, oh, so much math going on. That, yes, you could sit down and actually calculate like what's going on inside of a neural network in principle, if you had all the time in the world and the super powerful computer. But doing those operations you know by hand or like line by line would take greater than the lifetime of the universe to actually do. So of course no one actually does it. So we basically take the workout by by doing by using matrices. And then, as we mentioned, we have GPUs that are optimized for vector math and matrix multiplication, so we can get this big speed speed up with the GPU hardware, but there's still explainable math that's happening at each step. If you were if you had the ability to zoom in on a single iteration, a single gradient update, we will see the operations that we saw happening in lecture five or like updating a single weight is just remember all that's happening at a massive scale. And we think about modern applications, it is just truly mind boggling. So that makes it impractical to calculate these gradients for these giant gigantic composite functions, especially in the high dimensional space. So for example, you know, what's the derivative of sine of x, cosine of x right so we learned that in in trigonometry. And so we can see like how we can actually calculate this. Right, so they create on 100 evenly spaced points from negative two pi to two pi, and then plot the sign. Right. So, why equals sine of x and then the derivative would be cosine of x, I can plot these right this is what we would expect. So, now what I can do is I can, I can do this using pipe torch and tensors. So I can use the value of sine of x, right those numbers that I created, I'll create a tensor from that array. Here's the tensor. And so then, currently I look at this, this argument is a parameter of this tensor requires grad. I'll get to what requires grad in a minute. So it says false. So right now requires it does not require grad or required gradient. So I set that to true, right, just by, you'll see the function version has an extra underscore there. So I set requires granted true. Now I can see that it will give me this extra little bit of information at the end so we'll set that set requires grad equal true and will tell me that whatever I print out the tensor. So now when I'm writing requires grad equal true, I'm forming this computation graph, this backwards graph that is the history of every operation. So, when we think about how a node can calculate itself its own values from its input, and then also its derivative, and those values are going to be dependent upon the children, or the parents, the values of the children subsequent nodes are going to be dependent upon the values of the parents. So now I'm going to create this graph and so this backwards graph is going to be this history, right, so if you think about the actual operations being performed. If you start from the input you perform an operation over you perform a few executed function over that operation, you then take that value another operations performed over and so on and so on and so on. And so, each of those operations can also be part of the graph. And so setting requires grant through will allow me to basically automatically calculate every component of that sub graph every every component of the sub graph within that graph, as I'm performing operations. Alright, so this will aid in calculating these composite gradients. So I'll just look at this grad FN it currently says none will start assigning values to it soon. So now I can define the sine function, right, so of course I'm going to define why being sine of x. And so now here's why if T. And so here's the sign you can see that we're basically starting from a value really close to zero, and we're ending up we're getting closer and closer to one and then eventually we end up with close to zero again. So, no doc stream for that one. So the shape of this is 100. So, just a tensor that has 100 100 elements in it. Okay, so now if I look at the backwards, so why t dot backward. It's confused the gradient of the current tensor, with respect to the graph leaves graph is differentiating using the chain rule we've seen that before. So it accumulates gradients in the leaves. So you may need to zero grad attributes to them to none before calling it we'll see what happens if you don't zero out your gradient. So, backward will basically compute the derivative for every x in y t that has required requires grad set to true. So, why t is basically constructed by in this case performing the sine function over some, some other opera some other value in that case, So this will basically compute the gradient for every value in that in that array or in that tensor. Okay. So, so now here, if I just do dot backwards over an array of one 100 ones. So this argument will represent the gradient of y sub t, with respect to itself, and it's going to be all ones. So now if I look at xt dot grad. God, there it is. Okay, so now if I look at xt dot grad right I did, I did y t dot backwards with the gradient with respect to itself, not to look at xt dot grad, right we started one. We go down to zero. We end up back at one. And this is now actually the cosine of x. Right, so I started with just some, some values, I computed the sign for those values. I took the gradient of the sign. And now that original input that x is now set to cosine. How did that happen. So let's look at how we compute, why t right why t is generated by taking the sine function over xt. And since why t is created by performing this operation over xt. If I run why t dot backwards, this will keep a record of updated gradients for xt because xt was one of those arguments in the computation graph that was used to create why t. So this only does it if xt requires grad is set to true, which we did earlier. So, therefore, you just need to be alert to your in place calculations, right so if I'm doing operations over something that uses something else as an argument. If that other, if that argument has requires grad set true to it I may end up calculating the gradient of why with respect to that in the original input. So you got to be careful, right, you may have it you may not actually want to do that. So you have to you have to make sure that requires grad is set to true, only when you need it to be. So let's visualize some of these operations. So first you can see what's going to happen. I just want to plot xt versus yt, and then it's going to get mad at me. So why did it get mad at me, why do you think it got mad at me. Sorry. So dimensions. Let's see what happens. What it wanted so it didn't say anything about the dimensions. So xt and yt should both have 100 elements. The error is, can't call numpy and tensor that requires grad use tensor dot detach dot numpy instead so remembering all those operations to convert between numpy arrays and tensors. Those only will work, assuming that basically the tensor is not attached to the computation graph. And so, if I tried to do that it's basically saying hey I'm not done with my computations over this, don't try to take it away from me. But of course, pi plot being pi plot. So it needs to be a numpy array, I can't turn it back into a numpy array until I detach it from the computation graph. So there's this neat function dot detach that you can use to basically say, I've got this tensor, I don't want to do something with it like visualize it or use it as you know an input feature to another operation or something. I need to detach it from the current computation graphs remember when that docstring said that dot backwards accumulates gradients in the leaves. Right. This is one of those leaves. So I need to detach the leaf from the tree that is the graph in order to use it anywhere else. So we will use the detach dot numpy function and again when you're doing assignment, I want to say three. You may run into this issue. So tensor dot detach will return a new tensor, this will be detached from the current graph. And this result will not require grad. So if I have something that requires grad and I detach it from the computation graph, there's not much point in having requires grad or what I'm going to use requires grad for if I'm detaching it from the graph, there's no graph to compute the gradient. So I can do whatever I want with it and not worry about changing that due to other operations having been performed on the graph. And then the dot numpy will convert the tensor to a numpy array. So that's going to go back in familiar territory. So now here we go. So here is xt and yt right that is the blue line. So we have xt being the original input and then yt being the design function and then xt dot detach right that will give me the same x values and then xt dot grad is going to give me the derivative of y. So that's sort of that computation graph magic that we just discussed. And so now, instead of plotting, you know, two separate functions, sine of x and cosine of x, I'm really just using properties of the same two, the same two variables xt and yt to get the values that I need. So, questions. Be a little bit mind bending. So where are we at now? Like halfway through actually have to move faster. Okay. So, now, let me do yt equals the sine of xt, and then I'll run yt dot gradient again, or yt dot backwards. And so now xt dot grad gives me something like this. Hang on, we started to go down close to zero, and then the back at two. But I do know that the derivative of sine of x is cosine of x right that hasn't changed. So what happened. Let me plot it. Now we can see this. Right, so now it's telling me that the blue line is still sine of x, and the orange line computed by xt dot grad now is like cosine of x except it's bounded at two and negative two. So what gives I didn't actually change anything. So the magnitude of the derivative is twice what it should be. And this is because dot backwards is going to add the gradient values to the previous values. So I'd already calculated one gradient that was effectively cosine of x. Then I did it again, but cosine of x is still there. So it's like, okay, I'm just going to take whatever this is and I'm just going to add it to this. So now, my gradient is sitting there at two times the cosine of x. So we must explicitly zero out the gradient. If I do this, then the same addition is true, right, whenever I compute the gradient is going to add it to whatever whatever is in the gradient. But if the gradient is zeroed out, that's zero. And so what I get is the actual gradient that I want. Okay, so use this dot grad zero function, and this will print out the, this will zero out the gradient. So now what I can do here is if I do, you know, four, 10 times, I'll just create, I'll take the gradient of the sine function, and I'll print out that first element. Right. So the first the first term should be one for cosine. Now if I do this, you'll see that I get, you know, one, and then two, and then three, and then four, and then five, all the way to 10. Because it's every time I run this, it's adding that gradient. So I get that accumulation of values. So we add one to the existing value. If I put the grad zero inside of the for loop. Now it's 11111111 so I keep getting the right value here. Okay, so always know you need to be zeroing out your gradients. All right, so now, having done that, what we're going to do, we'll run through some examples of training some simple models using SGD and PyTorch. I'll compare the NumPy version and then the PyTorch version. So here's the NumPy version. This should look pretty familiar to you. So we see our X's and then my T is just not some nonlinear function, a square of X. I'm just going to use my SGD implementation that's similar to what we did in assignment one. And that gives me this. Right. So here is my T and this function Y. This is like the best linear function that I can fit to this. Right. So I'm still doing just linear operations. Now we do it in Torch. So what's different here, right, I'm creating these tensors from the NumPy arrays, and then I create my weights as Torch.zeros. I'm not using autograd yet. I'm still just doing the calculation manually. So gives me an error, expected scalar type long, but found float. So if I look at X, it says it's at 64 bit int. So this error is actually a little bit misleading. In fact, it suggests that some float should be an int, but X is already made of ints. But W is now made of floats. So rather than make W into ints, because integers do not make good weights, typically will make X into floats. Okay. So let me run a version of this again. All I've done is I've just added dot float here. And now this works just fine. Very similar result. Okay. So why are we actually using Torch if it looks identical to our NumPy code? What's the point of this exercise? And just to get you thinking about computation graphs, apparently. So where this really shines, let's take advantage of automated gradients. So if I've got this computation graph, the reason that I do it is that I can automate the calculation of gradients. So why am I dealing around here with this for loop when I can do everything much more cleanly? Okay. So here we go. So same up until this point here. Now instead of the update, I'm causing just calling MSE.backwards. Right. So MSE in this case, this is my loss function. And so to update the gradient, I just call whatever instance of my loss function I've created and just call dot backwards on that. Okay. Because the dot backwards calculates the gradient. So when I was doing like YT dot backwards and YT was the sine function, it's going to calculate the derivative of the sine function. But I'm always calculating the derivative of the loss function. And so I just define what that is and then call dot backwards on that. And then finally, I also do with Torch, no grads. So I can kind of not update the gradient. I can sort of do a temporary. I don't really detach anything from the computation graph. I just sort of pause the reading calculations on that so that I can use that to update the actual weights here. So you can compare that to what we did before in the commented code. And this gives me, again, pretty much the same result. Other things you can do is you can use predefined optimizers. So here, you know, I'm defining, you know, what define optimization function, you know, manually. So here I can do this using some predefined function. So where previously I had defined my SGD operation, I can actually just pull that out of the library. So this is going to look like this. So now I can basically instantiate my optimizer as an instance of Torch.optim.sgd. I can specify what weights I'm going to be optimizing and what the learning rate will be. And then after calling loss dot backwards, I can just call optimizer dot step. And this is basically this thing to perform one step of weight optimization. And so now for every training epoch in 100, I'm going to do one per step. And each time I'm going to zero out the gradient. All right. And this also gives me the same result. So, you know, rather than define SGD or Atom or what have you by ourselves, we can use this predefined optimizer class. And the storage of Optim is this package that implements a bunch of different optimization algorithms. All of them have like, you have to define different parameters for them, right? You have to find like the, of course, the ways you're going to optimize, but also for Atom, you need to define, say, those beta values and things like that. And at this link, you can see a list of all the implemented optimizers that you may want to use. Questions on that so far? The other thing we can do is we can use predefined loss functions. So what was my loss function to this point? What was I using? I'm just doing a regression problem. Mean squared error, right? So I'm trying to minimize the error. So here we go. This part, this is the definition of loss function right there. Now I can optimize that. But maybe I don't feel like doing that. Maybe I forgot the function for mean squared error. Maybe I'm doing a more complicated function like, you know, categorical cross entropy loss. Something that's like, I don't remember how to write this precisely and I don't want to mess it up. So PyTorch will give you the loss function like before. So here I can define this MSE func as torch.nn.msc loss. And so now I just calculate the MSE as this function, whatever it is, over my targets and my predictions. And then I just call it backwards and I can optimize my weights. So if I run this, I do get a problem. So what is this issue? Right. Found D type long but expected expected float. This is similar to the one above. So we see this where we had X made of ints and it wanted floats. This is cropping off the backwards pass. Inputs are T and Y. So we know Y is made of floats as it should be. So therefore the problem must be in T. And so now I can add dot float to T when I define it along with X. So the loss from it is the same. Boom, boom, boom. Here we go. Okay, I actually will talk about torch.nn module. Okay. NN stands, if you haven't guessed already, stands for neural net. So basically this allows me to create a lot of the infrastructure that I'm going to need for neural net and then define my specifics like how many layers and what types of optimizers and loss functions. This is not going to look simpler for our linear model, but if you try to apply it to multi-layered models it will. So all I have to do is define the specifics of my layer with a single line of code each. Okay. So number of inputs, number of outputs looks similar. And then everything is going to be torch.nn.sequential and then I define the layers here. So basically this is saying the only quote layer is just a linear function mapping from input to output. But if I had multiple fully connected layers, I would define each of those in turn. So now if I just print the model, you'll get this nice print out of what the model looks like. And so you can see what all the layers are. This allows you, like if you forget, say, what the input size to a particular layer should be, you can print out the entire model and say, okay, hey, layer four is expecting that things are going to be 256 elements. And so that's where my error is because my input somehow is not 256 elements. So if I print model.parameters, I get this basically this list of the parameters that I've solved for. So in this case, they're just they're random. Right. So I've just got some random weight and some random bias weight. So now I can actually use this to try and solve function. So this looks similar, except all I've done is I've added n inputs. I've added n samples and inputs. And so now I've also added y equals model x. So this is just saying my model is a function. I define it. However, I'm going to run that entire model over this input. So this is sort of the higher level version of running like hidden layer of y. There's all the hidden layers are all the layers in there. I'm going to run all of them single forward pass. And if I do that, then this gives me. Once again, I've solved the problem. You'll notice if you're paying attention, you'll notice that like these are not exactly the same solutions. In fact, some of them have like the orange line is like a little bit higher. So the slope is slightly less than the bias is probably a little bit higher. So remember, these are all estimations. You initialize weights randomly and it optimizes based on your error as best as it can. And so you may get slightly different outputs each time. It is, in fact, perfectly possible to write a neural network that has two inputs that are the number numbers two and three with a single unit. His activation function is a plus sign, and it's going to tell you the answer is four point nine nine nine nine nine. This is the thing that happens sometimes. So just be aware that this is all estimation. So by adding a hidden layer or two, then what I can do is I can describe the structure of the network like this. So we see here torsion and sequential. So one linear layer that maps from size of inputs to that first layer size. So in this case, both hidden layers are 10, 10, each function, right? Define my activation function to what function gets applied after I perform this operation. That output then goes into another linear layer that's going to map it to whatever size and hidden one is. And then another 10 H function that's going to map it to the linear, the output, whatever size that is, in this case, just one. And then everything else is pretty much the same. Run that. So now I'm actually get now instead of a linear function, right, it's able to optimize kind of the curve of the line. It's not like it understands that there's a squared function here or anything, but it is able to match those values pretty closely. We do get a bit of a weird zigzag here at the end. So maybe Adam will do better. So what I can do here is I can, for my optimizer, I will just define torsion.atom.adam instead of .sgd. And then I can change my learning rate appropriately. And then I run that. And there we go. Exactly where it needs to be. So basically optimize to this almost perfectly. OK, so now let's actually make use of the GPU. So it's trivial to move data and operations down to the GPU with PyTorch. So all we need to do, well, first I'll run it without the GPU. So here I'm just going to perform matrix multiplication for 1000 times over some random numbers and run this and we'll see that it takes about a second. So pretty fast. This is running on my lab machine. The CPU itself is quite powerful. But we do have this thing on this machine called CUDA. Torch.cuda.isAvailable will tell you whether your machine has CUDA or not. Generally good practice to write your code to accommodate either in case you end up on a machine that has no GPU or you can't find CUDA. At least you can still run even though it will probably be really slow. So now what I can do is I can do .to CUDA. This is going to say take this tensor and move it to the device. This is not done in line. You have to reassign the variable. So at.to CUDA is not going to change at. It's not actually going to move it. But I do at.to CUDA. Then every time I reference at, it will get the version that's on the GPU. So now we're still running on the CPU. So here we go. CT equals CT.to CPU. Using CUDA on the CPU is not much faster. It's actually slightly slower. Because CUDA is a GPU acceleration library. So why am I using it on the CPU? So what I can do here is I can define this function. Use GPU. And in this case, I have this code written so I can use the Linux machine or my other laptop, which I would have brought except you can't connect to the internet anymore. So now let's compare the speech of the torch.NN model on more data using the GPU and comparing it to CPU version. So first I'll just set use GPU equals false. And then I'll run this nonlinear model. Move everything to the GPU. And this is similar as before. Moving data to GPU. Moving data to GPU. Training took about a second again with GPU. And took 3.15 seconds without it. Okay. So now the torch.NN.module forward function. We just saw how to implement this using this combination of sequential and then linear and 10H layers. And the forward calculation for the neural network is defined this way. So now we can just define a class that extends torch.NN.module and define the forward function explicitly. So here I'll define an N net class that extends this class. And I can define the forward function that this is going to be very similar to the forward pass function in assignment two. So I start with, you know, I just set my y equal to my input. And now this allows me for each hidden layer, I just recompute y as the output or as the operation over the output of the last layer. And then this will give me the final output. So now I can set, you know, larger network, 100 nodes each, a larger learning rate. And then I can move the data to the GPU and run. And we can see that it takes about one second. So basically, the GPU magic makes it such that the larger model you have, the greater speed up you're going to get. The differences we see here with the simple model is like a factor of about three. But then as your model is growing, grow larger, it becomes a factor of 300, 3000 and so on. And so you can define your neural networks, pretty simple syntax, and all the other code kind of remains the same. And this allows you to very easily experiment with like different layer sizes and different hyper parameters and things like that. All right. I think we'll be it for today. So sit down. We're almost done. So, yeah, so we are I think we're done for today. So on Thursday, I will start introduction to classification. Good luck with the homework. And I have like a few minutes if you want to talk, but I'm going to have to leave early today.