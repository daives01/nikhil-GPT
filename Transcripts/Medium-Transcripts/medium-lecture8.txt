 Yeah, it's them. Yeah. Okay, let's get going. Very, very good chances. Class is going to be short today because if you look at the schedule today is the ninth and we actually finished this notebook on Tuesday. So we're actually like a day ahead and Raylu is a pretty short notebook. So I'm going to do that and then I will assign the second assignment and I didn't bring my GPU machine so I can't run half of the next notebook. So I'll just call it a day after assignment two and then I will set up my GPU laptop over the weekend. All right. So assignment one due tonight. If you don't have an extension, of course, some of you requested one, some of you have arranged one. So but for the rest of you, you if you hear me say this and you're like, oh shit, I did I forgot to request an extension and I really need one. I think the best thing for you to do is to get something turned in today. Okay. So for various reasons about how I run this class, it is much, much more to your advantage to turn things in on time, even if incomplete, than to try to spend a couple of days getting things wrapped up and then submitting it late because you will get a 10% late penalty, well, 10 points, not percent of the remaining grade, basically like minus 10 for every day that it's late. It is definitely in your best interest to get something turned in today, even if you feel it's not complete. Just trust me on that one. Okay. Are there any questions about? Yes. I lost our special one. Yeah. So yeah, I will have office hours at the usual time at starting at 330. I will try to give you feedback. So you have a rubric about different points that I will send you. Excuse me. What we try to do is basically say, count it off X points for not doing Y. If you do perfectly, we'll say like great job or something. So if you get 100, don't expect a lot of feedback. If you do something like super cool in your notebook that I can't resist commenting on, don't do it. But if you just checked all the boxes, you know, just take your 100%. If you do get counted off, we'll try to indicate what that is for. Other questions? All right, cool. So what we are going to talk about today is ReLU. Who knows what ReLU is? A few of you. So we talked about activation functions. Activation functions are these nonlinear functions that we apply to the output of a hidden layer. And meaning that when we back propagate the error through the network, the weights in that hidden layer are effectively optimized with the assumption that a certain nonlinear function will be applied to that output. So that is if I'm applying the tanh function to something and I want a specific target result and then there's some transformation that needs to be applied before applying the tanh, the particularities of that transformation are going to be different assuming the tanh is there versus when it's not. Right. So that's the activation function. The activation function is constant. It's usually not parameterized very much, except in maybe some very specific ways. But basically it's what you typically do is you apply an activation function to an entire layer. Right. So you've heard of this thing called ReLU, which is this weird term that people use all the time and no one really knows what it means. So my goal here is to basically demystify machine learning in this class. And so one of the goals is to tell you what all these weird incantations actually mean. So if we're going to address nonlinear problems, and assignment two is going to be nonlinear regression using neural networks, we need some way to include nonlinearity. We do that with activation functions and any sort of nonlinear function can in principle be used to introduce nonlinearity. Now, if I'm assuming that my activation function after my transformation of my inputs by my weights is the tanh function, then my optimization is going to be performed in a certain way so that my weights end up at certain values. If I change that function, of course, when I perform the same optimization operation, the weights are going to end up different. The reason for that is because when we are performing back propagation into hidden layers, one of the things we have to incorporate is the derivative of whatever nonlinearity that we apply at the output of that layer. So if we had something that you can do, and it's not really clear why you would do this, is if you have a neural network where you have no activation function on any layer, and it's just a linear pass through, you still end up with just a convoluted way of solving a linear. And so what that means is that when you're applying these nonlinearities, when you're applying the computed, if you had no nonlinearity, if you remember how we performed back propagation the output layer, it's just the error term times the input to that layer, then if I had no nonlinearity on my hidden layer, when I'm trying to apply back propagation to that hidden layer, and the derivative of a linear function is just 1, x equals x, then I'm not applying any extra term to my back propagation operation. And so if that were the case, then the backdrop for the hidden layer would be the same as the backdrop for the output layer. So what that means is that when I change the activation function, I just change that one term, the derivative of whatever that activation function is. Given that, we of course need our activation functions to have some desirable properties so that when you take the derivative, values don't vanish, they don't go out of control, they allow you to keep optimizing and taking the appropriate size step down the gradient toward that global minimum. So as you've probably read, because I was kind of rambling, we have the desirable properties include computationally simple, because we want these to be fast, we're going to be applying them at a high number at a large scale, you don't want to be computing some arbitrarily complex function for multiple units and multiple layers. We also want for initial small weight values, that function should be close to linear, because as the weight magnitudes grow, we want the function to be increasingly nonlinear, such that when you reach magnitudes of extremes, you're not having those values greatly affect the input much more than input values that are maybe closer to zero. Again, the derivative of the function should be computationally simple for the same reasons as item number one, I'm going to be taking the derivative during backprop again at exactly the same number of activation operations that I formed during the forward pass. So that must also be computationally simple. And then the magnitude of the derivative should decrease as the weight magnitudes grow, and this should perhaps approach some sort of asymptote, so that if I have an extremely high value, it's not disproportionately affecting the adjustment in weights during backpropagation. And then finally, the maximum value of the magnitude of the derivative is limited. And as we will see, some of these properties are more desirable than others, and in some of them we can actually let slide a bit when certain other conditions are met. So given these properties that we identified previously, we also identified two functions and their derivatives that have these properties. One is the sigmoid function, given by this, and this derivative is sigmoid of x times one minus sigmoid of x, and the tanh function given by this, where its derivative is one minus tanh squared. Both of these obey these properties. Sigmoid, of course, is this S-shaped curve bounded between which value and which value? Zero and one, right? And the tanh function is bounded between negative one and one. So these two functions effectively have the same shape, just with different bounds. And so then the derivatives also have kind of, they have basically a hump in the center. So it's just that the maximum value of that hump is different for the two functions. All right. What do these functions actually do? Let's take a look at them. So we'll plot them with code that you've seen before. So I will define sigmoid function, the tanh function, and their respective derivatives and plot them using different types of lines. So here is the plot. This is what we see. So the blue lines, that's the sigmoid and its derivative. Red line, that's the tanh and its derivative. And so we can see how they obey these nice properties where, when I'm close to zero, this is more or less a linear function, both of these. The maximum values as the inputs go to extremes are bounded. Same is true for the derivative. It has a limited maximum value for sigmoid. It's 0.25. For tanh, it's one. And then also that the derivative decreases towards zero as the values go to extremes. OK. So we see the derivative of the sigmoid and the tanh function decreases as x gets further away from zero. Now remember the three things that we need to actually compute a weight update over some predicted output. So one, the learning rate. Generally speaking, what's the size of the step that I'm taking along the gradient? The error between the output and some target. How wrong am I? If I'm very wrong, the total step that I want to take would be bigger, scaled by learning rate. And then the gradient at z. How steep is the slope? If I'm trying to approach the bottom, I'm trying to approach a place where the slope is effectively zero. And if I'm on a steeper portion of that gradient, then that's going to affect how far I move down. So absent fancy things like atom, which also further adjusts the total size of that step based on things like previous weighted averages and variances. These are the things that you need. So you take some combination of all of these, and this determines how far and in which direction I need to be moving along my gradient. So what this means is that if the gradient is small, then the weight update is likely to be small as well. Generally speaking, this is what we want. We don't want weight updates to be too huge and shoot us off into the abyss or actually trying to approach the abyss, the bottom of the gradient. That's going to shoot us off into space up the curve of the gradient. So we like these derivative functions that don't explode as the magnitude of x gets larger. And we don't want that weight up to be too large such that we miss this global here. I should change that to global. We don't miss the global optimum entirely. OK. But if the activation functions are too small, there are some downsides as well. So if you have some function h, that's our activation function. And if the output of that function is too close to 0, then the outputs in that layer with that function will be close to 0. And sigmoid actually has this problem, which is one of the reasons why you prefer tanh. Sigmoid, we have really negative values, which may often be the case. You have values close to 0. And so as opposed to tanh, where the set of values close to 0 is pretty small, tanh of 0 is 0. And as I move away from 0, it linearly also moves away from 0 until I start to reach more extreme values. Then also, if the derivative of that function is too close to 0, then you can't communicate useful information to back propagate the weights. Because I'm going to be multiplying the change in weights by, among other things, the derivative of the activation function. And if that's really close to 0 or is equal to 0, there will be no change. This last point in multiple applications is generally known as the vanishing gradient problem. And it shows up in a number of different circumstances, most particularly in RNNs. We'll talk about that briefly later in the course. And among other things, what this can do is it can cause the training of the neural network to just slow to a crawl or stop. Because if you end up with a place where all of your gradient derivatives are 0 or a bunch of them are, suddenly individual neurons that might be useful for the problem, those weights are no longer updated. So I could end up with my weights at a place where they're arbitrarily far away from the best solution given that neuron in that layer. But it has no useful information about which way to go. So it just stops. And we don't want that, of course. And so both the sigmoid function and the tanh function can suffer from this problem. Everybody clear on the motivation so far? OK, so we use this thing called ReLU. ReLU stands for rectified RE linear unit. So the unit here refers to the neuron. So we have a neuron that this ReLU function is applied to it, and we'll call it a ReLU unit. And so you can also have ReLU layers where the entire layer uses ReLU activation. And we also talk about ReLU networks where all the layers, all the neurons use the ReLU activation. And it turns out that these ReLU networks actually have some very interesting properties that we'll discuss briefly at the end. So in the example so far, we've been using the tanh function for everything. It's like this is our default non-linearity. This is the only one we've explored so far. We're just going to apply the tanh function to everything. We don't really talk about tanh networks so much, but you could. That would just be a neural network where you use nothing but the tanh function. ReLU networks, because they have some interesting properties, tend to be considered a single group because those properties apply or appear to apply to all ReLU networks. The function itself looks like this. So already you can see that unlike tanh, this is a what kind of function? Piecewise function. So it's zero if the value of the input is greater than or equal to zero. Otherwise it's x if x is greater than zero. In other words, this is basically taking the max of zero and x. So there are multiple ways to write this function. And we will use the for the for our implementation, we'll just use np.max. So if I have some input a and I take np.max of a, I get this. So all it's doing is just taking the positive part of x. So if I am zero or less, the output is zero. If I am greater than zero, the output is x. There are some small debates about whether you need to bound it at zero. So it's like less than zero, it's going to be zero. Or if it's greater than or equal to zero, it's going to be x. We end up mathematically end up in the same place. Although I think people who study like real deeply the mathematics of machine learning may actually have some strong opinions on this, because based on where you make that boundary, sometimes the properties of the network may actually change. But for our purposes, I don't think you really need to worry about that. So here's our function. And it simply takes the positive portion of x. So what's Rayleigh's derivative? Right. So we'll describe it here and you can tell me what it thinks, what you think it looks like. So if x is greater than or if x is less than or equal to zero, then Rayleigh of x is a constant zero. So the derivative of any constant is also zero. So for this portion of the function where it's less than or equal to zero, the derivative of that should also be zero. So that should make sense. Now, if Rayleigh of x equals x, the derivative of y equals x is one. So we should all know that. And so therefore, if x is greater than zero, then d Rayleigh dx is equal to one. So the derivative of Rayleigh, also known as the rectifier function, is called the heavy side step function. So what do you think the derivative of this function looks like when I show it next? So I see a line. Yes. A line and then another line. What do we think? So we define d Rayleigh as the heavy step function. We get this. And this is what we said. We, of course, are assuming it's we're drawing it like the continuous function. So it does draw does its best to draw a vertical line here. And really, it's drawing it between negative point zero or between zero and point zero, zero, five or something, because we're sampling 100 points. But yeah, what we're seeing is it's zero from negative infinity to zero. And then suddenly, it's one. OK. So let's see what actually happens when we apply this to some weights. So first of all, I'll define this term s being a weight sum. And then I will just take create some random numbers. So let's assume that these are inputs to a Rayleigh function. So first of all, let's see what happens if I take the tanh function to this. So I get this. So if these are my original inputs, this is the tanh of that. So tanh of zero is zero. The tanh of the negative numbers are increasingly negative as the negative number, the magnitude of the negative number increases, and then also increasingly positive bounded at one as the magnitude increases. So basically, what we have is I have a bunch of a bunch of weights and sort of squishing them into a distribution between zero and one. Rayleigh, I can also basically take a function like this for some input s. For those elements of s that are less than zero, I will set them equal to zero. And then that will give me the output. So Rayleigh of s looks like this. So this is tanh of s where everything is squished into that s-shaped curve. And this is Rayleigh of s where all the negative values are simply set to zero. So if I take there's s again. So now, what about the gradient of s? So, well, I guess point here, just this is one of those peculiarities of memory. So here I put in s and then I modified s in the function. So I return s and I can take Rayleigh of s and it's going to give me this. And then if I print s again, because I modified s there, it's going to give me s. So just to be careful in this implementation, what we'll do is we'll make a copy of s. So I actually have a deep copy of s. I'll store that as y and then y will be the Rayleigh version of s. So I can return y and then if I print y. Oh, what's going to happen? s and y. There we go. All right. Now, what about the gradient of s? So I will just define again, d Rayleigh. I'm sorry. The gradient of s. s and well, so s, this s is modified in line, right? So this is modified in sort of this version, this non-memory safe version of the function that I wrote. So really, s, what we want to be doing is returning s to the original value here. So if I were to do this again. So. All right, so let's start from the top here. All right, so I'm going to define s. OK, now, s is this. I'll take its tanh. OK, whoo, whoo. Let me define Rayleigh using s inside the function. I'm going to take the Rayleigh of s. OK, now I get s with all the values, negative values to zero. Oops, I changed s in line in the function. So I actually changed the value. Of course, if I apply Rayleigh to this, I will get the same thing because all it's doing is taking values that are zero or less and setting them to zero. And so the only thing that gets, quote, changed here are things that are already zero. So nothing actually changes. So a better version of Rayleigh will make a deep copy of s. Of course, this takes more memory. So let me define that function. Let me redefine s to what it was previously. So now I have the original s, y equals Rayleigh of s, y. And then s again. So there's the original s. So now I have these. All right, so now what about the gradient of s? So let me define my derayleigh function. So s will be just n samples by n units. First, I'll just create a deep copy. And so now I will just define the heavy sidestep function in line. So here we have dy, where those elements of it are less than zero. Those become zero. Where they're greater than zero, they become one. Now I still have to deal with zero. So here, because every value in the function has to be given an output value, I don't want to be, I don't want to define, I don't want to end the function here. Because then if I put in a zero, it'll give me some error, probably. So I will just decide that, effectively, the same thing is true, where if I just set this equal to zero, I'm doing the same thing as if I did that. So now derayleigh of s gives me this. So all the negative values have become zeros. All the positive values become one. Then there is one zero in there. That remains. OK, so there's that again. And there's just multiple ways of doing this. So I can take the, I can just copy Rayleigh. So there's another alternative way of doing this would be because Rayleigh is already setting those negative values to zero. I just copy the Rayleigh output, which where the negative values were already set to zero and the derivative of those would also be zero. And then all I have to mess with is now zero or the positive values. So that's one way. Gives me the same thing. And then another way would just be to make a copy of s. And then I'm just going to return basically a Boolean where if dy is greater than zero, this would be true, which if I do as type int, that turns it into a one. So this is basically just saying all of those all those elements of this input that are greater than zero will become true or one and everything else will become false or zero. So these are more or less efficient ways of performing the same operation. OK, so now the weird part, right? Why does this actually work? Because it doesn't really seem very intuitive that this piecewise linear function would be able to introduce nonlinearity into a neural network. Right. Why would this work? It's not totally linear in that it's got these two pieces, but it lacks the clean curves of the sigmoid of the 10h and just sort of looks like I lopped off the bottom half of my linear function for no reason. So why does this work? So let's remember that the sort of modeling curves like the following adapted from notebook five, just I'm using the 10h function. So again, what I'm going to do here is I'm going to create some some inputs and then apply a known nonlinear function to them. So in this case is just, you know, this is this is like basically linear function with some random noise. Then we're going to train a model to fit that. This is the same. This is just doing linear regression, except I include a 10h function here. So I'm sort of trying to optimize my weights with the 10h in the mix to model on a linear operation. So this gives me this. These are my actual weight values, weights and bias. And here's the model. Here's what the model predicts. The orange line. Right. So you can see that it gives me a very slightly curved line. This is effectively trying to do its best to straighten the 10h function. So it's applying weights that are really small, such that when I apply the 10h function to it, I get something that's like really close to the original input. OK, so there's a bit of a curve here. If I were to train for longer, probably this curve would flatten out a little bit. So what these functions do is they're going to bend the otherwise linear regression to better fit the data. So you may observe that by adding this noise, there is actually because of the random sampling, there actually is a bit of a curve to the data. Right. So if I were to model this purely with a linear function, maybe it wouldn't be quite as good a fit as I did it with the 10h function. So ReLU does effectively the same thing, except instead of having a smooth differentiable curve all the way through, it's picking one point in the line and making one bend. OK. So this is a lot more computationally efficient, obviously, because it's just it's really just a linear function with a bit of a twist, but it's also less precise. So let's say that I have this data below and they have these blue points and these orange points. And let's say that I wanted to somehow draw a line to separate them as cleanly as possible. Right. So what I'm trying to do is I'm trying to draw a line that will be about here and then here, you know, going to try and fit it as closely as possible may not quite get it because these points really closely overlapped and then go down again. OK. So we could probably do this with a 10h layer or two. So if you remember from Notebook 5, we had two distributions to these random noise samples where one kind of curved up and then one kind of curved down. And we can just apply a 10h function to a linear function and kind of fit that if there's a single curve. Then we had one that sort of more like a parabola. And so now I want a function that's going to go down and then go back up again. And you can do that with a single 10h, but I could do that with, say, two 10h functions. One that would put the curve for the negative values and then become flat. And then one that starts out flat and then at zero starts to climb. And so then if my weights, if I have two hidden units, each of which has a 10h function on it and the weights in one are effectively taking the negative values and making them close to zero, that's going to give me that curve that starts flat and rises. Then I have another one that takes the very positive values and makes them close to zero. That's going to give me the one that starts to fall and then ends up flattening. And then I add those two together and I'll get that nice curve. So that's how a 10h layer would do it. So this would basically draw this like smooth envelope around the orange points. Regularly with a single bend would have a much harder time, of course. Right. So it might, you might be able to infer that like, okay, I get one line and one bend. So I'm going to put my bend here where my cursor is, and I'm going to have straight lines on their side. Okay. Probably not too bad of a fit, although you do risk having certain points like maybe this one that fall on the wrong side of the line. So this is going to be less precise. But what if you have a bunch of different railway units, and each of them is going to be optimized to put that bend in the right place. This could approximate the same curve. So it's sort of like, instead of drawing a circle, you draw like a 36 sided polygon. It's going to look a lot like a circle. Okay. Or 100 sided polygon or your pick your number, the more you the more you get, the better it's going to look like a smooth curve. So by these railway units working together, this would allow us to approximate the same curve. So outside of this nonlinear function, all the neural network operation is doing for every unit is y equals wx. So that's just moving the line around, and then, and then stretching it, you know, rotating it, basically choosing the slope. And so then you moved around by some bias B to figure out where to place it. And so the right bias vector, it will allow you to choose smart places to place each of those bends and then the remaining ways will sort of tell you how I'm supposed to, how I'm supposed to rotate the entire curve. And so then each of these, let's say what I could do is I could optimize for like a small piece of the curve is kind of a shallow bend like that. That's going to handle some part of maybe from all the values like up to negative four or something like that, or up to negative two. And then I find another one that is optimized to place the bend in a slightly different region, and then so on and so on. And then I add all those together, and it's going to do a pretty good job of creating something that looks like that curve that I want to separate these regions. So what that means that ReLU's real strength is in numbers. And so we can have a really large number of these rectifier units that actually approximate the nonlinear behavior of functions like tanh. And they do it, even though you're using them in greater numbers because it's so much faster to calculate the ReLU function, they actually do it with greater speed and greater efficiency. So if you want more on like how does this work, there's this explainer here that you can check out. So these are some pretty nice examples of trying to separate, you know, sort of curved regions in data. All right, questions. Okay. All right, so now back prop is relatively straightforward, because as I've kind of foreshadowed already, the only thing that you really need to change is the derivative of that activation function. So it's not as if there were no derivative back prop in the hidden layer is the same as back prop in the outer layer. So all the only thing that I'm adding is the derivative of the activation function. So we have, remember, for our neural network operation, if V is some hidden layer, I'm going to multiply X by that, apply some activation function H, gives me some scalar value Z. I add a bias to those, and then multiply those by output layer weights W, and that gives me my prediction. So here is back prop. Let's just look at the W line first. This is back prop in the output layer. So this is going to be T minus Y. This is my error term, how wrong I am. I'm going to multiply that by Z, which is the input to that layer. I'm going to do this for all of my samples for the entire dimensionality of the output. Right. And then there's some learning rate in there. So the same thing, all I'm doing is, all I'm changing is basically this part of the equation, when I go from the output layer to the hidden layer. So now instead of just the error term, taking the error term times the output weights times the derivative of Z, in this case, the input to that layer. So what is Z? Well, Z is actually going to be a function applied, a function H applied over X, V. So that has to be incorporated into this operation. And so I need to know what the derivative of an activation function is. If H is tan H, then the back prop operation will look like this. So I'm going to have one minus Z squared because the derivative of tan H is one minus the tan H squared. If H is ReLU, then the back prop through V is going to look like this. So I'm going to have everything up until this point, the activation function is the same. And there it is multiplied by the derivative of the activation function, which because if the activation function is ReLU is this step function. So now it's going to be dependent upon what the precise values of each element in Z are. So I'm going to take in some weight matrix Z and those elements of it that are negative or zero become zeros and those elements of it that are positive get left alone. So let us view how those weights actually change. So what I'll do is I will create some data, I'll represent sample inputs and targets like you've been doing, and then I'll create some randomly initialized weights. And then I'm just going to simulate one iteration of training with the two different activation functions, tan H and ReLU. So I'll have basically versions of Z and the output as assuming that the activation function is tan H and then one assuming that it's ReLU. And then I'll store the error using each one, perform back prop, and then calculate the change and you do that. So these are the weights of each weight. So these will be the delta between the previous value of those weights and the current values. Alright, so we do that. Now let's view how the weights change depending on the layer and which activation function. So I'll just view this in a pandas data frame and just reshape the data so we can we can view it. And here we go. So if these are the initial weight values, just for say the first nine here. This is what happens after I apply the tan H function. Okay, so we can see that for example, for those values that are very close to zero, they remain very close to zero for those values and all of them are pretty close to zero. So you don't this effect is not really very pronounced, but those things that are a little further away from zero, you know, get a little closer to negative one or one. And then we can look at the amount of change, right, how much each each one has changed. Now, ReLU, we can see that after the ReLU function, there are certain values that have some change applied to them and certain others that are completely unchanged. And this is all dependent on the whether they were positive or negative after you take that and then multiply them by the inputs. Okay. So now we can look to the same thing for the output layer, right, and we see similar phenomena. The only real difference that here we don't have that many, we don't see any values where the value has not changed at all. And this is mostly because we have this additional computation in the way that hidden layer that's providing other information. So finally, ReLU is not the only linear unit type activation function you can use. There are plenty of others. ReLU is the simplest, but you can use JLU, this Gaussian Aerolinear Unit. This is the one that has been used in most state of the art language models. So I don't exactly know what GPT uses for certain, but I'm pretty sure that it's some sort of JLU function. And then there's the sigmoid linear unit, which is basically just x times sigmoid of x. This ends up being another smoothed version of ReLU. So it's sort of, it looks typically like the ReLU function, except it doesn't have, it doesn't have like a sharp bend at zero. It's got more of a smooth curve. So all of these, well, not necessarily all of these, many of them, they try to smooth out that bend in ReLU because it makes for a nicely differentiable function all the way through. Whereas, yeah. So let me do, let me do leaky ReLU first. So leaky ReLU is basically, instead of being, instead of clamping everything at zero before, for all negative values, and then just letting x pass all the way through, what we do is we let a little bit of that negative value through. So there's some constant value, usually like 0.1 or 0.01. And what it'll do is it'll take, for negative values, it'll do, you know, k, whatever that is, times x. So it's going to, a little bit of the negative value through, but not that much. Right. Maybe take one, one percent of that negative value and then the, and then the positive values just get passed through. So parametric ReLU is the same thing in that we're going to try and learn, or we're going to try and let through some of those negative values. But we don't know how much, right? So I could just leaky ReLU everything, but maybe that constant 0.01 is like not the best value. Right. Maybe, but I don't know what it is. I actually want to learn that from the data. So we basically have a tunable parameter that allows us to figure out what the best value of this quote negative leakage is. And so this can actually be learned during training as well. And exponential unit, what this will do is in order to keep mean activations close to zero, it'll actually exponentiate the negative values. So all of these are effectively methods of leaving the positive values alone and trying to do things, the negative values that are meaningful, usually in combination and also don't cause your training to collapse. And all of them are usually more computationally efficient than 10H. Some of them like parametric ReLU, you have to, you actually have to tune a parameter in there. But they all have different uses and they can be very useful for different types of problems. You will find that convolutional neural nets usually use combinations of ReLU layers. Okay. Questions about ReLU? Yeah. Oh, the place where you. No, I don't. Okay. So I guess there's two answers to this. Yes, you can. As in it's definitely possible to write a function that does that. Does it satisfy the nice properties that we want with neural networks? That's less clear. Because pretty much everything gets standardized. And so you're assuming that everything is going to get clustered around some mean, which then gets standardized to zero. And so zero tends to be the best place to start making differentiations about your data. So yeah, you probably could write a function that's like, I'm going to maybe learn where the best place to start letting positive values through is. But there's a good chance it's probably going to be like really overfit to a particular data set, because there might be some data set, like where you standardize everything. And for some reason, there's like a weird secondary node. And it might learn, okay, this is like a really good place to start my linear part of my ReLU unit for this data set. But once you train on that data set and you try something else, even like different tests, it's going to be like, okay, I'm going to try this. So it's just, you know, back of the envelope guess really sort of seems like this would not be something that would be very productive. And so you're going to have to train on that data set. And so you're going to have to train on that data set. But once you train on that data set and you try something else, even like different testing data, it's no guarantee you're going to have that same property in the testing data. So it's just, you know, back of the envelope guess really sort of seems like this would not be something that would be very productive, although maybe people have tried. Other questions. Okay, so let me go to assignment two, and go through that real quick. Okay, so general shape of this assignment is very similar to assignment one in that we give you some starter code, you to fill out parts of that starter code, apply, apply that to some dummy data to make sure that your functions are apparently written correctly. So you actually have to perform some experiments on it and discuss your experiments. So you will give you in this case a neural network implementation. If you do this assignment correctly, you will at least have a complete neural network implementation that you can use yourself. If you don't want to do that well, that's questionable because it's all written in NumPy, it doesn't have GPU acceleration. So you can use it. And if you're a talented hacker, you can probably translate it into PyTorch or something. But we'll also use a PyTorch neural network later so you don't need to do that. But again, if you succeed at the assignment, you at least have in principle a neural network implementation that you can use and experiment with and does give you a lower level of control than the PyTorch or TensorFlow implementation. So we do have that advantage. So you're going to need to do that to complete the neural network, define a function that partitions the data into training, validation, and testing sets like we showed in the previous lecture, apply to a data set, and then define a function that's going to run experiments with different hyperparameter values. And then you're going to describe your observations. So first thing you're going to need to do is you need to complete the optimizer class. So we give you a mostly complete optimizer class that has complete implementation of SGD. What you need to do is refer to notebook six and turn that SGD implementation into an atom implementation. So this cell will implement SGD, and then there will be some implementation of atom that you need to complete. So we give you, for example, we already give you like, you know, MT, VT, beta one, beta two values. What you need to do is you need to finish the atom implementation by updating these values and the values of self.all weights. Okay. So then you can test that. If you do, if you're correctly, you should get the same results as shown below. What do we see here? So I alluded to this property last time. So the test here is basically finding the minimum of parabola. SGD finds it exactly. We know the minimum of the parabola is at five. SGD finds it at five. So the atom gets very close, but doesn't quite find exactly five. But this is supposed to, this is the expected value, right? You're supposed to get 5.03900403. Okay. We can also see here that, for example, both of these are training for 100 epochs and SGD starts with an error that's very close. And in this case, because it's a very simple problem, arrives at zero pretty quickly. So, atom starts with an error that's like way off 16, but it also gets, it like closes that gap within the 100 epochs. So if you look at the change in the level of convergence from the start to 100 epochs, atom is basically eliminating this error of about 16, where SGD starts very close and only has a little way to go. Okay. So you can also see kind of some of the properties of the two functions there. Now you are going to implement the neural network class. This is going to call the optimizers functions. So we already give you that. The call to optimizers is done. So first you need to complete the use function. So you can make use of the forward pass function in your use function. I'm not going to give you much more information than that. You have to figure out exactly how you use the forward pass function in the use function. This should not be too difficult, I think, if you've been paying attention to what the forward pass is and what back prop is and what it means when you cease training. Okay, so do that. The rest of the class is pretty much given to you. So complete the use function. You can assume that X is standardized, but you have to return the unstandardized prediction from the use function. So then you test out this test neural network function. Your results should be the same as these below. So you can test that there. And so we give you these graphs and basically if your output matches these graphs, done it correctly. Again, as always, I recommend download this notebook, create a copy, just in case something gets messed up, you can always refer to this one and you also compare your results visually to this one and numerically. Okay, now what you're going to do is you are going to cut and paste, basically create a copy of your neural network class cell here, and then modify it to allow the use of the Rayleigh activation function. Why do I want you to cut and paste? Well, mostly because I don't want you to risk screwing something up that would throw off these results that you're trying to test. Okay. So yes, it does seem kind of extraneous to copy and paste this large chunk of code, but I think it will help organize your stream of thought and also prevent you from maybe creating undesired results. Okay, so what you're going to need to do is you're going to modify the neural network to allow the use of the activation function. So right now it assumes that the activation function is tanh. So if we go to in the train function, for example, you will see somewhere. Forward pass, radianf. Find this tanh. There it is. There's the tanh. Yeah, and the forward pass. Here's the tanh function, right? So what you're going to need to do is you're going to need to modify this to take an argument activation function that will specify either the string tanh or the string Rayleigh and depending on that value, apply the right function. There is no Rayleigh function in NumPy, so you have to define your own. So this should accept a matrix of weighted sums and return the Rayleigh value. The implementation of how to do that is basically given to you in the notebook, which you need to do is modify that for use with the neural network class and make sure that your inputs are all correct. You also need to define rad Rayleigh. Then you need to add some if statements to the forward pass and the gradientf function that depending on the value of this activation function argument will apply the tanh function or the Rayleigh function correctly. So this should be pretty straightforward if you just have a new class variable that has one of the self dot variables in the neural network constructor that will then accept the value of the activation function. Finally, the experiments. So you're going to use the auto.mpg data that we've used in some lectures in lecture two. So let's apply these neural networks to predict miles per gallon using different neural network architectures. So kind of doing the same thing that we did in lecture three, only this time we're doing it nonlinearly. So the data processing is going to be more or less similar to that. You should end up with 392 samples after you remove all the samples that contain missing values. Then you're going to need to partition the data into five folds as we showed in the lecture. So this should return train validation and test partitions for the inputs and the targets. And then you need to write this function run experiment that will have three nested for loops that basically do a grid search for you. You're going to try different parameters of the number of epochs, number of hidden units per layer, and the activation function. So these two can be whichever you want. Activation function is just tanh or Rayleigh. Now that allows you to compare the training time network architecture and activation function and see which combinations work best. You should try zero for one of the hidden units per layers because that will be a linear model. So if I have no hidden units in my neural network, it's just a linear model. The way we've constructed this, it just passes zero in and get that. So then for each set of parameter values, create and train a network using Atom, which you should have implemented, and then evaluate that neural network. So you can collect all the parameter values, RMSE, and then when you're done, construct a data frame that looks something like this. So then before you start this nested for loop, you need to call your partition function to form the train validation and test sets inside one experiments. Use the same partitions for all of your experiments. So a sample call to the functions would look something like this. So what you need to do is then change which values of these should all be constant, right, X, T, and F, since day five, and then you need to specify which ones you want to use there. So this is just a list of numbers. This is just a list of lists, and this is just tanh and Rayleigh. Then find the lowest validation error and the lowest test error and report on the parameters that produce this, and then discuss how good your prediction is. So just you need to describe these three different observations that you make. Plot the RMSE train valent test sets versus the combined parameter values. We did a version of this at the end of the previous lecture, so you can model it off of that. You can work with a partner on this. If you choose, you're not required to. This is the only assignment that you're allowed to work in pairs until the final project. So you're given a grader like before. It should be uploaded on Canvas. Same procedure as before. And so I recommend keeping the notebook in a folder with just a grader. So then you will get 70% for the code and 30% for the discussion and the experiments. You can also get some extra credits by look up the the swish activation function or start this article, and then you can implement that as a third choice of activation function. If you do that successfully, you'll get five points extra credit and you can just do my experiments. Just say comparing just comparing the three activation functions, say, take your best performing set of other hyper parameters and then try the three activation functions and discuss the results. Okay. Any questions. All right, great. I'll let you go.