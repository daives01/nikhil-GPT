 Thank you. Thank you. Thank you. Yeah, I mean I I'm looking forward to the day when it's like, you know, passable as real meat and kind of. And what. Yeah. I mean like I don't mind, you know, it's just sort of waiting for the price point to come down until it becomes. Yeah. Yeah, that's fine. Yeah, I mean it's like it's obviously not me, like, never gonna pass for like actual me, but, you know, I'm not huge but you know by you guys remember DZ of my mom's vegetarian and like for, but for like weird reasons basically my mom, some reason, like the post office my mom lives at my house even though she lives in Canada. So for DZ of steakhouse keeps sending her like coupons. For some reason they're like my mother and not me. I probably could it's only because of her name or anything I don't need beef though. So it's like, you know, the steakhouse, they probably have some stuff like that. Yeah. Yeah, I'm looking for it actually looking for like a good restaurant to take like visiting faculty candidates to so I like rare. Yeah. 2020 so like two and a half years now. Yeah, that's good to. Yeah. Yeah. Okay. Yes. Okay, so you're naming all the places that I've been taken like once. So, all right, I should probably stop talking about restaurants. Let's go ahead and get started. As I mentioned, like, if you have good for Collins restaurant recommendations I am looking for them for professional reasons and also because haven't gone out a whole lot since we moved here because pandemic and then baby so you know, tried like the same you know five restaurants and for Colin so send me recommendations if you got them. Okay, so I did post the link to that chat GBT event. If you're watching that not an assignment or anything, an assignment in my seminar. But if you are interested, you have two hours to spare, because I'm sure you all do. You know I recommend watching that we will probably talk about some of those same points when we do like you know ethics and machine learning at the very end so it could be interesting for many of you. Alright so where are we at right now is do a quick share screen and let's take a peek at the schedule kind of get you going on where we're headed next so we are now. We should be back on track, assuming I get through the end of notebook 10 today so I'm going to finish up QDA and linear discriminant analysis and then we'll do linear logistic regression so quite a bit of content, although we got through most of that on last Thursday. So thank you all for getting your assignment one submissions I believe everybody submitted on time which is great. There were a few people who didn't submit although I assume that this folks will probably end up talking the classes to see what happens. So keep that up. Also, so, as I mentioned, it is very much to your advantage to get things submitted on time. So the reason for that is as follows. So I'm going to extend a really great opportunity for assignment one for those of you who choose to take advantage of it here are the terms. I will write them out and send an announcement later. So, any late penalty you first of all you may resubmit your assignment ones for regrade is going to be due a week from today at midnight, no extensions, no hard deadline of February 28. You're not required to do this. There will be a five point deduction. So that is, if you do perfectly you will still get a 95. This is, you know, you're not required to do this. The reason is, you know, I don't want people chasing those last few points if you got like a 99. Stick with your 99 if you resubmit and get 100 still going to get a 95. Okay. If for some reason you do worse on the re grade will keep your original grade. But I know that there are some of you, not too many but some of you who probably would want the opportunity to rectify some mistakes. So this is going to be your opportunity to do that. So I will reopen the box. This evening. Any late penalty will still apply so that is if you turn it in a day late. In addition to the five points will get an additional 10 points off this is why it is very much to your advantage to get things submitted on time. It's also in future assignments, in case I decide to do this again right so the same policy will apply. So, automatic five points so the maximum you can get on this is a 95 although that may be very good for some of you. You can rectify any mistakes will run it through the greater again and will grade your discussion. It is due at the end of the day, one week from today so midnight on the 28th absolutely no extensions, no exceptions, you're not required to do this so if you're perfectly happy with the grade I recommend you keep it. This is intended to be an opportunity to rectify mistakes that is both overhead for you and for us. Any questions on this. This is not necessarily going to apply to all assignments I will decide on a per assignment basis depending on how things turn out better sign it so for this one. This is because it's extended for this assignment. Any questions. Okay, I'll post an announcement with the same details today. So let's get back into the material so where we are now. So first, so I'll finish up, notebook nine, hopefully get through notebook 10. If we don't finish that then we should be able to get through 10 and 11 on Thursday. Next assignment will be rolled out a week from today. So just keep an eye out for that and then a two is due by default on Thursday extensions for those of you who have blanket extensions should already be factored in. Okay, so I mean controls. Let's continue with notebook nine. Font size okay for everybody. Good. Okay. Alright so just to recap, we were looking at introduction to classification. What we ended up doing was we're basically looking for, we want to find distributions in the sample where things items that are closest to an exemplar of a class would basically fall into distribution that is represented here by a peak, a peak, where the further you get from that sort of ideal points in the search space, the more the probability of falling into that class will taper off. Right. So, the idea being that there are distributions where there are clusters in your search space, and you want to find the cluster the probability where a point falls close to that, that peak in the distribution as closely as possible and you want to find that you want to have that distribution represent the data as good as possible. And so what we do is we end up drawing these normal distributions saying I'm going to assume that there are these points where if my sample falls right in this point is basically 100% likely to be a member of this class as we get further from that point we get a lower probability of being in that class and, consequently, a higher slightly higher probability of being a member of other classes. So we model our distribution somewhat like this. What we want is that distribution is really multi dimensional, and it can taper off at different rates and in different directions. And so we're looking at the types of functions we can use to model that we ended up modeling this as a property of Bayes theorem, right so this is Bayes theorem here I have you know p of a given B equals p of B given a times p of a divided by p of B, then p of B is basically the sum of the joint probabilities of B and all possible classes. In this case, what we were calling a. So we went through all the math. And so using the assumption that each class is normal distribution. We can then try to define the boundary between two classes so this is going to be basically a line or hyper plane, where if I fall on one side of that class of that boundary member of some class I fall on the other side of the boundary member of another class. So we went through, you know, all the mathematics is basically a lot of exponents so we use natural logarithms to bring make this function is slightly more attractable. Good reasons for using logarithms include that we can now do addition instead of multiplication so I'm now not multiplying a bunch of small numbers together so my probability of ending and underrun is far, far lower. What we ended up with is basically have this discriminant function that will call delta so basically this is the discriminant function of a class K for sample x. And we just want to find that boundary, where the discriminant of for class, say, class, a of x is equal to the discriminant of class B for access to define where it's equal probable that that sample is a member of those two classes. So this is just the, the two class problem. This equation is quadratic and x we call this quadratic discriminate analysis right because the discriminant function is our quadratic function. So, um, all the math probably make some of your eyes water. So we'll now go through it in Python. So, first, let's just make what we do usually do will make some, some dummy data will define it in terms of D dimension so you can arbitrarily vary the dimensionality of the data as desired. So in this case we'll define our samples x and t, where now there's just a single, a single component to each sample, but this can be you know any size you want, and then 10 samples for each class. So now we define x one and t one is these are samples in class one right we're just going to our trade classes one and two. So we'll just say, there are some samples that are members of class one there's some samples that are members of class two. We define the normal distribution and then the shape that we want to sample that from the normal distribution into just one thing to note here is just for this data we've defined that there to be a wider variance in the samples belonging to class two. So we put all this data together. So what you can see here the way we stack this is we basically take the x one samples the x two samples vertically stack them. And then we take the, the t one samples and the t two samples. These are the targets and Berkeley stack them so now we basically have all our x ones or inputs x ones the next to that our targets t one and t two. And now we stack them together. So this is a standard way of representing our classes where we have the target label says we've done before. So our data setup and pretty much the same way as we have done. So now imagine that instead of the, the actual class labels we just have data right so basically don't know t one and t two. Right. We just have data we don't know how it's generated. We don't know the mean the covariance of the two classes so data might look something like this. Right. So, if I just have these are my samples and I've got some class labels I don't know how I generated these, these classes. So I don't know these means and covariance, co variances. So let me just try and and try and compute the discriminant function for this so let's start as before, I'll just separate the inputs, separate the input column of the target column. So similarly, right so ID dimensional data I'm going to take columns zero through D, and I want to see the last column to meet the target. I will define my means and my standard deviations over my data x and standardize it. So that's done before. Right, we do not standardize t of course because this is now class labels. And so now we're trying to predict basically a discrete value, rather than some continuous value that might have a specific mean and standard deviation. So now we need to find this QDA discriminant function. Here's the mathematics of that again. So, one, the one key term here. Right, let's break down what all these things are so discriminant function for class k of x is going to be negative one half times the logarithm natural log of sigma of k minus the one half times remember x is the sample use of k is the mean of that of the class and questions that's the distance. So this is a set of samples. So each of them is going to be a different x minus the mu for that for that sample should be the same so transpose that times the inverse covariance matrix. Again, times the distance, plus the natural log of the class, probably the class. Okay, so C equals k is just whichever class we're interested in. So we know to calculate all of these things except for that covariance matrix, sigma. So let's consider a couple of ways of doing that so we can define sigma. You know, as some arbitrary values, and then we can say take sigma times the inverse of sigma should give us the identity matrix. So, now I can use this other function pseudo inverse so P inverse. So we should get something that if I use the pseudo inverse function that is very close to the inverse and I'll define this in a moment. So we should get something that is very close to the identity function. In fact, I do. Right, so we have one and then some very very small numbers here. So, effectively zero. So now I don't find sigma slightly differently. So instead of this, you know, 1221 it's not 1212. If I try taking the inverse of this it's going to throw an error, right, this is a singular matrix. So what is a singular matrix we've looked at the senior matrix is basically a non invertible matrix so we have typically the two matrices, A and B, right, if B is the inverse of A, then, A times B will equal B times A which is going to be the identity matrix for whatever the dimensionality of A and B are. So a singular matrix is going to be one that's not invertible right for for whatever reason so we'll see if it's a, you know, a square matrix, but it's it's not an invertible for some reason. So, what can we do in that case well what we can do is you can use the pseudo inverse function so this can can be what we call the more penrose pseudo inverse of the matrix. And so this is going to be the generalized inverse of the matrix, using its singular value decomposition, including all the large singular values. So what this does is this now allows me to basically compute something that's close to the inverse of any of any matrix. So it doesn't necessarily if the matrix is non invertible for some reason, I can still use the pseudo inverse to to get something that's not invertible close to the inverse for that. So, if I now define sigma times the pseudo inverse of sigma remember remember what's what's sigma look like it's 1212. This is going to give me this is at least going to give me an output, right, so this is not the identity matrix of course, but it will allow me to, to, to compute this. So now I can define a function that allows me to use the QDA equation so same similar inputs that we have all of the, all the pieces that we need to put together right we've got the inputs. We've got the means of the district different classes, the standard deviations are sorry, the means of differences is mu, we have the means of the data the standard deviations of the data mu being the, the main points of the different classes, sigma is the covariance matrix, and then prior is going to be that prior probability of the class right so remember this is a generative model. There is a prior probability there as basically how, how common is my class overall than my data, right, so I need to know the base. So, you can see that we're going to take x minus the means times the standard deviation so these are my standardized x values. And now I'm going to take that and then subtract mu, right, this is so this is going to give me the distance of the standardized input from mu, the mean of the class. Then what I can do is I can then compute the determinant of sigma. So now, this is commented out here because it's not going to apply in this particular implementation. If the determinant is zero, then we will raise this error for the same recurrence matrix but instead we're going to do is we're going to actually use the pseudo inverse to allow us to at least get an answer for, for all cases, even if it happens to be singular. Okay, so now we can start putting the pieces together right so here are my negative one halfs right so negative one half times the NP dot log is the natural log by default of the determinant divided by a negative one half times the sum of the product of x times the, the inverse matrix times x, and then I'm just going to reshape that so that all my samples are organized in the right order. And then so this, this here should give me a single, single dimensional matrix. I want that to be two dimensional. And so then I'm going to just add the log prob of the class to every element. So to use this we need to calculate the mean the covariance and the prior probability so we got that right mu mean sigma covariance prior probability. So, what about P of C equals k, this is gonna be the prior probability distribution of class k. So if I have no prior belief that one class is more likely than the other, then it's just going to be the number of samples in that class divided by my total number of samples. In this case, if you look at the data that we use before is 50% because we had five or like 10 members of class one and 10 members of class two. We don't, we're gonna pretend we don't know that though. Right, so we don't know how this data is generated. So we'll just sort of try and figure, try and figure out if we can recover the right answer. So, let me see what all my classes are so I'm just looking at instances of T, where their class label is number one so I can just do this by basically doing T equals equals one right so this will apply the equals one Boolean function over my entire over my entire array. And then I can just reshape it to list all the examples out so now you can see that those first 10 are all true, because T equals one and the rest are all false. So, now I can represent my class one in class two is those indicator variables, right, where it's either zero or one depending on the which which classes in. So I'm going to have K minus one classes that are all zero. And then for that Kth class it's going to be one. So now I can define the mean, and the covariance for all of those for all those different classes. So I basically just pull out which samples in my, which rows of my data are members of class one which ones are in class two. And then for each of those, I can just take the standardized inputs for that class, and then compute the mean for each of each of my classes and same for the covariance. So now what we can do then is I will get the prior probability right so this is going to be, and one is going to be the sum of the class one rose, so that is just how many there are, but same friend to. And so and will just be land of t. How many samples do I do I have. So now I can compute the prior probabilities for class one in class two just by dividing those respective ends by the total number of samples. So, we can see that you know in this case we already know that our data is going to be the same. So we already know that our data is evenly split, but the same code would work. If I have an imbalance data set this will allow me to do that. And this will allow me to then factor in that prior probability. So in case I had a data set where I had 80% samples in class one and 20% samples in class to write the overall probability is some sample falling into class two should be lower and I want to factor that it, it being a generative model. Okay. So let's look at the covariance for sigma one right this is going to be this value. So now we can apply our discriminant function to some new data. So I've been able to define a function that given this information will give me the discriminant function, and then I can actually just compute the output probability for that forcing new data. So now I'll create 100 new samples. And so this is going to be kind of representative of similar data. So what I'll, what I'll define is two, two models right so this is a generative model you've always run k models for k classes. So if I like two classes I need to run two models. If I've got 10 classes I need to run 10 models. So I'll define two instances this QDA function and all I need to do is put in the distribution that I'm interested in right so the distribution in the prior probability so for the first instance, I'll put in those values for the first class and the second instance I'll put in those values for the second class. So now what I do is after I run this, it's going to put output values into D one and D two. And you can see that it should for each of these 100 samples, it should give me a label, basically, you know, true or false or a probability that when rounded would give me zero or one that tells me how likely it is it falls into for the first set into class one and for the second set into class two. So we'll look at it. If you were to run this notebook and increase the dimensionality of the data you would still work right because it's written to, to accommodate arbitrary dimensions. But if the data is more than one dimensional will just plot with respect to the first component. In this case, there is only one component. So to obtain the value of the normal distribution from the sample we've got two choices. So we can either start with the discriminant function value and then transform it into the full normal distribution value. Or we can just use our normal distribution implementation directly. So in this case, we will just define this normal D function, where that has the inputs and then mu and sigma. So print out those values right so I'm going to have different mu and sigma for each class. And you can see already that since this is just a single dimensional sample. The mean value for class one is around centered at point one two whereas the mean value for class two is centered, just shy of point point three or sorry, my, my mistake. That was the covariance. The first class is centered at negative point nine and the second class is second centered at positive point nine, and the covariances are given here and as we mentioned when we're generating the class to data, it's got a wider variance, if you remember that, hence this covariance being being larger. Okay, so what's the, what's the normal D function so x contains samples one pro and by D news that mean vector, so just D by one, and then signals the covariance matrix. So what we can do is basically try to recover the normal distribution by using the discriminant function basically reverse engineering. So if you look at this, this code, you'll observe that it's kind of very similar to the, the QDA code. So, just, I'm kind of doing everything in reverse right and try to get back to the normal distribution from the discriminant function, where previously we had defined the normal distribution, and then we drive the QDA function from that. So let me define that there's our new one in you to data again. So now I can plot what the discriminant functions look like for this data so all I'll do is obviously exponentiate the probabilities, and then I will apply the discriminant function to that. This will give me a nice, nice quadratic curve. And it looks something like this. Okay, so for this data. So you can see where that peak is for the blue curve being class one for the orange curve being class two. And you can see that you know the peaks are different right about negative point. This is on standardized. So, this is the other standardized like negative point nine and positive point nine. So this is probably about negative point nine still but then the peak for class two is probably about 5.5 something like that. And so we can see that there's that wider variance in the second class. Same thing, as we see here. So this is this this first chart shows the discriminant function, you know, the actual value plotted as a function of the input feature, and then QDA here this is going to be the probability from directly from the discriminant function so here you can see more clearly. These are those peaks that we were after. And then that kind of attenuation in the probability and so again here that wider variance in in class two is also evidence. So that here this one is just the QDA using the normal distribution. And you can see that we get a very similar output right so this is what we want we were able to successfully recover that that normal distribution from the discriminant function. Now they're only 10 training samples per class you can expect the results to change a little bit, because it's quite a bit of noise. But what if we have like more dimensions than samples. So for example, I could set D equal to 20 right now I have 20 components for each sample, but I still have 10 results, and I can run the exact same code. And what happens. Well, what happened here, we're not getting results. And sometimes I can run this again, and it may plot something for like one class but not the other. And even when it plots something will often see if it if it possibly for both classes will basically get a flat line here for the probability using the normal distribution. So, something goes wrong each time sometimes a different thing goes wrong. But clearly there's something not right with this. Shoot, not quite with this distribution. I need to accidentally exited. Okay, here we go. Here we are again. So that stigma is very close to singular, which means that the columns of x are close to colinear. So, the determinant of the singular matrix of course is zero, and it can't be inverted. So we'll discuss some ways of handling this in future. But we assume a single normal distribution. As the model of the data from each class. And this does not seem to lead to a huge complex model. Let's say of how many parameters there are if the in the mean of the covariance matrix. If the data is D dimensional for some some value deep. So that means that the me is gonna have D components right so I'm just going to take the samples that have D dimensions and I'll take the mean for each dimension. And so I'm going to have like this D dimensional vector that represents the sort of the expected mean of the distribution of that class. So the covariance matrix would have D squared components, right, because it needs to be D by D. So if it's one, if it's just like a single dimensional sample, we saw that our, our Sigma's just had a single value, right, one value by one value is one value. If we had 100 components then the covariance matrix would have 10,000 parameters. So in reality, the covariance matrix is symmetric. So it just has basically D over two plus D or sorry D squared over two plus D over two, or D times D plus one over two. These unique values, we can compute the remaining values just to pre know those values, but still quite a lot right and it grows. You know, not quite exponentially I guess but polynomial. And so we have one for each class. So the total number of parameters including mean is going to be k for the number of classes, D plus D times D plus one over two. Right, so a lot of a lot of samples. So if the data distribution is then under sample that class boundaries going to have a lot of errors in it, right, because I'm using. If I have 10 samples with 100 components. I'm not going to necessarily be able to find a very good mean for each of the classes that I have that I'm interested in. So, we're going to basically overfit to those few samples and if things. There's basically a lot of variants, the more, the more components you have that could end up on either side of that class boundary because you didn't do a good job of finding that. So we need to remove some of that flexibility from that normal distribution model. So, when we can, what we can do is we can restrict all the covariance matrices to be diagonal. And so, then we'd have these basically ellipses that you can draw in the data, these will be parallel to the axes will come back to this point when we do dimensionality reduction. This wouldn't really work well if our features are well correlated to each other. So now we can force all the classes to have the same covariance matrix by taking the covariance matrices for all classes and averaging them element wise. But this is doesn't seem like a great idea on the surface. Why not having single covariance matrix for all classes seems like it might. So no. What's the answer. So why is why is averaging all the micro variance matrices a bad idea to get a single covariance matrix for all classes. Yes. Yeah, basically that's it right we look at the variance in different classes could be radically different. And also the more components you add the more dimensions you add the variance in individual dimensions could also be different so even if I have a three dimensional sample. The variance in one class in dimension three could be way less than the variance dimension three for another class, while the variance in dimension one is a lot bigger right so averaging that you're basically losing a lot of information about in how much when I move along a certain dimension, my distribution starts to fall off right so this is not like a great idea. So what we can do then is we can use the average covariance for each class, and then weighted by the fraction of samples for each class. So why do we do this. When you can set it for and similar. Okay, you're off the hook again. They run around thinking as soon as because he skipped class last week. So yeah, basically, we, we can assume that if my classes are like uneven unevenly distributed, then if I'm averaging all the, the co variances, it might not be so detrimental this one class that like has a different sort of a different variance in it, in some dimensions but it's it's under sampled in the overall data set right so if I've got one class of which I have 10 samples in a data set that has 10,000 samples like if I neglected this somewhat it might not be the end of the world right because it's so rare. I don't know if fitting model to the means and standard deviations defined by these data that I have for that class is actually representative of what other members in the class would actually be right so it doesn't make doesn't make a whole lot of sense to overweight your lens too much credence to just a few samples that might be really noisy or whatever. So, we can actually do this and see a better result than, than using a unique variance matrices. So now to remember our, our discriminant function. So, this is the discriminant function. And so we're basically just trying to find values for where d of some k is greater than or don't have some days greater than some other k, and then use the same covariance matrix for every class. So now instead of sigma sub k we just have a sigma right so now we can we can use the discriminant function, plus the natural log of the probability for each of those classes. So this can be simplified as follows. So you're not going to go through the math and basically just like multiplying out all the terms, so that I am able to have a simpler functions and I determined function is now becomes something like this right So this is the function of x transpose of k of x is equal to x transpose times the inverse singular matrix or covariance matrix times mu sub k minus one half of mu transpose times the inverse covariance matrix times mu sub k plus the log prob of k. And so now you can see that unlike the previous function might not be obvious because I'm not sure you don't you probably don't remember the whole previous function we go back and look at it. So this is going to be linear in x, as opposed to taking a bunch of square roots and then trying to bring the exponents down in front. So now this can be rewritten as delta sub k of x is equal to x transpose w sub k plus constant. I'm sorry. It looks it looks a whole lot like that linear function right. So remember, we did linear regression we did neural networks we basically have some inputs x multiply them by some weights w, and you add a bias B which is usually just sort of subsumed into the weights and you train you train the way for that bias, but it's So this is the normal distribution. Right. If we started with y equals mx plus b, where m is a slope, and b is a y intercept, b is a bias right this is just going to be some constant value, and then the slope in this case is multi dimensional, just defined by weights is the coefficients that I'm going to weigh each each each input by. So then using the normal distributions as the generative models and restricting the covariance matrix. This gives me a linear boundary. So, this is now called linear discriminant analysis. So QDA and LDA are based on these normal distributions by modeling the data samples in each class. So I can say for some sample. What would this look like if I tried my best to model it using a normal distribution and trying to find that boundary between my classes. And so, QDA has this flexibility. And so LDA is actually often better in practice, in particular cases where we have under sampled data or high dimensional data, right, for reasons that we saw before, because we don't necessarily want to have that full flexibility of trying to define a covariance matrix for every class. When in reality I can model the data in a more flexible way, or in a more practical way with fewer computations using linear boundary. Alright, questions for the example. Yes. So yeah, if you have a data set and it's not very comprehensive, I'm going to put words in your mouth, and I'll say like, yeah, sort of, just give a sparse, sparse data set. So, yes, very likely you'd want to use LDA, because you can. The intuition is basically, you don't really have enough information to be very confident in the sense about what your covariance is going to be. So, trying to trying to fit a covariance matrix to like every data set you're going to probably overfit to any peculiarities of those samples. And so instead I can define a little more general. Would dimension reduction also help with this? You certainly could. Yeah, so let's say you have like some high dimensional samples, but you can figure out that like most of that variance is actually captured in say the first two or three principal components, right, then in that case you might be able to reasonably fit like a QDA model to that, where it's where it performs a little bit better, because you can basically infer that the risk of having that large variance in those higher dimensions is actually really low, right, because they're not actually capturing all the information so it could just be you have some sample that for some reason, you've captured 100 components of, but just the first few components are actually where what's really important for making some sort of classification distinction. And so those other say 97 components just adding noise. Right, so you could get rid of those and yeah you definitely could so just keep in mind just all these machine learning techniques are individual tools in your toolkit. And so, most of your job, you know if you apply this in your careers or in your research is going to be trying to figure out like what's the right combination of tools that I want to use for my data. Right, so do I want to do PCA on the data itself or do I want to do PCA for like visualization or something, you know, it may be helpful for one but not for the other. And do I can I do some dimensionality reduction technique that allow me to use a technique like QDA because it's faster than say, trying to fit a neural network to it because these extra 90 plus parameters are just like not really that relevant and if you figure that out, that you've probably solved that problem already. Okay. Other questions, comments. Cool. Yeah, so remember we had that Parkinson's data as we had samples to class to class problem. They basically have features extracted from the voice. And then you do the samples labeled as has Parkinson's or does not. So we're going to go back to that and then classify using QDA. So first let's calculate the means and the covariance matrices so I'm just going to use the same splits that I had before. So if you remember, we had like 170 or 195 samples I guess, or something like that. And then we split them into a train test split using I think 8020. So what I'll do is all 50 is generative models to each class, right, so two classes just zero and one. And so then I will standardize my trains, and then I will compute my means and my co variances for those two classes. Then same as before I'm going to run these two discriminative models over the samples from class one and samples from class two and see, given what I know the labels are how correct am I. And then the same thing over the test set right so again, I'm going to be using X test I'm going to be using the same. The same computed means and standard deviations for the data and then also the muse for the two, two classes and the co variances right. And so now, then I'm the last term here is going to be the prior probability of the class so my class zero is healthy, and then I'm going to be using the same numbers as Parkinson's that is going to take the total number of each of those classes, divided by the total number of samples. And then I can return that into some percent correct. And so you can see where we're going already but in this case, using QDA, right, the train percentage is like 98%, correct, and the test percentage is about 87% on this split, if I run it again I think it's slightly different numbers. But you can see that there's a significant underperformance of the test accuracy compared to the training accuracy. So, what we can do now is we can write this function is going to do it multiple times we're going to try different splits and run it multiple times so again as I think I mentioned, we want you want to typically you might want to try to average like over a bunch of different splits just in case you got a lucky split once right and you don't want to report those results, because perhaps someone trying to reproduce the work wouldn't be able to and they're going to be like well I ran your exact code and I got a different results so what gives, but what gives is that there's a different random seed or something. And you gotta you just have to get a lucky split of the data that no one else can actually ever reproduce. So, basically this function is going to do what we just did, just a bunch of different times. And so we can see that I will make a, you know, a split of the data run my two discriminant functions over the train and test data print out the percent correct. And so now what I can do here they can basically do this run Park, I put in the data file and my training fraction. So, give me that number, or do it again, right I get slightly different numbers you can see like now we're getting 92% test accuracy. So, the training numbers pretty much stay the same, right, this is about as good as I'm going to get using QDA on this data, but for different splits, you know, I may get sometimes radically different percentages so we have a range here from about 84.6 to 92.3. Right. And so we can see here, we compare these two in this case, the test accuracy is identical. Right. And this is not necessarily because we have the exact same split just happens to be that you identify the same number of samples as the incorrect or correct. So, um, let's just you know for for your review you can just consider how would you get the values for these for these different things using Bayes theorem, if you need some practice so you can just look at these. Look at these points, and then go up in the notebook and to see how we would get these these different values. So now what do we need to change where we're just doing this with QDA So what do we need to change for all this to run it with LDA. So let's write this LDA function and see if the same classifier, or the LDA classifier which assumes all the classes have the same covariance matrix does better than QDA on the Parkinson's data. So we showed that if we assume the same covariance matrix by weighting it. And by the number of classes. Then our discriminant function becomes as follows. So then what I can do is I can write disk LDA, that's implementing this function instead of the quadratic function over the same data. And so then I can redefine run Park to use this function disk LDA, instead of the QDA function so here we're going to run the QDA function that I run the LDA function, and then we can see how they compare. So, if I run this right now we can see, here's my QDA result and there's my LDA result and you run it five times. And then you all take a look at this and see, tell me, you know what you observe. How does, what's QDA doing versus LDA. Right, yeah. So a lot of this is based on that on the split right so we can base it based on which 20% were holding up for tests can have significant effect. But we can see that you know the the QDA percentages are like routinely north of like at least 95 often up into 98 99, where the LDA numbers are lower 89% 93% on the test fraction can sometimes be significantly lower actually for LDA but also sometimes a bit higher. So for example, here's, here's one case where basically the LDA is beating the test only that's the even beating the LDA train accuracy, and it's also significantly beating the QDA test percentage so we can see you're the QDA probably over fits and is a little bit more generalizable often, but if you run this again for example, you know, run it a few more times, you'll get some different data right so here's here's cases where for most of these numbers are pretty pretty identical to the test so he's like say this sample. Same test accuracy, even though the LDA train accuracy was quite a bit lower. So, then what I can do is I'll just like write this out into a file. I'll call it QDA LDA. And so then if I just run this, then we can actually see the probabilities. Right, so now if I look at this. Here's a sample that's class one is predicted as class one, and then the probability is actually, you know, 20% or something. But the second class is just point 001%. So the number of these classes don't sum to one is just looking at which one is more probable right so even if this is only 20% likely to be a member of class one 20% is still a lot more than close to 0% so that's going to be the answer. Yes. Yes. The one both. So you can basically see, there's no way to definitively tell that it's overfitting but it's very likely for seeing my train numbers being 98% and my test numbers falling significantly below that. So, it could fit really closely to this data and there's enough resemblance between the test data and the train data that it's it is lifting it up and it may actually get the higher than a, than the LDA model. Not necessarily usually going to produce a lower result but it shouldn't produce a result that's like so much lower than the train. Okay, so it's really, it's much more about that discrepancy between the train accuracy and the test accuracy if it's really overfit to the train data it's like your train data is going to be like close to 100% it's really good thing but the training data. If your model were good. You would expect to see a similar number on the test. Right, so if I'm seeing 99% train accuracy and 82% test accuracy for a problem that is this simple. So it's probably seeing overfitting. There are more complicated problems you know some like very complicated like objects or action recognition problems that the state of the art on the test set is like 40% or something, just because the problem itself is so hard. Yes. Yes. I think we assume that. So we will be covering the test data. Yes. The second is, we have both at 87 and 89. But, it's a 9891. Yeah. So how can we say that it's overfitting or underfitting. So, you, you're going to look at the discrepancy between the training data and the test data. So here for example this one right for LDA, the training test accuracy is a really close. So, this model is trained well enough to get 89% accuracy in the train data. It's also generalizable enough to get similar accuracy of the test data. Okay. So, if I look at this one for example, my train data is like 98%. I love to see that the test accuracy is a lot less than that. Right, so there's something in this way this is fitting to the data that it's like, not so flexible and it's able to generalize quite as well. So you're the ideal model is one where you just get like really good train and test accuracy. But in most cases you can't really expect to do that. What you don't want to end up with is a model that is somehow fit to some sort of peculiarity in the training data such that when I give it new data. It just kind of falls apart or just doesn't do as well. Yeah. But do we want to lower accuracy better fit more than. I mean it does it does depend of course. It depends what the use case is right do I want to, if I'm just interested in like just fitting to the Parkinson's data, maybe IQD is great, right, because actually in some cases it's doing better so like I could stick with that. But if I wanted a model that's like okay I just want to be able to handle an arbitrary two class problem, and I don't know where the data was generated from and I don't know those, I can just calculate the means and covariances but I don't know kind of the distribution the data was sampled from, I might want to err on the side of some that's more flexible. So it sort of depends on like what am I trying to use it for. If I were running a test that's like, I just really want to fit to Parkinson's data of this form specifically, then you probably just want to do whatever is going to give you the best time, the best test accuracy but it's like, I just, I mean we're running a bunch of different samples maybe we're going to allow every trying to do to class classification like a bunch of different diseases or something, just as like a filtering to send people to you know a specialist or something like that. You probably want something that's more generalizable. Whether or not, just keep in mind like, just because you can doesn't mean you should right machine learning may not in fact be the best tool for this if you think of like a medical environment that stakes are pretty high. So you may or may not want to use a technique like this. This is just a demonstration of like you know if we have data set up in this way this is how you can model the problem. So RMSE well I mean this case we're not measuring RMSE because it's a classification problem so RMSE is error on us on scalar values. Right. So, if you're having, if you're predicting continuous values than the RMSE will indicate you the cow. How close are you to predicting the correct value. For classification right your metrics are different, right so here we're talking about accuracy. You might also do like precision or recolor f1 like we talked about in. In the second lecture or like you know area under the receiver operator curve. There's a bunch of different metrics that you can use and part of your task is to pick the right metric for the task at hand but like if you're dealing with a classification problem like RMSE would not be the one to do because you have to it's basically squared error over some units what's the units of classification there aren't any labels. Yes. Yeah. Yes, yeah, yeah. Basically what I'm saying is like, if I look at like this class right this is a misclassification. So, that's the sample member sample one of class one is predicted to be a member of class two, because the model, when factoring in things like the prior probability of the classes in this case it's equal probable but then also like the features. It's saying okay there's an 8% chance that this is a member of class one there's a 9% chance or 10% chance is a member of class two. Neither of these is objectively good. Right. Wouldn't put money on this result. But these are the only two things these are only two models that have got. So, I must choose one. Right, you can handle the problem in a way that's like, if I don't get a value that's like above 50% or something I'm just gonna say I don't know. Right, I'm not, I'm not, I'm gonna, I'm not going to use this result. But if I handle my problem in this way if I set up the formulation this way, whichever one's higher wins it doesn't matter if that higher value is actually objectively low. Yes. So, the last this classification. That's the spot on. Yeah. Yeah. Why is that one was. I mean we would have to look at the exact inputs for that sample. Yeah. It happened to be that way but there might be you know, there may be something that's like a typical about that sample. So, you know, we're talking about like voice features for Parkinson's so like maybe that person had a particular timbre to their voice already, right, maybe they had a naturally shaky voice or something like that, or actually, maybe this is, I think, class, class zero. Maybe they just have like a really solid voice that even the shakiness that comes with Parkinson's doesn't really change that part of the vocal signature all that much. Yeah, yeah, yeah so it really could be I mean this is this is not a severe outlier but if I had a case where it's like this is 95% likely to be a member of the wrong class might be an indicator that it's not. All right. Okay. So, what I will do now I will start and run this notebook on your own. I'll start the next notebook here which is classification with linear logistic regression so we're continuing with linear classification, and we'll just follow up, kind of with an alternate method for a discriminative model rather than a generative model so just as a point of point of fact so a discriminative model and basically looking for, for the individual features. What is most relevant to this class. I don't know about the prior distribution, so much, I don't factor that into my model, an unbalanced set can have repercussions for how you fit your model. Because there are fewer samples to be pull your decision boundary in a particular direction. But I'm not going to be using like prior class probabilities and making this decision. So, this linear model use for classification we can have this masking problem where if I have to, too few samples of one class. This, this can result in masking so we had these different membership functions that other other than linear functions. So first we use these generative models to model data from each class, and then convert that to probabilities, using Bayes theorem, and then drive those quadratic and linear discriminant functions. So now we're going to instead of doing that instead of having like two model that's going to give me a probability for being in one class and being in other classes, I'm just going to directly predict the probability. And what is a consequence of this if I'm doing this, then I should end up with a probability that it is a member of one class that if it's greater than 50%, I'll classify it as a member of that class because there's no other comparison to make. I can't say it's 50% likely to be member of class A but 30% likely to be a member of class B there for class A wins. I'm basically just saying, is it a member of class A well it's 50% likely or more so therefore yes, or it's less than 50% and therefore no. So, in this picture of the problem was that this line for class to the green line this is too low. And in fact, we'll see that in the middle of the range all the lines are too low, right. So, if I'm looking here. Yes, the green line is the highest for the set of samples, but none of them are particularly high, whereas for the ones on the edges. We see values for these two functions of the red line, the blue line are quite high and so you could probably be reasonably confident of that. So, one thing we can do is reduce his masking effect by requiring the function values to be between zero and one, and then requiring them to some sum to one for every value of x so these sound like probabilities, these are properties of probability so we can actually represent the probability function as basically there's some function, some predictor function for x parameterized by weights w for some class and I'm just going to sum that or divide that by the sum of the outputs for all functions, all such functions for all the classes. Right. So if I assume that f of x parameterized by w is greater than zero. We haven't discussed exactly what f looks like yet, but we can see the w represents those parameters that you're going to be tuning to fit the training data. So now we're back in trying to optimize weights. So, we know that this expression will give me a value between zero and one for any x. So now we also have probability of C given x expressed directly, as opposed to modeling x given C for every class and then running all my models using Bayes theorem. So, this is going to be an arbitrary function, so let's just give it another name was called g for now right so g is the probability of C given x, which is given by the output of f for function k or class k divided by the sum, the sum values of f for all classes in m. So now we need to choose something for f and whatever that is we have to have some plan for optimizing its parameters. So, what's our plan. So now what we're going to do is we're going to try to maximize the likelihood of the data, so that is to say I've got some data. And I know that all my samples in these classes in my in this data belong to some set of classes. So we need to try to maximize the distribution of classes such that the likelihood of seeing this data is maximize. Okay. That makes sense. So I've got some data, and I want to see what classes do I need to define what distributions, should I infer, such that the likelihood of this data is the greatest that I can get it to be. So, if you have training data consisting of samples x one through n. And then these indicator variables for classes one through k. Remember these are all going to be ones or zeros, where it's one where it's a member of that class and zero otherwise. So, each row of this matrix should contain a 01 and a single one. And then we can also express my samples as an n by D matrix. But for the following examples will be using single samples more often. So the likelihood is going to be the product of all probabilities for the class of the n sample, given that n sample for that sample. And so a common way to express this using indicator variables would be this. So here's my indicator variable raised t sub nk that's the indicator variable. So the probability raised to this value right is either going to be is either going to be raised to zero, or raised to the one. So let's say I've got three classes, and the training sample and is from class two. So the product is going to look like this right so it's going to be the product, the probability of one given x raised to the t sub n one times the probability of C given x raised to the t sub n two times the probability of C given x raised to the t sub n three. So of course if I raise anything to the zero becomes one. So now, only one of these terms is going to remain. So let's say it's a member of class two, this is going to reduce to the probability of C equals two, given x. So now this shows that if we use indicator variables as exponents we can now select the correct terms to be included in the product because basically looking at what actually is relevant here. My class of interest is class two, I really only be looking at the probability that it's in class two. It's not I'm back to computing a single probability. So, if this is the data likelihood what we do to maximize the data likelihood. So again, I'm going to be finding some weights, w. So, that maximizes the likelihood of actually encountering this data. So, if L of w, right so if this is previously likelihood of the data b. So now I want to find the w that maximizes that data so basically just looking and trying to solve likelihood of w is going to be for, for all n and all k I'm going to take the product of those probabilities of that sample falling into that class. So now I'm finding the derivative with respect to each component of w, that is because it's a derivative in high dimensions and now back to computing gradients. But there's a whole bunch of products in here and what happens you multiply a bunch of fractions together. It approaches zero, right and so the more multiplications I have the closer it's going to get to zero. So I'm going to make it easier by working with the log problem, the log likelihood. So I'll just call this ll of w. And so now I can do things like convert all my products into sums, and then bring my exponents down in front. So now I take sum for all n of the sum for all k of the indicator variable times the log prob of the class. So now this becomes a lot more tractable, and I'm just trying to find, this is going to be a negative number, always, unless like, well, it has been a good number. I'm just trying to find the least negative number. So now unfortunately, still trying to solve the gradient. Of course the gradient of log likelihood with respect to the weights is not linear in x. So we can as before, simply set the gradient, the result equal to zero and solve w right so now if you're paying attention this sounds a lot like our neural network problem, right, it's now no longer a linear function we've turned by my linear function into a nonlinear function. So we'll do a similar technique. So we'll call this gradient ascent. And if you're wondering why we're doing ascent and not decent. Just think about the properties of a logarithmic curve that make this appropriate to think about the shape of logarithmic curve. So what I'll do is I'll initialize w to some some value, and then I'll make a small change in w in the direction of the gradient of log likelihood, with respect to the weights. So, should be back in familiar territory this is starting to sound a lot like neural networks, or just, you know, linear regression even with SGD. So I'll repeat this, this step until I seem to get to some sort of maximum value in the log likelihood. Right, so this is a form of convergence and it's just going to see my, my value does not seem to be increasing very much I probably reached about the maximum on this gradient I'm ever going to get. So, what we see here is now what's the value of w I'm going to take the previous value w plus some, some value alpha times the gradient of the log likelihood, with respect to w. So, alpha is going to be that that constant that affects the step size which sounds like learning rate, right. So, again, you know, also sometimes labeled alpha. So remember that w is going to be some matrix of parameters, let's say we'll have some columns that correspond to the values required for each f, of which they're going to be k minus one. So, we can work on this update formula, one column at a time. So, here we have this. This is for each column, and then I'll just combine them at the end. Right, so this this weight is going to be weight. The weight plus alpha times of the gradient for all of those individual weights those individual components. So now let's remember that we have some function was called h. So, the delta, the derivative of the log log of h with respect to x is going to be one over h of x. And then we have this function this probability function we've just labeled g right so now I'm trying to figure out what g is. So, I can now rewrite my log likelihood function just put g, g of x in place of the probability. So, now my, my gradient with respect to weight J of log likelihood is going to be some for all n for all k of t sub nk divided by g sub k of x sub n times the gradient of w sub j times g sub k of x of n. So, if you're wondering why the above works just remember what the derivative of the log of x is right so driven that for log is one of our x. So we can actually do that. So, now it would be really nice if the gradient includes the factor, g sub k of x of n is going to cancel with the other one in the denominator. So we can rewrite the function to get this. So if we define f of x sub n parameterized by weight sub k as he raised to w sub k transpose times x right so again we see this thing that should look familiar double weights times inputs. So if we rewrite this such that, g of g of case of x sub n so g sub k of x sub n is equal to f of x sub n parameterized by w sub k divided by the sum. So now we can work on simplifying this right so we take this function that defined here. I'm just going to rewrite it in terms of this new definition of f right so we get this. And so now, by take the gradient of this end up simplifying to something that looks like this right so if I have the gradient of w sub j times. The sum for all k of e raised to this quantity over one over that times, you raised to that same quantity. So now if I take a look at this. What's going on here so I have the gradient of W sub j raised to e. Sorry, times, he raised to the W times x. So now remember what our, what our indicator variables were doing right so if it was a member of that class I get some value. It's not a member of that class it's always one. So taking the derivative. I should get zero, if it's not a member of the class of interest. Otherwise I get an actual quantity. So, therefore, what we can end up doing after all this math is basically saying, for this value, what I can end up doing is I can just define this function, where it's going to output, some, some value if it's a member of the class of interest and zero otherwise. So now if we take this substitute back into the log likelihood expression, we end up with something like this. So we basically have okay so the change in W sub j of the log likelihood is going to be the sum for all n for all k times the indicator variable over our function g divided by or sorry times the gradient of W sub j times the times g. So now what I can end up doing is I'm just going to take this change in delta, delta sub j k minus the my function g times the input. So now this gives me this update rule. Right, so we see what we had before so previous or previous value of W sub j plus alpha, my learning rate times the sum for T sub n, sorry, T sub n j minus g sub j of x sub n times x sub n. Let's focus on this term here, like so went through all that very very fast. Let's focus on what's going on here so if we look at my indicator variable, either a zero or one. Right. This is going to now be some probability value. And so, if I have my samples that are set up basically saying it's got a probability that's zero or one or zero. Right, let's say it's a member of class two is 010. Now I want to predict something that's going to give me a meaningful error. So let's say about three classes where my indicator variable is 010. I want to subtract from this, something that is going to be of the same dimensionality that have to be three terms. I also want to be a meaningful error. So, what's a meaningful error in this case basically let's say that we've got three classes, whereby indicator variables are 010. Think of those as probabilities instead. Zero percent 100% zero percent. So now if I can have a predictive function is going to output probabilities that can be directly subtracted from a value between zero and 100. It's going to give me a meaningful error. So if you imagine that I have some function that says, okay, got three outputs and it is point three point six and point one. We can now subtract that from, we can take that and subtract it from 010. So we'd have zero minus point three, one minus point six, zero minus point one. So now we're back into the kind of traditional error formulation of how wrong am I? So I'm trying to predict my probability. My ground truth is saying, well, there's a zero percent probability it's a member of these classes and 100% probability it's a member of this one class. I want my output values to approximate those values as close as possible over all of my samples. So this function, whatever it is, and you may be thinking of names for this, is really just another way of representing error. It's just this time it's representing as an error in probability instead of an error in some scalar value. So let me finish this, I think. So just to summarize what we've done. So I have my probability of my class given some sample and some data likelihood we want to maximize. So what I did is I took my probability function and I'm trying to find some function that's going to model this appropriately. So for right now I'm just calling it J or sorry, G. We want G to have the following properties. It should be bounded between zero and one and it should sum to one for all possible outputs. So this value should be raised, this value raised, e raised to the w sub k times x sub n. Remember, this is just my input x times my weight t, or my weight w. And so this should raise, equal this value if k is less than k. Otherwise it'll be one if k is equal to k. So now what I can do here is then for the likelihood of w is going to be the product for all n and all ks of these probabilities raised to the indicator variables. Remember, we're writing the probability function in terms of as this. So now the gradient of the log likelihood with respect to w is going to be something like this. So I now take the gradient, it's going to be the log likelihood. So I'm turning all my products into sums. Because I'm taking the derivative of the natural log, I can now bring the indicator variable down and then also divide it by my function g. And now I'm going to multiply this by the gradient of my weight g, or sorry, my weight w times the function g of just k of x sub n. So now what I end up with is the simplifies to for the sum for all n of x sub n, my input, times my error. Right, so I get some output here. And I subtract that from my indicator variable is the can be thought of as a probability. And then I multiply that by the input. So that's the gradient this now allows me to turn this into an update rule which tells me how much I need to move along that gradient in order to optimize those weights. So, last few minutes, questions about this we'll pick this up again on Thursday. So the function of the physical alphabet. Yeah, it says function of f of x sub n. I thought you mentioned that k is less than, it's not equal to capital K, then it should be zero. So k is the number of classes, so big K is the number of classes. So k here is an individual class and basically it's going to be there's a class of interest. Maybe it's like 012. And so it's going to be the, the probability of falling into the individual class. When for all classes, right, this would probably actually confusing. This should probably be like a summation I think. So yeah, I think I'll fix that it's a bit of a typo. So, remember here we have the output of this is going to be what's the probability of it being in this class, and there's all this probability should sum to one. Okay. Alrighty. Yeah, so I will go back to my office. Come to office hours if you want to be there until 430, and good luck on getting a two completed.