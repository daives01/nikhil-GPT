 I think that's a good way to make forward that. Yeah, we might be able to get a little bit more of a sense of what we're doing. We need to get a little bit more of a sense of what we're doing. I think that's a good way to do that. And that's a very important thing to do. I think that's a very important thing to do. I thought you were going to say something. I think that's a good way to do that. Thank you. Okay, let's go ahead and get started. Sorry, we just found out that we have a paper to submit tomorrow and our co author is flying home from India and I thought he was the one who's going to submit. So, slight emergency. Okay. Just making coordinating with my student. Okay, so let's go ahead and get started. Let me share my screen. I want to, one point that I think I wanted to make real quick. Where is it. So, right, so I'm not going to, you know, remember the part about don't put you know Christ for help in your code comments. But I don't want to, I don't want this to be taken as like, don't ask for help. I'm just saying don't, you know, put something that could be taking this way in the code comments. If something is wrong, the best thing to do is to come tell me, because there are there are resources that you can be pointed towards. There are also options to manage the coursework to if you're having you know, issues with like course load or some things you know just don't, you know, put a don't put a cry for help in your own comments to be found you know when it's when it's too late. So just wanted to make that clear. But today's topic is me. Windows shade this here. So, which one Oh God I have so many things. Of course overview. Information theory. Okay, so today I'm going to do for you the first 30 minutes or so. Just talking about information theory and evaluation so basically how especially when we come to classification, you will be performing evaluation of your models and why we do it this way, because there are some very solid mathematical principles for performing evaluation in the ways that we do. And then following that I will begin the first real technical lecture on linear regression, which we will almost definitely not finish today because it's very long and so we'll finish that up on Tuesday. So, if there are no questions, I will go ahead and start. Yes. Yes, so they will be posted on the public facing page. So this one I think still says 2021 running from the contents of the slides are the same as long as throughout the updated version after class. Yeah, so just just as a reminder all the information should be available on the public facing page. Yes. First notebook. Oh yeah so I'll fix that. So the issue with that is that if you look at the link it says like envy or dot iPython dot org change the ipython to dot Jupiter, j y t e r it'll work. So that's just because I Python Jupyter notebooks, we call it ipython notebooks, and because it's courses existed for a while I think the URL is just inherited from a previous version so if that first one's not working, change ipython and the URL to Jupiter and it will work this afternoon I will fix the link so you don't have to do that. Okay. Anything else. All right, great. So, what is information theory and why is it important. So first let's just do an introduction to what it is. So information theory is the study of quantifying and communicating information, so information, we're kind of colloquially raised to think of that it's an amorphous quantity or quality so really, that is not something that is easily quantified or measured. So you know, you don't think of information as having, you know, mass or dimensional properties, when in fact it actually has all of those things. And we'll get into why that is now. So, as a field, information theory was first established in the 1920s by the two gentlemen on the left so Harry Nyquist and Ralph Hartley. And then it was codified in the 1940s really became a field, due to the person on the right, named Claude Shannon so Claude Shannon is kind of referred to as the quote father of information theory. And so, generally as a field this lies as the as the intersection of probability theory statistics and computer science, and really is just about what is one of the properties information that allow it to be measured numerically, and what does that entail. What does that allow us to do. So, the answer in information theory is the concept of entropy. And you've probably heard of entropy in, say, physics class. And so basically, the greater the uncertainty involved in the system, the greater the entropy. Right, so we clean up my well my wife cleans up my daughter's playpen in the evenings. And then my daughter goes in there when she's not a daycare and throws everything around the system trends toward disorder. And so the entropy if I take the playpen to be a closed system at the beginning of the day it has lower entropy the end of the day it is higher entropy one day, one way to think about this is My daughter has a whole bunch of toys that when they're put away properly or a little basket. Right. And so at the beginning of the day, they're in the basket. At the end of the day they're everywhere. And so, if I want to know where a particular toy is when the system has less entropy that is the toys are cleaned up, I least know that's more than likely to be in the basket. And I'm usually correct when the system has greater entropy when the toys are strewn everywhere if I want to find a particular toy, I don't know where it is. So there's greater uncertainty about a particular property of the system. And so it has greater entropy. So just in general terms that's a specific example. How much uncertainty is involved in the value of a random variable, or the outcome of a random process so the outcome of random the value of the random variables like position of the toy in within the playpen outcome of the random process is what's the way much which my daughter is throwing her toys everywhere. What's the process by which that happened. And so she doesn't throw her toys everywhere in a different place every day. In the same place every day is actually in a different place. So what's the uncertainty involved in that outcome. One another way to think of this is you have a fair coin flip. There are two possible outcomes. Whereas if you have a fair die there are six possible outcomes. So they're both fair, you know they're not they're not unbalanced in any way. But the coin flip has lower entropy because there are fewer possible outcomes. Okay. So, why is this important. So we begin with this quote by by Shannon, the fundamental problem of communication is that of reproducing at one point, exactly or approximately a message selected at another point. And he was thinking about this kind of in terms of telephone lines so remember this is the 1940s telephone technology was present but wasn't very sophisticated. And so you had basically static lines, or what he called the noisy channel. So that is, there's a person on one end who wants to say something, and there's a person on the other end is receiving the message, and there's some level of interference in the middle. And so, person B who's the recipient has to decode that message and they might hear the wrong word or something. Right, they might hear, you know, the, the, I don't know. And so they might hear a word that's like common, you know, hop, when the person meant to say cup. Right. And they might be able to disambiguate that when they hear the following words being of water. And they're like, well, a cup of water makes a lot more sense than a cup of water and so therefore I'm going to interpret this as being cup. Okay. And so this has a lot of implications for a number of different, a number of different concepts like the entropy and redundancy mutual information, the channel capacity of the Gaussian channel and the notion of the bit. The two in blue are the two that we're going to focus on in this lecture and for this class. So, if you've heard of entropy in physics class. There is actually a close correspondence between physical entropy and information entropy so in particular we talked about Boltzmann entropy gives entropy and Shannon entropy. Information entropy is Shannon entropy. It's given by the formula here through the denoted H. And so it's basically going to be the sum for all possible states for the times of the probability of that state times the base two log of the probability of that state. So, H is going to be the bits per symbol of an information source. So let's say, an alphabet of symbols, or a vocabulary and piece of eyes the probability of occurrence of the ice possible symbol so in our fair die example, the probability of every possible outcome is one in six. In an evenly distributed set of six classes, the probability should roughly approach, one out of six. Okay. So, if you have an unbalanced set then whichever set is whichever classes overrepresented is going to have a greater product base rate probability of occurring. So now, the other two types of entry boltzmann entropy is written with the following formula s equals case of the times natural log of w, or s is the entropy of an ideal gas so little bit Bolton was studying gases and the composition thereof case of B is the constant Boltzmann constant and w is for showing the kite which is German for probability. In this case the probability is the probability of a specific molecular configuration of a gas so I've got a vessel full of air. And I'm like, what are the, what's the probability of the molecules being arranged in a particular way. Well, you can't actually count the individual number of possible arrangements of molecules, but you can come up with a probability distribution that describes that right, what are the probability that they're all clustered over on the right side vanishingly small, what is the probability that they're relatively evenly distributed throughout the vessel, probably somewhere in the fat part of the bell curve. Gibbs entropy is a generalization of boltzmann entropy, and it uses a similar formula. So, negative case of the times the sum for all I have the probability of the state times the natural log of piece of I. So, the probability of the system case of B is the boltzmann constant and piece of I is the probability that micro state I occurs as a system fluctuates by gases in a vessel. The molecules are moving around and I micro state I is just like some, you know, choose some configuration was the probability that the gap the molecules are arranged exactly that way. So let's take a look at these formulas. So, let's take a look at the Gibbs entropy formula, Gibbs entropy formula and Schenn entropy formula. Do you see similarities? What do you see? Sorry, you mumbled. Probability, right? What else do you see? Logarithms. What else do you see? Constants, right? So let's take a look at how these things. What happened? I used to have like some. So, the probability is be a piece of I, what's W stand for? The German word for probability, right? So this could just as well be written P. We have natural logs, we also base two logs but it's pretty trivial to convert between bases and logarithms. That's not a big issue. And then we have a constant, right? In some cases, the constant has a negative but that's just a constant times negative one. So if it's a constant, I can factor it out. And then in this case, we have a sum and then in Boltzmann entropy, basically I'm taking the sum for all possible states, right? So if I sum this and W is just considered to be probability, then I'm taking all possible probabilities of microstates and summing them. So, effectively, these formulas are equivalent. So, how does this apply to machine learning? What are the information theoretic concepts that we will be learning in this class and why do you need to know them? If we take entropy that is H, so H sub X would be the entropy of a discrete random variable X. So that is, if I know the distribution of X, what is the measure of uncertainty in the value? So if I take this curve, which is going to be the entropy of a Bernoulli trial, which is binomial as a function of success. So if I have two possible outcomes, X equals zero and X equals one, then the entropy is maximized when the two outcomes are equally probable. So what is an example of a system that has two possible outcomes? A coin flip, right? And so let's assume that success is one possible outcome. Success is just arbitrarily defined as being landing heads up. So if the coin is heads up on the ground, then the probability of heads is what? 100 percent, right? If this is the event that I'm observing, the probability of that event must be 100 percent. So if I'm observing my penny being heads up, then the probability of tails is zero. If the coin is in mid flip, right? What's the probability of success? In this case, heads being up. 50-50, assuming that it is a fair coin. So the probability is maximized when the two outcomes are equal probable in the case of the coin that's in mid flip. So if I don't know anything about the outcome of the system, if my possible outcomes are evenly distributed, then every individual outcome is going to be equally probable and I will basically fall in this part of the n dimensional curve, where n equals the number of classes. Another concept is expected value. So this is going to be written with the bar E. This is just a generalization of the weighted average. So this can be the arithmetic mean of a large number of independent realizations of X. So if I have a fair die, possible outcomes one through six, and I throw it a bunch of times, right, the expected value of that is what? So if I have a fair die, right, and I throw it a bunch of times, so if it's a fair die, the weighted average is every possible outcome is one out of 16.66% likely. So let's say I throw it 100 times, and I get the average of all those possible outcomes, the value of that number should approach what? 3.5, right? So that's a little counterintuitive, right? The expected value of a fair die is 3.5, but we can objectively say that a fair die will never show 3.5, right, because it can't. It's not one of the possible outcomes. However, this is the expected value of all the possible outcomes. And so that's just because the formula is the generalization of the weighted average. So now I sub X, or sorry, I of X is called the self-information or, quote, surprise of X. And that is going to be the negative base two log of the probability of some possible outcome. So if you think of, say, Scrabble letters, right, every letter is assigned different point value. Q is 10 points, E is one point, and X and Z are also 10 points, and rare letters are more points. What this is, is those point values are effectively a realization, numerical realization of the surprise of each of those of each of those outcomes. So there are very few Qs in the bag of Scrabble letters. And so you're surprised and maybe a little bit excited when you take out a Q because you get a lot of points. So another way of thinking about this is that E is the most common letter in the English language. It's unsurprising and it contributes pretty pretty little information. So if I think about where E can fall in a bunch of words, right, if we count the number of E's in the header for the slide, Theoretic Concepts in Machine Learning, one, two, three, four, five, right, there's five occurrences of E. And it doesn't tell you a whole lot about what comes next to it. So if I think about where where do the E's fall? They're next to H, O, C, P, N, and then L and A. Whereas Q, for example, usually falls before U in English. And so if I'm looking at words like quit or quake or common English words or sequence, if I see a Q, there's a very high probability that there's going to be a U. Now, it's not always true, right. For example, you can have loanwords from like Arabic that have Q in them. It's not followed by a U, but those are comparatively rare. OK, and so that's why, among other things, Q is worth 10 points in Scrabble, because not only is it rare, also in order to make real English words, you usually have to have a U. OK. So now the entropy of X will equal the expected value of the self-information of X for X in the set of all symbols produced by the information source. So now the entropy of X can be given by this formula, which is going to be equivalent to the expected value of the formula on the previous slide. And so this can also be written as the negative sum for all X of the probability of every possible outcome times the log 2, the base 2 log of the probability of that outcome. So this is a powerful formula because you can combine it with when you have multiple variables. OK, so the joint entropy of two variables, X and Y, is going to be equal to the same formula, except I'm just going to plug in X and Y at the same time. So this is pretty straightforward in that if I want to take the joint entropy of two variables X and Y, I can then take the negative of the sum for all X and all Y of the joint probability of both of those times the base 2 log of the joint probability. So in other words, if X comma Y is the position of a chess piece on a board, then H of X, Y of the row and the column is the entropy of the position of the piece. Similarly, I can do almost the same thing with conditional entropy. The only difference is that I have to instead I can't just do the expected value of X bar Y, because that doesn't really make sense. I still have to use X comma Y. But effectively, it's going to be very similar to the formula on the previous slide, except now I'm going to take the conditional probability of X given Y. So the bar here means given, that is, if I know something about the thing on the right side of the bar, how much do I know about the thing on the left side of the bar? So in other words, if X is today's weather and Y is the season, then the entropy of X given Y is how well season Y predicts whether X. Right. So it is winter. And so there is probably a high probability that it's going to be snowy or Court Collins gets 237 sunny days per year actually when I interviewed for this job they said it was 300 and I looked it up and that was a lie. But 237 is still pretty good. But these those days are not evenly distributed, right. We're more likely to have sunny days in the summer. Yes. But this here. So if I take the if I sum for all values of X and all values of Y, then I take the joint probability of so let's say my I have four values of Y. So I have a spring summer fall and winter. And then I have, you know, let's just say sunny or cloudy for X. Right. So then if I take the joint probability that it is a cloudy day and it is summer, or it is a cloudy day and it is winter or it's a sunny day and it is fall all possible combinations. So those two there's going to be some joint probability associated with both of those things occurring at the same time. But that's not going to be the same as the probability of the weather conditioned on the season, because it's more likely to be cloudy and winter than it is in the summer. Okay. And so I'm going to compute the for all possible combinations, the joint probability of that combination of values. And then I'm going to compute the conditional probability for the same combination of values which is not necessarily I take the base two log of that and then they multiply them together. I compute all of this as a product. And then for all possible values of X and Y, I take the sum of that and then I take the multiply that by negative one. Now give me the conditional entropy. Other questions. So now the cross entropy, which is the key term here. So we're talking about cross entropy loss. For now cross entropy is basically if I take two distributions. So let's say I have a distribution P and a distribution Q and then X is a single event. But the value, the probability of that event is going to be different depending on which distribution I sample from. So then the negative, the negative of the sum for all X of the probability of X according to P times the base two log of the probability of X according to Q. That is called the cross entropy. So what this means intuitively is that this will be the average number of bits needed to identify if an event, identify an event, if the label set is optimized for Q rather than the true distribution P. So let's think about, let's say I'm trying to classify the season given a bunch of factors about the weather or something. And I have the true distribution of outcomes. So I say I take hours of sun. That's like almost a perfect predictor. Let's say I take like inches of precipitation and cloud cover and wind speed or something like that. And those are my three inputs. And I use that to try to optimize a model that will predict the season based on those three inputs. There's a true distribution that is like, this is the weather in Fort Collins for the year 2022 and that's going to be my true distribution. And then I try to optimize a model to predict that. It will become arbitrarily close to the true distribution, depending on how well trained my model is. We'll get into the training parameters later in the class. But it's probably not going to be exactly right. And so how off is it? Is it way off? Does it predict that the weather is winter a few times out of say 100 or 65 times out of 100? So that optimized distribution is Q, the true distribution is P. And so this is going to be how much more information do I need to identify that event if I'm sampling from Q rather than P. And so this is important in cross entropy loss because those labels that are incorrect, those they're drawn from say a poorly optimized Q will signal some difference in Q from the truth P. And the amount of that difference will tell you how much further you have to optimize your model to get close to the truth. Right. So far, everybody. Yes. So when you use that to like compare the different distribution. Yeah, you certainly can. Yeah, so, you know, one way, for example, in the process of optimizing a model and I'll go into model optimization. I'll go into model optimization like in the latter half. But let's say I have a model. I'm just trying to find a function that maps from an input to some output. And there's going to be it's going to be parameterized by some weights. My job is to solve for those weights and that's the optimization processes. If I'm let's say halfway through optimizing and I haven't quite optimized to convergence yet. My model may be kind of correct but not very correct. And so this is a measure of like how wrong am I, given the current state of the model, and this will tell me you know do I need to kind of shift my distribution further in one direction or another, and that's just going to be in multiple dimensions so but for now you can just think of it as like, do I move it left or move it right forward back, etc. Any other questions. Yeah, you first. What is what is a label. Okay, so in this case, you can think of it as basically being a distribution of clusters. So if I have let's take the example of the weather again. If I have three inputs of wind speed cloud cover and precipitation, and I plot them in three dimensions let's assume for a moment that these are good predictors of the season. And that means that you should have a bunch of points that are like winter weather, they're clustered close together, and a bunch of points that are summer weather that are cluster close together in spring weather and then autumn weather. And if these are good predictors, then those clusters will be closely defined. And so, in the optimization of a model the label is the output that I'm trying to use to map the inputs to. Yes. So, whether whether that's the output of this. So basically the cross the cross entropy of q amp a p amp q remember these are distributions. Yeah, yeah so the end of being a number. Right, and we'll see you know how we calculate the number and how we use it. So when I when I say how wrong am I, I mean I'm saying that you can, you can actually assign a number to that value which sounds counterintuitive, but it's always going to be relative to the information that the how you're representing the information that you're dealing with. Okay. So it's not like, you know, it's not always like between zero and one you can normalize it between zero and one, but sometimes how wrong am I is like 3.5. So that doesn't in and of itself mean anything, but it does relative to the data that I'm using to train my Alright, mutual information. So we talked about self information. That was I have x mutual information. I'm not entirely sure why the use the semicolon, but basically we have I semi colon y. And so this is, you know, how much, what do I know about why if I know x. Now, they can be calculated using the following formula. So again, for all values of x and y, I take the probability the joint probability, and then I divide that by the, by the base two log or multiply by the base two log of the joint probability divided by the marginal probabilities multiplied together. So, if I have two events that are conditionally independent. Who knows what that means if I say two events are conditionally independent What do I mean by that. Yes. They have no relation to each other so effectively it's saying if I don't if I have x, and I have why I don't know anything about x if I know why. Right. Now, another way to represent this is if I just take the overall marginal probability of probability of x p of x, and the overall marginal probability of y. Then, if I multiply them together if they are truly conditionally independent, the joint probability will be exactly the same, because there's no difference. There's there's no difference in the probability of these events occurring together, then there is of them occurring together separate. So make sense. Yes. If they if they were truly core if you had vector representations of these things yeah then then you would get a dot product or something something very close to one. That's assuming you're representing them as a vector which we'll get to in a moment. But for probability distributions. It's sort of like, yeah. I will have to check the math and that and get back to you to be sure but I think they think that intuitively that seems like a good way to think about it. They are generally, they're generally inversely correlated. So one way of thinking about this would be like, if it's dark at 7am. And it's, and what's the problem being dark at 7am and what's the problem being cloudy. Right. Those two things are probably related somehow, right, because if it's cloudy may raise the probability of it being dark 7am. Whereas, if I'm asking two completely unrelated questions like, what's the probability of there being a car crash on college, and what's the probability of there being a hurricane in Florida. Right one usually has no bearing on the other except maybe with an extreme interpretation of the butterfly effect. So if I have the probability of these two things they have individual probabilities and the joint probability of them occurring together is just the multiple of both of those. Okay, so this can also be represented as the expected value of the point wise mutual information PMI of the events x and y. So, point wise mutual information refers to single events. So again if I have car crash of college and hurricane in Florida. It's like how much information do I, does that car crash give me about the world versus how much information is that hurricane give me about the world. So, those single events, whereas mutual information in general refers to the average of all possible events hence the expected value. In other words, the, the mutual information of x and y is going to be the information entropy of x minus the information entropy of x given y. So, how do we get to the next bullet. So that is if what do I know about why, if I know x. So this is telling me what's the entropy of x given y so why has some conditional bearing on x. And so if I take the overall information entropy of x subtract that from it. This will tell me what do I know about why if I know x, because I'm assuming that x has some bearing on or why has some bearing on x. So in English, this would be like, if I see a queue, there's a higher probability that you comes next. And so these two things have a relatively high level of mutual information in English. Yes. So, I'm going to ask you again. Usually so the vertical bar means given. Okay. And so if I have x given y, then why falls in the right side of the bar. If it's why given x you just flip them around. Yeah. So the last of the, the conceptual definitions is KL divergence. So this is also known as information gain. And so it's written as D sub KL, and it's usually denoted with the double bars. So this can be realized as the sum for all x of the probability of x times the base two log of p of x divided by q of x. Once again, p and q are different distributions. Right. So if I have some distribution p, that's the truth, and some other distribution q, that is arbitrarily closer far from the truth. If I perform data compression, assuming q, while p is the truth, then D sub KL is the number of extra bits I actually I need to actually compress the information according to p. So in other words, if I have two people, Alice and Bob, and they're drawing colored balls out of a bowl. Alice knows the true distribution. So like the number of balls in the bowl that are black or red. And Bob has a different distribution. So let's just take very simple cases. Alice knows that all of the balls and bowl are black. Bob thinks they're all red. So Alice's distribution is p the truth, Bob's distribution is q, the erroneous assumption. And then when Bob takes a black ball out of the bowl, D sub KL will measure in bits how surprised he is to see that. Assuming his distribution, this is subject to like how many do we know how many balls are in the bowl total. And do we know that only black and red are the options. Right. So if we throw other possible options there, the number would change. But given other parameters that we know, then you can use KL divergence to measure in bits basically information and surprise. So another way of thinking this is information gain. So if Bob takes the black ball out, he has gained some information about the state of the world. Right. So what how much has he gained? Well, that's debatable. But we know a couple of things. If you thought they were all red, and he pulls a black one out, at least he knows that one of them is black. He doesn't necessarily know that all of them are black, but he knows now they're not all red. Right. So what bearing does this have on how we actually evaluate things in machine learning models. So we think about the probability densities. So now P and Q again, P is the density of the true labels as distribution over all the classes we have. What's the distribution of samples into those classes, and then Q is the density of the model label sets. We assume that the same number of classes right out of distribution is a different problem. It's a topic of open research. Let's assume that we specify the number of classes in our model. And then the only differences in just like the density of the distribution of classes in the ground truth versus the model. So if I draw predictions from Q, that's going to give me some prediction of the underlying data. And my goal effectively is to get Q as close to P as possible. And so I need some way to evaluate how incorrect the model is quantitatively. So classifiers make errors, but they make different kinds of errors. Right. So for example, if I have, well, I guess in the paper we're talking, I was just talking about with my student, we have this case where there's a severely unbalanced sample. We basically have comparatively very few positive samples that we're trying to extract in a whole bunch of negative samples. Right now. I could have a model that just says if I have like 99% negative samples and 1% positive samples, I'm going to say everything is no. Right. My accuracy be very high. But that's not a very good way of evaluating my model because I'm probably not going to get much higher than 99% accuracy, but I will miss all the relevant information that I'm trying to extract. So we need a better way of classifying errors than just plain old accuracy. Accuracy is great, but only useful in certain circumstances. So no model is perfect. So we think about spam filters, search engines, COVID tests. I'm trying to retrieve a certain set of relevant samples while minimizing the number of irrelevant samples that I retrieve. So if I have a COVID test that is quote 90, 95% accurate, and I gather 100 samples, how many tests are wrong? Well, how many tests are wrong? Five of the tests are wrong. But how are they wrong? Right. That could be important. So for example, if I test 100 people for COVID-19 and let's assume that 10 of them are actually infected, that's the true distribution. If I have two tests, A and B, and then A finds five out of those 10 positives, it gets 95 samples correct and five samples incorrect. 95% accurate. If I have test B that finds 15 out of 10 positives, it still also gets 95 samples correct and five samples incorrect. It also is 95% accurate. Are these two tests the same? No. So accuracy is how many do I get right divided by the total number of samples? So that is the true positives plus the true negatives divided by the total population. So if COVID test A finds five out of 10 positives, there are five false negatives. There are five instances, five cases of COVID that this test missed. If I have COVID test B that finds 15 out of 10 positives, then it's got all of the true positives and it also retrieved five false positives. Right. Now, which of these is better? Which of these is better? B. In this case, it depends on the use case though. In this case, the slides from 2021, so like written early pandemic, I guess now we just like don't care anymore. But you know, we didn't, we really didn't want to miss positive COVID infections. And so in this case, and you know, and many people still do not want to miss positive COVID infections. So in this case, COVID test B that runs the risk of retrieving some false positives, but has a far lower probability of leaving a negative on the table is the one that you want to use. So there are two measures that we use here. Precision, also known as the positive predictive value, which is the number of true positives divided by the number of true positives plus false positives. So in this case, the precision of A, we have five true positives divided by five true positives and no false positives. Five out of five is one that has a precision of one. Precision of B has 10 true positives divided by 10 true positives plus five false positives. That's 10 out of 15. And it has precision of point 666 continuing. Right. So this is basically precision is I have my dart board. And if I throw one dart and it hits the center, then I have 100% precision. Right. But if I throw 100 darts and one of them hits the center and a bunch of them go wide, that's going to lower my precision. Recall is kind of the inverse of that. It's also known as sensitivity. I'm not sure why I got cut off there at the bottom. This is 10 and that's 0. That's the number of true positives divided by true positives plus false negatives. So in this case, the recall of test A would be five false positives, five true positives divided by five true positives and five false positives. So a recall of point five. And then the recall at this point five again, this is those five positive cases that it didn't find. Right. The recall of B because it got all of the relevant samples, all the positive cases of COVID has no false positives. So it has a recall of 10 out of 10 or one. So recall is, you know, if I have my dart board and I throw just my success is defined as like having 10 darts at the board. If I throw 100 darts and 90 of them go wide, but 10 hit the board, then I have achieved my goal there. So the trick is if I test 100 people for COVID, you know, at least early pandemic and really still, we don't know how many people are actually infected until you do the test. And even then the test is not perfect. It's just kind of you have to take multiple tests. So we don't actually know this. So, we don't actually know how many people in a population actually have the disease. And so the consequence in this case, we're in 2021, of a false positive was a two week quarantine, which sucked. And you don't want to have to do it. But if that's it, and you know, you're not going to be locked down forever was a relatively low stakes consequence. So in this case, should we prioritize precision or recall? Recall. Right. We don't want to miss those positive tests, but it depends on your use case. So we will prioritize recall in this case. But there are other cases where you may want to prioritize precision. So there's a quote from this British jurist called William Blackstone. It is better that 10 guilty persons escape than that one innocent suffer. Right. So if, for example, as in 18th century Britain, the penalty for a lot of crimes was death. No going back for that. And so you don't want to accidentally execute an innocent person. And so it is perhaps better to let someone go. Maybe if they committed like a minor crime, then give them a very permanent, very severe punishment. So again, precision recall, what metric do you want to use? It depends on exactly what you're trying to measure and how you're going to use the results. But in most cases, we actually have to balance these somehow. So accuracy is one way of doing this in that you can take the number of true positives plus true negatives divided by the total population. But when you have more than two categories, this often is not meaningful, especially if they're not balanced, even if you have two categories and it's unbalanced. Also, accuracy may not be a meaningful metric. Let's take the example of an image classifier. And I have three classes, cat, dog and rabbit. And I'm trying to classify images as one of these three animals. So the precision of cats in this case would be the number of true positives, that is things that are cats that are classified as cats, divided by the number of true positives plus the number of things that are not cats that are classified as cats. So basically, in this case, true positive and false positive need to be read as members of the class that are correctly classified versus numbers not of the class that are classified as this class. And then true negatives would be number of things that are not of this class that are not classified as this class. False negatives are things that are of this class that are not classified of this class. So positive and negative just in this case are going to be always interpreted relative to the class. So in this case, if I write it out like this using what we call a confusion matrix, I can just calculate the precision by summing across all of the rows and then dividing, take that as the denominator and then dividing the number along the diagonal by that. So I take 13 divided by the total in this row, that's 25. I take 16 divided by the total in this row, that's 22. And I need 13 or 10 divided by the total in this row, which is 13. So has a relatively low precision of cats, 52, but higher precisions for class dog and class rabbit. And so what this tells me is what types of classes tend to be more problematic for this model. And so that can tell me where to optimize. Recall is the inverse. So I'm just in this case, I'm just going to sum down the rows, right? But it's just the number of true positives. So that number along the diagonal divided by the sum of all the numbers in the rows. When reading confusion matrix, I'm going to take the number of positive and negative. So it's not always as simple as just sum across the rows or sum down the columns. You need to pay attention to which axis is which. And in the assignments, you usually use like a preset function that does the same thing. So I'm going to take the number of positive and negative, and I'm going to take the number of positive and negative. So I'm going to take the number of positive and negative. So now what do we do? What's another way besides accuracy to balance between precision and recall? So we have another metric called F score. And it will the most usual case is that you can just use the function to calculate the accuracy of the So now what do we do? What's another way besides accuracy to balance between precision and recall? So we have another metric called F score. And it will the most usual case is F1 score. This is a harmonic mean between precision and recall. This is a harmonic mean between precision and recall. The formula is two times the quantity of precision times recall divided by the quantity of precision plus recall. So in this case, what I would do is I'm going to compute the precision for all the three classes. So here I take the three numbers for each of the classes, divide by three, and then do the same for recall. Multiply those two numbers together in the numerator, add them in the denominator, divide it, and then multiply by two. And here I get an overall F1 score of point six six. And so this is a typical measure that you often find of assessing the quality of a classification model. Now, this is macro average. That is, we are assuming our sample is relatively balanced. And so I'm not going to weight these numbers by the number of samples in the class. So you can do micro averaging, which does do that. So if I have an overbalanced sample, it's overbalanced in one direction, then I will weight that accordingly when computing the F1. Yes. When the weighting is micro average. Yeah, good question. So generally macro averaging will work for balanced samples or things that are close to balanced. It's kind of, it's a bit of a, kind of a qualitative game. So you'll notice that our samples here are not completely balanced, but they're close enough, right? So it's like, it's pretty close. It's like, I can probably get away with just doing macro averaging. If I did micro averaging, I wouldn't get like a hugely different number. On the other hand, if I had like a lot more rabbits than cats or dogs or something, you know, if I put all my animals in a barn and come back a year later, I'm going to have a lot more animals, a lot more rabbits, different barns, I guess, dogs will eat them. Then I will then I need to weight my sample accordingly. So this is F1 is a special case of the F measure, which is usually written F sub beta. This is just a tunable parameter. That's basically, how many more times do I value recall than precision. So in this case, like my COVID test that I still value precision, but I maybe value recall more, right? I don't want to throw precision out completely, but I want to value recall n times as more than I can set this value. And so instead of two, excuse me, one plus b squared, and then in the denominator, I'm going to have b squared times precision. And so of course, if beta is set to one, it comes after this formula. So this is just like a nice little schematic. It's actually from Wikipedia if you go to precision and recall. So let's say we have just some samples, and we have relevant samples here on the left side. So of those relevant samples, the things that are retrieved, those are true positives. And then on the on the right side, those things that are also retrieved are the false positives. Precision, how many items are relevant. Recall, how many relevant items. And so this is just like a little schematic you can use to remember that. Okay, I think that is all for this slide, Dexter, if any questions about these metrics. Right. So, now, let me proceed to the Jupyter Notebook that will get started. So, the first topic is linear regression with SGD SGD stochastic gradient descent. So this is our common optimization model or algorithm. So when I talk about optimizing a model, I want to bring the weights that parameterize my model closer to those weights that will predict the true distribution. So in this class, what we will be doing for pretty much all the units is we're going to do for whichever unit is going to be the linear version first, and then the nonlinear version. So, the first unit is on regression and regression, as you are probably aware is fitting lines to points. Right, so most machine learning. So, let's say all machine learning in some regard, is effectively glorified curve fitting. And it's just about what am I trying to do with that curve? Am I trying to fit, you know, point fit to the data? Am I trying to separate regions in the data? You know, am I trying to predict a trajectory? All of these things are done with lines. And so it's just a matter of how do I draw that line. So, if you're really enthusiastic about fitting occurs to points, you will love this class. Given a set of observations. So if I have n observations from one to n, and then a set of target values. What's the simplest model, we'll call it g of x that you can think of. Well, the simplest model is probably just g of x equals zero or a constant, but that's not going to be very useful. And that's all my data is laid out in a horizontal line. So, assuming that's not it, the next simplest model might be something like this. So if I have a model g, that is a function of inputs x parameterized by weights w, what are those values of w? So here you see bold, this can be assumed to represent a sequence of values. So bold x is going to be a bunch of different x's. Bold w will be a bunch of different w's. So these weights w maybe some weight zero plus the first weight times the first input plus the second weight times the second input and so on, until I go through all the inputs. In other words, I can rewrite it as the sum from for i from one to d of w sub i times x sub i. And so if x of zero equals one, then this can be written as just the sum from zero to d of w i times x sub i assuming that x sub zero will equal one and therefore factor out. So this can be written also alternatively as this vector of w's times the vector of x. What do you want to do with the font size increased? Yeah, okay, sure. So, we have this thing here, w, t, x, what is the t for? Anybody know? Transpose, right? So what does what does that mean? So let's just write it in code real quick. So let me define a vector w and x, and this will just be 0123 followed by 1230. So if I just do element wise multiplication, right, so if I print w, print x, and then I print w times x, this gives me the result 0260y, 0 times 1 is 0, 1 times 2 is 2, and so on. So this isn't what I want, right? I want g of x parameterized, but w to give me a single number. How can I get a single number out of this? Well, in the last lecture we had this matmo operator. So that sign is the matrix multiplication operator in NumPy. And so if I do w at x, I get 8. And so what if I just write out the matrix multiplication operation by hand using summations and multiplications, we can verify that it's correct. And so that is in fact correct. Okay, great, but you haven't really explained what t is. So let me try it again with two by four matrices, right? So if I have these two matrices, w and x, that are now two by four dimensions, I can print w dot shape and x dot shape. So this will show me exactly what the layout of my different matrices are. So I'll print w, I will print x, and then I will print w again element wise, times x, I get this. And then I'll print w matmo x. So what happened? So I've done this work so far until this point, but now it's throwing an error when I try to get it to print w times x. Yes. So if I look at two by four, remember how we do matrix multiplication, you basically take every element of a row and then multiply that by the first element of the column, right, that becomes the first value. And so what that means is that if I look at the shapes of my matrices, these two values here in those inner values have to be the same in order for it to multiply together. So I can't multiply these two by four matrices together using those shapes, right? And so you will see this error frequently, probably when you're doing your homework. And so you will find mismatch in the score dimension zero with GU func signature, blah, blah, blah. And people don't really know what this means. Often what it means is that the shapes of your matrices are wrong. Right. And it is either because you set up the data, you did, there's some bug in setting up the data or you need to reshape the matrix. So now let me transpose this so I can run NP transpose over w and then print these, print the shape. So now we have four by two and two by four. So my sense is this should probably multiply. So now if I transpose w and then multiply that by x, this actually works. And so if I wanted to get a two by two instead of a four by four, what would I do? I transpose x instead of transposing w. Right. And so now you know what, if you know what shape you want to get out, you should be able to identify how you need to reshape your matrices. So an n by k matrix times a k by m matrix will always equal an n by m matrix. And so if n equals one, then the transpose of w is actually equal to w internally in NumPy. Right. So if I just take the transpose of 0, 1, 2, 3 and print it, it's just going to give me 0, 1, 2, 3. So w transpose x is nice because it's linear in the parameters of w. And so we can do optimizations based on derivatives and we can solve this analytically. It's not so nice because it's also linear in the inputs and this greatly limits the complexity of the model. So one infamous example is that linear models can't solve the XOR problem, which we use to motivate neural networks. But a model that's linear in the inputs might be the best you can do. Let's say you have a sparsely sampled distribution, then maybe you only have a linear model that would actually be able to fit to this data. So fitting data samples to a linear model, let's assume the following situation. If I have a force F exerted on a spring, this is going to be proportional to the length of the spring. This is Hooke's law. The potential energy stored in a spring is proportional to the square of its length. So let's say that we want this rod here. So let's say I have some springs and there's a rod suspended between them. And I want that rod to settle at the point that minimizes the potential energy instead of springs. This is basically equilibrium for this system. So we can conduct a series of measurements of the potential energy with the rod in different positions and we'll store these lengths as a vector w. So given by this formula here, T sub n will represent the nth experimental measurement. So that is, in this case, potential energy. And then if G is an affine transformation, so that is its linear plus a constant. So if X are the lengths of the springs, and then G is some affine function over that, this can be written as we did above. So this is basically going to be the same thing that ultimately resolves to X transpose w, sorry w transpose X. And so this will have the parameters w0 through d. So when we have this function, G of X semicolon w, this w, those are those weights that parameterize the function. And so if I know the value of these weights, and then I can put in my inputs X, and then just perform this linear operation on it, this will give me the output for any new value of X according to those those weights. So now my goal is I need to find these the best value of these parameters for weights. So which ones give the best fit? That would be the one that you take the argmin of w for all possible for all the sum of all values for all our experiments, right? So T sub n is going to be the actual experimental measurement. And then G is going to be the output of the function. So this should just give me the least squares distance, right? So here what I can do is I can set the derivative, also known as the gradient. Yes. I'm sorry, say that again. The argument. Argument is the input to a function. So, argmin. Argmin is the argument, the input that gives the minimum value. So basically, I'm trying, I'm looking for the values of w, that will minimize the output of this function. And so if I, if my very simple version would be like, what's the, what's the argmin of X squared, right? I'm looking for that value of X that produces the lowest value of X squared. And this case it would be zero, right? Because that's the, because it's parabola. And so even if I go negative, the X squared value goes up. So what I can do here is I'll set the derivative, which is the gradient is the key term we use, but for now we'll just use derivative with respect to w to zero, and then we'll solve for w. So, you know, hopefully you're all, you're familiar with linear algebra and what a derivative is and what it means to set a derivative with respect to a parameter. Now we can do this with matrices. So, matrix formulas get a bit simpler. If we assume a couple of things. One, if we assume that the first weight w is zero is multiplied by the constant one, and then X i zero, that's the first component of the sample i is that constant one. So what we can do then is we can collect all of our observations into matrices and effectively allow us to stack up these matrices such that there are correlations between the inputs and the weights and the desired outputs and then solve for the value of the weights. So T is going to be the observations I want to fit to. So in this case, this would be my actual experiments where I'm trying to find a model that fits to my experimental data. So I'm going to collect these into a matrix T, given by this, and I'll collect the examples into matrix X, right, and is the number of samples. So this will be the Rose, and then D is the sample dimensionality that is the number of observations you made the number the number of things that you measure each time so in this case if I like four springs. I would have four measurements for each of these right. And now, W, these are the weights that I'm trying to find. So, in code. Let's set some dimensions. So n is three so that is will be the number of observations, this is a toy example. So, so let's say, and is the number of measurements. D is the number of variable values per observation so number of things that I measured here I will just initialize these values to random values of the desired dimensionality. And then I'll print them. So now you can see that I have one, one three dimensional array, or, let's say, one, one by three array, and then I have a three by four array or before matrix, and then a one by four. So, the collection of all the differences is going to be T, the targets, minus x times W. Right. And this should be an n by one matrix to form the square of all values and add them up I just do a dot product. And this only works if the value is a scalar which means that he has to be a column matrix. If you want to predict more than one value for each sample, T will have to have more than one column, which we'll get into later. So let's continue, assuming that he just has K columns, meaning you should want a linear model with K outputs. So let's compute this team minus x w. And this will give me according to these random values here. So this this output this is basically assuming these weights w. And given these inputs x. How wrong are these weights w and coming when it comes to predicting the output. So I can do the, I can use this to get a complete scalar value so this might be one way of measuring the distance from the predicted value given these in this case randomly initialized weights from the true value. So if I want a simple scalar value, I can just take that dot product. Also, just in point of fact, dot t is the same as NP transpose so I recommend using this just because it's shorter and requires less typing. So now to find the best value for w, get through this part. We'll take the derivative of the sum of the squared error objective, set it equal to zero and then solve for w. So, here's all the math what you will find is that in this class. We're probably not going to, I'm not going to dwell a whole lot on the details of the written math, because what we will find is that will end up doing the same thing in code. For those of you who are interested in how the mathematical operations work. The math will be presented for you. But effectively what we find is that I want to, you know, I want to take the, the derivative of the square error objective with respect to the weights w. And so this would be if I take the derivative I can bring the two down in front. And so now I can take the sum of the differences times DG with respect to the w. So this all works out in the end to negative two times what we see here. So, T sub n minus xtw times x. So basically look at, look at this, this is the target, minus the predictions, right, x times w times those inputs. So this gives me this, this again also uses all three of those values. So here's where we get the benefit of expressing inputs. So I'm going to use these two examples as matrices, because the song can just be performed at the dot product. Right, so these are all expressed as matrices, then I can just take that entire matrix x transpose it and then multiply it by matrix t time minus x times w which is itself would be matrix of the same size. So, if we just check the shapes and the size sizes of each matrix in the last equation above if I take x dot shape. It's three by four x transpose dot shape is four by three t dot shape is three w shape is four. And so now we can set this equal to zero and then just solve for w. So if I take the above equation set it equal to zero. Well, I can just divide by negative two is a constant. So now I can just divide by x transpose times x and that's going to give me w. In Python it looks like this. So, here if I take, I'll just use the NP dot linear inverse function, and then I can compute those values. There are a couple of different ways to do this in basic NumPy you can use the solve function. This will assume that the solution is going to be a function of the same size. So I'm going to use the same solution, and then I'm going to use the same solution to compute the same size. So I'm going to use the same solution to compute the same size. And then I can compute those values. There are a couple of different ways to do this in basic NumPy you can use the solve function. This will assume that x transpose times x is full rank that it has no linearly dependent columns. So that is you can't have a column that's can be represented in terms of other columns so I can't have a column that's like 123 and then another column is 246 right because 246 is just a constant times 123. So, solve function assumes that it is full rank. So basically this assumes there's no linear dependence so not a constant times a column not a constant plus a column, not two different columns added together. Assuming that is true. Then, I can, I can use this function, or better yet, use the least squares function this will not make that assumption. So, least squares actually produces a number of different things. So, what are those things. Well, one thing you can do notebooks is you can use the dot string so just put a question mark after the function, and it will pop up this little box or something. Lots of little box that gives you the doc string for that function so you know what the inputs are, and what it actually does. So this returns least squares solution to a linear matrix equation. It assumes the following parameters, and it will return the following things. So what I'm going to do here is this will return the least squares weights, the residuals, the rank, and value s which it said was singular values. But hold on a second, these solutions do not appear to be the same. So, if I look at the all close function. The easy all close function before. So, if I have two values that are equal to each other, like zero equals zero which return true right. So, what if I have zero equals point 000001. That's not going to return true. But remember in machine learning everything is estimation and so it is entirely possible to let's say write a neural network where you have an activation function that's a plus sign and inputs of two and three and it tells you that the output of that is 4.9999997. So, you have to get comfortable with approximation. So the all close function will basically say are these values close enough within a distance. So, it takes a very small epsilon value, and you can take any two arrays, it can be individual, just single numbers or huge matrices, and will tell you, are these things you know close enough to be equal to within a rounding error. Okay. So if I run this, it will actually say these two things are true. So, close enough. So all of these even though they may not on the surface appear to be identical, they are actually close enough to be the same. Now, there may be multiple solutions to a system of equations right many times, you know, a times, b equals a time C or a dot b equals a dot a dot c and b and c are not the same thing. Right, because I can actually multiply them by a different matrix a and get different values. So, the least squares and solve functions can be written with simpler arguments, because they're designed to find the value of w that minimizes the least squared error using this matrix product as an argument will simplify the regression implementation so linear regression. Right now we use the simpler version. So, let me just get w out of this. So now I can just use least squares that just takes in x and t and I'll pass our cond equals none. And then I'll get back w which is what I'm interested in. So, according to our random values that we initialized above here. This. These are the best weights to solve to basically map from those randomized inputs to those randomized outputs. So, what if I have thousands or millions of samples. In this case, x and t can be pretty large. So to avoid dealing with matrix operations on huge matrices and you just see. Okay, I get down to example of SGD in action. These can be quite large so to avoid dealing with matrix operations on these huge matrices. You want to derive a sequential algorithm that finds w. And what this does it uses the fact that the derivative of a sum is the sum of the derivatives. And so now you can express this derivative as a gradient, that is, a derivative and high dimension, so I can now represent my single derivatives in every dimension as just a vector of derivatives. So if you imagine, we're used to thinking in two dimensions where I have like some curve, and the derivative is the is the slope of a line at a given point right. So now imagine you have a three dimensional surface. So, what's the slope of the curve. Well, it depends which way you're looking right if I'm standing on the side of a hill. The slope is going to be different. If I'm looking at say east versus I'm looking south. Right, because it may have the surface is going to have you know different slopes in different dimensions and so if I am you know standing on a if I'm like leaving my, my house, and my driveway faces north. Then the northward slope is going to have like some negative values of slopes down, but then if I turn and face east it's a flat surface so like the eastward slope would be close to zero. Right. And so each of these dimensions is going to have its own derivative that can be represented as a vector, right can be negative and and zero or something like that. And that's the gradient. And so, gradient is just a high dimensional derivative, usually written with this upside down triangle symbol pronounced Dell, and it can be as big as you want. Okay. So here's the math for that so recall that what I'm trying to do here is I have my function of experiment rise by w is just a linear function. I now have going to, I'm going to take the, the error here so easy error with inputs xt and w. This is represented as just the sum of all targets minus the predicted value squared. So if I take the gradient of this, then what ends up going on here is now I have the gradient. I can now bring the square down in front. And so now I can take two times, T of n or T sub n minus g of x, w, times the gradient with respect to all w's of the right so now this is the error gradient. Eventually this simplifies to this thing at the bottom. So I can bring the negative two out in front. And so now this is going to be the sum of all the errors times the inputs. So now instead of summing over all of the samples. What if I just take a sample of the gradient. So if you update the associated weight for each sample, based on the gradient for that sample. So if I'm on my driveway and I'm facing north and it's, you know, sloped in a certain, a certain direction that might tell me that I might need to update that value more than the grading the other direction which is zero. So another way to think of this is, I guess in the last four minutes. If you are standing on the lip of the Grand Canyon, what's the fastest way to get to the bottom. I know someone wants to jump. No, you're going to die. What's the fastest way to get to the bottom without dying. Well what you might do is you might look in the direction of the steepest slope and move that way. Right. May not be the wisest choice is like a lot of cacti and stuff. But that would be one way to do it and so if I'm trying to get to the bottom, an inefficient strategy would be to walk along the lip or something like that right because it's going to maybe it's going to slope up and down, but it's not going to get me. It's not going to really get me toward the bottom very fast. And so I want to assess in which dimension. Am I going to have to fast, find the fastest slope, and I'm going to move in that direction. And so that's the gradient is this high dimensional slope. And what we're going to do is we're going to descend the gradient. So, and that's gradient descent. And I believe that's kind of all the time that I've got today, I will field questions in the last three minutes if there are any. So, we will continue with this on on Tuesday.