 Okay, let's go ahead and get started. So hope you all made a start on the assignment. So it's going to be due a week from Thursday, by default. So hopefully that's going okay. I will have office hours this afternoon. And as I mentioned, I think I'm on the faculty committee and I may have to cancel some office hours depending on what we're doing with that. But currently this week it looks like I'm going to head off. If you are interested in our faculty hiring process, I encourage you to come visit the faculty candidates talks. There's usually going to be on usually a Monday or a Thursday at 11 a.m. We will send out an announcement to the CS list when those are scheduled. Other than that, anybody got any questions, concerns about the assignment? Yeah. You don't specify if they're just separated. Good question. You don't need to do that here. That is probably a good skill to practice. I had this conversation the other day. You don't need to do that for this assignment. We will have one assignment that is explicitly about that. And from that point on, you will be expected to do train test splits. Any other questions? All right. So who's ready to talk about neural networks? So I understand neural networks can be somewhat scary. So I'm going to introduce the neural networks in the easiest way possible. Everybody, please open your textbooks to page one. This is neural networks for babies. I'm going to read your neural networks for babies real quick. This is a ball. This is a neuron. It sends messages throughout your body. Give the neuron input and output, and it can help us learn. Give the ball input and output, and it acts like a neuron. The neuron can have one input and output or many. It starts to sound familiar. Is there a red animal in this picture? The neuron can tell us based on its input. When the neuron has an answer, it sends its own message. Does this animal have eight arms? The neuron could tell us based on its input. When the neuron has an answer, it sends its own message. Two plus two plus two plus two equals eight. Where do the messages go? Neurons talk to each other. They connect in a network. Input neurons look at parts of the picture. Output neurons have answers to the picture. Neurons in between don't see the pictures or give answers. They're hidden. How do the hidden neurons learn to decide? Training data can have correct labels on them. After training, the network has learned to label new pictures. A really big network can solve even harder problems with the help of computers. Now you know neural networks. Thank you. OK, class is dismissed. Now you know neural networks. Please go home and do assignment two. Now, as you can see, there's a whole lot probably missing from that. If you know anything about neural networks, the treatment of convolutions and back prop is not up to the standards of a major AI conference. So I think the part of the end where it says, now you know neural networks is a little bit weak. So for the remainder of this, we will go from linear regression to neural networks, not the baby version. We'll fill in some of the details. All right. So people don't believe me when I say this book is real. So I had to bring this up. Oh, I read my kid repeatedly. Now my daughter is at the point where she vastly prefers my wife to read things. I'm not offended or anything. Because she wants to read the same books over and over again. I don't have to do that, which is fantastic. But now she had a while where this was her favorite book from a week ago. So she was taking it to my wife to have her read it. And my wife is a historian. So she's very smart, but not in computer science. And when she gets to the part of now you know neural networks, no, I don't. All right, let me share my screen. And we'll go through neural networks for adults. All right, so if you remember, hi, this, that. Shoot, I put in the controls. OK, so if you remember from the linear regression lecture, I briefly mentioned that linear models can't solve the XOR problem. So I assume everybody knows what the XOR problem is or what the XOR operation is. Basically, if I have two inputs and then I'm only looking for those cases where one of those things is true. So if I have x is 0, y is 0, 0 is false. Neither of those things is true. So x, x or y is 0. If y is true but x is false, then x regular or y is true. And so is x, x or y is exclusive or. The same is true if the ones in 0s are flipped. But if both of them are true, x and y would be true, hence the one. x or y, one of these is true. But x, x or y is 0 because it's the exclusive or. Only one of the inputs can be true for the x or to be true. So we can also graph this. So in the version below, we have the blue x's where the x or is true. And then the red, the black circles where the x or is false. So if I have 0 and 0, it's false. If I have 1 and 1, it's also false. But in the cases where only one of them is 1 and the other one is 0, then it's true. So you can tell by looking at this already, if I'm graphing like this, this is clearly not a problem that a linear model can solve. There's no way to fit a line to these points when the points are in a square. So that should be pretty evident. So what I'm going to do now is I'm going to basically present the solution to the x or problem done with matrices to motivate how we use the introduction of nonlinearities to solve more complex problems. So basically working backwards, this is the solution to the x or problem. So if I have the following weights, 1, 1, 1, and 1, and then c equals 0 and 1, w equals 1 and negative 2, and then this bias is 0. So let's write these just in numpy form. So let me set those two variables. Now I'll set x to be some matrix that contains the inputs. So the first column can be taken to be x. The second column can be taken to be y. So 0, 0, 0, 1, 1, 0, 1, 1. So write this as a numpy array. We get the following. So now if I take those weights w and then multiply the input matrix by that first layer, I'm going to get the following. So if I do x at w, I get this, 0, 0, 1, 1, 1, 1, and 2, 2. So now I take this bias vector c, add that, and then I get the following there. Now this is going to be this nonlinear step. So so far I've just done a linear operation. The inputs times some weights and then I add some bias. That gets me to this point. This is not the solution to the XOR problem, as we can see. We have 0, negative 1, 1, 0, 1, 0, and 2, 1. It's not clear how that maps to the solution to the XOR problem. Now what I need is some function that's going to allow me to take the input converted to an output that begins at 0, rises to 1, and then drops to 0. So I'm going to apply this nonlinear transformation, where if z is less than 0, it's 0. Otherwise, it's z. In other words, I'm going to want to take the max of 0 and z. So I could write this function f of z that does that. And so then f of z will give me the following output. So now I have 0, 0, 1, 0, 1, 0, and 2, 1. So all that's done is basically taken this part here. And the only thing that's changed is this negative 1 into a 0. The result is that now I have these things mapped into a learnable space. So what's going on here? We have the input being 0 and 0. The output is 0. If the input is 2, the output is 1. And if the input is 1, the output is 0. And we have two cases here. That's why I only see one point there. So the output of these linear steps, effectively, among other things, turned this point here into basically 2, 1. This one here used to be before the nonlinear operation used to be down here. This is 0 and negative 1. So if you see where my cursor is, and then the blue x and then the point in the top right, you can see you fit a line to that. So that was the linearization of the input. But now applying this nonlinear step, this gets me this. So now it's in a continuous space that I can actually learn. So now I multiply that final weight vector, w. So we had input x times big W, which is one set of weights plus a bias. Now I'm taking that, applying some function to it to turn it into this space here. Then I'm applying another set of weights to it. And that gives me the output here, 0, 1, 1, 0, which, if you remember, corresponds to the XOR. So now all in one function, I can effectively take this neural network where I have the weights and biases pre-specified. That will give me the output that is the XOR for those inputs. So if my inputs are 0, 0, 0, 1, 1, 0, and 1, 1, I apply this function, XOR nn, and it gives me 1, 0, 0, 1. So I've done this all in one line. So it's kind of hard to tease apart exactly what the different components are. So I'll write it more legibly. I'll do a version of XOR nn. So we take the hidden weights. First of all, we take this input x. We have specified weights w and bias vector c. So now the hidden weights is going to be, I should probably call this hidden output, I guess, x at w plus c. Then active stands for activation function. We should go into a minute. This is some function f that we defined earlier over that value. And then output is going to be the output of this, so the activation, times another set of weights. And all that will give me the XOR. So by introducing this nonlinear function that allows me to take some inputs that are mapped to a line and then deform that and then multiply some other weights by that will allow me to solve this problem that a linear function was not able to do. Any questions? OK, so this example, of course, involved no training. I basically gave you the solution of what the right weights were and then showed you how once those weights are in place, we can use that to solve this problem. If you want to know how we actually train to solve this particular problem, there is an article here that you can peruse. I recommend not really doing that until we get to the end of the notebook because it's going to assume that you know neural network operations like backpropagation. So I'm going to go ahead and show you neural network operations like backpropagation, which you presumably don't yet because you haven't got there. But once you do, if you're interested in how to actually train a neural network to solve a problem like XOR is very logical, this can show you how to do that. So basically, the takeaway that I hope you have seen is that all neural networks have the same basic form where we have some function applied over some other operation where that operation is just a linear operation. So if we take the inside of this function, wx plus b, this should look exactly like what we were doing in the previous lectures. We just have some affine transformation weights w, shift it by some bias vector b, and that gives me the output. So I have a line, and I'm just trying to take inputs and map it to someplace on that line. So what's f? f is some nonlinear function. OK. These functions are usually called activation functions. And so last time we saw how we could do these fixed nonlinear inputs to introduce nonlinearity when it seems like there's not a linear solution to the problem. So this is somewhat labor intensive in that we first look at all of the data and see that there are some places where there actually appears to be some correlation between some parameter and the output, but that correlation is not linear. So if you remember in lecture four, we had resistance of the vessel on the output. We had resistance of the vessel on the y-axis and this thing called the Froude number on the x-axis. And there was some sort of quadratic looking curve. We're basically trying to figure out what actually is this curve. It looks quadratic. Is it actually quadratic? It turned out that the best answer is probably x to the power of 13 or something. But that took a lot of effort trying different feature functions over those inputs to see what exponent allowed me to convert that input into a linear function or to convert the input into a form where I could apply a linear function and get the output. And that's not going to scale very easily. What if we had some arbitrary way of introducing nonlinearities? So we don't know which nonlinear functions to use, but what we can do is we can pick a form of nonlinear function that has its own parameters or weights that will determine the grade of the nonlinearity it introduced. I know I'm going to deform the data in some nonlinear way. What kind of nonlinearity and how nonlinear am I going to be making that transformation? So we want the parameters of this to control the actual shape of the function. And there are a bunch of possibilities for such functions. I'm sure you can think of any number of nonlinear functions that just satisfy the property of being nonlinear. But we need some desirable properties that I'll illustrate here and then go into why we want those properties later. So first of all, I want to be computationally simple. And one major reason for this is that we're going to be doing this a lot. We don't need to have some wild polynomial being calculated for every data point over 1,000 training epochs or more. We also want initial small weight values. We want the value of that function to be close to linear. And then as the weights, the magnitude of the weights increases, the function becomes increasingly nonlinear. So now you can think of, let's say, on the x-axis, we want it to be closer to linear. And then as the values grow to extremes, we want it to be more nonlinear. We also want the derivative of the function to be computationally simple for similar reasons as we want the function itself to be computationally simple. We also want the magnitude of the derivative to decrease as the weight magnitudes grow. And perhaps we want this to be asymptotically. This is not always true, but it's generally a desirable property. We also want the maximum value of the magnitude of derivative to be limited. So we want the derivative itself to basically have some known maximum value. So let's go about deriving some properties or from these properties what a desirable activation function might look like. So first, let's start with just a linear weighted sum using the familiar formula, xtw. So for some input sample x, we want s, the output of this function, to be small if the magnitudes of the weights in w are near 0. As those magnitudes increases, we want the magnitude of s to also increase. So for example, if we have some data here, just weights that are arbitrarily chosen numbers and then inputs. And then if we change these values here, let's just say let's make a bunch of them much bigger. We can also see the value of s also increases. That should be pretty intuitive because we're still dealing in the world of linear functions right now. So now, if we want this function s, let's try to construct a function where s is the shape of the derivative that we want. So basically, s is going to be the derivative of some function. I'm going to derive the function f, where s is its derivative by first constructing the derivative and then seeing what the interworldly antiderivative of that function is. So first of all, we know a couple of things. If I were to take the negative of s and use that as an exponent, this means that I can take whatever function that is and make it asymptotically decrease to 0. So we want that according to point number 4. So we can use base e. Base e has a lot of nice properties, one of which is that e's derivative is e to the x. And the antiderivative of e to the x is e to the x. So e allows us to deal in natural logarithms, has some very nice properties with regard to differentiation and integration. So first what I'll do here is I will just plot some inputs that are evenly spaced. And then I will plot the e to the negative s. So of course, as we all know, we can see that as the value of s increases, e to the negative s is going to decrease and approach 0. So now remember we're constructing a derivative. We want the maximum value of that to be the limit the maximum value of that derivative. So here what I can do is I can then take, say, divided by 1 plus e to the negative s. And so now unlike this one where the maximum value is basically going to be infinity as I get more and more negative, and the minimum value has a limit, by doing this, I now have basically limits on the minimum and maximum value. And so the maximum value is the important one that we want to control here. And by virtue of that also, we would also want the negative of this to also be limited. Right now we're in a window of an interval of 0 to 1. So we don't really have that problem. But we'll see what happens with that in future. OK, so this does what we want as s grows more and more negative. But we also, as s becomes more positive, we want to bring this function down to 0. So one way I can do that is just by taking 1 minus 1 over. So now this is going to start at close to 1 and eventually decrease to 0. So I want some combination of these. So I basically want something that starts like this and then rises and then falls again. So I want the left half of this function at the top combined with the right half of this function at the bottom. So what if I just multiply them? OK, so that looks pretty good. This is doing what I want. It adds asymptotes to 0 at both extremes. And then the minimum value is limited to 0.25. So the last desirability that we want is we want this to be computationally simple. We take a look at this function. And we can see that there are some common terms. So for example, what I can do here is I'll just create a bunch more points. And then I will recompute this function. So we see we have basically e to the negative s. It's in terms of 1 plus 1 over. So all numbers we know how to deal with. 1 and the e, effectively, with an exponent. OK, great. So now what I can do is I've got this. And I've got this function. Now let me see if it has property, if its antiderivative has properties that are also desirable. One library you can use is this thing called SymPy, symbolic Python. This is the only time we're going to use it in class. But it is kind of fun to play with. Basically, what this allows you to do is define operations and then actually perform differentiation or integration on them. And it will give you the resulting formula. So effectively, you can use this symbolic Python to sort of solve those pre-calculus problems that you've probably done before. So first, need to init printing. Allow me to use Unicode. So I'll define a symbol, s. And so this s symbol will just be the symbol s. So now I can run this .diff function. So what this does, this is just going to differentiate f with respect to the symbols. So in this case, I have some function f of s that I want to differentiate with respect to s. So what I do here now is I define my function, so s to the fourth. And I want to differentiate with respect to which symbol s. And now we can see, if you remember your calculus, we should expect the derivative of s to the fourth to be 4s to the third. And so this will actually put that out for us. OK, so we see the SymPy.integrate function should be pretty evident what that does. This is going to integrate the function with respect to the symbol. So what I'll do now is I will define the function that I defined above, 1 divided by 1 plus e to the negative s. So I'm just writing this in terms, in the SymPy formula, in terms of the symbol I just defined, s. So now I want to integrate the function y times 1 minus y with respect to s. So this is going to give me this, 1 over 1 plus e to the negative s. So if f of s equals this, then the derivative of f of s is f of s times 1 minus f of s. So we are now just derived at the common sigmoid function using neural networks. So remembering that s above was defined as just a linear function xtw, then we have some function x, some function of x parameterized by w, equals 1 over 1 plus e to the negative xtw. And so remember what this is. This is just going to be the output of that linear operation where we have our weights and then make some sort of prediction by multiplying the input by those. Questions about this so far? So let me define some helper functions. So I'll just define f of s. And then I will define its derivative, so df. And then I'll plot the function versus the value of s and the derivative of the function versus the value of s. And so we get this. So this function s in blue, we can see that it rises asymptotically to 1, whereas its derivative is that function that we saw before, and it caps out at 2.5 and has asymptotes at 0 at both extremes. So this is called the sigmoid function because it looks a bit like an s. And it is bounded between 0 and 1. And so now we can use this function because it has these nice properties that we have identified in neural network operations. So first thing we're going to do before we get to the neural network of hidden layers is to just apply SGD to fit the sigmoid function to some data. So we're still working more or less in the world of linear regression. It's just we have this final step of applying the sigmoid function so that we have some sort of nonlinear output. All right, so what we're going to do is we're going to find weight values that satisfies the sum of squared errors in the output of this function. So what I will do here is I'll just define, again, some points. I'll take this function here and just add some noise. So basically, this is I'll take t times or t equals x times 0.1 plus some randomly sampled noise from a uniform distribution. And it's going to give me some data that looks like this. This looks like I could reasonably fit a line to this data. It might not be the best fit in the world, I could do it. But it might be nice if that line had a bit of a wiggle in it. Maybe that would fit to the data a little bit better than just a straight line. So if you think about what this data represents, the values in x are, at this point, those are just inputs. They don't actually represent any real data points. The values in t, though, are our targeted values. And these are derived, in this case, by some known function applied to x where we've applied some random noise to it. So those targets are not going to be neatly identifiable. They're not deterministically identifiable from x. But you should be able to still fit a curve to it. So here's a training function where I will put in my inputs, my targets, my learning rate, and some epochs and train for a certain number of steps. So what I'll do here is I'll just train this for 100 epochs and then plot the results. So here we go. After training, it ends up with these weights, in this case, negative 1.7, 0.4. And then if we apply f here, which is our sigmoid function, this is going to give me this output for the sample according to the inputs and then apply times the weights and then apply some nonlinear function. So we get this. And we can see that it's probably a decent fit. You can see that it's nonlinear. And it does fit to this data decently well. I'm not sure that it's necessarily a better fit than the linear operation. It's hard to say. But you can see that also the nonlinearity being applied is quite slight. This is not hugely different from a line. So let me create some other data. So different distribution, different function. So 1 plus negative x times 0.1. So now we get this. Probably mostly linear, but again, could be fit to with a nonlinear function. Let me try this. And that fits pretty well as well. So seems like I got a decent way of taking my sigmoid function, computing an output according to a linear operation, applying the sigmoid function to that. And I can use that to fit to what might be some somewhat nonlinear data. So the question now is, if I have this data, how could I fit this function to that data? This is nonlinear. I would want my sigmoid function to be able to capture the nonlinearity that is obviously present in this data. What I've got is the ability to train weights. And once those weights are trained, take those inputs times the weights, apply my sigmoid function, and then fit to some nonlinear data. So if I try it with this data in this curve here, what I end up with is here are the weights that it's trained. And the result is that it gives me this. That didn't seem to work very well. Let me try it again. Compute some more data. So it fits decently well on one part of the curve, but not the rest. So we can see here, when x is positive, it seems to be kind of fitting to that OK. But when x is negative, it's just more or less a line. So there seems to be some sort of inflection point here around x equals 0. And we want to find out what kinds of weights will make the sigmoid function go down from negative infinity to 0 and then rise again. This is what will be needed to actually fit to this data here. Whereas here, we're only kind of getting one side of the equation, if at all. So we don't know what those weights are. And they're not really easy to find, because the only weights that I've got here are two weights that effectively, the first weight here, that's a bias, as before. So how much am I shifting up and down the y-axis? And then the second weight here is, what do I apply to the linear operation, or what weight do I apply in the linear operation before I apply the nonlinearity? So I'm not really doing anything that affects the slope of this nonlinearity in any real way. All I'm doing is I've defined the sigmoid function, and I apply that over the output of the weights times the input. So we could try maybe using two of these functions and adding them together. So what if there were one set of weights that caused f of s to decrease until around 0 and then remain roughly flat here, and then another set of weights, we'll call those v, such that if I take x times v and apply f over that, it would be roughly flat and then start to increase. So we'd expect there now to be two sets of weights. We'll call them w and v. So now we're talking about the world of multilayered neural networks. One layer is going to have, in this case, two units that output f of x times w, each with their own w. And the second layer is going to have a single linear unit with its own w. So does everybody get the motivation for having these two different sets of weights? OK. All right, so now let's talk about linear models, as we're familiar with, as neural networks. So what I'll do here, and I cannot guarantee that we're going to get through the entirety of this notebook today, because it is fairly long and there's quite a bit of math. But what I'll do, like usual, is I'll present the mathematics first, go through, effectively, how we are deriving the different operations that we're going to be using in the construction of the neural network. And then at the end, I'll have the Python version that translates the mathematics into code. And demonstrates how you would actually write these operations. So remember how we do just linear modeling. We have inputs x, targets t. And then we have, for every sample k, we want to find the weights k that minimizes the squared error in the k-th output. So we'll say for x sub k, I want to find the weight vector w sub k that minimizes the output between t sub k or between the prediction y sub k and the actual output t sub k. Then we'll use that to make predictions. So what we'll do to make this go faster is we'll take all of these weight vectors w sub k. We'll collect them as columns in a big weight matrix w. Some, the x with the total above it, I'll use to denote x with the constant 1 column. So remember, we always add this bias column so that we have values against which we can train the bias weights. The target value for the k-th output for the n-th sample is going to be t sub nk. So remember how we set this up in a linear model. If I have n samples, number of things that I've got, each with d dimensions, number of things I've measured about that sample. And then I have k things that I want to predict about that. So this could be just a single value. Could be multiple values. So if I have the first sample, and I'm trying to predict the second thing about that in the output, this would be, assuming that we skipped the first one, actually, the second one would be indexed at 0. This would be t sub 1 comma 2 or something like that. So we're using this to calculate the error. So we have e of w. And so what I'm going to do is I'm going to take for all samples, for all outputs, so sum of all n over the sum of all k. I'm going to take that output for that sample minus the prediction. And then I'm going to square it. I need to sum this for all combinations. So now I'm looking for, effectively, the value of w that will allow me to minimize this function. Now w can also be calculated as, remember, if we rewrite these as matrix operations, as x sub t, let me increase the font size a little bit, x total tx. Then I transpose this, or take the inverse of this. And then x total transpose times t. So we can compare this to solving for the value of w in notebook 3. So what's the contents of all these matrices? Remember, w is going to be w associated with every output and every input. So there are k things that I'm trying to predict, and then d dimensions to every input. That's going to have a weight value that's correlated with each of them. In a linear model, what I can do is I can actually look at my weights and decide what was most important for predicting the output in a neural network that becomes less easy because of these hidden layers. So if w looks like this, then my prediction y is going to be biased x times w. What are the shapes of these things? So I have n samples, d dimensions, add 1 for the bias. So x tilde is n by d plus 1. w has to be d plus 1 by k in order to multiply with this, of course. And so if I multiply these two things together, then y must be an n by k matrix. And so this should make sense because, the things that are represented in y, those are going to be the k predictions for each of the n samples. So if I have 100 samples and I'm predicting two things about, I should have 200 individual numbers represented as 100 rows in two columns. So for every element of that output matrix y, it is going to be equal to the nth sample with the bias times the kth weight vector. And this can be drawn kind of like this. So what's going on here? I have my different inputs, x0 through xd. And I'm going to multiply each of those by the associated weight, and then sum those. And that's going to give me the associated output. So now, if I want to add nonlinear combinations of inputs, what I'm trying to do is I'm trying to transform x into some function. We'll call it phi of x. So for example, if phi of x is phi over this big weight matrix, and if I'm trying to add nonlinear combinations, what I might do is I might raise this to a power or something. So this is the same thing as introducing nonlinear features in the inputs. So now what you can think of that is that instead of raising it to a power and stacking a bunch of those things together, I'm going to have some arbitrary function that is applied to this. And then I'm just going to replace the output x by phi. And so now I'll use phi to represent phi of x. And so therefore, phi of phi sub n is going to be this function phi whatever it is over x sub n. So I'm searching for a function. But we have learned now that functions can also just be represented as a linear operation with a set of weights. So if I have f of x parameterized by w, my goal is try to solve for w using some algorithm like SGD that allows me to minimize the error between my predictions and my outputs. So what I'm doing here, this is the part that's going to fit into the interior of the neural network. And what I'm going to do is I'm going to try and predict the weights that give me the output and then minimize the prediction error when there are multiple transformations being performed at every step. So when we talk about, I should have pasted this again down there. So when I say all neural networks have the same basic shape, which I did way up here, I think. So I say all neural networks have the same basic form. What we can now do is we can expect this function to kind of be stacked on top of each other. So I'll take this output. I'll perform another operation over it and maybe another operation over that. And every layer in this neural network represents one more function being applied. So all of these things can now just be nested. Different functions are the same? Different functions, yeah. So remember, a function here I'm just talking about f of some x parameterized by w, which means that the w could be different each time. So when we talk about neural networks, I knew this was going to come in useful at some point. So you've seen this diagram that you've seen neural networks written like this. So every layer here that is every column of red dots is a function. And so each one of these is parameterized by a different set of weights, meaning each one of these is a different function. And our goal is now we're just trying to solve for those weights that parameterize each of these individual functions at the same time. It's a function of a function of a function of however many layers you have. Yeah. So are we implementing these functions to the basic function that we have and then we're getting the output out of that? Effectively, yes. So I'll come to the mathematics in a moment. But you can kind of think of it like this. So if I have just a linear method, a linear model, it's got one input and or a known input and just like a known set of outputs, it's a linear transformation between them. I can compute the error and then use SGD to optimize the weights that are going to minimize. Let me insert a hidden layer in the middle of that. If we just look at those first two layers, and let's just disregard the activation function for a moment, it would be just like a linear operation in that if I knew what the output of that second layer was, I could use that to minimize the error between the predictions of the outputs. But the problem is the targets that I have, I have a two-layer neural network actually correspond to what come out of that second layer, which I haven't even touched yet. So the input is going to be transformed by some function into the hidden layer. But what that number is not clearly correlated with the actual output is because there's another function that must be executed over this intermediate number to get the output. And I don't know what that function is just yet. So the output we get is not the actual one. Right. The output you get can just be considered some sort of scalar that is not interpretable. That's why I talk about these as hidden layers. So if I get the output, I know what the units of the output would be, what it's supposed to represent. Is it a class? Is it a miles per hour or something like that? But that hidden layer in there is going to give me some scalar outputs. It doesn't really have units or anything like that because it's kind of combined multiple channels that input data on its way to getting an output. But I haven't got the output yet. So I can't assign any meaning to it. Would you say it would be a bit more optimized for the original data that we had instead of the way of getting it to be? Sure. It can be. So the idea is, and we are getting a little ahead of ourselves, but this is interesting. When you've correctly optimized a neural network for a task, you can actually get very useful representations out of the interior. So for example, I think I mentioned in the first class, a lot of my research involves these things called embeddings, which are basically continuous representations of classification labels. And so you can actually take these embeddings that are hidden layer representations and use them. They're just numerical values or numerical vectors. By themselves, they're not interpretable, but you can use them for other tasks. And they actually preserve a lot of information. So basically, these hidden layer representations preserve the information that is necessary when the network is well-trained. But a human would have a hell of a time trying to figure out what the actual meaning is. You can do things like cosine similarity to figure out where are the clusters in this thing, is it similar to, but it's not something you can say, like, OK, this number represents a bird, this number represents a feather, this number represents a microphone. Right. OK, so we have some function. Sorry, any other questions? So we can easily spend two days on this notebook, so that would be fine. I think that's how much time I actually built into this. OK, so now let's assume we've got some function phi that is going to be an operation over x. So now what I want to do is if phi times w is going to give me my output, now I want to do the derivation that's going to minimize this error. So whenever I multiply phi times these weights and then square the sum for all my samples, that's what I want to minimize. So I want to find, so you can see now that I end up with w is going to be equal to phi times phi raised to negative 1 times phi times t. So now I can use it in the same formula that I had before, where I have y equals phi tilde times w. So now I'm focusing on if w is this output layer, this thing is actually going to produce the prediction, I want things that are going to be useful going into w, I want things that go into w to be useful to predict that output. But the trick is there's kind of the separation between the inputs and the outputs. So the x is the input. It goes into some weights, we'll call them v. That produces phi. I want phi to be useful when multiplied by w for predicting the output. So now I have these two functions that I'm trying to optimize at the same time. But there's no generic way of arbitrarily separating those. It allows me to do this over a large number of samples at scale. So if I know that this is a nonlinear function, I want to introduce some arbitrary way of having nonlinearities and nonlinear operations performed over my data so I have activation functions. So get to that, details on that in a moment. So if we take a look at this, this is what I just discussed. I have the inputs. Something happens to them. We'll call that thing phi. And then whatever happens to them, I can multiply by weights w to get my outputs. And so this yellow box is the black box of a neural network because I don't know the nature of this transformation. So this is passed through some nonlinear function, multiply by w to get the output y. And now I'm just trying to figure out what the hell goes in this yellow box. So focus on serving its purpose. Can we use training data to find out? Training data can have correct labels on it. So if I know what the correct output is, maybe I can use the training data to actually optimize the weights. So the textbook, so to speak, talks about vision. We'll talk about convolutional networks in lecture 13 or something. Right now, it is doing regression. So it's all still numbers. But the same principle applies. I can use the training data to figure out what needs to go in phi. So we've now just entered the world of neural networks where 5x is going to be the output of some layer of adaptive units. So 5x, we'll call it h. H is typically what we use for the activation function. So I've used a bunch of different terminology here. F of s, phi of x, h of x, all referring to the same thing. So because the neural network is a universal function approximator, it's not really useful to talk about the function in abstract. You want to know what function I'm talking about. So we'll use h to represent the activation function. In specific, the activation function is this nonlinear function that's applied over the output of a hidden layer. So this sort of looks like this. So now, before, we had something that looked like this side of the equation with the x's going straight into these blue nodes here and producing outputs. So the only thing that's different here is that instead of x going directly into the blue nodes, it's going into these yellow nodes, where they have weights v. These x's are multiplied by the v's. And then before the output is produced instead of a sum, you have this activation function h. So let's say x goes into the first hidden layer. It's multiplied by weights v, which at start is just going to be arbitrarily initialized. It gives you some number. You apply the function h over that number. It gives you a different number. We'll call that z. And then you have a bunch of different z's. And z's go into the next layer, which have weights w in them, that also maybe randomly initialize different numbers. The z's are multiplied by these w's. And then each z, you take the linear sum of z times all the w's, that gives you your output. So now, I'm still in the world of trying to optimize weights. I just now have two separate weights, two sets of weights, v and w to try and optimize. So the dimensionality of each step will be as follows. So n will be the number of samples. d is the number of things you measure about n. So x tilde will be x times x by d plus 1. v is going to be d plus 1, because it's multiplied by this, times some other dimensionality m. And I'll just specify how many m's I want to get out of that first layer. So now, z tilde. If I have x times v is equal to z, this means that it should be of dimensionality n by m, because we have the d plus 1's, those should cancel out. But this also has to go into another matrix operation. So I need to append a bias vector onto it. So now, this will be z tilde is going to be z with the bias, which is going to be a dimensionality n by m plus 1. So w naturally would have to be how many things are going to come in, m plus 1. How many outputs do I want? k. So this should be m plus 1 by k. And so z tilde times w, the m plus 1's should cancel out. And so we end up with an output that is of size n by k. So the final operation looks something like this. So if z tilde is h applied over x tilde v, so here's h. This is x tilde with the bias times v. That gives me z. Then I append my bias again to z. This gives me z tilde. And so then z tilde times w is equal to y, which means that I can write this all as a single function. So here we have y is equal to h of x tilde v. I apply the bias again to that, multiply that by w. So the two layers in this case are called the hidden layer and the output layer. In larger neural networks, the last layer, of course, is always the output layer. And then anything besides the input layer are going to be hidden layers. So we talk about the last hidden layer or the first hidden layer. And we can talk about the things that are represented at different points in the neural network. h is the activation functions for units in the hidden layer. If you have multiple hidden layers, you may have different activation functions. And we'll talk about some of the different activation functions. And so we'll be doing gradient descent in the squared error. So we want an h that has some of those nice properties we outlined before. We want its derivative to not grow out of control as v grows and want that derivative to be easy to calculate. So let's try a couple of functions. What about a polynomial? We can plot a polynomial and its derivative to see if it satisfies the properties that we want. We have h given by this, and then its derivative given by this. We can plot the derivative with the dashed line and h with the solid line. So does this look like, take a look at this, do you think this is a well-behaved derivative? No. It's kind of the opposite of well-behaved derivative, right? Well, what we want is we want things that as the magnitudes grow, the value doesn't grow out of control. This is doing exactly that. It's a polynomial function. So we'd expect that. So we don't want this because remember, the gradient descent procedure is going to take steps that are of a size proportional to the derivative. So this derivative gets huge. And so it's a high positive as a increases, and it's a high negative as a decreases. And so the gradient descent, it could be very unstable. So what we want, we don't want to be skipping back and forth across that global minimum again. And so if the gradient grows out of control, we risk that. We also have things like the exploding gradient problem. This can be solved. The exploding gradient problem can be solved through relatively simple techniques. Bigger issues, this thing called the vanishing gradient problem that we'll get to. But for the moment, we don't want these derivatives to have properties like this. So two common choices for functions with well-behaved derivatives are the sigmoid function, as we saw before, right? 1 over 1 plus e to the negative a. And then this thing called the tanh function. So the tanh function is given by this. Differences are the, well, do anybody know the difference just off the top of your head between the sigmoid and the tanh function? What are the bounds on the sigmoid function? It's bounded at 0 and 1. What are the bounds of the tanh function? Negative 1 and 1. So the sigmoid is an asymmetric function that is bounded at 0 and 1. And the tanh is an asymmetric function for its bounds. So let's work out their derivatives. So we'll work out the derivatives. And then I'll give them to you and we'll plot them. So these are the two functions. So h1 is sigmoid, h2 is tanh. And then we have the derivatives of both of them. So now let's take a look at these. So the blue lines, that was the sigmoid function that we plotted earlier. We see the value top out at 2.5. We also see those nice bounds at 0 and negative 1. Then there's the tanh function in red. So similar. It looks like a very similar function it is. It has the same overall shape, except it's bounded at negative 1 instead of 0. And therefore, we have a steeper derivative here around x equals 0. And so we actually have a maximum value of this at 1. But both of these functions will still satisfy those nice properties of derivatives that we want earlier. So it's got a maximum bound. It doesn't grow out of control as the magnitude increases. So both of these are friendly functions. So these derivatives are computationally simple. So are there anti-derivatives. They decrease in magnitude as the weight magnitude grows. In this case, both of them do so asymptotically. And they have limited maximum values. You satisfy a lot of nice properties. So anyone know what the sigmoid function can be used for? Say I have some arbitrary scalar number. The sigmoid function will squish this into a range between what are the bounds of the sigmoid function again? 0 and 1. So if I take a 10, the number 10, take the sigmoid of 10, it's going to give me a value that's pretty close to 1. So sigmoids are nice for turning things into binary probabilities, for example. And so you can use sigmoids for, say, binary classification tasks. Tanh function is useful, particularly in hidden layers, because what it does is it will actually preserve some negative values. These may be useful, because there may be things that are inversely correlated with some input to that layer, be it a hidden layer or an output layer. And you may actually want to preserve that. So generally, you will see tanh functions being used in the interior of neural networks. It's not the only activation function, of course. There are plenty more that we'll go into later. Sigmoid functions are useful for things like binary classification tasks and more often will be seen in output layers. All right, questions? All right, so now this is like the gnarliest part of this, I think. Training by gradient descent. So remember the intuition behind gradient descent. If I assume that there is a high dimensional derivative, I'm trying to descend that and trying to find where it is closest to 0. So the gradient is going to be defined by the error and try to minimize the error. So I want to get to a point where I am so close to the true solution that when I move along the gradient, the error update is being so small that I can be said to have arrived at something arbitrarily close to the solution. So remember that we use the mean squared error in this case. So between each target value t sub nk and the output predicted value y sub nk, I'm just going to take the difference. I'm just going to square that. And because every target and every output is going to be defined for a given sample and a given measurable output that I'm interested in, I'm going to sum this over all n, all k, and then average it for the sizes of n and k. So now e is no longer a linear function in the weights. So this means that we can't set the derivative equal to 0 and solve for the parameters like we did before. So instead, what we can still do is we can do gradient descent in e by making these small changes to those individual weights in v and w in the negative gradient direction. So it's not I can't use linear algebra to solve for the inverse function anymore because e is no longer a linear function. But the intuition behind gradient descent trying to find some global minimum or as close to it in this high dimensional derivative as I can still holds. So I'm sort of doing this a little bit blindly in that I don't know where I'm going, but I know where I've been. And so I'm just going to look where I've been and walk backwards, go down the slope. So the update is more or less the same is that if I want to update this value for v sub jm, I'm going to take whatever previous value it was minus some learning rate rho times the derivative. And this is the same for v or w. So often for this, I'm going to have these two learning rates, rho h and rho o be different. But often they're presented as the same. There are cases where you can actually have different learning rates in different layers. And this can help convergence. Most of the major packages don't allow you to do that by default. You have to do some kind of pie-shorter or TensorFlow hacking to get that to work. In most cases, a constant learning rate or at least a single learning rate across all layers is what the package will give you by default. But there are cases where it may be desirable to have different learning rates. So we want to use this to find the global optimum. That is the values of v and w that minimize the mean squared error. So for this take a more simplified view. I'm not sure that this looks very simple to you with all the errors, but this is the simplified view. So we have this full picture. We want to focus on modifying a single way. Let's say v1,1, this one here. This is going to be based on a single error between the target t1 and the output y1. So for the moment, to make things a little bit cleaner, let's drop the subscripts. Let's just focus on the single hidden unit and the output unit that is relevant to this computation. So this input x, whatever it is, goes into v. And then this gets multiplied by all those elements in the matrix v. Those get summed. We then have some value that then gets turned into putting the activation function, which then deforms that by some non-linearity. This gives us z. z is then multiplied by w. And I take the sum. This gives me the target, the output. And I just want to measure the difference between the prediction y and the target t. So the forward calculation, this is simplified. So I'm going to ignore the bias and all of the terms. Right now, we're just looking at single terms being multiplied. So no need to worry about the matrix multiplication. So if y equals w times h of v times x, in other words, y equals w times z, z equals h of a. And a equals v times x. So since e is equal to t minus y squared, dE dv should be d of t minus y squared dv. So chain rule to the rescue here. So basically, I'm trying to represent t minus y in terms of things that I've already calculated here. So the error is going to be d of t minus y squared with respect to dy times dy dz times dz da times dadv. Because each of these terms, like y is represented in terms of z, z is represented in terms of a, and a is represented in terms of b. So again, if this looks intimidating, no fear. This is presented for your interest if you are interested in how the mathematics works. When it comes to the code, all this will be done as matrices. Basically, I'm going to be doing this for the individual elements, show how it's done as matrices, and then show the code, which most likely won't happen until Thursday. But we'll get there. All right, so now if I take the derivatives of all of these, I end up with something like this. So 2 times t minus y times negative w times dha da times x. So what this term here, of course, is this will depend on what h is, which function I'm using. So for the moment, we can assume that if that h is tan h, the derivative of tan h happens to be 1 minus tan h squared. So this page here at Stack Exchange will explain why, if you care to go into that. So now we have a formula that I can actually plug in for this. So remember that z equals h of a, so I can rewrite this as z. So 1 minus z squared can be written as 1 minus the square tan h of the input, because the tan h is h. And so now the entire thing ddv can be reduced to negative 2 t minus y times w times 1 minus z squared times x. So let's break each of these terms down. This is the derivative of the error, this first thing here. So t minus y, that is the error between the individual sample. w, that's the weight that's being updated. 1 minus z squared, that's the derivative of the activation function. And x is the input. So with the exception of the activation function here, this is the same as we were doing linear regression. The three components I need are the error, the weight, and the input. The only thing here that I'm adding that is new is this activation function, because there is this nonlinear function that I've performed over the input. All right, questions about that? What was 1 minus z squared? We go back to this. So we broke down ddv in terms of all of these things above. So these are the individual elements of the weight multiplication. We can use the chain rule to break it down into each of those, basically the multiple of these derivatives. We can easily take the derivatives of all terms except for dz, because we don't really know. We know that z equals h of a, but if we don't know what h is, we can't actually turn this into a formula. But there are a limited number of things we can use for h. It's got to be an activation function that has one and has a set of nice properties. The only one we've talked about so far in any real depth is tan h. So we will assume that h is the tan h function. The derivative of the tan h function is 1 minus the tan h squared. It is. It's a fact. And because if h of a equals z, then z is effectively tan h of, let's say, x, the input. So this 1 minus z squared can also be thought of as 1 minus tan h squared of x. Everybody cool with the rest of? So far so good. This is more or less just a linear operation with an added nonlinear function and its derivative. That seems intuitive. OK, let's add another output. OK, now we look like this. So same thing. Same things are happening. I've got a single value being multiplied by some weights v. Apply my function h. It gives me z. OK, now z is actually going two places. So there's a weight w1, and there's another weight w2. So just like in linear functions, I want to predict two things about the output. I have two columns in my final weight matrix. So that's pretty straightforward. So this is going to be that first value. This is going to be the second value, and they're going to give me different output values depending on what those weights are. These have some meaning that I can use to compare to the prediction. And so now I'll get a different error for each one. So now things get a little bit hairier. So chain rule again. So what's new here now is that instead of just having t minus y squared, I've got two things. And I've got a sum. So I need to take the sum of a derivative is equal to the derivative of a sum. And so what I'm going to do is I'm now going to sum these errors, these squared errors. And now this has to be taken with respect to v. So this works out like before. The only thing that's different here is now the numerator of this equation. But now we can distribute the, or sorry, I'm going to try. I now need to compute this with respect to the different y's. d y1 and d y2. So if I compute this with respect to d y1, I can then put this other term out here, d y1 with respect to d z. This is the equivalent to what's going on here in the single output version. But now I have two y's because I have two outputs. And so now I need to compute with respect to both of them. OK, so now I can have d y1 with respect to d z and d y2 with respect to d z inside the parentheses. I can compute these derivatives similarly. So this works out OK. d a d v is just x. d z d a can be again be written as d h a d a. And so now I'll put in the derivative of the tan h function here. So now we can think of the errors calculated in those output units as being sent backwards to the units in the previous layer. So if we'll call these delta values, then the derivative expressions will refer to those as delta rules. And those delta values are back propagated, so sent backwards into the previous layer. So this is that back propagation you might have heard of when you were discussing neural networks and is notably not addressed in neural networks for babies. So that's basically the small error value that is being used to update the weights. So just like in linear regression with SGD, we use that error value to update the weights. The thing here is that in those weights w, the value that is being used to optimize w is dependent on the value of v. So if v is very, very wrong, w might also be very, very wrong. And so the output could be very, very wrong. So if I get an output, if my target is here, let's pretend an arbitrary space, and my output is here, and they're way, way different, I don't really know, is this because the output weights were wrong or it's because the hidden weights were wrong. I have to allow for both of them. And so I have to assume that there might be something that's wrong about v that's making the prediction when z is multiplied by w also wrong. So this error backpropagation should be used not just to optimize the weights of the output layer, but also to optimize the weights in the hidden layers so that when an input flows through the hidden layer into the output layer, it's going to get me a better result. So intuition behind backpropagation, at least everyone clear on that, you can think of like, if you're a play like one of those pachinko machines, you put like a coin into the top and it bounces down to the bottom, you can think of like those, the bottom slots, those are your targets, right? You know where things want, you want things to go. And let's say you want a coin of a certain size to end up in a certain position. The weights then would be something like the sizes of the pegs in the pachinko machine. So you want to increase, decrease the size so that your input of a certain value is going to go the right way through the pachinko machine. The non-linearity could be something like you replace the peg with a spinner or something like that. So it's effectively just sort of this big machine where you want to, you know the input, you know where you want it to go. You want to just mess around with the interior of this machine until all your inputs get where you want them to go as closely as possible. It's just instead of wooden rods, it's numbers. All right, let me see how much, what we got left. OK, OK, that's going to be like bad. OK, maybe I'll get as far as the full version of that problem. I'm a little behind. OK, so remember these derivatives. If you don't remember these derivatives, you can forget these derivatives and just come back to them before class on Thursday. So we're basically differentiating with respect to two things, W1, which is going to be an element of this output layer, and V, which is going to be an element of the hidden layer, currently the only element of the hidden layer in this simplified example. So DEDV is the function that we see up here. It's got these components. It's the summed derivative of the squared error times the derivative of the activation function times the input. And then DEDW is much simpler. There's only one output. So there's one error. And then the input to this layer is Z. What happened? Yeah. So that's much more straightforward. So the hidden layer, with respect to V, that's much more complicated because you have multiple outputs flowing backwards into this single unit. And you have errors for both of them. Yes? Is it not required to derivative with respect to W2? Well, it's the same thing here. So the formula is the same. The only thing is that the value in this, the specific numerical value in this would be different. So this could just be cloned for W2. So now, if you go back and look at those update rules that we had earlier up here, now let's just plug those back in down here and see what falls out. So the update rules for the delta. So new value of W is W minus DEDW. Same for W2. And then so that's going to be W plus the learning rate times the error times the input. So this delta is going to be rewritten like this. So now V is going to be previous value of V minus DEDV. What is DEDV? We gave it up here. So now this is going to be the learning rate times the different errors times the individual weights times the derivative of the activation function times the input. One question you may be asking, where did this negative 2 go? At this point, this is just a constant. So if I multiply this by some constant learning rate, we can just assume that this is going to be factored into that. So if you're worried about this, don't. So now delta H is going to be delta O delta 1 O times W1 plus delta 2 O times W2 times 1 minus z squared. So this here, how do we do this? The reason we have this here is because of the update rule for W, we're just taking this error to be one of these delta values. And so now if there are two outputs, there are two delta values. However, this function here for the V update accounts for an arbitrary number of delta values. All I need to know is which one is which and just slot it in the right place and you add all of them. So we have two minutes left, so I'm going to stop here. If there are any questions, let me know. I will have office hours starting when I get back to my office. Thank you. I will see the rest of you on Thursday.