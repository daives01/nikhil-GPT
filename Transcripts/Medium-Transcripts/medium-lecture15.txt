 All right, let's start. So weather is absolutely awful, and like a bunch of people are feeling sick. This time of year. And today is my birthday and so my present to myself is having a short class. Thank you. I mean, I'm turning like 36, which is like, mathematically, it's actually kind of interesting, but legally it means nothing. And it just means that you're like that much closer to 40. So yeah. Okay, so does anybody need to come to office hours like after class? All right, yeah, that was gonna be my other present to myself was canceling office hours, but I sort of felt like kind of a jerk doing that. But if you don't need to, then I'll do that and I'll hold office hours in the regular time on Thursday. So all right, a one re grades are back for those of you who submitted them on everybody did at least slightly better, even with the five point deduction. So that seems to be paid off nominally for most of you. And Sarah has finished the age two grades, I just need to review them and I will post them. So I will endeavor to get to that in the next 48 hours at least. And then under about a three is due the Tuesday after spring break. So you know, if I Thursday would be the last time to talk to me about it in person, I will be online if you need to reach me over break. But I may not be able to respond like very promptly on when I'm traveling. So just F.Y. you do have some time, but it's because of the interveying implications for accessibility. So just make sure that you stay on top of that. Any questions or anything? Yeah, I am not here the twenty eight and the thirtieth, I think. And there's in the end that I'm going. Yeah, yeah. So I'll be here for that intervening week. And I'll be accessible. It's just I'm probably either do a remote lecture or recorded lecture those weeks that I'm on. OK. OK, any other questions? All right, cool. All right. So last time we talked about a common convolutional neural network. So this is effectively just a network that includes a weight matrix called a filter. That way you multiply it by things that, quote, match that weight matrix should output a high value. That's like this high activation. But we use these handcrafted filters. And so, of course, the question is, you know, why do we waste our time hand crafting the filters? Isn't the point of machine learning to basically look at a bunch of data and figure out what matters automatically? So this lecture and the next will actually be about performing the training operations on convolutional neural networks and how they differ from, say, the standard feedforward and what CNNs are able to do that feedforward networks are not able to do. So just as we recall, we take our image, we break it up into these patches. This allows us to, among other things, kind of simulate the scanning continuous motion of the eye over an image. And then we can take each of those patches and try to tease out what the important information is in that patch for any particular class. So this is a classification problem. You can use CNNs for regression problems. We're not going to cover that here, but we'll just treat this as an image class. So this is sort of this canonical computer vision problem that we're going to be talking about here. So what we're going to do is we're going to be taking our neural network classifier class that we had from before and turn it into a convolutional version. So the classification part is the same. Right. I have K classes and then that means I have K output nodes and I run my softmax function over the values in those output nodes that gives me whichever one is going to be the most likely class. I take the argmax and that's the class output. So, again, I have indicator variables and my error is going to be the probabilities for all of my classes. And then we can take that and subtract that from my indicator variables, which is a bunch of zeros with a one in the place where the class is the correct answer. And that makes it 100 percent probability minus whatever probability it predicts for that class. Do that for all the other classes. And my goal is now to try to get as close to that distribution of all zeros and a one as I can for all samples. So in this example, we're going to be just using a single convolutional layer to demonstrate how the mathematics works without getting too complicated. And then we'll assume that our samples are these two dimensional arrays. We'll just assume black and white images. So we'll have them be square and have the same number of rows and columns. So and then maybe not this one, but the next one, we'll talk about how to do this using pipe torch. And using multiple convolutional layers. So our neural network classifier CNN class consists of pretty much the same components as the existing neural network classifier class. So we have our init functions as the constructor. We need two more arguments in additional to in addition to the usual ones that we have in the CNN or in the neural network classifier. So that is we start with the things that we've we've had before the number of inputs, number of hidden per layer, the number of outputs, and we have the activation function. So the first three and this fourth one, those are the things that we've got already in our existing neural network classifier to make it a convolutional neural network. We have to add the patch size and the stride. So the patch size is given my input image. What's the size I'm going to break it up into in patches and the stride is going to be how much overlap do I have between those patches? That is how many in this case, pixels am I moving from patch to patch? So if my stride, my patch size, like seven by seven, my stride were also seven. This would mean I'm going to take a seven by seven patch. Then we're going to move seven pixels to the right. There would be no overlap between my patches. So we don't typically want that. What you want is a stride like that's going to be lower than the patch size. So if I had a patch size of seven by seven and a stride length of two, once I clip out that seven by seven patch, I move over two pixels, clip out seven by seven from that location. And keep going. So these are the two in addition to the standard four arguments. So the patch size that's going to be the same as that kernel or filter size. So we want our filter to match the size of the patch that they multiply together so I can actually get a meaningful scalar value out of that. So to make the right number of weights, previously we had this kind of make weights and views function that would automatically take all your weights and squish them into an array. And then in our constructor, we can create the appropriate size of the different matrices that allow us to perform these operations between layers. So this becomes a little bit more complicated in that we have to deal with the convolutional layers in a slightly different way. So first, I'll initialize the weights and I'll just build this matrix of all the weight shapes. So I'll have my inputs. I'll build this list of shapes. And then first, I'm going to build the shape of the matrix for the convolutional layers. In this case, we're just going to have one. So right now, we'll just assume there's going to be one convolutional layer followed by N fully connected layers. So shapes will be initialized with the following. So you're going to have self dot patch size times self dot patch size plus one. And then it's going to also have the number of hidden layers in there. So let's just focus on what this means first. So this is going to be self dot patch size. Let's say it's a seven by seven patch. This would be actually taken five by five just for simpler math. So if this is a five, we'll have five times five. That's twenty five plus one. And this one is our bias. So this should be a twenty six by whatever the hidden layer size that I'm projecting into. So, again, remember what happens when you create a neural network. I have an input size to that layer and then an output size of that layer that's going to be projecting from N dimensions into N prime dimensions, whatever those values are. So in this case, because the convolutional layer happens on the input, the input size should be the input to the entire network. So this is going to be my patch. So I'm going to take the patch size plus one for the bias. I'll take, say, a five by five patch, string it out into twenty five individual numbers plus a single one more number for the bias. And that's the input that goes into the convolutional layer. Then the convolutional layer should, for each of these inputs, map it into dimensionality equivalent to the size of the next layer. This should be the number of hidden units. So I'm going to take, say, a ten hidden units. I'm going to take my twenty six dimensional input and I'm going to somehow map that into ten dimensions. That's going to be the size of that convolutional unit. And so then the input size would be the square root of the number of inputs. So if I if my number of inputs is seven hundred and eighty four, then my input size would be twenty eight, because this should be a twenty eight by twenty eight image. And so then and in would be the input size minus the self minus patch size divided by self dot stride plus one. So what this does is it's going to basically tell me for the next layer how many inputs I should expect to have into each unit, assuming that I'm segmenting my image according to the specified patch size and stride length, allowing for making sure that I'm taking just the floor division to make sure that I always get an integer. And then I multiply that by the number of units to make sure that I'm getting this that many samples that go into each of those units. And so then for each hidden layer in my hidden per layer, except for the first one, I'll then append the number of inputs plus one and then the number of hidden. This is going to be the shape of that next transformation. So from this. This part highlighted, this all deals with the convolutional layer and then this deals with all of the fully connected layers. So because I've just specified kind of by Fiat that there's only one convolutional layer, I just deal with it all here and it's specified once and then I can use this for a loop to deal with all the the full connected layers. Yes. I think this is just basically a reflection of what's in the existing code. So probably this doesn't necessarily need to actually be here. You probably just get away with deleting, with removing it. But this is currently what's in the neural network classifier code. It's just leftover. Other questions? Yeah. Yes, absolutely. Yeah. So for for the task that we're going to do, which is basically classifying basic shapes, one convolutional layer is definitely enough for more. Let's just take RGB images, for example, you're going to have three channels going in. So you're going to want to, for example, have something that's going to be able to select features, relevant features from each of those channels. It gets a good deal more complicated. I want to get, say, my pooled or gisted version of those input features, then feed it to another layer that's going to tease out like higher level features. What you typically see happening is like for actual images you do, you can see in the earlier layers, often things happen that are like detecting edges, your features that kind of do things like those diagonal edges and basic shapes. Then you combine those into more structural features. So the deeper you go to the network, the more structure you get. That's how you can actually recognize real images of people and things and real pictures in the world. OK. Other questions? All right. Good questions. So we'll define this make patches function. So what we're going to need to do is we're out to have some way of automatically converting that input matrix into the patches. So this is going to be very similar to something that we saw in the previous lecture. So what I specify is just my input and then the patch size and then the stride length. So these are the things I need to actually segment the image. What the trick here is that I am going to be passing in X as a flattened array, right, because I need to have some fixed input size and then one initial term for the bias. And so then I need to turn this back into something that can actually be segmented horizontally and vertically. So for X, for the shape of X, I will take the square root of that. And that's going to tell me what are the dimensions of this image on each side. So we will assume that all our images are square at this point and even more complicated convolutional nets. Usually you squish the image into a square shape. It just makes the math easier for arbitrary image sizes. So I'll compute the number of patches. This is basically going to tell me how many for this patch size and stride length, how many images I should expect to have. And so then I can use that stride tricks library that I showed you last time to basically compute the actual image patches for every individual patch. As we shape that into an appropriately sized array, and then I'll return that so that now I can move through all of my samples and then have all of my patches accessible for each sample. So now that we have the make patches function, we need to modify the use function. So what I do here is similarly, I just start with the standard unstandardized X. I'll standardize them. I then convert those flattened samples into patches. So remember that this X at this point is still a single dimensional array representing the pixel values. Run this through make patches. So now I have X patches. And so then I'll run that through the forward pass, which we'll see next. And this will actually perform the convolution operation over the patches and then return the the predictions. I then run my softmax over that last element in the prediction. So remember, Y is here is basically the output of every layer like we've done before. It's accumulating this list. So I take the last element of that list. This should be the output of that final layer. Run the softmax layer over it. This should give me the actual probabilities for every class. And then I just take the argmax to actually tell me which which class index is predicted. So, again, if we have 10 classes, if one of them shows up at 11 percent, that's the highest, even if it's not very high, is going to predict that. So always good to look a bit more at kind of at your your actual probability distribution rather than just relying on the output label. So the forward pass, we actually have to monitor it a bit to handle the input as patches. Though this also has to flatten the image from the output that we get from the convolutional layer to feed it into the fully connected layer. So a common tactic is not necessarily globally useful, but one that is very common is to have some number of fully connected layers after the convolutional layer. In many cases, this has been shown to improve accuracy, but it does so at the expense of compute time. You don't really always need to do this and often just having convolutional layers, depending on the task, can be enough. Nonetheless, it's very common to see these fully connected layers appended to the end of convolutional net. So in the forward pass, what we'll do is if I'm in my convolutional layer, so this is all kind of the same, right? This is the forward pass. I have my regular activation or my my 10-H activation, depending on which activation function I specified. And so now, if I'm in the convolutional layer, which just in my example is known to be the first layer, then I need to find each sample into a vector to output that into the following fully connected layer. So that's what this does. And then for the other layers, once this is flattened, then it just gets fed into the next the next layer and then the same operations that we've seen before occur. So I finally get like the last weights. And so then I can append the outputs of that last layer times that last weights plus the bias. This was going to be that final prediction. OK, so train is actually not that difficult to modify. So the only thing we need to do is actually just create patches from X, because the way they make patches function in the structure, it's going to have them already in the right shape to feed into train. And so then I'll use that as the input matrix to the optimizer call. So now the way this is written, we have this function arguments for the optimizer. Whereas previously it was going to be X and T for regression problem, X and T, I or T indicator variables for a classification problem. So now this is still a classification problem. So we still use that again. We still use T indicator bars. But now the inputs is going to be like the patched version of X. So the way that this is written, this is actually a very straightforward change. Error F, we actually change nothing. So straightforward. So why do we not change anything in the error function? So what's the error in the classification problem? Disclassification. It's missed probabilities, basically. So I'm just trying to compute a sort of a distance metric, how long I am, how wrong I am. This is going to be one of these terms is going to be 100%. And I take that 100% minus some probability distribution. If it says it's like 93.4, then it is, you know, the 6.6 off. And all those other numbers are going to be some number off as well. I'm trying to minimize that distance. In the convolutional net, has anything changed about the classification aspect of it? It's the same, right? So the error term is the same. What changes is the backdrop part, right? This is the gradient. So error is the same in that that final error term is just the difference in predicted probabilities, but how we actually backprop through the network to account for those convolutional layers is quite different. So I'll show you how to do this here. Once you move to PyTorch, you don't have to do this because we still can just use autograd and do, you know, loss.backwards. Nonetheless, I think it's very useful to understand exactly what's going on here. So the the backprop loop is going to step backwards with the layers, and you have to add this special case when you reach the convolutional layers, in this case, the first layer. So going back to the fully connected layers is the same as we've been doing. But once you get to this convolutional layers, you have to understand where exactly the error gets back propagated. So if you consider that in a fully connected net, I've got some weight values that sort of live inside those nodes. I want to optimize those in the convolutional net. There's a weight matrix, right? That filter is the weight that sort of lives inside the node. One, there's multiples. Two, it gets applied to every patch of the input. So it's not just like a single component. And so you have to allow for all of those differences. So here, basically, the way we end up with that, that delta that's backprop from the fully connected layer is going to have different values for each convolutional layer. Those different values result from the application of those convolutional units to each patch, because when I apply the same kernel, same filter to different patches, I'm going to get a different value. And so then the delta that corresponds to each of the applications is going to be slightly different. So what I'll do is I'll sum those delta values by multiplying each one for each convolutional unit by the values in the patch. So because the patch is different, the values in the patch are going to be slightly different, and the patch is different, it's going to result in a different output. So I'm going to weight that error differently. So in order to do this, I'm going to reshape that delta matrix to the right form, done like this. And so then I need to reshape the convolutional layer input matrix to a compatible shape so I can multiply them together. So I'll take that input. So this is that input term that you multiply by the error to actually get the amount by which you update the weights. So the only trick here is that because I have now a matrix of delta values, I need to have an equivalently shaped matrix for the convolutional inputs. So now you can calculate the derivative of the error with respect to the weights with the convolutional layer with a single matrix multiplication. And then the fully connected layers just work as they have been. So the only real trick here is just understanding that you have multiple applications of each patch of each filter to each patch because the patches are different. The resulting values, those feature maps are going to be different from the same filter applied to the different patches. And so then the the error that you're going to get out of those features that come out of those different patches multiplied by that filter is going to be slightly different. I have to account for that. I do that by collecting everything into a matrix and then making sure that my inputs are reshaped into the shape that is compatible with that matrix so I can do the whole thing in a single operation. All right, questions about the components of the CNN classifier. So what we'll do is we'll in my code that's hidden from you, I'll define the neural network classifier as a new class that will extend the neural network classifier. And so then you can use it as in the following. So what we'll do is we'll make some simple images. These are either squares or diamonds. So I'll first define my square so you can see here I've got a bunch of zeros and then some ones that define the shape of a square. Similarly for the diamond. So you can see here if you look closely and see those ones, I'll draw those images by defining this drawNeg image function. So now you can see I've got an example of a square and I've got an example of a diamond. And these are centered within my within my image frame. OK, so this works fine. But right now, if these were my only images, it would be my image frame. So if I'm only using the images, it would be my net would be really good at classifying squares and diamonds in this exact position, which is not very useful to us. Right. If I'm trying to detect zebras, I want to detect a zebra, whether it's on the edge or in the center. So one of the key benefits of CNNs is that they're invariant to translation. Global Trans-CNN will basically be able to pick up features corresponding to an image class, whether it's in the center of the image, whether it's off from the edge, even if it's from the camera or closer to it, as long as it's not so far away you can't see it, of course. OK, so what we're going to do is we're going to create a function that will generate a bunch of images like these, but we're going to randomly shift them left and right and up and down. And so this would challenge the fully connected nets, but not the convolutional net, because remember, the convolutional operator is sort of like having this pinhole camera that scans over the scene and says, oh, I see a corner that looks like it might belong to a diamond there. That's important. It doesn't matter the exact place that it's that it matters that I see it. So I'll define this make images class. I'll make 20 or 400 black and white images, the diamonds or squares, and I'll sample a few of them to show. So this is going to create training data that has sort of randomly arranged squares and diamonds for classification task. So I'll create, in this case, 100 of each class, and then I will split that into training test. So we get something like this. Right. So each time I run this, I'm going to get a slightly different set. And so we can see here, there. Yeah, so now I get a different set of images. You can see here that we've got big squares that are nicely centered, squares that are in the corner, some squares and diamonds that are like tiny and shoved off to the edge. So at least this should challenge a fully connected net and allow me to demonstrate when a convolutional net is actually able to perform. Yes. All of these squares are fully within our brain. Yes. If we were to introduce ones that were cut off, so I swear that it's important. Yeah. This network could be a more advanced network. It might challenge this one somewhat. I suspect it probably wouldn't challenge it a whole lot, because if you consider if the square is cut off in the corner, you still have very square features. If my two classes are square and diamond, if you imagine a square and a diamond that are cut off in the corner at the equivalent position, I'll have a square that looks like this. And then there's the frame in the corner. The diamond might just be like a slanted line. Right. And that could be enough. Basically, what we would end up with is possibly filters that are optimized to detect, say, not the whole diamond, but just the slanted line. But that also then connects to something in the output layer that has a high activation when it gets for the diamond class, for example. So for this example, probably not for a real example. Probably. Right. Especially if you have things that are like rotation. You know, I'm trying to classify animals from different perspectives or even chairs. There's a you've probably heard of the ImageNet data set. It's a common thousand class computerized data set. You know, some researchers showed that because the images are nicely cropped and framed, you're sort of getting your chairs and cups and bottles and things. But they're all like this canonical pose. And so if I take a picture of a chair from like up here, all of a sudden that doesn't show up in ImageNet. So an ImageNet trained network doesn't recognize that as a chair because it doesn't it doesn't have the features of a chair. So cases like that, you want more you want some combination of more sophisticated network and maybe just like better training data. But in this case, if that were in the training data, it probably would be OK at that. You can. There's a couple of ways to do this so that you can have an additional channel that say you have depth data, for example, that actually operates with the depth data. So some like gesture recognition stuff can be trained here at CSU. They use the depth channel for that. One of my students actually defended his master's thesis yesterday, did a thesis on convolutional nets over 3D geometries, so actual meshes. And there's some very interesting properties of meshes that you have to account for to make them invariant for a CNN. And they're still it's still challenging. And then also there are 3D CNNs. So they're actually instead of a 2D convolution, you actually have a 3D convolutional operator. And that can be used for both 3D data and also say like multi-channel like video data. You can have the RGB channel and then a third channel that is like time. And so you can actually look over like multiple frames. But these are way more compute expensive, of course, the moment I add another dimension, I'm basically going from like X to X squared to X cubed. And so every additional pixel basically have to process that three times. Yeah. What if you inverted the pixels on your picture? If I trained it on black on white and tested it on white on black, it probably would have a lot of issues. Yeah. So again, we run the same issue that your training data must more or less resemble your testing data. So if I trained it on images that look like this and then I inverted this and used that as the testing data, I suspect we'd have a lot of problems. If you train it on both, then it probably would be able to accomplish classification of both, assuming that the network size is big enough to accommodate the filters required to optimize well enough to accommodate both of those features. So I suspect, again, if we had like the square diamond task, if we did that sort of bi-color version, might have to increase the size of the hidden layer a little bit, but it probably wouldn't be too much of a big deal. Other questions? Good questions. All right. So now we've got, we've created our CNN. We've made the requisite updates to the patching, the forward pass, the use function and the back propagation. We've created training data that would challenge a fully connected net that is probably not going to challenge a CNN. So now we can actually try to train this. So our net has been defined to accept these two dimensional input matrices. We need to flatten each image. So first, what I'll do is I just look at this, look at my train samples. So we can see like I've got 10 squares here. Let me take a sample input for train and test. I've got 200 train samples, 200 test samples. These are 20 by 20 images, so each of them contain 400 pixels. So what I want to do is flatten those. So what I'll do is I'll try two units of the convolutional layer followed by one fully connected layer of two units. I'll use a patch size of five and a stride of two. So what are my classes? I can just run np.unique over t train. So I've got two classes, diamond and squares. I'm going to have two output nodes. And so now what I will do is I will import my neural network. I will track how much time it takes to train. And so here this is X train dot shape. This should be that 400 number. This should be my flattened input size. These are my two hidden units and these are my output size. And then I specified my patch size and my stride length. So I'll train for 2000 epochs of the learning rate of 0.01 using Atom. Let's go for a minute or so. Not even that. And this is this is our results. So it took four seconds to train. Perfect accuracy on the training data and pretty close to that on the testing data. So what I can do now is I can look at my individual samples and see what the probabilities are. So these are the this is what I predict. So class zero, these are squares, class one, these are diamonds. This is the probability for each. And then the blue line is the actual ground truth labels. So we can see there's basically one sample. In fact, it looks like the very first maybe not the very first node is the very first sample. It appears to be the very first sample. It got it got wrong. But it got the rest right. So of the 20 testing samples, you got 19 correct. And one of them appeared to classify a a square as a diamond with slightly above 90 percent. But in all other cases, it got it correct. And the most the one thing that got closest to being wrong was this one where I thought like, it's about 19 percent likely to be a diamond. And it's actually a square. So pretty good performance overall. It's a simple task, but this demonstrates what a convolutional neural map can do. All right. So now let's see what our units actually learned. And I can do this by drawing images of the rate matrices. The first I'm going to look at the shapes. We have 26 by two. So this this should be a five by five patch plus the one bias weight. So I'm not going to visualize the bias weight, not least because I can't turn this into a filter. I can't turn a 26 sample array into a square. So I'm going to lop off that bias weight, reshape it into two five by five arrays, one representing the first unit, one and the second unit. And we'll see what it actually learned. So what do you notice here? The square looks funny. Do you think there seems to be something that looks like a diamond, right? Do you think it's actually reflecting the fact that it learned the shape of a diamond in that configuration? Not really. Why not? Yeah. Right. Visually, and especially in this case, because remember, the diamonds are like are randomly distributed across the frame. So this is just sort of coincidental. It might be because of the way the weights were initialized or other factors of the training. But let's not be misled into thinking that because we see the sort of diamond shape here, that this is this is necessarily like the unit that has a higher activation when it sees a diamond. It might be. And maybe there are a higher proportion of diamonds that are nicely centered from our maybe just kind of unlucky with randomization. But it's comparatively unlikely. Sorry, who's calling me? Let me again. Like this time today, like my my undergraduate university, they called and asked for money. So anyway, CSU will do to you, too. I'm sure. Anyway, so what we can see here is that although we see a diamond, it isn't necessarily indicative of the fact that that filters like learning the diamond features in this configuration. So let's print the weights. So this is the the first unit. This is this is going to be this one. So you can kind of see, let's take like this first value. This is like pretty low. This is negative point six, whereas these two on either side of it are point five, three and point five, eight. So you can see that this corresponds to the values we see there. So each hidden unit contains this five by five pixel filter. And then the individual pixels represent the value in that matrix. So unlike the last lecture where we had this very specific kind of edge, these units had been trained at the same time to optimize for both squares and diamonds of different shapes in different places in the image. And so then also, it's the it's a five by five filter over like a twenty by twenty image. And so as the as we move across those those patches, these individual values are going to reflect some sub segments of the image. OK, and so they do pretty well at this task, but they don't really resemble a square diamond features visually. So if we train the CNN to detect dogs, would you would you expect the filters in that network to visibly resemble dogs? Not really. Sometimes you will see when you say take the filter, the inputs times the filters, the feature map, not the filter itself. Sometimes you actually see like high activations on like features of a naturalistic input image. So you can see if there's a filter that appears to activate more for certain types of edges, you'll kind of see the image of like a German shepherd or something in places where maybe there's like a diagonal edge for a certain type of filter that activates with that type of feature, you sort of see that it matches those those features in the image. But as you get deeper and deeper into the network, those visualizations become like more and more obscure and opaque. So kind of it's easier to visualize maybe for those more intuitive features in the early part of the convolutional net, but much not not so much later in the net. Similarly, these images, these filters can be optimized for multiple things. So both of these filters are optimized for squares and diamonds, you know, maybe some more than the other, but it doesn't both at the same time. And this is generally true for all filters in a convolutional net. They're all optimized for all classes to a certain to basically varying degrees. So you have some that are going to be more optimized for certain classes and then less optimized for certain classes, but they should all activate a bit, you know, at least like a non-zero amount or mostly non-zero amount, depending on what they're most optimized for. Okay, so now let's repeat. We'll use four convolutional units and two fully connected layers after the convolutional layer. So here we have, these are my four units and then I'll have the two fully connected layers with 10 units each. And this time I'm going to train for 1000 epochs instead of 2000. I'll use Adam again at the same learning rate. And so if you remember the last one took four point something seconds, this took 5.8 seconds, even though I trained for half the number of epochs, right? This is because it's a, there's one, there's twice as many convolutional units. There's also these two fully connected layers. So it's a bigger network. There's more back propagation due, there's more operations. The test percent is still good. It's 98.5 instead of 99.5. So if I visualize those probabilities, it looks something like this. So there's some samples. It still seems to get that first sample wrong. In fact, you got it more wrong in terms of the actual probability of the incorrect class. And then there's two samples of diamond that is also getting wrong. So I'm going to use the first sample, which is the first sample, and then I'm going to use the second sample, which is the second sample. And there's two samples of diamond that is also getting wrong. So the training converges faster, but the generalization of the test data is actually not as accurate. So we might say that, you know, perhaps this is like overfitting to something in the training data. And that's quite likely in this case, because the task is so simple that the bigger your network gets, the more likely it is to learn kind of spurious correlations in the data. So we will visualize what the four convolutional units learned looks like this. And then once again, you know, we can see that there's no clear correlation between the actual physical shapes of the input images and anything that's being learned in these convolutional filters. So you can see, you know, where they're like, for example, in this filter, these two pixels here represent in the right hand side. Individual values of that filter that tend to have high activations kind of regardless of what they're looking at, at least relative to what's in this data. And then this one has like a high activation with or is as high value in the filter, just like right in the middle, but not a whole lot of visual correlation. So let's see how a fully connected non-convolutional neural net would do. So let's look at sort of the non-patched version. So I got my two 200 samples training test. So instead of using my CNN classifier, I'll use my normal classifier. So you'll notice that I don't use the patch size as an input. I don't use the stride length. And also because this is now an instance of neural network classifier, not the CNN in that function, it's not going to call like the make patches or anything. So the way you've written this code is that in the CNN, it's set up in a way that will automatically do the patching for you if you instantiate this type of networks. So I don't need to do that pre-processing of the data before I instantiate the network. It does it for me by virtue of the type of network that it is. So if I have, you know, let me run the convolutional version first and then I'll run the fully connected version. So this is four hidden units and then two fully connected units, 99 percent test accuracy. If I do a fully connected net with the same network architecture. So one, you'll notice it trains a lot faster, right? Because the convolutions mean there are more operations that have to be done because I have to apply every patch, every filter for every patch. So trains a lot faster, you know, two seconds versus five ish. But test fraction is only 83 percent as opposed to 99. So we plot that and it looks like kind of a big mess, right? There's a bunch of squares that are being misclassified as diamonds and a decent number of diamonds that are being misclassified as squares. So what did our filters actually learn? So we get this. So now take a look at this. So what do you think it's tried to do here by looking at these filters? So remember what our inputs look like, right? We've got squares and diamonds randomly scaled and randomly moved about the frame and it tries to optimize for all of the data at once. That's what a neural network does. So let's take a look at some of the things we might observe. For example, we might see, you know, there is sort of a line here of high values. There's also one at a similar position in this other filter. So it might be that, for example, there could be a lot of squares that are in this region or there's lots of horizontal lines in this region in the training data. And so it's learned that, well, there's probably something here at this location in the input. So if I optimize to detect that, I will tend to get a higher accuracy. So it may have learned effectively to look for a certain type of feature at a specific location in the input. Similarly, you might observe that there's a diagonal line like here and maybe another one there. So you can see some sort of diamond esque features here. And so this might be another indication that a lot of diamonds happen to occur at approximately this location in the training data. And so it tried to optimize for that because by predicting that it got a better it got less error and a better better performance. But if these things don't occur in the test data or don't occur in as significant a proportion, then it's going to incorrectly predict things that may be a square that has a pixel at this location is going to go, oh, well, this is correlated with being a diamond in my training. So I'm going to predict that I'm going to predict wrong. So the invariance to translation is one of these key features of the convolutional net that a fully connected net is not going to pick up on because it's trying to learn everything it can about the individual pixels in the data. And it generally just doesn't do a very good job because maybe if I had a bigger network, it would do a better job with a simple task like this. But generally speaking, it's not going to there's not enough space in the network to represent the information at a per pixel level where the actual position of the pixel actually matters. Any questions? OK, so the the fully connected net tends to overfit to the straining data and so but it tends to it fails to perform as well in the testing data. So there are basically a few things that a convolutional net requires. So it still requires fixed size inputs like your fully connected net. But the reason it can do invariance to scale is because there is in there's an imposed ordering on the pixels in that the neighborhood of a pixel is the pixels on either side. So we can actually learn basically relations between pixels at certain points and pixels around them, regardless of the absolute position inside of the image. And so then there's also this or this there's this implicit ordering and then there's the neighborhood around the actual pixel. So this allows us to use the stride the striding operation, the convolution and that filter to segment images according to relevant information relative to each other, regardless of the actual position inside the image. OK, any questions? Yeah. In the images, are we not considering the lower values like the black spots on the left side of each image? They're pretty similar. Yeah, they're pretty similar. And they're probably close. They're pretty close to zero, I think, if I'm looking at like the edges. And in this case, this is also probably learning correlation with with the data in that we. We actually don't allow images to go off the edge, for example. So it's very likely that that first pixel on each side is empty in most samples. So it learned effectively that there is zero correlation or close to zero correlation with the edges and the class, because at most you might have like one pixel or one row or column of pixels if it's a square in that edge. But otherwise, in the vast majority of samples, it's probably an empty, empty space. Other questions? All right. So to summarize, convolutional nets use fewer overall weights, but they learn more generalizable matrices or filters. These filters don't necessarily match the input features visibly, but we can train them to recognize multiple different types of features in sometimes very diverse input sets. And we can recognize them at the same time. So these can be squares or diamonds, straight lines, diagonal lines, curves. As the network size increases, the capacity for learning more types of filters also increases. What we need to do is we need to take the image segmented into patches, stride over those patches. We get a significant amount of overlap between the patches. And so now these filters can learn the difference and similarities between neighboring patches that have slight differences between them. So after you standardize your input values, so these these just like intensity or in RGB, they're just color. So you're going to standardize them into a distinct range, convert that entire matrix into patches. We have a function that does that for you. And in both training, you usually do this so that your input is of the same format in both functions. So now the input to the forward pass is the patched version, not the raw input. And so then if the output of a convolution layer is input to the fully connected layer, you flatten that into a single vector. So now it just becomes basically a feature representation that goes into the fully connected layer. So the hardest part is back prop. So because we have multiple applications of each individual patch and each individual filter. And so those delta values are going to differ depending on the different features in the patch. And so they're going to have multiple values for every convolutional unit. And so that has to come from the application of each unit to each filter. So we sum that from each unit to each patch, we sum that for all the delta values, take the whole delta matrix, reshape it into k by n, where n is going to be the number of units in the convolutional layer that you want to back prop. What's the other thing we need to multiply? So we have the error, we've got the features, then we have the input. So then we reshape that input matrix to n by p, where p is the number of values in the patch. So now k by n and n by p will multiply together to give me k by p. This k by p will give me the gradient of everything but the bias weights. So this is going to be the gradient associated with k classes or k outputs and then p patches. So then I'll sum every column that reshaped data matrix, delta matrix, and get the gradient bias weights. So you compare this to back-propping through a fully connected layer where all you need to do is multiply the inputs to the layer by the delta. And then we have no delta matrix to reshape, it's just all been single values. OK, so final questions. OK, I'm going to take it at your word that you don't need to see me in office hours. If you do, I will be there, but I'll be writing. But I will give you back 30 minutes and I'll see you on Thursday.