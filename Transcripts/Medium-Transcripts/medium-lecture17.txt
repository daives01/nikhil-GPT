 Okay. Welcome back everyone. Hope you enjoyed a good break. So, I, let me start with the schedule, where we're going from here. Okay, so what's going to happen, like I mentioned before break I'm going to be gone next week. What I'm going to try to do is actually get us a day ahead. Mostly today, because we have a review of softmax and CNNs that I kind of always build in after spring break, but usually we don't really need to spend the whole class on that if any. And so I'm going to try and basically get through notebook 16. So I can move 17 to Thursday and then do 18 remotely next Tuesday, and then, ideally, we can all actually take the 30th off, because I will be traveling, and you all can have a free day I guess if we, if we manage to do that so hopefully we can that's my goal. So, okay so then next Tuesday, I will be in California, but I will just do lecture remotely so you can come to the room and attend on your laptop is really wanted to. But you can just you know, watch from your, your house or, or wherever, or if you can all come in here and someone who rejected you have a watch party for lecture. Huh. How am I supposed to know where you. You do that and then you like write a reflection on how this was a different experience for you and two pages minimum then you get some extra credit. Okay. Yeah, and you can't use chat GPT to write it for you. So, um, a three is due tonight I. So here's one thing you should know about a three that I discovered. People have mentioned there's some, you're having some issues with the automated greater and some things like you're seem to be doing it right and maybe it's giving you some different answers so in some cases, I discovered, how many of you are using an Intel or an M1 Mac to do your homework. Okay, in some cases and this might be the case with other types of newer processors as well there's some slight differences in the architecture of the chip set that actually caused some slight discrepancies in the way it ends up performing grades and operations. So, I sorry I'll send you all the discussion that we first kind of found this last year, and we, we seem we think we can't put the way to address it. So this means that it might take a little bit longer to get your a three grades back because we have to sort through some of these issues. But it's going to warn you so if you are worried that the auto greater is maybe not is giving you things that you can't track it down I recommend you come talk to me or send me the the issue first like and maybe review but also know if you're using things like a new Mac. Sometimes if you're using like Python 310. It's maybe a slight issue. So I wouldn't necessarily get like too upset about that if you're like losing points on one of the issues and it seems to be like there's a slight discrepancy. Yes, in the back. I think we are grading using 3.9. So if you want to just be assured that you're writing into the same version that we're grading on us 39. That's not necessarily going to change anything with like the architecture of the chip side or something like that. So that's also an issue. Unfortunately, you know, well, unfortunately technology continues to evolve. And so our notebooks that were written back in like 2019 or whatever, are not necessarily like fully automatically compatible with everything that's out right now. Probably not. No, if you're getting full points the auto grader, especially even if you're using like a newer computer, most likely, you'll be the same for us. If you want. One thing that may help is like, you know, if you're concerned you can submit along with your assignment take like a screenshot of the auto grader and make sure that the output is like, present in the notebook. A lot of people don't do this never clear your output before you submit because then we have to rerun the whole thing and we're not going to see what you actually run. So like, you know, run the auto grader, show us that your output is, you know, you're getting full points. We'll run it again, of course, but if you notice some of these issues then that at least tells us that like we can check with you ask like what your, what your system settings are. Check it against a machine that has those same settings or as close as we can get. So yeah, apologize for that that is an issue mostly with like the GPU part of the code. But it's one of these things with this was just newer machines, and the way that they handle certain types of instructions. All right, so today, I'll lecture in whatever form it takes, which is hopefully mostly just being an also assign assignment for assignment for tends to be the trickiest one for a lot of people, fair warning is a lot of different moving parts to plug together I'll get to that when we, when we assign it. And then on Thursday project proposal so I know some of you come talk to me about your ideas. So, if you have an idea and you want some preliminary feedback on and I am available. The proposal. I'll assign that and then you will have, you know, little over a week I guess to write your proposal, send it back to me, and I'll have time to give you feedback and request any changes or clarifications. So, that being said, does anybody have any major issues that you want to review regarding classification convolutional neural nets or the softmax function. And if you do, let's take a few minutes to talk about that before I go to lecture. If you don't, then I will start reinforcement learning. So, I feel pretty good about softmax. Remember it's just like you're just computing error in the same way. You're just now you're subtracting probabilities from probabilities convolutional nets. You basically have a patched version of your image that allows you to simulate kind of the scanning function of the quote I over the image, and then you allow you train your filters to have higher outputs when they put match certain things in the input, and then with a set of properly optimized filters, you're able to do classification for things that have different types of image level features. Any questions as we go through the homework, of course, we can come back and review this material in office hours. But hopefully I think everybody's at least somewhat confident on this. I have no way to say no. I just want to stress that once more. If you're not confident about it, there's probably other people in the class who are equally not confident, but less brave than you. So if you speak up, you're probably doing your classmates a favor. But I'm assuming from the silence you all are completely expert at the softmax function and convolutional nets and you're going to have no problem with assignment four. So I have no choice but to take you at your word. So I'm going to start to lecture 16. That's assignment four. There we go. Okay. So this is the beginning of our unit on reinforcement learning. So we'll have about four lectures on this. So the introduction we're going to do is just sort of the simple, we'll call tabular reinforcement learning. So no neural networks involved at this stage. On Thursday, we'll show how you can use neural networks. So reinforcement learning can, for some people, be very intuitive and for some people it can be very non-intuitive and it might be kind of how you've been conditioned to think about machine learning. There are a couple of different ways to think about reinforcement learning. One is that you are training and using your model at the same time. Another kind of similar way to think about this is this is sort of like supervised learning, except you're not getting all your samples at once. This is basically you're in a quote world and you have to explore the world and that process of exploration is what gives you the actual samples. And so this is in some ways it's like learning by trial and error, which makes sense because a lot of human learning, especially when we're say infants learning to manipulate things, is done by trial and error. So the example that I gave last year was when my daughter was like kind of learning to eat with a spoon. And so she likes the taste of applesauce. And so if she loads up the spoon with applesauce or reloaded the spoon with applesauce for her and she sticks like the handle in her mouth, she doesn't get the applesauce. But if she sticks the spoon part in her mouth, she gets the applesauce. She likes the taste of applesauce. So basically, the agent has things that make it quote happy or sad. And it wants to be the most happy it can be or at least the least sad it can be. The way you specify your problems is analogous to that. So this is sort of trial and error learnings like this agent world model where the agent explores the world and learns about what types of inputs give it positive reinforcement or positive rewards and what types do not. And it tries to maximize the positive rewards or minimize the negative rewards. So, for example, I assume we all know how to play tic-tac-toe. So if you look at a board setup that looks like this, this is the current state of the board where X and O has made two moves each and your X and you're trying to choose the best action, right? You have a number of different possibilities. So this is not, I guess, this is not totally exhaustive. Or actually maybe it is. So basically we have five open squares on our grid. And you can place your X in any of them. Of course, depending on where you place the X is going to be a better or not as good move, right? So if you look at this, this is sort of a qualitative judgment. But if you look at the ones that are outlined in green, these are good moves. So for example, if I put X here, as X, I now have two paths to victory, right? So if O puts their blocks by going here, I can still win by going there. Similarly here, right, there's one path to victory. So maybe it's not as good as this one. Maybe this is like obviously the best one, but this one is still pretty good because like O could block me there. But then I still have like a couple of other places that I could go. This one maybe is not as good, right? So this is not really, it's not really obvious like how I set myself up for victory here because I put X there and I have to fill out like at least two more squares before I can win. And I don't really open another path to victory without an additional move. And like maybe these are marginally good moves, but like not this one. This one is actually very good. I'm not sure why this circle is dark green. I would say like this one and this one are like obviously the best moves. And like these two are OK, but maybe not the obvious best. And this one is probably like one of the worst moves you can make. So basically these may not, these moves may not like immediately lead you to victory, but you can see how they lead you to victory in like within the next move. Or you can see how like this one, for example, it's not immediately clear how in the next say and moves you're going to, you're going to be able to win the game. So all these can be represented as quote, so we have a set of possible states which is S and all these can be discrete. So, for example, in tic tac toe, that is the cardinality of S is going to be less than infinity. So we have tic tac toe game positions, right? So at most, even the board is empty. There's like nine possible moves that every player can make. Position in a maze, so assume that kind of a grid world, I can move like one step in every direction. But depending on the size of the maze and how many squares are blocked off is going to be a finite places, a set of places that I could go. Sequence of steps in a plan. So there's a defined goal. And I'm trying to figure out how do I get from where I am to the goal. There's going to be, and there's a set of discrete steps. I can only choose a finite, one of these defined things. So if I can't really do multiple things at once or something like that, then this would be a finite set. These could also be continuous values, right? So these could be joint angles of robot arm or this could be the angle of an inverted pendulum page, for example. Or you could have the position velocity of a race car, right? If I'm trying to control a vehicle, I have continuous values that I can pull at any given time. And they're not necessarily integers or they could be just like fractions to a specified decimal. You can also have parameter values for a network routing strategy or something. So any type of basically planning problem, planning or control problem, can be realized as a reinforcement learning problem where you have to define the states that you can be in. Now, when you're in a state, you must decide what action you take in that state. That action is going to move you to other states and depending on the correlation between states and actions, an action could keep you in the same state or it can move you to a different state. And the actions can also be discrete values, right? So the next moves are where I see the X's in red. And so this is going to, if this is my state here, each of these actions is going to put the board into a different state, right? And that's going to then dictate which actions are more or less advantageous in that state. So in the maze, where I am is part of what I need to know, but also I need to know where could I go, right? And so that's going to be from where I am now, what possible moves are available to me. It could be there's a wall on one side so I can't move there, right? That might restrict the set of actions that I have available to me. And then like rearrangements of a sequence of steps in a plan. So, for example, there might be multiple ways to get to the goal from where I am, right? It doesn't necessarily matter which one of these things I do first at this state, as long as I do some subset of them that gets me to a place where I can then move toward the goal. But these can also be continuous values, right? So if I'm my inverted pendulum or my robot arm, there's a torque that you're going to apply to like the pendulum or the joint. And that's going to be a continuous value. So, you know, I think in terms of this project that we're working on to control inverted pendulum using a joystick to the joystick, right, that would apply a torque. And that's going to be a continuous value. And then in the race car example, right, so if I'm trying to actually control the car, if I know where I am, and I'm trying to like move, learn how to like drive on a racetrack, I'm going to have to steer the car and apply a certain amount of acceleration in order to move the correct way along the track and not say, you know, drive off the racetrack or drive off the road or something like that. And then like in the network routing strategy, you like there are certain parameter values that you're going to need to set in order to route traffic through your network. And so these are going to also be continuous. So what we want to do then is given a state and a set of possible actions, I want to choose the action that's going to result in the best possible future from the state. So in order to do that, quantitatively, I have to have some way of representing that future outcome. So, for example, what should this what does value represent? So in tic tac toe, I might want to determine if I make this move, what's the likelihood that I'm going to win the game from this position? So, for example, we can see if you know the rules of tic tac toe, if I am if I make this move, and I'm X, I should be able to tell that by doing this, there's 100% chance that I'm going to win the game. No matter what O does at this point, I'm going to have at least one path to victory. Whereas for like one of these, maybe it's more likely than not that I would win the game, but it's not certain. I might have say 67% chance of winning and a 33% chance of it being a draw or something. So I have to quantify that somehow. In the maze, it's going to be like how far am I to the goal? Right? So I know where the goal is, and I want to be able to figure out like what the number of steps is going to take me to reach the goal if I am in place P and I make move M. So, you know, in planning, basically, this might be like efficiency and time and cost or something, right? There may be multiple ways of getting to the solution. I probably want to choose the one that's going to be like the most efficient, right? Or has some added some benefits. So maybe if I'm like playing Pac-Man or something, right? I might want to choose the path that's going to allow me to pick up the most pips or something like that. There are multiple things that you might be wanting to balance. Robot, I might need to, I might have some energy constraint, right? So I want to choose the move that's going to allow me to conserve energy the best. Race car, right? Time to reach the finish line, but also taking into account that if I make certain moves, I could say flip the car over, right? And that would, of course, delay me getting to the finish line, if ever. Never frowning, maybe your metric is throughput. So all these things, what you decide to measure is going to be, again, critical to determining what type of strategy you actually arrive at. So I'll show, if time permitting, I'll show an example at the end where basically like by measuring certain things, we can learn a different type of task, right? But of course, that's contingent upon the ability to measure that thing. If you can't measure that thing, you have no way of quantifying your progress toward that particular goal. So with these correct values, these multi-step decision problems can then be reduced to the single step decision problems. That is, I'm just going to look at a set of possible actions. I'll pick the one that has the best value. So this is going to be guaranteed to find the optimal multi-step solution. So the dynamic programming problem is basically, I've got some multi-step problem and I'm going to be looking for the optimal sub-solution by solving for the optimal, sorry, the optimal global solution for solving for the optimal sub-solution in every step. So that is, if I know where I'm at right now and I can find what the optimal solution is for my current situation, then I can be guaranteed that whatever I choose now is going to be part of the ultimate best solution that I arrive at. And so the cost of a single action is going to be what we call the reward of that action from that state. So then the value of that state action pair is basically the expected value of the full return. So I have a full sequence. I have at each step, I'm at some state, I take some action. For each of those state action pairs, I'm going to get my reward or my reinforcement. And then at the end, I'm going to get the full return as the sum of all of those rewards. So for example, if I look at this example, if I start in one of these states and then I get small r for every reinforcement, and then the return is going to be the sum of all those. But if you look at this, just if you take a look at this, you'll see that these numbers don't add up left right. So if I find me here, it looks like my reinforcement is 0.2, my return is 0.9. So that doesn't really make sense. But if you add it backwards, that is right to left, it does. Why is that? Well, that's because I'm trying to get to the end here. And I use that by trying to predict the sum of all reinforcements that I'm going to get when I get to the end. So that is, if I'm here, then maybe the optimal action that allowed me to get here would be going from this previous state. Let's assume this is just like one path through a lattice that we'll see in a moment. But basically, if I add this up right to left, all of a sudden, I can see that if I follow this path, then I'll get the total return of 0.9, according to the sum of all these little reinforcements. So let's say that I'm in some state, S sub t at time t, upon taking A sub t from the state, I'm going to get the one-step reinforcement and then the next state. So if I'm here and I take this action, I'm going to get into this next state, I'll have the reinforcement in this case of 0.3. And so now I can continue this until I reach some goal state k steps later. And so then the return R sub t starting from state S sub t should then be the sum of all of my reinforcements. So that's why this works. So effectively, what I'm trying to do is if I'm in any of these steps that precede the goal, I want to be able to predict what the best action is. So I can do that by estimating the sum of all possible actions that I can take from this state and try to figure out the best sequence. So then I'll use the best returns to choose the best action. So this will become a little bit clearer if you look at this example. So if I have this lattice here, my goal is to get to one of these end states. And let's assume these states are actually all the same. So there will actually be a closure here, but we're not showing that. So basically, if I'm looking at this, if I go along this path, then my return is going to be 0.9. If I go along this path, my return is going to be 0.8. If I go along this path, my return is going to be 0.6. So which of these is the best path to take? The first one. You think the first one? You all agree? What if I'm trying to minimize costs rather than maximize reward? So the last one. So it really depends. Are you maximizing or minimizing? That is, what does your reward or cost function actually represent? So for example, if it's the race car and you're measuring time, you probably want to minimize time. Whereas if you're playing Pac-Man and you're measuring the number of pips you collect, you might want to maximize that. Or if you have like, are you trying to conserve some resource or are you trying to gain some extrinsic reward or something like that? So if it's like energy that you want to minimize, then you might want to take like this bottom path. So let's say that this is actually a cost. What is the energy required to move between each of these states? And so this is interesting because in this case, the first reinforcement you're going to get is actually the highest value. So if you want to minimize the total return, it doesn't necessarily make intuitive sense to take the path that has the highest reinforcement if the reinforces actually a cost. But you can see that from this state, you then actually can get to a sequence that allows you to take a bunch of really kind of small cost steps. And in one case that has no cost in exchange for taking a big cost step at the first time step. And so this actually ends up being the best the best path to take if you're trying to minimize, say, expenditure of some resource. Right. So conversely, if you're trying to maximize, if you're in this state, it might make sense to maximize. It might make sense to take this because you get like the largest reinforcement. If you think that's a reward that might seem like the best action, but ultimately you're going to get the lowest return. Right. So you may want to take one of these sort of lower reward first steps, because you can figure out that you're actually going to get a higher reward later on in the sequence. So to do that, we need to know these values. Right. So you get the same sense of what's going to happen if I'm in some state and I take some action. I need to try to estimate this for basically all combinations states in action. So where do these values come from? So we have a couple of options. Run one is I can write the code to calculate them. So this usually isn't possible. Or if it is possible, it's not really a problem. You need to solve the reinforcement learning. Right. I can kind of brute force my way through it or use some other type of solution. And you can probably do this for tic tac toe because it's a fairly constrained problem. And there's a relatively limited set of things that you can do that will and you can typically account for all of those. You could use dynamic programming. This is also usually not possible because it requires knowledge of the probabilities of the transitions between all the states for all the actions. And if you don't have that, then it becomes a lot more difficult to infer what those rewards would be. So you can compare this to things like the hidden Markov model. This is a type of model that we use in natural language processing to try and figure out, like, what's the probability of, say, moving from state X one to state X two or from state X two back to state X one. And then we also have these things like what are my observations that I've ever stepped on. You can use certain types of dynamic programming algorithms to solve certain types of sequence problems. But again, if you can do this, your problem is probably constrained well enough that you don't need to be doing reinforcement learning for this. So a true reinforcement learning problem is one that you are looking for examples, basically looking for lots of examples of both solving the problem successfully and not solving the problem successfully. So that is I want to see for my inverted pendulum, if I'm trying to balance, I want to look and see like given a an angle and velocity of the pendulum, what moves the same trying to balance the gear, try to balance like a pen on your hand or something like this. Right. So I'm trying to keep this balanced. I do like really bad job. And then over time, maybe I become better, better and better at it. And I learned that by figuring out if I can feel see the pendulum tilting a certain way, I know that I shouldn't move and I should maybe make a certain movement that's like small or large, etc. And I do this a whole lot. Right. So in some sense, you know, learning to walk, you know, is some sort of reinforcement learning or at least some kind of reinforcement learning feedback happening between the brain and the body. So basically, we can teach AI agents to learn to quote walk by running them through an obstacle course, and they sometimes come up with these really weird walks like you know they're not necessarily by people and you can have like a three limb agent that sort of has this walk. And so it learns actually will come out through, through the environment by this, by this example sampling. So basically these examples are going to be represented as these five tuples so that as you have the state, where I am the action, what I do, the reinforcement, like the return the the reward that I got from making that action that state, and then where I end up, so that is the next state and then the next action. Yes. Assuming you're trying to maximize. Right well again you have to formulate your problem. So if you're assuming that you're trying to maximize the return, then yes the one that has the highest return is typically going to be, you know, at least an instance of a good plan, possibly the best plan, it gets replicated as you have like more continuous scenarios, but generally yes now if you're trying to minimize of course then you're basically just inverting that it's going to be like the lowest returning. So there are a couple of different techniques, broad families of techniques we can use. And so I'll will focus on temporal difference and also talk about Monte Carlo I'll do a demonstration at the end, time permitting. So basically, in Monte Carlo sampling what I'm going to do is I'm going to take an average of the observed returns, and then assign that to every state action pair so that is I have basically some value function that takes in my state and my action. And this is going to approximate the mean of the return from that state and that action. Right. So, in order to do this, I need to actually wait until the episode concludes that is I either reach a goal or I fail, like I timeout or something. I need to wait before I update my state action pair variable. So for example, I'm trying to solve a multi sequence problem. And I semi randomly explore the world, and then I reach my goal that I can see that I got my goal and I can see oh hey this is actually a really good sequence of events. I want to do more like this in the future, or if I say I got a time limit of like you can take 10 steps and then you're done right you get maybe 10 attempts to solve the problem I try to move through the world, and in 10 attempts I do not reach the goal. Right so then you can see that and say well, this was not a good sequence of events. I should do less of this in future now. The trick is that in maybe a suboptimal sequence of events maybe I made the first three steps of my 10 were actually optimal right from where I started, those took me directly toward the goal and then I moved off track or something right so doesn't necessarily mean that all of the everything in that sequence was bad, just means the sequence overall was bad. Similarly, if I do reach the goal. I might have started off that, and then I stumbled upon, you know, a path toward the goal. So again, just just like the opposite. I do not necessarily want to adhere to everything in that sequence, because there might be a better way right maybe I reached the goal in like seven steps. But I actually the best path could have gotten me there in for right so maybe I don't want to continually just exploit the strategy that I happened to stumble upon, because there might be a better one. Okay now temporal difference is the other strategies to basically what I'm going to do is I have the value function I'm going to evaluate that over the next state next action pair, that is, given where I end up if I take this action that I look at all the possible actions that I can take in that in that state, and I'll sample each of those actions in turn and then put that action and that then my current my next state into the value function. So I'm going to use that as an estimate of the return from the next state, and then we're going to update the current state action values that is value of s of t as of t. So that is the value function of s of t as of t is going to be approximately equal to the return from the next. The Sorry the reinforcement at the next step, plus the value function at the next step which is an approximation of the entire return. So that is this is small r, this is an approximate approximation of the big R. And now what this does is then I can update my state action pair immediately after taking the action. So I can basically see I took this action from the state, and I have a pretty good estimate of how good or bad it was, so I can see if this is going to get me closer to the goal or not. So if we take a look at this, this little graph here, I'm trying to estimate the return R from state B. So this is where I am right here. And so basically I've got a situation where I can move from one of these states through state C, and then from state C you can either move to state W or L, W versus when L is lose. So pretty straightforward scenario, I want to end up in W. And I have some examples. So that is I have, I start in A, I go through C, and I go to L, and I do that 100 times. So basically 100 times I go from A to C and then I lose. So I have 100 examples of failure. And then the 101st time I go from A to C to W. And the 102nd time I start in B, I go to C and I go to W. So I'll do this as a contrived example. Right. So I have one example that shows me what the estimate of the return from state B is. So I'm trying to figure this out. What this would look like for these two methods is basically for Monte Carlo, every example starting in state B leads to a return of one. This is trivial, because I have one example starting in state B, but it leads to a return of one. Right. So every example in starting state B leads to a return of one. The mean of this is one, which is a prediction of a win. Now the temporal difference method would show me that the reward from C to B is zero. And then from state C, I have 100 rewards that are negative one and two that are one. So in this case, what this would end up being is a value of negative 0.96. So this is a very likely loss. So temporal difference takes advantage of the cached experience given in the value learned for state C, whereas Monte Carlo is going to take just those samples that have the entire sequence, including my start. So basically, what do I do in this situation? If I go from B, if I start in B, if I'm trying to win, what type of method would I go with? Monte Carlo. In this case, it's pretty trivial, because my Monte Carlo method gives me the only prediction of a win starting in state B. So this is not necessarily how you would actually choose. You would have a better distribution of your actual samples. So any questions so far? Let's see where we are. All right. So let's take a very simple maze example, like a stupidly simple maze example to the point that you look at this image and you think like an image failed to load. But here's our quote maze where G in the corner of these walls represents the goal. So I can be any position here. We'll assume there's a grid. And I need to decide whether to move up, down, left, or right. So to do this, we need an estimate of the number of steps needed to reach the goal. This would be big R. So I'm going to go from A to B. So I assume there's a grid and I can I need to decide whether to move up, down, left, or right. So to do this, we need an estimate of the number of steps needed to reach the goal. This would be big R, the return. We need to formulate this in terms of the reinforcements, that is the R. So first of all, what reinforcement do we get for making a move? So for every move, it's going to be one. Because we don't really know if any of these moves are going to get us closer to the goal. Then big R is going to be the sum of those to boost the number of steps to the goal from each state. So now you can see that if I have a bunch of cells in this that represent where my agent is, it's going to look sort of like one of those little minesweeper grids. There's like a number associated with how close, how many steps it's going to take me to get from here to the goal. So this is the first step. I want to basically traverse the shortest path. So the Monte Carlo sampling will assign this value as an average of the number of steps from the goal to the goal from every starting state. And the temporal difference will update the value based on one plus the estimated next value. So the next step is whether we do Monte Carlo update or temporal difference. Let's look at this comparison on this maze problem. So we've talked about this value functions that is the V of S and A. So how do we actually represent this function? So the simplest way to do this is actually just as a table. So you can imagine that let's we take this maze example. So we're going to take this state that would be represented as some cell, say numerical cell, and then action that's going to be one of those four things up, down, left, right. And then the value of that should be some representation of how many steps it should take me to get to the goal from the state, given that I take this action. So we're going to take this other function and we will actually write this function called the Q function as this table. So this state action value function is basically you take in both the state and the action and the value is the prediction of the expected sum of future reinforcements. So in the maze problem, those future reinforcements are basically a sum of a bunch of ones, each one representing a step through the maze. So terminology Q comes from this thesis by this guy called Watkins from the University of Cambridge. And so what we're going to do is we're going to select our current belief based on the optimal action is in the current state by taking the argmax of the Q function or the argmin of the Q function, depending on if you're maximizing or minimizing. So again, what we will do is I think for the maze problem, I believe there's some typos here, like it says argmax, we actually mean argmin because we're trying to minimize distance. But typically, you can think of another way to do this is you can just realize your cost as negative rewards. So now if I think like it costs me to take a step and therefore the return for taking a step is negative one, all of a sudden it's turned argmin problem into argmax problem. So typically I'm going to be talking about in terms of minimizing, but usually most people are going to are trying to maximize. And if you have this problem of you've got a cost function rather than reward, it's pretty easy to just like invert your reward values and suddenly you turn your rewards into a cost. So we're going to represent the Q table for this maze world, which is going to be so I have the Q function of S sub T and some action. I'm trying to find the argument for a that gives me the best value. So if we have argmin of these possibilities, let's say this is the goal and S represents the state. So I have effectively a set of Q values for this state and each of the possible actions up right down or left. And so I'm basically going to evaluate this Q function for each of these combinations and then choose the value of the action that is the right hand side of the second argument that gives me that in this case minimum value or maximum value if you are treating them as costs. So that is we can let the current state be a position in X, Y coordinates and the action would be integers one, two, three or four. And so therefore I'm looking for the element of one, two, three or four that's going to minimize the value of this function where my state is represented as these two values X and Y. So if I assume a grid, then I can have say zero zero one zero one one or something like that. You know, you can start from the bottom left if you want. So let's try to do this in Python. So first, we need to know how we can actually implement this Q function. So we know what the arguments are already. It's going to be a tuple consisting of an X, Y coordinate and then a single integer representing the value. So we know we can enumerate all the states, which is going to be a finite set of out of 100 possible positions. And then we can enumerate all the actions. So that is also finite, which is four. And so then we can calculate the new state from the old state in an action and then represent all of these state action combinations in finite memory. So a fairly nice compact table that we can use. So we'll just store the Q function in in table form. So this case, we're going to use three dimensions X and Y. You could in principle use two dimensions. Right. So I could just say enumerate these 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, next row 11, or next row 10, 11, 12, so on, so on. It is maybe just a little bit cleaner, easier to visualize if I keep them separate. So that's what we're going to do. But it wouldn't be too difficult to switch between these two representations. So we'll have a three dimensional Q table where I'm going to have one dimension is X, second dimension is X, third dimension is X. And then the third dimension is the action. So you can think of this as kind of this cube where I've got each of these states on the X, Y plane. And as I go every level deeper, that's representing what the Q table is going to look like for each of those different actions. Now, the actual value is going to be the steps toward the goal. So we're going to be minimizing. And so for the images above, in order to get X closer to G, I'm going to be moving either right or left. So we have some intuitive representation of what correct Q values should look like, which allows us to calculate them. So how can we represent this three dimensional table of Q values in Python? So if we have X or X and Y have 10 possible values and there are four actions, so we can create this table, basically three dimensional numpy array that is a 10 by 10 by 4 array. So now we can represent this three dimensional table as a numpy array. So how should we initialize this table? This above line is going to initialize all the values as zero. So think about what effect this is going to have for the Q values for actions as they're updated to estimate steps toward the goal. So what's going to happen is basically we're going to have a table that's going to be a numpy array. So this is going to be the actual goal. So what's going to happen is basically all of my actions are going to have the lowest possible Q value that is zero. And so this is going to force the agent to try all of the actions from all the states, because it has no notion of what might be better than the other. So it's basically going to start like here. Every possible action that it could take is going to be zero. It won't know what the possible return is until it takes that action. So going left is just as good as going down. Right. So this is lots of what we call exploration. OK, questions. Right. So now updating the Q value using the temporal difference updates will look something like this. So I'm in a state, I take an action, I get some sort of return, and then I'm in a new state and I can see which possible actions are available to me. So then I can calculate the TD error. So that is the return at this this next time step, plus the value function that is the Q function. And then I'll use this. I'm going to take the Q value for the next state, next action pair, subtract the Q value for the current state, current action pair. That is where I was and what I just did. And then I'll use it to update the Q value that's stored for S&T. So it's going to be some cell in this three dimensional array that has the value in it. That's by default initialized to zero. And I'll update that with some new value. So if I, for example, start here and I move to the left, then this should give me I'll have the return, which is one. Right. In this case, my return is always one. And then I'm going to have the Q value for this, which is going to be different steps toward the goal, which in this case, let's assume like a seven by seven group, just count them. So we'll have one, two, three, four, five, six, seven. Let's say I get Q value for the next day, next action at seven. Compare this to where I was one, two, three, four, five, six. OK. So now I have one plus seven minus six. So then that's going to be that'll give me the value that allows me to update the Q table there. And so then how much do I update it? Well, I don't necessarily want to update it just with the raw value. I'm going to scale that by something. So I'll have a constant here. Row is going to be some learning rate. So I could say, well, maybe this is not like an atrocious thing to do, but it's suboptimal. So I want to maybe discount this one a little bit. Point one point zero one, whatever this row is just a scale factor between zero and one. And then I take that and then I add it to the current value of the Q function at that at that point. And so that's going to give me the new value. So in Python, it looks something like this. So let's say the position is going to be represented as a two dimensional array or a two value array. And so then we return this here. Reinforcing this case is always one. So I'll take my Q old, which is going to be basically the Q value using my old state. This is a two two elements array and then my old action. And then Q new is going to be the Q value of my current state and my action. So remember, what we're doing here is this is after I've already taken the action. So I'm updating Q new as my current state. This is where I am now. Yes. What I just did, what I just did to get here. Yeah. Yes. So keep in mind, like. The Q new here is the values at time step T plus one, whereas Q old is at the values of time step T. So you think ahead and kind of one step in time. But I want to update the values of the table where I just was, which I think of as being the quote current time step. So I'm projecting forward, seeing where will I be if I make this action? How good or bad is this? I'm going to use that value to update my values where I am now. Yes. What do you want row to be zero? Yeah, actually, you know what? This should probably be non-inclusive. You wouldn't want to have a row to be zero. Non-inclusive. You wouldn't if row is zero, you would never learn. Right. So you would have no update. Yeah. So this is probably more accurate. OK. Other questions. Right. OK. So here we are. And so now how much how much do I update? So my TD, my temporal difference error is basically this part here. Right. So again, I'm trying to figure out what my expected return is compared to the Q value of where I currently am. And then I'm going to take my current value and then update it by my learning rate times this error function. So now this should start looking pretty familiar. Right. I have an error term. I have a scale factor of how much I update it. And then I have some value that is being updated. So this is going to be performed for every pair of steps that is T and T plus one until you get to the final step. Of course, that has to be handled differently because there is no next state. Like once I get to the goal, I'm done. And so then the update given as previously is going to become basically I'm just going to update my current Q value with instead of taking some error from the subtracting the Q value for the next step. From the current state, I'm just going to take the next return minus or the next reinforcement minus my current value. So in Python, this becomes adding a test for the goal. Yeah. In this case, yeah. In this particular example. Yeah. So it's not not necessarily always going to be one. But I think we change to the goal. Yes. Well, because we're taking steps through a maze. And these are discrete. So basically, imagine if I had a two dimensional maze where sometimes I have to jump on top of a box. Right. That might have a cost or something of two. Right. Or if I have a continuous, I can decide how much I move. Right. Maybe the me were like playing mini golf or something. Right. And you can either hit it hard or softly. Right. You go to like, what is it like on Mountain Avenue? They have like this mini golf courses. That I suck at. And so like you're trying to figure out like how much do I need to putt? Right. So they could be I have a force of like one Newton or whatever. And then I have a force of like point two. So these you always have to think about what your what the problem we're trying to solve and how that is represented in this case, it's discrete steps through a maze. All I can do is step one square in any direction. So it's got a cost of. It is not necessarily always known, but you basically get that from the environment by making the action. OK, so I'll I will show you a block stacking example at the end and I'll explain how that's going. So I'll try and get through this thing. OK, so now I need to check and see if I'm at the goal. So in this case, the maze is represented as a character and I have a letter G at a goal position. All I'm going to do is just check and see if where I'm going has the letter G at it. And if it does, then I use my my kind of goal update rather than my normal update. OK, and so now to choose the best action for some state X, Y, I just need to look at the argument for the Q function at the row that contains where I'm at, given my given my current state. And then if we store the available actions as this array, then I can just update the state based on the action by looking at a where a is the argument of the Q function. So this is this agent environment interaction loop that is the agents in the environment. It does something. It gets some sort of feedback from that and uses that to decide what is best to do the next time. So we have to implement the following steps. That is, I initialize Q, I choose some sort of non goal state can be done randomly, and then I repeat the following. So I'm at the goal, then I update my Q function with the temporal difference error that is one minus the Q function or R minus Q function, and I take a new random state says I'm at the goal I'm done. I reinitialize and I try to solve the problem again. Otherwise, I'm not at the goal, I select the next action. If I'm not at the first step that I'm going to update the Q old with the temporal difference error within this case is one plus Q new I'll shift the current state and the action to where I where that action landed me, and I'll move those to the old ones, and then I'll apply the action to get to the new state. So in the 10 by 10 maze is going to look something like this in Python. So I'll initialize my Q table, choose a random starting position, and then for however many times I want to train. I'm going to execute the following. So if I'm at the goal, then I'm going to update my Q function using one minus Q old, and then I'm going to reinitialize right so this is now re initializing my state. Otherwise, I'm going to select the next action. If, and then I'm going to update my Q old using this formulation temporal difference error, and then a shift my current state of my current action to my old ones, and then apply my action to get to my new state. So in order to solve perform an RL problem you need to know the following things. You have to have the state space that is, in this case is the size of the maze, what is the extent of things that I could that I could possibly where I could possibly be, then the action So these are the things I could possibly do. So in the in the maze example this is going to be moves you can make in balancing an inverted pendulum the state space is going to be you know have the angle and angular velocity would say of the pendulum and the action space is going to be like how can I move my my hand or what torque and I apply to my pendulum to keep it balanced reinforcement for every state action, or at least a method to calculate it so this can be shoved off to another function. And then you need to have some method of extracting this so you can get very sophisticated with this like in the example that will show you actually draw that directly from the simulator. So you don't have the, you don't have any knowledge of it ahead of time but you basically use like a representation of world physics to figure it out for you. And so this can be either a cost or reward so usually we just call everything reward and you inverted if it's a cost. And you also need to know whether you're minimizing or maximizing returns. So depending on this, you know you can just apply like a negative reward there are some subtle differences in behavior with more complicated problems but generally this holds. Okay, so the Python solution to the maze problem looks like the following. So we'll start with text file that specifies this 10 by 10 maze looks like this. So now I'll print the maze so here's my representation and I have some walls here, and I have my goal so basically I'm going to start somewhere in the white space and have to find my way to G. So I'll convert this into a machine readable format of course. So I basically flatten this into a list. And I'll turn this into a 12 by 12 array. It's 12 by 12 because we have a 10 by 10 maze with walls on all sides. Right, so we're just representing this is basically places that the agent cannot go, but this is mostly just for for readability right I could represent this without these walls here. It would just be pretty confusing to look at. All right, print it out again to make sure we didn't screw it up. Great. That looks good. So, I'll need some functions, I'm going to create one that's going to draw this queue surface over this two dimensional state space. And then one to select an action, given the queue surface. So that's what these are. This is really just a drawing functions not going to go through that for time. So I'm going to construct these arrays that holds the queue table and updates it using temporal differences, and then we're going to use one that will update the queue table using what's Carlo. So remember the difference between temporal difference in Monte Carlo temporal difference I get to update my values every time I make an action and see what happened. Monte Carlo I have to wait until I get to the end. And then I can see, based on an average of how good every action was I'll update that. So I need to have four possible actions for each position. These are represented as changes in rows and columns so they have plus one and minus one. And then I'll set the Q value for the invalid actions to infinity. And so an invalid action is going to be basically where I run into a wall. So if I end up here, right going right should be an invalid action because there's a wall there. So, by doing that I can basically say that this is by saying this has a cost of an infinite cost I'm never going to take this because I'm not I'm not allowed to. So now for some parameters so we'll run 100,000 interactions, that is 100,000 updates all that my learning rates be point one and then I also have this epsilon. This is a random action probability. So what's the random action. Well let's imagine that I explore, and I end up at the goal. So I could take one step forward one step back one step forward one step back one step forward one step back and one step down that step down lands me at the goal. Right. Doesn't mean I need to take like three forward back steps and not a dog trying to find a place to nap. And before I, before I step down. Instead, it's a possibility that maybe I took seven steps and I could have taken one. Right. So there's a random action allows me to basically pop out of some sufficient strategy that's still suboptimal. And so I can do that by taking a random action it might get might allow me might maybe take like some random actions that never get me to the goal, but there's a chance I'm going to get a better, better solution. So I'm going to keep history trace of positions and reinforcements and then I need to use that to update the Monte Carlo version of cube. So I'll store this trace of x, y and a. And so now this is just for initialization display. So now let me initialize. So here's my start position. And then this is my, my first action that's just the index of where I'm going to look for my for my actions. I'll keep track of the trials and the number of times that I've hit the goal. So now we can see okay if I found the goal I'll perform the goal update, if not I'll perform the regular update, and then the Monte Carlo update is looking, I'm going to look in the trace, and then I'm going to update this based on an average of all of those, all of those Then if I'm done, I randomly start new position. If I'm not at the goal, then I pick the next action. This can be done randomly. If I choose a random value that's less than epsilon otherwise I'll choose the best action from the cube function. And then every 100 steps I will print. So, let's watch this run. So, here's the TDQ policy. Here's the Monte Carlo Q policy. This is what it's done, the most recent trials that red dot obviously is the goal. And then this is going to be the number of steps per goal at each trial. So, just take a look at this and see you know what do you think is better at solving this temporal difference or Monte Carlo. So, if I'm here I'm trying to get to the goal, the arrow represents like what the best action is for this cell. So, let's say I start here. Follow the arrows. Great. Let's start in the same position in Monte Carlo. That's this one. Right. Okay. Oh, oh crap I got stuck. Pretty clearly for this this type problem the temporal difference is a better solution. You can see, obviously in the Monte Carlo I find plenty of places where if I start at some trajectory, I end up, you know, in a place where I just sort of go back and forth if I adhere to the Q policy. I might be able to pop out of this by doing a random action, but there's no real guarantee. Okay, so now let me show you a quick example of Monte Carlo learning for different tasks. So let me. Alright, let me run this environment so this is basically a unity simulator of block stacking. So, when the sometimes it crashes hopefully won't crash. If it does crash because I show a video or something. So I'm going to train for 300 steps okay so we have two blocks. My goal is to stack the. There he goes. Okay, my goal is to stack the C block on top of the B block and what it's doing is it's selecting an action relative to the surface of the B block. I mean you can see that there are times where it's basically jumping around and moving you know it's not even touching the surface of the B block. And sometimes it does. So we can see here in these updates every so often, one of these, one of these rollouts will pop out, showing me like what the reward, the mean reward that it's been getting so this is using Monte Carlo learning. What this is doing is for the first 100 steps. It's really just randomly exploring. So trying to see if I randomly explored the space, what looks like it's a viable solution versus what's not, and then the actual learning starts. So what you can see here is kind of out there exploring the space is really just sort of verifying that there are no good moves out here. And then eventually it might sometimes it takes a while to converge. So I'm not sure it's going to get there in 300 steps. But eventually it will kind of going off spinning off in space and see it doing its thing there. Eventually it might find. Oh hey here's a good action this actually gives me a good reward. I should try more of this. I'm not sure if I'm going to get there. I did not. Okay, let me try again. Usually I run for like 500 but sometimes that it as it starts to succeed it takes longer, and this one just takes like a bit more time. So it's still learning. Okay. Not learning a whole lot, or at least it's learning what bad moves are it found something. So it should try to do more like that in the future but right now it's still in the random exploration phase. So let's see if it. We do 100 steps of random exploration, still there. Okay, so now it's found it may be found a couple of actions that might be pretty good or at least closer to optimal. So you can see now it's starting to stack. So now it does some more exploration. It tries some some more stuff. We can see that it's getting mean reward of roughly 37 or, and in this case the reward is like 1000 for stacking the right the first time and then at the discount of 100 each time. I'm not sure it's actually going to learn anything here. There we go, starting to get something. So now you can see it's starting to kind of learn a bit. And then it's. We'll see what the reward is. So here now mean reward is like 32, it should start to get slightly higher rewards here so 31.2. So now it's like 68.4. So now it seems to kind of start to converge on some sort of viable solution. So this is an example of Monte Carlo learning because it has to terminate the episode in this case it gets 10 attempts, or it stacks successfully. And so once it reaches the goal or a times out we'll basically look at all the actions that it took and see how good or bad they are. This is using a particular type of method called a deterministic policy gradient so it's actually using a neural network to optimize the weights. In particular, there are two neural networks one called an actor and one called a critic. So as you can imagine, the actor chooses an action, and it also predicts how good it thinks the action will be. And then the critic basically says, yeah you're great or boo you suck. Based on the action. And so the actor tries to make better actions, the critic tried to get better at predicting how good the actors actions are. And so then both of those losses flow backwards into those respective neural networks to optimize those weights. So this should terminate relatively soon, and we can see what the final word was, and we can try and evaluate this briefly. So this probably could have done with training for like a little bit longer. So you can see had a mean reward of 169, so it kind of. Yeah. In this case it's 1000. So basically if you were ever perfectly stacking the first time you would get a reward of 1000. So if it's, if it, the way the reward is set up here it's 1000 for stacking perfectly the first time, minus 100 for every additional attempt so if it stacks right the next time it gets a reward of 900, and then like 800 the third time and so on. Then it gets a reward of negative one for missing the block entirely, and then a reward of nine for touching the block, but not stacking successfully. So, so again we're using the world physics here. So basically if it, if it stacks off center but it stays stacked. That would be a reward in the hundreds depending on which attempt it was on. Perfectly yeah yeah. But of course you're more likely to stack perfectly the first time if you put it exactly centered. Right. So, if it trains long enough you know it should it should approximate that. Okay. Questions. So, what does the reward actually tell us about the 169. So in this case, we think about if we know the maximum reward is 1000. Right. So we can think that on average. This would probably given what you know about the reward shape. Alright, 1000 for the first attempt 900 for the second attempt. So basically between this number falls between what would be a good stack between the eighth and ninth attempt. So, if you average the model as it stands right now would probably stack successfully somewhere between that eighth and ninth attempt, maybe a little bit better, because there's a lot of noise in this and so when you actually evaluate the model sometimes it performs better than it appears to at the end of training. So, it's all about you have to understand like what the values in your reward actually signify. And so this in this case the reward value were chosen quite deliberately to kind of encourage it toward a solution with the appropriate amount of exploration like once it finds solution, it gets a very high signal, saying I should be trying more of these things. Would it be right to assume that the reward is no, it's not very efficient learning? That may be the case. So you can you can use like these reward shaping strategies to specify like what types of things you want to encourage the agent to do. But you could just as well train this with say a reward of negative one for failing to stack and just a reward of one for stacking. It might take like a little bit longer to converge possibly, but you might actually converge at like a better solution. So for example, with like the off-center stack it might get a really high reward and that's as good as it ever gets. So it's like, oh, well, this is a really good solution. It's not like perfect, but I'll keep exploiting this. Whereas with a more with kind of a rewards of lesser magnitude, you might encourage it toward like a more like a more perfect solution. Though it might take longer. Yeah, so you're you because you store the action at every step. So if you see the action is represented in zero to a thousand. Basically, in this case, values that are closer to 500 mean it's closer to getting it centered. So in this one we have 578 and 530. So the reward is nine. So it got it on the block, but maybe it didn't quite stack it fell off. And so you keep track of all of those, each of the rewards and each of the states, and you can say for this action that I took I got a reward of nine, which is like not terrible but also didn't get me what I want. This action got a reward of like 600. Right. So this is better than this other one that got me only a reward of nine. Yeah, until the until the end. Yeah. Other questions. Good questions. Yeah. Yeah. Yeah. In this case, yes. Yeah, so this case, if it just sort of plops the block out there in space and it never touches the destination blocks that word of negative one. So when you saw like the block kind of moving around in space is really kind of exploring the space and learning like there are no good actions here in this region. So eventually I want to try and move out of this region. Okay. Other questions. One last question. Okay, go ahead. So, yeah, so we get the actions over there. The one which has minus one reward and the one that has three more. Yeah, they won't pretty much have like about 500 as the quarter. Yeah. And if I remember I mentioned that if they're above 500 they get very poor from there pretty cool. Well, so I mean, keep in mind that like, imagine a large space where the block is in the center. So the block is defined is bounded at zero and 1000. So values very close to zero or just like way off in space, very close to 1000 or way off in space values close to 500 are going to be close to the block. But the way that I constructed the action space here is that the block is like really small, because it's, I'm not sure about like the span of the block in the in the action spaces but it's very very small. If you if I picked 500 500 exactly, it should stack. We also add. We also make this more difficult actually because we add a little bit of physics noise to it. So if you imagine this is a virtual environment, I can be hyper precise. So that but whenever you actually stacking things, when you release it there's a slight motion. Right. So basically add a little bit of a jitter or push. So sometimes, even if it stacks well. There's a bit of noise there that actually like sort of simulates this release and it actually ended up falling off. So there's a number of things we've done to make this problem kind of harder and more realistic. That makes it you know, at least, it's still pretty easy for reinforcement learning to solve but like, you know, at least somewhat challenging. All right, I better talk about assignment for before we adjourn. So, what you're going to be doing here is classification of hand drawn digits so you're going to be given a solution to assignment to similar to what you've seen before. And then you need to extend it into a neural network classifier class. This is using MNIST but it's not a convolutional net yet this is still fully connected net will do convolutional nets next. And then you're also going to need to find the confusion matrix function. So here's the neural network class should look a lot like what we've seen before. So, what similar to some of your A2 solutions. Then we have we got our optimizers we got an instance of neural network. So what you need to do, then, is you can test using these functions. You need to extend the neural network class. So again, you'll have access to all the same functions, but you need to override the train function the error f function the gradient f function and the use function. In addition, you also need to define make indicator bars and softmax functions. All of these are given in some form in previous notebooks. It's just this you're gonna have to pull from. So, you're going to need to do that in 9, 10 and 12 probably, I will double check that. But there's a couple of different places you're going to look for some of these different things. So, you want to look in the implementations of classifiers that we've done and look at how we override the train function and the error function gradient function and use function, and then create versions of the make indicator bars and the softmax function You can do just create a sample test of this new class. So basically what this is what this is doing is just classifying random numbers into instances of class 01 or two depending on whether they're less than 25 between 25 and 75 or between or greater than 75. So basically, you can test that function using using this. So this would look something like this. And these values are just offset so you can see them both so we just we show t plus five just so they don't overlap perfectly. Then for the hand drawn digits. So you can download MNIST. This deep learning site goes down a whole lot but you can also get it from here. So we've, there's a page at the Washington CS department. If you have trouble with this I have MNIST pickle saved so like if some for some reason either these sites work, just let me know I'll put it up on canvas. Pickle file is already partitioned. So open it. You've seen this before 50,000 training samples 10,000 valid test samples. So these are these these training classes these are the digits. If we look at those 784 columns we now know that those are the pixels right so if we if we split all of them up, you'll see that we just plot the values and you have a bunch of things between zero and one. But these are actually pixel intensities for the image right so you have pixel intensity of zero, and then suddenly we start to see actual pixels that are non zero. So you can see that these are the pixels that are laid out in rows. That's why I had this periodicity in this plot. So you can rearrange them into a square. So let's reshape them look at the numbers. We print this image we can see it's a five, give you a function to turn it into a grayscale color map. And then you can pop the negative image. So we have about 100 images and plot them using the labels as titles so these are the first 100 images. So you want to check the proportions of each digits see the roughly even, we found that they are roughly 10% belongs to each, each class. And so they're all very, very class close to point one. So let's do an experiment. So training neural net five hidden units in one layer and a small number of epochs, and so you can train your neural network classifier using this, these, these settings and see what the final training error is. So now you have to run some longer experiments so first you have to write code to do the following. So you have to try five different hidden layer structures doesn't really matter what they are. So you can choose you to train that for each one training network for 500 epochs, collect the percent of samples classified for the given train validate and test partitions. So create collect these into a pandas data frame. You want to log the times this is going to be how long the network took to train in seconds. So we've done that before you see you start a timer, and then subtract the end time from the start time or the start time from the end time sorry. And then what you're going to do is you're going to retrain a network using the best hidden layer structure as judged by the percent correct on the validation set. So basically you've done a network search using the percent correct to the validation set use that to figure out which of your network architecture is is the quote best one. So you can use this network and then use this network to find the find several images where that it gets wrong. So as you look in the test set and see where the network's probability the correct class is closest to zero. So that is these are the images which your best network actually does the worst on. Right. So then you draw these images and you discuss why your network might not be doing well for those images, for example, are there a bunch of fourth of look like nine just something like that you know depending on how your network is performing. So you could write a confusion matrix function. So this is going to return the confusion major any classification problem as a pandas data frame. We've shown this in lecture 12. So this needs to take in two arguments the predicted class and the true class, so it should look something like this, where you take in why classes and t test and output something like that. If you want to do some of the coloring, you can use what we showed in lecture. I think to do that. Okay, so 50% for the correct code 50% for the experimentation and discussion will have the greater I have not put this up yet I will do that this evening. Same, you know, same procedure, so put this in your, in your folder with your, with your notebook and run it. Finally, you can do extra credit for combining the train test and validate partitions into two matrices, and then use Adam And then you use Adam ReLU and then a single value of learning rate number of epochs compare several layer architectures by applying the crossword validation as defined in lecture 10, and then show the results and discuss which architectures, you find works best and how you determine this. Okay, we're out of time, so let you go. I'll go back to my office and have office hours in a few minutes.