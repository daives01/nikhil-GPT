 Okay, let's go ahead and start. So here's where we're at, we are still a day ahead. So this is the 23rd, we finished lecture 16 already, so I'm going to do 17 and then 18 next week remotely. And then hopefully we'll be able to take the 30th off. It'd be great for all of us. So today I will do reinforcement learning with a neural network and then also discuss the project proposal. So this is going to be due, we have two weeks to do this, but I recommend that you get it in sooner because then I'll be checking for those to come in and the moment I sign off on it you can start even if it's not the 6th of April yet. So I'll talk more about this at the end. Before I start, I just want to like reinforce a couple of points about office hours ethics. I wish more people were here. Just a note that we need to be able to assist everyone who's up fairly. And so I would request that if you come to office hours, my policy, this is like pretty much open door and as you observed you come in. If I have you do something, you are welcome to stay and try it, but I recommend, I would ask you that you like move to a different chair or if it's going to take a while, go out in the hall and work on it because there may be other people coming in. So just, I don't, we really can't have people there like monopolizing our time. Office hours for me start effectively right after class most days because I can start talking to you guys if you need, but they end at 4.30. After 4.30, I've moved on to other things. I'm not there for office hours. And I think the same is true for lab ops, right? There's a specified time. I come there for class questions. After that time, we are not obligated to discuss class content. We both have plenty of other things to do. Start your assignments early. Again, if there's a flood of people coming to either of us, you know, insisting on help at the last minute, very likely the answer at some point is going to be no, you should have planned your time better. So just, it's not like a global problem, but it's been prevalent enough that I need to make this clear again. So if you start coming at the last minute asking for help on your assignments, very likely, you know, I might help you a little bit, but at some point, you know, it's on your own. You need to be responsible. You all are adults, as I said at the beginning of class. Part of being an adult is learning to manage your time. And this is the best time of your life to learn how to do that. Frankly, the habits you develop now are going to stick with you the rest of your life. So learn to manage your time now and I promise you it will serve you very well. Okay, so hopefully that is clear. So please be respectful of us. Please be respectful of your fellow students when it comes to making use of office hours resources. All right, so any questions? Very hopefully you have, if this hasn't scared you into starting your assignments, you will your assignment after this class because that is going to be due in two weeks from Tuesday. So please, please bear that in mind. All right, so I will go into the lecture if there are no further questions. I had the controls. Okay, oh, good. Sorry, this one, this is big. And sometimes when I reload it, it takes like forever to render all of the all of the formulas for C is already loaded. Okay. So if you recall, last time we introduced reinforcement learning with this thing called a Q function with Q function is basically the function that you're trying to predict given a state and an action. How good is this going to be for for my objective of achieving my goal? And also, where am I going to end up if I am in this state and I take this action? What state am I going to end up in and what actions can I take from that state? So previously, we had filled this out as a Q table, right? So the Q table is going to basically have dimensions for the elements of the state and then for the action. So in our case of solving like a maze, we have a two dimensional state, basically xy coordinate in a maze and then an action, which is one of four things move up, down, left or right. So we were looking for the sequence of action that either maximizes or minimizes the sum of reinforcements along the sequence. So depending on whether you're you're trying to minimize your cost or maximize your reward, what we're going to do is we're going to be looking at lots of examples and looking at those reinforcements and returns. That is, if I'm in an action, if I'm in a state and I take an action, how good is that according to some some measure of goodness? Right. So again, I gave you the example of if I'm trying to solve a maze and I'm in state s and the the goal is right in front of me and I take one step to the left or the right, one step to the left, one step to the right, one step to the left, and then I move forward. I don't want to just repeat that sequence because there's like six unnecessary steps in there when I may have discovered that from this point, if I take one step forward, I reach the goal. Right. So I don't want to just find one sequence that works and continually exploit it. So we thought the Q table as this function that takes the state and a possible action and returns some value for that pair. But we this this table is actually a function. So if I have a function, we have certain types of mechanisms that I can use to find the value of that function. So now let me give you something we've talked about before. If I have some inputs and some outputs and I assume that there is a function that maps from those inputs to the output, my job is now to find the values that parameterize that function. So I need something that can say approximate that function. What type of mechanism might I be able to use? In the title. Yes. A neural network, a neural network, universal function approximators. So if it is a function, I can use a neural network to approximate it. I can, for example, if my function is y equals x, I can approximate that function with a neural network. I don't know why you would do that, but you can do it. So I can do with a function of arbitrary simplicity. Of course, I can do it for a function of arbitrary complexity. So if q is just a function, let me just optimize for the value of that function. So the objective of a reinforcement learning problem is going to be reduced to the objective of acquiring this q function that's going to predict the expected sum of future reinforcements. So now I'm trying to find this big R value. This is going to be the thing that I'm going to try to predict with a neural network. So we're now back in a pretty standard neural network paradigm. I'm going to take in some inputs and predict some outputs. My output now is going to be the sum of future reinforcements or the total return, and my input would be some representation of the state in the action. And so the correct q function, once it's optimized, will best determine the optimal next action. So now my objective is to take this approximation and make it as accurate as possible. So here are the components of this. So for every step, I want to compute the return for the sum of the reinforcements from here to the end. So little r, this is basically I'm trying to predict what big R is from this time step to the end. I'm going to do that for all possible steps. And that's going to be the value of the q function. So to break this down a little bit more, this is usually formulated as a least squares objective. So that is going to minimize the square of the expected value. So because the sum of reinforcements is some sort of scalar value, I can still use a mean squared error metric to approximate this. So this is no longer a classification problem so much as I'm trying to optimize the values in my network to predict the sum of future reinforcements. So now I'm back in the territory of trying to minimize error. So I have an established q value and I'm trying to minimize the error between that and the predicted sum. And I'm going to square that. So the expectation is going to be taken over a couple of sources of randomness. So there's the action selection. We talked about how I can I have a set of actions that I can take. But sometimes, let's say I don't want to I don't want to exploit a correct but suboptimal solution, such as moving back and forth left and right before moving toward the goal. So I want to have some level of randomness. I'm going to take a random action. So that random action that that's that epsilon from the previous lecture, that's the thing that allows me to determine with some probability, I'm going to take a random action instead of exploiting what my best strategy is right now. So if I find a strategy that works, but it's not the best one, I'll keep exploiting that where in fact, there could be a much easier solution. But I have no way of finding that because I've kind of found this rot in the search space. And I just keep doing that over and over again. There could also be some randomness in the state transitions. It may not it may not be deterministic what which state I enter given that I take an action. So if I'm in a state and I have two possible actions, I take the left fork or the right fork, there's some probability distribution. Maybe they're not evenly distributed and that I'm going to take either of these actions. And then finally, the reinforcement received. So if this is a continuous, continuous problem, I can approximate what I think my expected reinforcement would be. But it might not be exactly that due to other factors, depending on the complexity of the environment. And then, of course, the key problem is that we don't know we don't know this. We don't we don't know this term because this is asking me to perform a sum over infinite elements and I can't do that. So what I'm going to use is I'm going to use the Markov property. Basically, does anyone know what the Markov property is? They heard of this term before. Kind of any sense in what context you read this term? I've heard of chains. That's a good place to start. What's the Markov chain? Like it's the last out, right? Yes. And does it use it to repeat for those in Zoom a Markov chain, you take the last output and that is the input that determines the next output. Does a Markov chain use the previous output before the last typically? No, I mean, if a Markov chain, if the first order Markov chain, which is by default a Markov chain, it does not. So Markov chain is kind of something that we might call a drunk walk. So if I have a bunch of grids and I am in one grid and I take a step forward, well, then the grid, the square that I'm in now has influence on where I go. I could go to any of the adjacent squares, but it doesn't know that it just came from the square behind me. So I could just as easily just take a step back. So now I do this a bunch of times. I'm staggered around like a drunk person, hence a drunk walk. So you can maybe you might stumble upon a path that you actually do start moving forward and then suddenly you'll verse to the left or you step backwards or something like that. So the Markov property is basically this memoryless property that is where I go next depends only on where I am now. It does not really take into account anything previous such as where I came from. This doesn't sound like it would be very useful for solving a sequential problem. But in fact, it actually is because we can now approximate the sum for all k of r sub t plus k plus one as the sum of r the next return. So r sub t plus one plus the sum of all returns from that point. OK, well, I just kicked the can down the road by one by one step. But if I can use this highlighted up here to approximate, let me draw this out a little easier. So if I can use this part here to approximate this part, now I have to approximate this part. What can I do to approximate the part that's just in the box? I just shifted again. Right. So now the sum from k equals one to infinity of r sub t plus k plus one can be approximated as r sub t plus two plus the sum from k equals two to infinity of r sub t plus k plus one and so on and so on. And so at some point, this goes on to infinity because I don't know how many steps it's going to take me to reach the end. But at some point, I will reach the end and it's no longer infinity. I'm just looking at the return that I just received. And so if I approximate it this way, then I should end up with a relatively close approximation of the entire return. So. So, basically, because we have realized the Q function as this sum, then I can approximate the entirety of the Q function as the next return plus the Q value of the next state and the next action. Everybody clear on kind of how this is working so far? All right. And so now I'm back and trying to figure out what the actual value of this Q function is, approximating that with a neural network. So we'll just assume that the Q function is some function of an arbitrary level of nonlinearity. So I just have to find kind of a good network size is going to allow me to capture that level of nonlinearity and approximate the values in that in those ways. So the gradient of the objective. So now our minimization problem is like this. So Q is going to be some function parameterized by weights W. And so now I need to minimize the gradient of this function. So if I have the error gradient, J sub W or J W is going to be equal to the error of the expected return minus the value of the Q function for this input parameterized by these weights. And I'm just trying to solve for W. So in other words, because we now can break, we can break this down into the steps above, this can be written as the return plus the Q value at the next state minus the Q value at the current state squared. So if this function represents some kind of gradient, what kind of operation could we find to get the lowest point on the slope? What might we do to that gradient? We could descend the gradient. Eventually, if we descend the gradient successfully, we will find the lowest point in the gradient. OK, so this is now hopefully back in some level of familiar territory. So we take the gradient of J with respect to W and do gradient descent. And then we'll see here Q appears twice in the expression. So a pretty common approach is to take the respect or take the gradient with respect to Q at the current state and then we treat the next the next value of Q as a constant. This isn't necessarily correct, but it works well and makes the algorithm a lot simpler. So it requires less compute. So now I only have to compute the Q function once, given its current values rather than twice. So in more recent methods, a second Q function called the target Q function has been used to calculate the Q value at the next state given a set of different parameters. And then at certain intervals, we're going to copy my source weights to my target weights and then update the target Q function to basically keep these two roughly in line. OK, so just in principles, let's recall that the expectation operator is going to be a sum of possible states weighted by probabilities. So if D is the value on top of the fairy die, the expected value of D is actually three and a half, which doesn't make a lot of sense because it's not a possible output. But the gradient of the expected value is going to be the expected value of the gradients. So the gradient of J, we can just take the gradient of the above formula. And so this is going to be two times the return plus the value at the next step minus the value of the current step minus the current gradient of the value. So the expectation operator requires knowledge of the probabilities of all those random effects. We don't have that. So what we'll do is sample from the world. So this is now because I don't have this, I still need to see for my states and my actions, what kind of returns do I get if I take those actions in those states? So that is more probable events will occur more often. And then if we just add updates to W for each sample, you will get the expectation. So if I'm in a world and there's a distribution of events that can take place, regardless of the state and the action, the state of the action that I take, things that are more probable will occur more often. So, you know, you can think of like continuous problems when balancing my inverted pendulum. If I'm already over the marker here, I don't use the eraser. So if my pendulum is already tilted over here, it doesn't really matter what I do. I'm very likely to continue to drift to that one side. So given the state and this action, the state of the world, you can keep the markers, OK? Given the state and this action, then it's very likely I'm going to end up in the same place regardless. Whereas if I'm more balanced, then maybe there's a wider distribution of possibilities that could occur. So by sampling the world with the current state, I can basically get a decent doing that repeatedly. I can get a decent survey of what types of things are likely to happen given the current state of the world. OK, any questions? OK, so first, let's look at the gradient of the Q function as a table, and then we will look how and see how we can update that using a neural network. So when the gradient, when Q is a table, we first have to decide what the W parameters are for a table. So these are actually just the entries in the table. So if you think of what I'm if I have a state that is two dimensions and an action that is one dimension and I have my three dimensional table, I basically just go, OK, X, Y, Z at this cell in the table. This is the return. This is what I what I expect. And so what I just I just hear what I can do is I can take that input and multiply it by those weights instead to get the return. So you're not going to get like the identical values, basically just get an alternate way of saying, hey, instead of just retrieving this value, you actually want to take what's in here and multiply this by your input. And this is going to give you the expected value. So since W also weights, you can formulate the selection of this correct parameter as a dot product. So if we have X of T as a column vector, this will be equal to the number of table cells and values. And these are all zeros with a single one. So that one designates the cell corresponding to S and A. And so then Q of that is going to be that that input times those weights. So this looks a lot like the prediction step in a neural network. So therefore, the gradient of the Q function will basically just be the gradient of X of T transpose times W. And that's since this is taken with respect to W, this is just equal to X of T. So now, if I want to actually update the the weights, I need to define the temporal difference error. So in this case, this is the formula we've seen before. So delta sub T is going to be R sub T plus one plus the Q value at the next time step minus the current Q value. And so if we substitute the gradient of J in from above like this. So now this we can just replace with delta sub T. And so now the gradient in respect to W of J is going to be two times the error, delta sub T minus the gradient of the Q table at the current step. And so since we've established this is going to be X of T, then this ends up just being negative two E of delta sub T times X of T. So, in other words, for because our input except T is represented as a one hot vector, where one that one indicates the cell corresponding to S of T, A sub T, then this value. So this is delta sub T times one. And so now this value ends up just being negative two times the error of delta sub T and then elsewhere is zero. So now we can replace the expectation with samples and then we'll get the temporal difference update for the Q table. So this is just the standard weight update. I take my current weight weight value and then I subtract the gradient and store that in the new weight value. And so if the gradient is delta sub T times X sub T, I just multiply that by some learning rate row. And this is the update. So W times row times delta sub T times X sub T. And so this is really the same as what we've seen before. So I'm updating the Q table. Q of S of T, A sub T is going to be Q of S of T, A sub T plus row times the temporal difference error. And this is the same as what we did in the previous lecture. So previously, the update to just the cell was implicit. So this is also the same as the weight update in neural network. That is, I update a weight W based on an error, in this case, delta, a learning rate row and an input X. So same components as we've seen before, just written differently and kind of arrived at using a different type of formulation. OK, questions about that? OK. So now the neural network version. So we've shown how it corresponds to a table and that the tabular update is effectively interchangeable with doing a weight update, if we assume that those weights are effectively just entries in the table. But using a table for Q has limited use. Reasons why maybe the size of the table might be too large to store in memory. So imagine if I'm trying to solve some sort of complex environment, I might have a continuous function or even if I don't, I might have too many possible state action pairs. So I might not be able to store the entire table in memory. Learning could be slow. So I could learn something from if I have two similar situations in different locations in my environment. I could learn something from doing something at one location and eventually find my way over to the other location and I'm not able to solve it. It's like if you learn by reinforcement learning that red means stop and green means go, but you learn it at that traffic light like on Meldrum north of the parking lot and then you go to another traffic light and you have to learn that all over again. That would be very inefficient and also very dangerous. So instead, what if I have learned that red means stop and green means go and now I can see, well, I've gone somewhere else and I see also red and green happening here. I might have learned something previously that's relevant to this. So instead of having to represent every single state action as a set of cells, like every intersection in Fort Collins being a distinct cell, I actually can learn from features of the environment and then be more likely to reproduce good actions when I encounter those features elsewhere, even if I learned about it in a particular location first. So we can use this universal function approximator or a neural network. So to do this, we have to make a couple of changes in the derivation of the gradient, but they're going to be things that you've seen before. So we need to approximate this Q value parameterized by W with a neural network. So we already know the values of W are the weights and all the layers. So if you have two layers, then W is going to be composed of hidden layer weights V and big W. So it looks like the neural network should have a single output and inputs for S and A. So to find the best action, I could input S, try all possible actions A and then calculate the output of the network for each one. Then I pick the action that produces the best neural network output. This is not how you would actually want to do it. But if you think about it this way, then the math makes sense. What you would want to do is basically want to accumulate enough experience that you get a good distribution of the various different possible actions in S. Yes. I can see W, but the other hidden layer is the hidden layer. W is all the weights, big V and big W are the individual layer weights. So little w is all of the weights in the neural network. V is the hidden layer weights. Big W is the output layer weights. Other questions? OK. So really what I would want to do is sample enough experience to say, OK, I was in the same state S a bunch of times. And half the time I did action A, half the time I did action B. And those are really my two possible actions. But then I know what the distribution of expected returns or reinforcements would be. So let's just remember the objective function we're going to minimize. So we have Jw given as before. So this is the approximation of the sum and then the gradient, as we saw before. So what we'll do is instead of the expectation operator, we'll replace that with samples at time t. So now I can look and see. So instead of the E, sorry, I kept calling that error previously. I'm going to say expectation. I apologize for that. So we have we replace these with just a bunch of samples. So now I have the return that I got at time step t plus 1 plus the Q value for this sample. So now there's going to be a particular input sample and then the weights associated with that sample. And then minus the current Q value times the gradient with respect to those particular weights of the Q value. So what remains then is to calculate this last term. But we've actually seen this before when calculating the gradient of the output of neural network with respect to the weights. So remember, W is the set of both of both layer weights. So the only change the previous algorithm for if we want to train now a neural network to do a nonlinear regression is that we're just going to be using the temporal difference error instead of the target output error. So remember, previously, target output error was basically I know my ground truth value is t. I make some prediction why I measure how wrong I am is the difference between t and y. So the temporal difference error is now as given above. So this is delta. This is basically going to be the predicted return, according to my approximation, minus the actual Q value at that at that point. So what's happening here? The Q value as it stands is sort of my best prediction, as in this is what my my neural network or my Q table predicts is going to happen. It may be very wrong and may be close to correct. This is my approximation of t. So that is this is my best estimate of what's actually going to happen if I take this action. And it could be very close to what the Q table or the Q function actually says or very far. So this is what I mean when I say in reinforcement learning, you are using and training the network at the same time. So here I'm going to get an imperfect approximation of what I think my full return is going to be. I'm going to use that as the stand in for my target value, because it's the best thing that I have. I cannot get a better approximation from this because I don't have a bunch of samples. I only have the experience I've accumulated so far. And then I'm going to use my prediction. This is the analog for why that is my current Q value. So where does what does this do? Yeah. No, R is the row is the learning rate. Greek letter Rho is learning rate. This is the learning rate. But we've always used Rho. Sometimes you use alpha for this. We almost always use Rho. The little R refers to the reward at some time step T. So R sub T is the reward of time step T. R sub T plus one is the reward of time step T plus one. Big R is going to be the sum of all those rewards. That is the overall return. OK. So let's review the gradient descent updates that we had for nonlinear regression with neural networks. So for hidden layer weights V, we multiply those by my inputs. I apply some activation function H. This gives you some intermediate scalar values Z. I multiply that by my output layer weights W. And this gives me my prediction Y. Y is going to be more or less wrong when I compare it to T. So now T minus Y, this is my error rate. So I'm going to use my prediction Y. So now T minus Y, this is my error term. And so these are the updates that I use to update V. This incorporates the error and then also incorporates the derivative of the activation function. And then these are going to be the updates for my output layer. So here is Rho. We have, in this case, different learning rates for the two layers that you need not necessarily have that. Now the key change we need to make is in the error. So as I alluded to just now, what I calculated my error over is basically the best estimate I can make at the time and what my neural network before updating currently says the expected reward should be. So basically my neural network, I have two estimates of the reward. I have whatever my neural network predicts. And then I have something that's probably slightly different from that based on a little bit more experience. So I have predicted that my Q function for state where I am now plus moving forward is slightly positive. It gets me a little bit closer toward the goal. I then take a step forward and I fall into a giant pit like, oh, that was bad. It's not going to be closer to the goal. So I have a little bit more experience and I can now use that to update my Q table, my Q function. So this key change is in the error. So the targets are now going to be the reinforcement plus the next step and then the predicted Q values. So then we'll be assembling a bunch of these samples into matrices. So these inputs will be the states in the actions of these tuples be collected as rows in X. Those will then pass through the neural network will give me Z and then Y. Y equals Q because the prediction of my network is just the output of my Q function. That is its best estimate of the expected return. And those reinforcements will be collected as rows of R. So now I just need these Q of S of T plus one, A sub T plus one. And you can think of these as this is just the Q function once I've taken my next action. So this is just rows of Y shifted by one and maybe slightly slightly altered. So what we'll do is we'll call this Qn for Q next. So now I can just rewrite the gradient descent updates, except we're using R, Qn and Q. And then X is the same. So I have X, R, Qn and Q. All of these except X have one column where X has however many dimensions I have in the input. So now T is equal to R plus Qn. So this is that approximation of best return. And then Z is tanh or some activation function of X times V as before. And now the update functions are pretty identical, except you'll notice that I replaced Y with Q. I placed Y with Q because the prediction of my neural network is the output of my Q function. So to make this even more obvious, we'll just replace Q with Y and you can see it here. So now T, I have a way of calculating that as my return plus the next row in Y. And everything else is pretty close to being identical to the standard neural network update. The only thing that's missing is we don't have this one over K that we had there. But because that's a constant, we can effectively just factor that out. Questions? So dealing with infinite sums. So for these tasks that have no final state, so let's say if I'm trying to balance a pendulum and my goal is just to keep it balanced, it doesn't end if it's perfectly upright or something. My goal is just to not let it fall over as long as possible. So for these, the desired sum of reinforcements is going to be over an infinite future. I might say it's going to time out after a certain time. My goal is actually to keep it balanced like 100 time steps or something like that. So this is going to grow to infinity unless I force it to be some sort of finite value somehow. So what we'll do here is basically if I'm just trying to keep something going as long as possible, I can't accumulate all of my experience from one to infinity because eventually I'm going to blow up. So instead, what I'm going to do is I'm going to discount reinforcements. I'll actually discount ones that occur farther in the future. So at some point, I'm going to say, well, I predict that this action is going to keep me in a good place for the next 200 time steps and I can't really predict much beyond that. So I'm not going to worry about that right now. In 100 time steps, I'll start worrying about that when my time horizon actually reaches that. So what we'll do is we'll add this factor gamma. And so this is going to be some discount factor between 0 and 1. So as the further I get into the future, I'm going to kind of scale down how much I take this reinforcement into account. So I need to introduce that into the objective function. So you saw that we put that inside the sum. And so when I rewrite my sum as the approximation of the next reinforcement and the sum of future reinforcements, I will also use that gamma term. So this is going to be some value of gamma sub 0. And then I'll use gamma sub k in the sum. And then this is just, of course, we can rewrite this sum as the sum of the t plus 1 reinforcement plus all futures. And so then finally, remember, this is just the same as the Q function for the next state, next time step. And so gamma will also be out in front of this. So what I'm just going to do, I'm just going to simply add the multiplication by gamma to the next state action Q value in the temporal difference error. So if I'm using a bunch of updates, these are batches. So what I'm doing, nonlinear regression, we had input and target samples in x and t. And so now one set of samples are collected for one set of weight values that is for one Q function. So after the updates, we're going to have a new Q function, which is going to produce different actions. So what I do then, I need to generate more samples. So I need to do these in smallish batches so that I don't train too many iterations for each time. Otherwise, if I do training convergence every time, then the neural network can forget everything that it might have learned from previous samples. So I want to say train for like 200 at a time. And it doesn't give me a perfect function, but I'm getting closer. Then I train for another 200 samples. It's like, OK, this is getting me closer. So I'm iteratively improving. I don't fully train to convergence with every batch. So any questions before I do the example? So we'll do an example. It's called a 1D maze. What's a 1D maze? It's a number line. Don't let it fool you. It's a number line. My goal is to land on some desirable place in the number line that I would just specify. This is a cartoonishly simple example, but it does illustrate. So let's take a chain of states numbered 1 through 10. And so I can be in any state. I can move left. I can stay put, or I can move right. And I can't move off the end. So if I'm in 1, I can only move right. If I'm in 10, I can only move left. And so I want to get to state 5, for example. So let's model this as a cost function. So for every move, it's going to be negative 1 or a negative reward. And then if I end up in state 5, I get a reinforcement of 0. So at this point, I'm just trying to maximize my reward, which is the same as minimizing my cost. So modeling the reward in this way will drive the agent to get to state 5 as quickly as possible, because once it gets to state 5 the first time, it'll see, oh, I didn't get dinged for this, so I want to do this more often. So the state is going to be an integer from 1 through 10. So let's approximate the Q function using a neural network. We'll have two inputs for the integer, so that is the state and the action, six hidden units, and one output. So these are the states and actions. It's just 1 through 10. Negative 1 for action left, 0 for action stay, 1 for action right. And then the state is going to be bounded between 1 and 10. It's just taken to be S sub t plus A sub t. So if I'm in state 4 and I move left, I should end up in state 3. So pretty straightforward. I'm basically just trying to find a desirable place in the number line. So here's my neural network class. This should look pretty familiar to all of you at this point. So I will store this. OK, actions available. Stay left, or step left, stay put, step right. So representatives changes in positions, we'll model them like this. So we'll just have an array of valid actions, negative 1, 0, and 1. Now, I need to force exploration by taking random actions. But because this is a neural network, as it learns, I want to decrease the amount of exploration in this case. We may not always want to do that. It depends on how well your neural network actually fits to the environment. This is very simple. So if I've got a really good policy for arriving at state 5, I don't want to risk disrupting that by taking a random action at that point. So we use epsilon to indicate the probability of taking a random action bounded at 0 and 1. So given a QNET, call it QNET, the current state and the set of valid actions in epsilon, we can define this function. This will return a random choice from valid actions or the best action determined by the QNET. So that is, if I have a 20% chance of taking a random action, then when it's presented with my action policy, I will either take the best thing predicted by that according to the state, or I will do that 80% of the time. The other 20% of the time, I will take the random action. As this is called the epsilon greedy policy. So I'll define a function epsilon greedy that does that. So basically, this is the important part. If I choose a random number that is less than epsilon, then I'll make a random choice out of valid actions. Otherwise, I will take the greedy move. So that is, I will run all my samples through my QNET and predict the best action for each sample. So now I need a function. What this will do, this is actually going to do the batching. So this make samples function will collect a bunch of samples of state action reinforcement, next state, next action. I can make this a generic function, my passing in functions actually to create the initial state, the next state, and the reinforcement value. So I'll define those functions first. So this is initial state, next state, and reinforcement. So basically, the initial state is just going to choose randomly from 1 to 10. So bounded at 11, because it's exclusive. And then new state is going to take in the current state and the action, and then add state to the action to give me the new state bounded at 1 and 10. And then the reinforcement will return negative 1 if I'm not in 5 and 0 if I am in 5. Very simple reinforcement policy. Gives me a syntax warning there, but it doesn't matter. So here's my next function, make samples. So I pass in the QNET, and then I pass in these functions for initial state, next state, and the reinforcement function, as well as my valid samples and the number of samples and my value of epsilon. So now I'm creating my X, R, and QN matrices. So these are just initialized with zeros. And then I will generate my first state. And then my action is going to be the epsilon greedy policy over the, as dictated by the QNET, given the initial state, S, and then the valid action of the epsilon. And then for every step in the samples, I'm going to sample the next state, compute the reinforcement at that state, and then choose the next action using the epsilon greedy policy, and then advanced one-time step at a time. And then this is plotting. So we'll just basically draw you a bunch of plots showing a bunch of ways of visualizing the output. So now for the main loop, we'll create 20 interactions. We'll call them steps for each trial, then update our QNET at the end of each trial. Then I'll run for 5,000 total trials. Now what I'm going to do is when updating the neural network Q function, I'm just going to run Adam for 20 epochs. So basically I'm going to perform a little bit of training to try and update my function. And then I will put in the next set of steps. So I will collect 20 interactions, update Adam for 20 epochs, collect 20 new interactions, update Adam again. So this allows me to basically perform these incremental iterated updates, not necessarily training to convergence every time. OK, so then I will create my neural network architecture here. We have 50 hidden units sitting above. I guess it's doing 50 now. I must have changed this. The description above it says six. Now gamma, this is my discount. So I'm going to scale every future reinforcement by 0.8. So the reinforcement at t plus 2 will account for 80% as much as the reinforcement at t plus 1. And then the epsilon decay, basically what this is is I have my epsilon value. I want to reach this final epsilon. When I'm done training, I want the probability of taking a random action to be 0.01%. So I need to start from 20% decay to 0.01% over the number of total trials. And so I'll calculate how much I decay every trial. So the epsilon decay in this case is 0.99. So now all this, here's my initialization of my neural network. I need to set the standardization parameters now so that the QNET can be called to get the first set of samples before it has been trained the first time. So I'll create this function. This is going to create my standardization now. I will set up my standardization with my inputs. So this is going to be 1 to 10. And then I will use the following means, 2.5 and 0.5, and then 0 and 1 for the mean standard deviations. And then the rest of this is just plotting. And now we call makeSamples. And I actually train my network. OK. This is just tracing the value of epsilon. This is actually performing the epsilon decay. And then the rest of the loop is for plots. So let's run this and watch it go for a little bit. So this will continually update. So basically what we can see here is I have the queue for each action on every set of trials. So you can see this updating. I'm getting a bunch of different randomly initialized samples. So for this one, this is going to be queue for each function. You can sort of start to see it start to take some kind of shape. So you can see that I'm getting some sort of peaks around 5 for the action stay, and maybe getting some peaks that are more correlated with moving left to right for the other states. So this is going to be the value of x for the last 200 samples. This is not easy to see. This is basically the sum of total returns because it keeps accumulating. So it becomes this big kind of block. Let me see if I can go through some of these other ones. It's not terribly easy. Don't want to. This one is the action. OK, so sorry for the jumps. It might make you some of you kind of seasick. Let's focus on the action for a little bit So you can see at the state, when the state is 5, the best action, this is plotted just as a line, but the best action between 5 is around 0, whereas the best action, now it's done. The best action for those states less than 0 is 1. The best action for the states greater than 0 is negative 1. This is the value of epsilon. You can see it decaying. This is the smoothed value of the total return. So you can see that it does train, and it does start to rise and kind of approach 0. And this is just plotting what each hidden unit is doing. So this is kind of hard to interpret. And here's the temporal difference error. So ultimately, this is actually what happened here. It basically plummeted for a bit, and then after more exploration, the TD error actually kind of starts to even out. OK, so now if I hit Define Run, I will not run this, because it says don't run live, because it takes about six minutes. So what this is doing, this is going through a grid search, trying to find what type of hyperparameters are actually best for this task. So we'll try a bunch of different numbers of trials, numbers of steps for trials, epochs, architectures, values of gamma, learning rate, et cetera. So let me skip to the bottom of this. I'll just plotted it in a Pandas data frame. So let me go through each of these, and then we'll talk about the results. So this is the number of trials. I've sorted them by our last two. I'll get that in a moment. So you can see for a number of trials, steps for trial, number of epochs, network architecture, value of gamma, et cetera. So the last two columns here are basically, this is the total return over the entire training process. And then this is the last two returns. So this is over those last two batches what the returns were. That's the value sorted by. If you want to take a look at it, do you have any sense of what network architectures look like they might perform well? Or network architecture in conjunction with other factors, perhaps? Yeah. So do you think the goal, the final goal, is the same or is it different for each trial? The goal is always the same. The goal is always to land at five. So if I change the goal, can it still land at both trials? Yeah, it could. Yeah. So if you can give an example of all fitting and registration, the ring function and the genome? Yeah. So if you can change the goal, can it still land at five? Yeah. So if you can change the goal, can it still land at five? The ring function and the genome? So this network is overfit to cases where the goal is five. So if I change the goal and retrain, I'll find it. This network has been trained to arrive at a goal that is five. If I change the goal to six or something, this trained network isn't going to work. But I can retrain the network. I might even be able to take this network and update it or something and tune those weights to update for goal six. Let's talk about generalization. Do you remember the block stacking example from last time? So what you can do is if I train it to stack two blocks and then I just change the environment to allow it to stack three blocks, because all it's doing is choosing an action on top of the topmost block, it probably would do an OK job of stacking multiple blocks. Because what it's predicting is basically, here's an action. It's relative to something very specified in the world. So it's very localized. And then I can basically update what is actually executed in the world, like what my agent actually physically does. And they can probably learn to stack three blocks. Or I could maybe take that train network and tune it slightly to be better at that task. But the state here is basically learning that if I'm in state four, the best action is to take one step to the right. If I'm in state six, the best action is to take one step to the left. If I'm in state five, the best action is to stay still. So if I change my goal to six, that is I change the environment, if it gets to state five, it's going to say the best action is to stay still and whatever reach the goal at six. So the environment, of course, is critical. So here, does anybody see what might be something of a discriminating factor in terms of success of this type of network? Is the number of hidden? We got 10, 10 here at the top. We also have 10, 10 and 10, 10 here at the bottom. The number of steps for trial looks like it probably does. So some of the worst performing ones all have 100 steps per trial, whereas the best performing ones usually have fewer. So one reason for that is that this is actually training less per trial. It's just pouring a very small iterative update, such as training for 10 epochs or something, updating the optimizer. Whereas here, it actually might be training closer to convergence. And then the next trial comes along and it has to forget everything that it learned in order to optimize again. OK, all right, questions about neural network reinforcement learning? I'm sure there are many. I can talk about this at some length if you want. So if you imagine a more complicated environment like solving a game level or something. Yeah, question. No, go ahead. I have a question about choosing activation functions. OK, yeah. Is there anything more that we need to consider now that we're doing reinforcement learning that I've never done before? Yeah, so choosing activation functions. So I think that's a good question. So I think that's a good question. That's really good. Yeah, so choosing activation functions. Let me think about that. So you have a couple of you do want to be kind of choosy about what activation functions you're using depending on the nature of the problem. So generally speaking, if you use a ReLU activation, it's possible you may lose some important information about, say, bad actions, for example. But also that would depend on how you formulate your problem. Because if your reinforcements are negative, maybe ReLU is a worse choice there because it's squishing out negative information. But if your reinforcements are really less positive, basically if the worst thing you can get is 0, ReLU might not be a bad choice. But if you're modeling it like we did here, maybe ReLU would be not as optimal choice. Other questions, comments, thoughts? So if you're trying to solve a more complicated environment where there were, say, multiple recurring similar circumstances that you're trying to solve some sort of game level or something like that, with a neural network what you get is you can increase the size of the network to accommodate different types of conditions. Yeah, question? On this Super-L, are there any activation functions like outside of any ReLU to consider? Yeah, yeah. So we didn't talk about that a whole lot, but there's a brief sidebar at the end, I think, of the ReLU notebook. There are a bunch. So the LU part of ReLU is linear unit. There are a bunch of other linear units. So exponential unit, Gaussian error linear unit, parameterized ReLU. So I'm talking about some of those, briefly, just to refresh your memory. So let's take leaky ReLU as one, for example. So ReLU, 0 until 0, and then y equals x. Well, maybe I want to squish out most of the negative information, but not all of it. So leaky ReLU kind of lets a little bit of it through. So it might be like 0.1 times x if x is negative, otherwise y equals x. Parameterized ReLU is similar to that. It lets through some of the negative information, but it's not a constant. It's usually a tuned value. So I can kind of decide, or I can try to learn the value of how much of the negative information I don't want to let through. Maybe I want to let through more of the less negative information closer to 0, and I really want to disregard most of the really negative information. Gaussian error linear unit is basically, imagine what ReLU would look like if it were a nicely differentiable function. So basically, we're trying to put the smooth curve in at 0. The Gaussian error linear unit is the one that is used in most transformers. So most of the chat bots, natural language applications, those are using Chibu units. Yeah. Other questions? Yes. Is that message for loop strategy that generally the best we can find? Oh, for grid search? I mean, if you're doing a grid search, yeah, this is like the way you would do it on if you're just doing your own. There are a number of libraries out there that you can use now. So Hyperopt, I'm not sure, doesn't do neural networks very well, but basically there's a lot of parameter search libraries out there that they will. One of the best ways to do it is basically, I have a bunch of parallel processors. I will put one instance on each processor and run them all at once. And so that way I can try a bunch of things at the same time, get all my returns back and figure out which one of these was best. But that, of course, it's going to be difficult to do just on your laptop. Yep. OK. Any other questions before I start on the project? OK. Let me open that up. Let me see. Where is? Where's the solutions? These are project proposal. And then here is an example. This is the report example, actually. OK. So the project proposal, so basically you can work in teams of up to four, as I've mentioned. The scope of your project needs to reflect the size of your team. So it needs to be more ambitious the more people you have. The project, as I've mentioned to some of you before, is really you think of this as an opportunity to maximize your own success. So that is, if there is a technique that you're particularly interested in or an application that you're particularly interested in or you have some outside area of expertise that you want to apply something to, basically what keeps you interested is what you probably would want to do for your project. If you have an existing research project and you want to put some sort of machine learning spin on it, that is fine. I do not object to that. So there are a couple of options. This is not necessarily exhaustive, but what I want you to do is be inventive and pursue some topic of your interest. But for example, if you need some ideas, take a neural algorithm that we've covered in class and apply it to some different data sets of interest and do some analysis and draw some conclusions. The analysis has to be more in-depth than what we've done in class, more than just like a bigger network work better or whatever. What I want to see is like, yeah, do the analysis of network strategies and different hyperparameters, but also I'm looking for error analysis. What are the things that your implementation fails on and why? Hypothesize. You can use other outside tools if you want, if it helps with the analysis. You can download code from the internet that implements an algorithm we didn't cover. So we talked a lot about neural networks. I will briefly talk about like KNNs and SVMs and stuff at the end, but by then you will be pretty deep into your projects. If there's some other machine learning algorithm, could be neural, could be non-neural that you want to explore. And maybe you compare that to something that we did do in class, that makes sense. If you have a bunch of coding that you're already doing, you're like, I can't bite off another coding project, you have the option to write a research report. Or if you just simply want to write a research report regardless of the amount of coding you're doing elsewhere, you can do that. What that means is you need to study at least five research articles of a topic of interest to you, again, a machine learning topic. You need to present a report that summarizes each article in detail, describing similarities and differences between the papers. And then you also need to provide a conclusion section that basically summarizes your takeaways on that topic. If you were to look at chat bots or something, you would study five papers on chat bots, and then you would write a report summarizing each approach and then talking about what are common approaches, what the current state of that field is. What you need to put in your proposal is basically just a confirmation that it is appropriately scoped. So remember, you start this now, you could probably get approval if you're really fast as early as like middle of next week, and you could potentially start then, but let's assume that the starting on fire is on April 6th, that gives you roughly a month and about a week to get everything done. So you need to, don't bite off too much, make sure this is something you can do within about a month to five weeks. In the project proposal, you need to show that they're appropriately scoped for time period and team size, and make sure you put effort into both the implementation and the analysis. Talk about what questions you're seeking to answer, and then what hypotheses you can make about the data that you'll be exploring using whatever methods you use. You need to explain why you wanna do this project, and the steps you will use to complete the project describes the methods you will use. So I need to see the sources, the data, are you gonna define new algorithms from implementations? Are you gonna use them from an online source? Where are you gonna get them from? So basically, do your due diligence, show that you have cited your sources and then go there and see what you're gonna be using. If you are working in a team, you also need to provide some definition of how the work is gonna be divided among the team members, so who's gonna be primarily responsible for what. This is not intended to be a hard, great wall between the team members. Of course, I expect you will be collaborating, you're gonna help each other out, but kinda need a sense that one person's not doing all the work and the rest of the people are just free riding. Possible results. So you can speculate on possible answers to the questions you provide in the introduction. This is gonna be a little bit different between the coding projects and the research projects. The results of the research projects are basically what you're gonna be looking for in your discussion. So what types of contrast do you think you might find between different approaches? What do you think this might tell you about the particular topic that you're researching, for example? Possible results for the coding project or of course, more along the lines of what do you think the likely outcomes are gonna be? You need to provide a timeline. So I wanna see like four entries with some dates and describe what every team member will accomplish by these dates approximately. Your grade will not depend on meeting these deadlines, but this is basically for your use so that you can come to me if you have problems, for example, and I can help maybe try to get you back on track. Okay, so this is pretty short, right? Doesn't need to be more than like two pages long. Submit this in Canvas and there's gonna be a drop box. So it's gonna be last name proposal or last name dash last name proposal and then don't email this because it's gonna get lost. So grading of this, grading is basically complete or incomplete and if these are not satisfied, what I will do is I will send it back for revisions. Once you revised it to satisfaction, you'll get the full credit. So this is basically 15% of the project grade and you will eventually get that 15% as long as you've heard it in and complete the revisions. So don't worry about like getting dinged on like if your proposal isn't clear or something, I will send it back to you for revision, needs to be made. So then also you make sure you're not like under committing or over committing. So like I said, this is 15 points. So do this and don't give these points up. So basically the grading on the project is gonna be 15% for this, 15% for the lightning presentation on the last two days of class. So what I will do is once I've gotten all the proposals in, I will count how many there are and then that will determine how long those presentations are. Usually ends up being about two and a half minutes for each one. So I will go into more detail of what you wanna do for that. But really for those last two days, every team is gonna submit a slide deck that has probably about three slides, maybe more, if you have a lot of teams, maybe longer presentations, but basically say, what's your problem? Not in an aggressive way, what is the problem you're trying to solve? I'm trying to solve what progress have you made towards it and what approaches you're using and then like what is left to do or what have you learned so far or something like that. Okay, any questions on the project deliverables? Yeah. We will, yeah. So I will do at least one lecture on transformers toward the end. Yeah, you could do, yeah. Oh yeah, I gotta do this first. You may not use chat GPT to write your proposal or your project. You may not use any chat, but like, can't use Bard, you can't use all the other ones that came out, you can't use like the one that the Chinese government put out the other day that didn't do so well apparently. So yeah, you cannot use a chat bot to write this. I want you to write your own words. So basically here's my attitude toward the chat bots right now. Right now I'm actually literally, you know, in and out of meetings with people that tilt trying to help come up with some sort of coherent positioning towards chat bots. So eventually I do feel we will come to some sort of coexistence and understanding of what is and is not an appropriate use of chat bots in the university. My attitude toward writing is if you're using a chat bot, you're not using your own words, okay? If you may possibly use like a chat bot to maybe help with like the mechanics or something, but ultimately it needs to be rewritten. You need to submit your own writing. And I will be checking against, you know, your written report and say your previous homeworks. You know, I'm gonna be looking for things to see like has the writing style drastically changed? Is were you making grammatical errors and suddenly it's like perfect, right? I'd rather see the grammatical error version, but I'm not gonna be grading for grammar unless it's like so pervasive that I can't understand. So if you spell things wrong occasionally, you make the occasional grammatical error, I don't really care that much because one, it's telling me that you're actually doing your own work and two, this is not an English class, okay? So I think at also the other thing, if you wanna study, if you wanna use like the outputs of chat bots in your project, that's perfectly okay. So using a chat bot as an object of study is a okay. I think that would actually be really cool to see. So just for some guidelines on like what my positioning is toward the use of these tools right now. Okay, other questions? But yes, we will be talking about transformers at least once. Okay, yeah. What? Yes, I will go about LSTART office hours at 3.30. Okay, bye, thank you.