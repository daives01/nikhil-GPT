 Yeah, yeah. Yeah. Yeah. Yeah. It's really great. It's like the number three bed hospital in the nation or maybe it's even the number one now and. Which is like it's a lot right but you consider how much how much treatments cost. Okay. Yeah. Okay. Let's, let's get guys. So, let me begin with note, mostly to the people who are not here, so I'm just gonna like tell you guys and those of you who are on zoom around this time of semester is when I start to notice that people are not coming to class. And this class is roughly 75 in person and maybe another dozen or so online so like I count the number of faces and it's like a lot less than 90. So those of you who are watching this recording. I do appreciate it when you let me know right and there are some people I can I can see who have already requested clearance to attend remotely and I appreciate that. So, if you're not one of those people and you're just like skipping class. I do start to notice, especially when it's like not negative 15 degrees outside there are certain days where it's like a lot more forgivable to skip class. It is now currently 31 degrees so today is not one of those days. I'm going to give this a warning that I'm addressing to the folks in person who don't need the warning. Not coming to class without a reason is going to affect your participation rate again semester and I assume that most of you don't want that. Of course just keep in touch with me you know if you are sick for a long period of time I understand you know just all the usual things but if you don't let me know I'm going to assume you're just skipping class. So, and then what am I supposed to do. Okay so you guy who was talking to me about the ethics cloak. What's your name by the way. Hunter, so I'm trying to learn folks names and it's hard to do in a class I said. So Hunter was asking me about the ethics colloquium today so if you're interested in hearing me and some other folks talk about chat GPT that's going to be happening in the LSC theater. So basically you go in the LSC and like, kind of as you're going in turn away from the food court down that like green line hallway and sort of keep going. And you'll get there eventually, it will also be recorded I'm told. And so I'm going to get the link from them and I'll post it, you know if you weren't able to attend. If you're interested in watching some of that discussion so it should be pretty interesting. So as a result I have to be there for. So I will have to help to close office hours at 345 at the latest. In case any of you are going to come by so, oh, eventually office hours will begin like the moment classes over if you need to talk to me I recommend just like catching me on the way back to my office and when we get there we can sit down and chat for a bit. But I will need to leave my office at 345 or so to head over to LSC for the event. So we are, I think we are almost done with grading a one I say we when it's sorry doing all the grading. But I think probably the weekend right. Okay, so yeah, so, so she'll give me the grades by about the weekend and I'll just need to spend about a day or so reviewing them so hopefully we'll be able to get back to you by the end of the weekend maybe So, I'll be there at 345 at the latest, which should be in time for you to take any feedback from PA one and and incorporate it into your assignment to submissions. So, if there are no other questions I will start the lecture on classification. You guys got anything you want to discuss. It's me I forget who all I think it's like five people it's from all around the university so it's basically like, I'm the only person in the university who does an LP as a primary research area as far as I understand so like now I'm getting like lots of requests to talk about how I'm handling I'm not sure the university policies. As far as I understand there is no distinct policy yet. My policy is don't. But it's going to be me. And there's like some folks from I'm gonna say like economics or the business school maybe the English department. I don't recall exactly but there's like four, four to six people total, especially a panel discussion for about two hours. So you know me and other people just riffing on the topic I've literally no prepared remarks, aside from what I rehearsed in my head so I would assume so I will say we'll see what happens right I mean I think it should be an interesting discussion I went to another one I did I was a I went to another one last semester. And then they decided to chat GPT so they asked me back. But it's a good discussion, usually it's pretty lively and there's a lot of interesting questions and things we discussed so if you can't make it in person I definitely recommend watching the recording and it just should be pretty informative and I'll probably touch back on some of the material we talked about when I do the last lecture on on ethics and and abuses of machine learning. Okay. So, if there are, would you have a question. I am told it starts at four. So I see the commences at four. So, yeah, I'll probably be there at least 10 minutes ahead. Yeah. Okay. So, let me share my screen. And let's start talking about classification. Okay. So, so far we've talked about regression problems, which is basically taking scale, mapping them to scale or output so my input is some set of numbers and my output is also a continuous number or set of continuous numbers. But many of the AI tasks that you might be interested in are not necessarily just predicting numerical values. Right, some of these what actually want to assign a meaning to things. So that is to say, how can I represent meaning numerically. Right, one is an abstract quantity that kind of resides in our brain and we have interpretations of, and the other is a quantifiable metric that we can actually you know perform numerical measurements on right how do we get these two things to align doesn't seem very intuitive. So we've done regression using linear models, and then also nonlinear models okay neural net so now that we're starting this unit on classification in similar form I'll be basically showing you linear examples of classification, and then nonlinear classifications using neural networks and we can see what is most useful for in which circumstances so let's just take a look and see how linear models are not suitable for classification. So let's we have some data like this, let's say we've got 13 students are taking a test. And yes you could plot the, the grade as an actual numerical value and then maybe it could be solved for using a linear function, but let's say that instead we have this data plotted as x being the number of hours that every student studies for the test. And then the y axis is the letter grade, let's just say ABC. So instead of a numerical value between let's say 70 and 100 is just one of three categories. Right so it doesn't matter so much if this person gets like an 83 and this person gets an 87. This is a big. This is what we care about right now. So, we could still try to do linear least squares on this by using the integers one, two and three for the different classes right so now I could have x being the hours of study, and then just plot one, two and three almost as if this were And you could do that, and then you could linear function to the data. Right. And so you could get a line that maybe looks like this given this data. This is not a great fit. Right, if I were to say, try to calculate the R squared value of this or something. It would not necessarily be very high because if you look at whatever value this is, you know, four hours. It, you would expect the value to fall here somewhere around 1.5 right when in fact the value is two. So, even though this kind of fits the data sort of relatively well, it's not a great idea because you have to convert these continuous y axis values into these discrete integers just one, two and three, 1.5 is not allowed right 1.5 is not one of my classes. So, without adding more parameters I need to figure out like where I have to split these, and I could use this general solution of like splitting at 1.5 and 2.5 and if it's like above 2.5 like I'm just going to round up and say it's a three, right, which would be an A. But if I do this, then rats. Chuck Anderson wrote this part of the notebook obviously because when things don't work for me I use more colorful language. Nonetheless, rats, the boundaries are not where we want them to be, because we can see here that if I'm just if I have my 13 samples and trying to arrive at an even distribution. I'm basically grouping one sample that I know is a B with the C's and another sample that I also know is a B with the A's. Right, so already, there's noise in my output set. So, I can already tell that by doing this, I'm going to have a very suboptimal model. So for example, if I have this, if I say X equals four, it's going to say okay well the predicted output is 1.4 which is below the 1.5 boundary and therefore it's a one or a C. So already, there's going to be at least two samples out of my 13, third is classified. I can do a lot better than this. So one solution would be to represent the class labels, not as numerical values, but to rather decouple the modeling of those values from the actual boundaries. So, using one value to represent all the classes doesn't allow me to, doesn't allow me to have that kind of flexibility. So instead I want to use three different values that are all orthogonal. So now, there's no clear linear dependency between say class B and class C, right. So even though the data may be broadly monotonically increasing, if you plot you know the grade versus the hours of study, it's not a good fit because basically the representation of the different classes is kind of orthogonal to each other. So this could just as easily be three completely different classes of, you know, subjects, maybe that maybe if you're in a study that or if you're a field that requires like on average fewer hours of study per night, for whatever reason, just because the nature of the field, you could use that to predict, you know, this, predict what field someone is in based on their hours of study per night. So class modeling, basically what you want is all your classes to kind of be orthogonal to each other. So one way to represent these orthogonal values are just to think of them as say vectors, right. So if I have three classes, this could be a three dimensional vector where every every class is just perpendicular to all the other classes. Okay, so one way to do this would be just to represent these what we call indicator variables. So if I have three classes of indices zero one and two, and I'll just put a one where it is a member of that class and zeros elsewhere. So indicator variables or one hot vectors, right, are basically the term that we use here. So we can model class one as one zero zero class two is zero one zero class three is zero zero one. If you think of how you might plot these coordinates in 3D space, you basically end up with something that looks a bit like this. So if my thumb is the x axis, my index finger is the y axis, middle finger is the z axis, you can see how if we assume that all my fingers were unit vectors, all these vectors would be orthogonal to each other. Okay. So now I can just let the output be this, this triplet, this, that has you know for however many value however many classes I have it has that many values, and I want to maximize the predictions that I get close closest to the indicator value that represents the class that is the actual true value. So then I'll need to convert to do is take these values and convert them to a class by picking the max. So, for example, if the class is 001, then this should y1 should be one or close to it, y2 should be zero y3 should be zero. If I take the argmax it's going to say okay index zero aka class one is the most probable class. Okay. Yes. Both it should be one. Both it should be one. But we're approximating so eventually we're going to get it as close to one as possible. I mean a little ahead of myself with the, with how we calculate error and classification, and we won't really get to it so much until the next lecture. So for the moment you can just assume that you want class one to be one. Well, if I represent my outputs this way, then I can just have two values on the y axis zero and one, and wherever that the class in question is one if it's one else rate zero. So, if I plot the data like this now we can see that for example all my A's are ones in class three and everything else is zero, and these would be all my B's, and these would be all my C's. So, it would still fit a linear function to this data. Right again it's not going to be a great fit, but it would look something like the following. So then what I can do is I can overlay them to see which one is the maximum value so for example, the, the blue one is basically the line fit to the A data, the red line is the line fit to the C data, and the green line is the line fit to the B data. So these are the boundaries between my actual classes I just look and see for each of these lines in which class, or which, which model has the highest value at that point. Right so the red line is my C's we can see that for these samples, the value is higher than the other two. So therefore these would be in numbers of class one, the green line is the highest for for this set so then these would be members of class one, and the blue line is the highest for members of this set these members of class three. So, what if the green line were slightly too low. Right so now if we look at this sample here, right, this one, which line is the highest at x equals this value, where the mouse is the red one is the sort of the class zero line or the C line if you will. But for this sample that we know is a B it's actually predicting a C because the C line is highest. So, why might this be the case. One reason that we could that could be the cause of this would be a masking problem right we have too few samples of class two. So if my class if only have three samples of class to my data looks like this, and the line that is fit to this data is going to be lower than this green line right is actually going to be much more like this. And so, if I have too few samples of a particular class then novel data that belongs to that class is kind of not going to be modeled as being a member of that class. So there might be no values of x which the second output y two is larger than the other two given a linear model. So that is class two has been masked by the other classes. So if I have say, five samples of class zero and five samples of class three, only three samples of class two, it becomes harder to pick out new samples of class two from new data. This is the same issue, as we would encounter with an unbalanced data set right it's more probable that something is going to be a member of class zero or class one or class three. And so I'm going to get a lower error if I predict those more often. Right, so this is going to, this is going to make me under predict instances of class two. So, let's think of what other shape of function would work better for for data like this. So, you can chew on that if you have any answers, you know you can you can spin them out later. Just hold this thought and let's try an example, real quick. Well, mostly try an example for the rest of the lecture so not really all that quickly but we'll go off to an example and come back to this later. So we'll be using this other data set from the UCI ML archive and particular data set of Parkinson's disease. So what this data set contains is actually, it's a nice bit. So what this data set contains is 147 samples from subjects with Parkinson's disease, and then 48 samples from subjects who do not have it. For these each of these samples, they extracted 22 numerical features from voice recordings so basically they recorded the voice, and then they run you know a bunch of like you know spectrograms and things over it and Fourier transforms And then they run you know different features. So, the feature engineering is kind of already done. These, these features are assumed to be relevant features from vocal analysis to predict Parkinson's, and then they're labeled. So, we just have zero for healthy subjects, and one for subjects with Parkinson's disease is actually from a collaboration with the University of Oxford and the National Center for Voice and Speech in Denver. So we'll just read the data read it in, and then just do some statistics over the data. So, if you look at this, we can see that we have 195 samples that is 147 plus 48. And then 24 indices, so we have the 22 numerical features, plus one for the label, and then the other one is just like an index, so 24 columns. So if you look at all the columns we see here. Okay, so we have name right this is the index, and then we have all these different features. So, there is status in here somewhere there so that their status. This for some reason is not the end but we know that this is the actual value. So we're going to extract that status value, and this is going to be our targets. So now we've taken the 24 features sliced out one of them. This is actually the feature trying to predict, and there are 195 by one values in that array. Okay, remember that other, there's that name column, that's just an index this is not going to be a predictive feature right you can't predict whether someone has Parkinson's if you know their name, or if you just have like some numerical index representing the study that's not correlated. So we're going to drop that and then we're going to drop the status of course because you don't want to have the thing we're trying to predict in the training data, because then what would it learn. Yeah. Or whatever. It would just ignore it could just learn to ignore everything but the status right the status column and the input is obviously a perfect predictor for this for the output so it's like well, here's the answer. I don't know everything else and I don't learn anything. Right. So, one thing that we do in in analysis is just sort of you look, look, like is, is a model. Does the training data effectively answer it obviously. And if it does then that's contaminated training data, but also are the features indicative of the answer right you you had you want to give the the model enough information to figure out the answer, without just giving it the answer. So these are the features that we actually actually use so I don't really know what all of these things necessarily mean maybe some of them mean something to some of you but they're, they're just, you know, features like frequency and volume and vocal quality and things extraction of the local recording. So, let's put the values for each of these so we can see that the ranges are very different, right, we have some that have means that in the hundred and some that have means close to zero and some that have negative means, and then there's a wide distribution as given by the standard deviations as well. So, let's look at the occurrences of the individual values, so we can see here that as, as was mentioned in the description of the data, we have 48 samples of zero, that is, people without Parkinson's disease, and 147 samples of one, that is people with. So this is basically just a binary classification negative positive for for this trait. And you can see that the sample is is unbalanced right we have way more samples who have Parkinson's than without. So, this is going to be one challenge with this. Okay, so for we have the small sample size overall under 200 samples so not huge even though we do have some pretty good data for each one. And then it's very unbalanced that we have like 100 more samples of one class than the other. What we want to do is we'll just force equal sampling from the proportions of the two classes and use that when building our training test partition so we'll use 80% for training and 20% for testing this is a very typical split that you will see. So here we can specify the training fraction so you can run this notebook and message this value to your liking. And then we can extract the class for each of the subjects, and then permute all the, all the data, take the training fraction, and this should keep roughly the same proportion in the training and test sets as occurs in the actual data. So we can see now. So for the training set we've got 156 samples. And for the testing set we have 39, and this should be roughly the same proportion I can verify this by taking the number of class zero divided by the number of class one, and we can see that it's roughly one third. Right, this is not going to be exact. But it's close enough. And so 32% of healthy subjects versus 34% of healthy subjects, or three to one ratio I should say between healthy subjects and Parkinson's is, it's decent, right. And then this compares to the original data set. These numbers are very comparable. This is what we want to see. Okay, so the least square solution would be first will standardize the inputs. We don't need to standardize the outputs because now they indicate the class. Also, the outputs are zero and one. So if you standardize them, I'm going to put something very similar. So what you don't want to do is when you're doing classification as compared to regression, you don't standardize the outputs because they're basically numerically indicators and actual class value. And then there's just some mapping that you use or just convert the numbers to say, a string label or something like that. So, then you just calculate the linear least square solution. So the training function for this would look something that looks pretty familiar. So we have, we calculate our means, standard deviations, we standardize that. We then insert our bias column. And then instead of doing iterative training using SGD right now, we can just do linear least squares. So I can use this, the linear algebra library from NumPy and just do that. And then I'll have a use function that basically will do the same thing. We just, we have the model, we standardize it, insert the column of ones and then multiply by the weights that were in this case we're calculating using least squares. So let's take a look at the data real quick, right. These are those 22 columns that are supposed to be interesting features. And so then I will just take this, feed that into my train function. And then I'll insert the bias. And then because this is a linear model, what I can do is after training, I can actually see what weights are assigned to each of those, each of those values. So, here we go. So of this, right, just take a look at this list real quick and see you know which ones do you think appear to be the most important, right. Remember how we interpret those weight values. So, what do you think is the most important feature? Shimmer? Yeah, shimmer, right. So this is basically highly, well, so this one is highly positively correlated and then there's this other one that is highly negatively correlated. What else? Jitter is pretty high. So, those three probably and then this MDVP, RAP, I don't know what that means, but it also has a fairly high negative value. So like those four features are probably getting us most of the value for our model, right. However well this model does, it's doing mostly on the strength of those four features. The rest of those values are like pretty close to zero. So now let's test the linear model, right. So to compare the target values of zero and one, you need to convert the continuous output to zero or one, whichever one is closest. Okay. And this would be something that you can do with a number of different functions, but we'll have this one function convert to zero one that's basically just computing the distance from the target that is either zero or one, and then just figuring out using the argument, whichever one has less distance. So, if I convert some of these samples where you can see this function at work, so whichever one of these guys is closest to, it will then assign to be that value and you'll note that is not bounded at zero and one, right. We can go above, right. We can have an output that's 1.1 that is much closer to one than it is to zero, so it must be an instance of class one. We've also got negative values in there. So I'm going to have to make things like 0.56, right, which is very, very slightly closer to one, but I must make a choice. And so it's going to choose one. So classification algorithms, just to keep in mind is basically, I'm giving you a fixed set of possible classes and you must choose one of these things. Unless you explicitly have, I don't know as an option, it's never going to say I don't know. It will say there is an infinite infinitesimally greater chance that it is a member of class one. So it must be a member of class one, right. So this is the difference between like 0.50001 and 0.49999. So instead of essentially saying a certain level of confidence, like if it's closer to this amount, I'm confident that it's this. Yes. Yeah. So we'll talk about that when we do classification with neural networks, and it's pretty easy to do that. So you can actually quantify sort of your confidence interval and say this is, I believe it's one and I'm 90% confident that it will, it's going to fall in the range that's going to be classified as one. You can get it to do that. Yeah. So you can usually most models will not do this by default, but you can write some code that will do it for you. Okay, so let's use our, let's use our model. So I'm just going to, I'm using the model over the training data right now, just to sort of test how it's doing over the data that's been exposed to right we see if it's at least a good fit to this model. And then I will do the same thing for the test data. Right. So what do you expect might happen you expect to see a difference between, say the percent correct train and the present correct test and how much right speculate. Okay, 10 Yeah, or, you know, I was just looking for like a lot or not much. Okay, but here we go so let's let's run this and we'll see we just convert all of these to zero and one, and then convert that to percentage. So in this case, actually, they're the same. And this might be, you know, you run this a couple of times with different training splits and you'll see some difference. Yeah, there's a whole lot of difference. In this case because the features are quite predictive, and the test data is just, you know, there's so many features in there. The test data is very very resemblance of the training data so it's unless you get like in a really unlucky split and there's like some peculiarities in the training and the test data. So, you know, random splits are kind of this double edged sword because you can, could you could get a lucky split right you could, I could do this. And I could say, oh wow, I got the same testing and training accuracy and therefore this is as good a model right but if I run it again, and it has a different random split I may not see that. And so, when you're reading a paper or evaluating model. You need to look out for whether they mentioned you know is this like an average of 10 runs or how are my splits done is like the best of and possible evaluations. And when you're writing a paper, you know it's good practice to report that as well. And also think about how you're, how you're presenting your results, you know, are you averaging over multiple runs. Are you using random splits or are you doing like participant wise splits. Yes. For assets that are like, I guess, they want to use the ensemble classifier to kind of alleviate the random splitting. Yeah, yeah so you basically for something like this exactly when it's a small test set right there's a higher chance of maybe getting some slightly odd samples in either training your test set and so if you are unlucky enough to get that, then you either have an overvalued evaluation or like a suppressed evaluation. So you can use like different different models and ensemble them you can run multiple times and average them. You can do your voting, there are multiple ways of handling this, but for for small for small test size for small test sizes and things like this it's generally good practice to maybe like get a second opinion, like this one model is doing really well. How do I know it's not just something weird that this particular model picked up about my training data. So it's good practice to evaluate it multiple times, multiple models, etc. So, let's do some visualization, right so what kind of visualization, could I use to check the results so what I can do is I can plot the true class of the output, and then each training samples you basically have to to series, what the actual class is and then what my model predicted that. So, that may look something like this. So we can see that the blue line underneath is the actual class we can see that these are like my 30 odd zero samples and these are the rest of the one samples. And then those things where the orange dot doesn't hit the blue line those the misclassifications, right. And so we can see, for example, what do you notice like which class has more misclassifications. So, which of the classes zero or one has more misclassifications, so this is zero, right, and that does make sense. Right, if you think about the imbalance in the data set, because you're going to minimize your error more you're going to get a greater So, it's basically losing more or losing less by misclassifying zeros and is my misclassifying ones. If this set were more balanced may not be the case. Look at the testing data we can see kind of the same phenomenon, right, there's fewer samples of course, we can see there's only one misclassified one class and three misclassified zero classes. So, remember we did for convert to one is basically saying, I'm just gonna have a decision boundary at point five, and doesn't matter how far it is from that decision boundary. If it's above it it's one of the below it is zero. So we may want to know for these samples do we have any that were just like real borderline in cases, right, there could be one where it's like, well I classify it as a zero, but just because my output value was like point four or nine for you know whatever. It's like okay. You just easily could have gone the other way if there's like a slight slightly different split or something like that. So, if you're like really borderline maybe don't take that as like absolute truth. So here's the actual continuously predicted output. Right. And so remember, it's not bounded at zero and ones we have values that are below zero and values that are above one, but we can see for example if you look at some of these misclassifications right these samples here. These are the ones that are like just barely below point five. So if you classify it as zeros, they're actually ones but it's like these actual values are probably you know point four or five to point four nine somewhere in there. So you can you can think of this excluding the, the ones that are above one or below zero just think of it as almost, it's 49% likely to be class one, which means it's 51% likely to be class zero so it's like if you're when it's when it's election season, I like read 538 like religiously, and so like the other predictions like saying well X is like 51% likely to win this race or no, and then they lose. What happened well it was a coin flip right it was not you did not have a good prediction for that. So like you're saying it's 51% likely does not mean it's a sure thing. Right. So this is a cognitive bias that people have similar things we see in the testing data, right. So we can look at the sample of samples. The, and one thing we can observe actually is that if you look at the samples that are misclassified as one that are actually zero. Those output values are much higher, right, these are falling in like this is like point 8.6 point seven something like that. This is also an artifact of that sample imbalance, these output values because there are fewer zero samples kind of drift arbitrarily toward that one boundary. Okay. So, what is the shape of the boundary. So imagine that you've got just two variable attributes, x one and x two within our least squares model will create make a prediction for some sample. And it's just going to be our linear model right w zero plus w one x one plus w two x two. And so then, for the Parkinson's problem will just have this decision boundary at point five. And if it's less than point five this output values less than point five, then that is zero and otherwise it's great. It's, it's one. So, the boundary is given by the equation. You know the sum of all weights times the times the inputs equals point five. So, what shape is this actually. So the above methods what we call discriminative, that is, they're trying to they're picking up the features and trying to say, how do I wait these features so that given a set of input features I can discriminate one class from another, right, it's not taking really into account. Things like how many classes, there are, or how many instances of each class there are just like in in the data set or in the wild. So another alternative approach is to basically create this probabilistic model from each class. So that is, this is a generative model so that is there is a underlying base rate for each class. And that's going to influence your prediction. Right, so if, if I'm looking at you have to know something about your data I'm looking at classifying Parkinson's. One of the things I want to know is like what's the overall base rate of people who have Parkinson's in general, but I might also be more interested in like what is what's the base rate of people who have Parkinson's who come into this clinic, because this is where my data came from. Right, so if I'm if I'm looking at data from a particular source. I may be more interested in the class relative to that source as opposed to just like the class in the wild so this is one of those. One of those things we just have to be aware of like your environment so like a lot of studies that are done at universities, of course, there's a bias towards the sample population right so we there's this acronym weird Western educated industrial rich democratic. And so you know, most people who come to a university are going to fulfill like at least three of those five categories. And so there is a bias toward people who come from those societies, and against people who do not come from those societies because they're left out of the data. Right, so maybe more interested in like, what is the prevalence of this class in my capital weird population rather than what is the presence of this class in the entire world because your sample is not of the entire world your samples of a particular group or set of roots. Okay, now before jumping into simple generative models. Let's just do some review of the probability theory that we talked about in lecture two. So, if you want to review this many of these concepts were raised in the entropy section of that lecture. Let's take some boxes of fruit. So, here are boxes, which jars, this red one is a blue one, and they all contain each contains some number of number of apples oranges and strawberries. So, if we count the number of fruit in each jar, we can see in the blue jar there's two apples, six oranges and four strawberries and the red jar. There are three apples green ones. One orange and then two strawberries. Right. So, there's 12 fruit in the blue jar and six fruits in the red jar. So then the probabilities of a fruit from a given jar, that is in the blue jar. There's two out of the fruits to the 12 fruits or apples, six out of 12 oranges, four to 12 or strawberry so there's like a given the, the blue jar, there's a 50% chance that if I pick a fruit out of that it's going to be orange. Right. Whereas if I have the red jar, there's a one in six chance I'm going to draw an orange out of there, because there's only six fruit and one of them is an orange. So, now, let's say that first I choose a jar, and then I choose a fruit from that jar. Right. So now instead of a single event that is, here's a jar pick a fruit out of it. There's two events. First you pick a jar, then you pick a fruit out of that jar. So now we need to know the probabilities of picking the jar in the first place. So let's just say for argument sake, that the probability of choosing the blue jar is 60% to the red jar is 40%. These are just numbers, there's no real reason behind this. You could say these are equal probable, because there's only two jars and you could choose them randomly, but that wouldn't change the probabilities, which is what I'm interested in showing here. So let's just say that these are. So then the probability of choosing the blue jar and drawing an apple out of the blue jar is that joint probability or the product of these two choices. So, 0.6 times 0.167 or 0.1. So with all the multiplications, we can see that the joint probabilities of these events are these different values here. And you'll notice that these don't sum to one. Right. Why would they sum to one? They wouldn't, because they, you're basically picking the jar first, and then you're picking the fruit from within that jar. So whatever value you finally get is predicated on the probability of those two events. And you'll notice that the sums here are not exactly due to rounding, but they're really, really close to 0.6 and 0.4. So basically what I'm saying here is that the probability of picking the blue jar and then any other event relevant to that happening is the same as the probability of picking the blue jar. So if I have a limited number of things that can happen after I pick the blue jar, the probability of any of those things happening after picking the blue jar is the same as probably just picking the blue jar. So it's literally just what's the probability of event X and the probability of literally anything else happening? Well, the probability of literally anything else happening is always one. So just multiply by one. So I'm going to combine these into a two dimensional table and show the joint probability of the different events. So it looks something like this. I'll have the jars on the rows and then the fruit on the column. And they look something like this. And so you can see that here are my numbers that are just the sum of the probabilities of the jars. And then these should be the probabilities of just picking a fruit. So I can just add all these together. Add all these together, they should sum to one. So there's where I get my sum one. So now just symbolically, let's just represent these as variables. So let J be a random variable for a jar and F be a random variable for a fruit. And so you can represent them like this. So here are the joint probabilities just given as P of J comma F. So this is really written. You can usually just eliminate the equal sign here. So you just maybe write this as like probability of blue comma apple. And the assumption is like, you know which one of these refers to a jar and which one of these refers to the fruit. So these can be used in Bayes' rule. And so what we just saw was an example of the product rule. So that is this. So if I take the joint probability is going to be equal to the probability of some event given another event times the probability of that event. So if I have some event F and some event J, then the probability of F comma J is going to be the same as the probability of F given J, right, vertical bar means given times the probability of J. So now, since these two, these are joint probabilities so I can just swap them, right, the probability of F and J the same as the probability of J and F. I'm just looking at co-occurrence. There's no conditional dependency. So we also now know that if I swap these, then I can rewrite this equation as probability of blue given orange times the probability of orange. And so now we know that this equation here and this equation here are the same. So now I have P of blue given orange times P of orange equals P of orange given blue times P of blue. So now I can divide both sides by P of orange. So what I do is I end up with P of blue given orange equals this side of the equation P of orange given blue times P of blue divided by P of orange. And so now the general form is this. This is Bayes' rule. P of A given B equals P of B given A times P of A divided by P of B. So if you want to remember the pattern, it's A, B, B, A, A, B. And you just need to remember where you put your lines. So A, B, B, A, A, B. So in other words, the posterior probability, don't laugh, is this thing here. So this term, this will equal the prior probability. It's the latter term in the numerator. That is the initial degree of belief in A or the base rate of A and then times the evidence. That is P of B given A divided by P of B. So that is the support that B provides for A. So on the right hand side of Bayes' rule, all the terms are given in the fruit example, except for the probability of orange. So I know I can get all of these terms, these initial probabilities and the probability of the jar from the tables. I can just pull those directly out of those values. I have to still calculate the P of orange, but I can do that using the sum rule. And so this would just be the probability of the fruit, the joint probability of the fruit orange and the jar being some jar J and just sum that over all jars J. So in this case, if the jar is blue, the probability is point three and if the jar is red, the probability is point zero six seven. So we end up as point three six seven. So in other words, Bayes' rule can be rewritten by having in the denominator the sum for all events B or sorry, events A. So that's the probability of the joint probability of B and A. So given that, we can then put back the equivalence to joint probability that we calculated earlier in terms of conditional probability. And so for all J sum the conditional probability of F given J times the probability of J. So these are all multiple different ways of representing Bayes' rule, depending on what values you actually have accessible to you. And this is known as marginalizing out and can be done using the joint probability or the conditional probability, depending on what you actually know. So now we can do this in Python. We can actually represent a conditional probability table as a two dimensional array. And I kind of doubt going to get through this notebook today, but I believe I planned on that. I'm a day ahead anyway. So let's include the row and column names as list and then write a function to print the table. So I have jar names and fruit names and I write my print table function. And it prints out a nice little display like I had earlier, including the sums, right. So we got like 18 total fruits. That's what the bottom right here means. 12 fruits in the blue jar, six fruits in the red jar, five total oranges, six total, five total apples, seven total oranges and six total strawberries. And here's the distribution in every jar. So now I can just like sum across the axes to get those sum values. I can calculate the sums of the fruits in each jar by doing this. And now I can calculate the probability of drawing each type of fruit, given that we've already chosen a jar, right. So I take the jar sums and then just divide the counts by that. And so now I can create this conditional probability table that gives me those values that I saw above. And you'll note that when we do this, we actually see sums to one at the ends of the rows. Because what's the probability of drawing a fruit, a given fruit or a known fruit given a known jar, and then you sum all of them right that should equal to one. What's the probability of drawing a fruit, whatever it is out of a jar, it's one right if you're not concerned with the actual characteristic of the fruit is. So we can do a bit more if we code the probability of selecting a jar as an array of point six and point four. And so now I can use this to calculate the joint probabilities just by multiplying those conditional probabilities by the jar probabilities. And so now here's my joint probability table. And now we can see that sum to one only occurs in the bottom right as it did before. So this is this table is all possible results and so this better sum to one. Now we're back and like, what's the probability of an event. So now it happens anything I don't care about its specifics, therefore it's one. So how do we get the probability of a fruit from this table. So we just marginalized out to remove the jars by something over the jars. And this can be accomplished just using NP some over the axis that represents the jars. And here we see those product those fruit probabilities that we saw before. So the probability of a jar, given that you know which fruit was drawn. And so now we can calculate that as P of jar given fruit equals to your fruit and jar divided by a p of fruit. And so now we get this. So, just we can represent all these as NumPy arrays and with some pretty simple operations, just keeping track of your axes, you can get all these joints or conditional probability values out of it. So if you don't know which of these values you need you can use these operations to get it. So now let's use Bayes rule for actual classification. So instead of our fruits. Let's look at hand drawn images. Right. So now I'm trying to say, I pick a image out of a quote jar. And I want to look at it and want to see okay, what digit does this represent zero through nine. So let's let I be a particular image, and then to classify I some digit like four, we need to know the probability of a digit for given an image I right so it's assumed there's some a bunch of samples of hand drawn fours and other digits. Right. And so those are individual instances, and they all belong under a class right that is the digit. And I just want to be able to classify the image as the digit so I need to know the thing that I have is the image. What's the probability of each of these 10 classes given this image. Now we probably only know P of I given digit. Right. But if we assume that I is one over the number of images. Right. So what's the probability of drawing this specific image. It's one out of however many images I've got. Right. And then how many, what's the probability of the digit being for if we assume it's evenly distributed, then this is going to be one out of 10. There are 10 digits and like I'm just choosing them randomly should be 10% chance of getting any single one. So then we can use Bayes rule by plugging in all those values. I'm sorry. In this data set now. So we'll assume that these, these are all unique samples. They look like really similar, but they're all unique. So, Bayes rule will give us the following. So, P of four, given I is going to eventually simplify to P of I given four times point one, like that's the P of digit being four divided by one over the number of images. So, for a simple generative model above we had used linear function as a discriminant functions, there's a boundary. And if it's on one side of the boundary it's one class of us on go side of the boundaries and other class. Now if we had three classes, you would end up with three discriminant functions, and then you compare the values to find the maximum value to make the class prediction. So a discriminant function is this describes the curve that separates points in the data that describe different classes. So if you remember from the neural network lecture I had the one example of like a curve of blue points and a curve of orange points. Right. So if those represented different classes I'm just trying to draw a line between them. Right. So instead of trying to draw a line to fit data, and now trying to draw a line to actually separate individual points. So for n number for any classes the number of discriminant functions is going to be either n minus one, or P, where P is the number of predictors that is important features that are strongly correlated with each, each class, whichever one of these are smaller. So a different way to develop a similar comparison is to define a probability distribution over each of the possible values, which are generative models so basically I have for every class I have a different model. And this is going to incorporate things like the base rate of that class in general. And so then I just run all of my models, and I'll see whichever which one produces the highest result. So these all eventually should generate some sort of probability, or to say like, if I have three classes one two and three it's like okay it's 50% likely that it's a 140% likely that it's a two and 60% likely that's three. Right, it's going to say three, and those classes don't have the probabilities don't have to sum to one. Right, because each of them is a separate model it doesn't know anything about the other models. So how would you like to model this probability distribution or a typical cluster. If you believe that data samples from a class have actually does tend to be close to a particular value, that is that the samples cluster around a point. So if I have some n dimensional representation, all my samples from one class kind of cluster close to each other, right, or at least they're closer to each other than they are to other things from the cluster could be very loose or could be very tight. But the point of a cluster is just that things in that cluster are closer to each other than they are to other things. And we can do clustering algorithms and we will later. And you can also do like a K nearest neighbor to assign new values to one of n clusters. But we want to kind of cluster over some central point of the sample space you want to pick a probabilistic model, it's going to have a peek over that point. So whatever point this is in n dimensions. If I describe the distribution that distribution should peak close to that point. So that is things that are closer to that will have a higher value according to this model. Therefore, it also falls to zero as you move away from that point. So to construct such a model we want a couple of different characteristics so that is the value of that model will decrease as we move away from that central point. So I have a sample that is far from the central point for a given class, it should have a low probability of being a member of that class. And then the value will always be greater than zero. So the least probability that you can have of being a member of any given class is going to be zero. And so if X is the sample and mu is some central points you can achieve this by taking one over the distance between X and mu. So it's magnitude of the vector. So let's take a new to be 5.5 and make a plot. And we can get something like this. Right. If I use the function that I defined previously, then this will give me something on the order of this, but it meets our criteria of flustering around a point, but it goes to infinity at the center. And you can't control the width of the decay in which central samples may appear. So first of all, we can take get rid of that infinity issue by taking the distance to be an exponent so that when it's zero, the result is going to be one. So that is, if I take two to the one, that is, sorry, two to the zero, right, if these two are exactly the same, right, this is just going to be one over one. So let's use base two for this to make things simple. So now we can want to see how we do a calculation with a scalar base and a vector exponent. So let's try to do this. This is not going to work right because I cannot exponentiate an into a list. Data types are not compatible. But if you use a numpy array, you actually can do that. Right. This will allow you to do it element wise. So neat trick there. You don't need to stick into a for loop. All right. So if we do that, OK, so this is better, right. It doesn't go to infinity as we approach this point. It is capped at one, but it still falls off way too fast. Right. So I want things that are like really close to this, but not quite at this. I probably want that that probability to be like point nine something not I don't want that probably to fall all the way down to like point eight. Right. So we don't want to be so exacting that you must hit this this central value exactly in order to actually get a reasonable probability of being in that class. So we want to change the distance to this function that's going to change more slowly at first. But when you're close to the center, but then continue to fall off. So what if we square this right now? I have two to the distance squared. This looks better. Right. So it starts to fall off less slowly. So this is a much nicer shape. And so now we can scale the square distance to vary the width. So if we scale it by point one, this means that the probability of a sample being in cluster X is going to fall off ten times as fast. So this looks. Better. Right. This is a pretty nice, nicely shaped function. So we could just pick a center and scale factor that best matches sample distributions. But let's make a single one more change that won't affect the shape of the model is going to make the calculations simpler. And so that is we're going to change the base of the logarithm. Right. So, you know, I assume you're all familiar with some of the properties of logarithms and we'll see how they come into play to fit a model to a bunch of samples. But the logarithm of say two to the point one times the square distance, or that is we'll just rewrite this as Z. So what's the logarithm of this? We know we can we can convert between logs. So if we're talking base 10 logs, then log of two to the Z would be Z of log two or Z times log two. So this means we can pick the base of whatever we want. So there is a number, as you're all aware, that has some really nice properties when it comes to logarithms, which is E. So the logarithm of the so we use the natural log. So the natural log of E is one. And so now if I take natural log of E to the Z, this will be Z times the natural log of E, which is equal to Z. So this makes things a lot simpler. And so now our model is going to be the probability of X is one over E to the raise of the scale factor times square distance or E to the negative scale factor times the square distance. So this is just going to be some constant. If I plot this again, this is really not going to change the distribution. So the scale factor is a bit counterintuitive. So that is the smaller the value, the more spread out the model is. So let's divide by the scale factor instead of multiplying it. And we'll just call it some variable sigma so we can tune that value. Let's also put it inside the square function so that it's going to directly scale the distance rather than the square distance, which also makes the calculation simpler. So now we end up with something like this. So we're going to end up taking derivatives, should not be surprising. So we'll be taking derivatives of this function with respect to parameters like mu. So then let's multiply by one half. So then we bring the exponent down. It's going to cancel with that one half. Again, this is all just to make the math simpler when we actually go to do the computations. All right. So now the one remaining problem is that this is not a true probability distribution. So this probability distribution must have values between zero and one, and then it must have values that sum to one over the range of possible values. So we satisfied the first requirement, but not the second. And we can fix this by calculating the value of the integral and dividing by that value called the normalizing constant. This is called the Gaussian integral, which ends up being the square root of two pi sigma squared. So if you want to go into more about why that is, check the article. Now we finally have this definition. Right. So we scale by one over the Gaussian constant, and then we take e raised to the negative one half times the square distance over sigma squared. And so now we've arrived at the normal or Gaussian probability distribution, or technically the density function thereof, assuming mean mu, standard deviation sigma, and thus varying sigma squared. So now you know a bit about why we use the normal distribution. And it's so prevalent because it has these really nice properties and desirable characteristics that are useful for constructing probabilistic models. Okay, so now you just hear some more about probability theory from various authors. So you can read more about that at your leisure if you so desire. So now before getting into Python, we need to define the multivariate normal distribution. So we should go to multiple dimensions because we don't know how many classes are going to be dealing with, and we need to have a normal distribution that's going to allow for me to do that. So in order to handle multi dimensional data and not just scalars, we'll go up to multiple dimensions. And so now if we say have two dimensions, that hill we've been drawing is going to be a mound, right, looking like those hills that we drew in the neural network lectures like Lecture 7 or so. So basically we're just going to have a two dimensional base plane and define some coordinate. And then we want to have this distribution fall off appropriately in all directions. So we'll define now X and mu to be these two dimensional column vectors. And we're doing two dimensions here so we can visualize it well. But once you've established the convention in two dimensions, it can be easily scaled up to however many dimensions that you need. So what should sigma be? So you need scale factors for both dimensions. This will allow us to stretch or shrink the mound in both directions, right, because you may not want to fall off equally in both directions. You might be more clustered around in a particular dimension or along a particular axis. So in two dimensions, the difference vector is going to be basically X minus mu. And you end up with these two values, d1 and d2. And so then the square distance is going to be d1 squared plus 2 times d1, d2 plus d2 squared. And so now you can see where the three scale factors go. So we have S1 here and then S2 here and then S3 here. And so this can be written in matrix form if we collect the scale factors like so. We have two instances of S2 because we multiply it by two. Right. So now I can put S1 here in the top left, S3 here in the bottom right, and S2 here along the opposite diagonal. So now you can think about, yes. So think about if you have multiple dimensions. So think about how we expand a polynomial. So if you have just a binomial, you end up with basically a squared plus 2ab plus b squared. So we have a cubic function. It's going to be, I'm really bad at rallying off functions at the top of my head, but like a cubed plus 2a or 3a squared b plus, yes. Then the coefficients get arranged in the matrix. You're going to have like three instances of S, or two instances of S2, like three instances of say whatever S3 would be, and then two instances of S4 and then S5. So now what we can do is we can now have D transpose times sigma times D. Okay. And so now if I do this, if it's the inverse, the identity matrixes would just cancel out to be sigma. So if I have D transpose times sigma times D, then I have D1, D2 times sigma times the original D. And so this will eventually expand out to this where I have every instance of the scale factor, the right number of times. Okay. So it's more intuitive to use scale factors that divide these distance components, rather than multiply them. So in the multidimensional world we can achieve this just by taking the inverse of sigma. Right. So now we're coming back to that inverse matrix. So now the normalizing constant is a little bit more complicated. This will involve the determinant of sigma. That's going to be the sum of the eigenvalues and basically a generalized scale vector. So what's an eigenvalue? You can go to that here. Right. So eigenvalues and eigenvectors. Characteristic vectors of linear transformation. So this is going to be this nonzero vector that changes at most by scale or factor if you apply some linear transformation to x. So you could, you take a vector and you stretch it, you rotate it, you scale it, but that there's going to be a value in there that will let most change by the scale. And that's going to be the eigenvalue, the associated vector would be the eigenvector. So you can skim through the Wikipedia entry on determinants or whatever source you wish. But basically the multivariate D dimensional normal distribution is going to be given like this. So now you can see that this bears certain resemblances to the function that we had previously. So we're basically trying to figure out what the value of sigma is. And so we can see all these terms in terms of D and sigma, big sigma, that are basically defining what little sigma is. So if I have a multivariate distribution, there's going to be a standard deviation in basically all of these dimensions. Right. So I need to figure out what that value is going to, what those values are going to be because my probabilities might not fall off evenly in all dimensions. So I could have different values in the different dimensions and I can collect them all into a matrix and then represent this function in terms of that matrix and those. Okay, so definitely not going to get through this all today. So all this means is that the Gaussian distribution is a nice choice. Its integral sums to one, its value is always non-negative. It has a derivative of a natural logarithm, which is very nice and very convenient. So we can divide p by this kind of nasty function, but it contains all the terms that we need. Right. X minus mu is the distance from the sample to the cluster center. And so now I can take these are distance constants. Right. So now I can multiply them by the covariance matrix. And then I reuse that covariance matrix in the calculation of the normalizing constant. So now if mean mu is some d dimensional column vector and sigma is a d by d symmetric matrix. So in addition to the above reasons for this, this has a number of interesting properties. So that one is the central limit theorem. So that is the sum of many choices of n random variables of 10 to a normal distribution or random variables trends towards infinity. So let's play. Let me just go through like, which can we reasonably get through? Okay. Maybe I'll get as far as QD in the next 15 minutes. Let's go for it. So let's play with this theorem a little bit in Python and we can use this interact feature. So what I can do is I can plot a uniform distribution and then the sum of all the samples and I'll then be able to mess with this value a little bit. And so you can see that as if I start with n equals one, they're basically just almost evenly distributed. Right. Not entirely, but close. And then as I increase the number of samples, it very rapidly starts to approach this curve. Right. So as I get closer and closer to infinity, right, I get a nicer and nicer Gaussian curve. So this is what I'm trying to approach. And I could increase this value. It just take the notebook. It might crash. I'm not going to do that. But you can play around with it on your own. You can see that as we approach greater and greater values, this curve starts to get smoother and smoother. So. There's that. So now how would we check the definition of probability according to what we just calculated? So first you need a function to calculate P of X given mu and sigma. So that is P of X vertical bar mu and sigma. So now if I put in my normal distribution function with inputs X, mu and sigma, where X contains those samples as an N by D matrix, mu is the mean vector and sigma is the covariance matrix. So what I can do here, let me just run this code. If I look at normal D, this now reproduces the the doc string for that. Let me check out the shapes, the matrices and the last calculation. So diff V is going to be X minus mu. This is that distance. So all my distances divided by that or minus that cluster center. So these should be all those D values. And then the normalizing constant is just one by one. And so this is going to multiply by something that is N by D. So and then take the dot part of N by D times D by D. So this should come out again to an N by D. So one by one times an N by D. Then I take an N by D. This also multiply comes out to an N by D. So I can sum across all of the axes or sum across all the axis one. This is just N and then I can reshape it into an N by one matrix. So now if I take this and then I get an answer is one for each sample. Right. This is what I was after. So I have a bunch of inputs and I transform them into a single column vector or column matrix that has an answer for each sample. So let's look at it. Do just an example with some some dummy numbers. So if I create an X, a mu and a sigma and print them all out, here are those values. So now I want to see if it's if I run normal distribution, this will say if you know for given this this mean and this covariance matrix, if my input samples are one, two, three, five and then two point one, one point nine, right. These are the two coordinates. These should be the probabilities of falling into each of those classes. So we can see here that given this this mean and this covariance, this model is not a great fit for this sample, whatever it is, because it has a low probability of falling into all those classes. It will predict that it's an instance of class three, because that is the highest probability, right. Although it's again, if we're talking about, let's say confidence, it might not be very confident in that just might be slightly more confident than that than anything else. So to really see if it's working, let's plot these. So we'll need to make a surface plot in three dimensions to show the value of normal D. So we can do the 3D plotting that we did before. So I'll use the axis 3D. I'll create my mean and my covariance matrix and I'll just create the Z mesh. And this looks something like that. So given those that mean and that covariance value, we basically have a probability distribution for this class that looks like this. So just imagine for however many classes I have, I just get a bunch of hills like this in different locations, right. And they may be wider or narrower, right, depending on how rapid the fall off is. Okay, so finally back to that masking problem. I'm going to zip through this and I will review it next time. So if you were thinking of the radial basis function to fix the masking problem, you're right. I don't know if any of you were actually thinking of that. But if you were, congratulations. But remember what a radial basis function resembles or if you don't know what a radial basis function resembles, you can know that it resembles a normal distribution. So let's say we come up with some gender distribution like the normal distribution for some class K. So I'll just call this the distribution of P of X given class K. So how do we use it to classify? So for each of those classes, we run that model and take the highest value, right. So we can do actually do better than this. Think Bayes rule. So we want to know the probability of the class given the sample, not necessarily probability of the sample given the class. So how do we get this from the probability of the sample given the class? We can apply Bayes rule. So given this, let's just jump to the second line here. So if P of K given X equals P of X given K times P of K divided by P of X, we can get the joint probability, we can get the value of P of X by marginalizing out over all the joint probabilities. So P of X and C. And in other words, because this is equivalent to the conditional probability times the probability of the class, we just now need to sum over probability of X given K times the probability of K. So for two classes, one and two, we can then classify a sample as class two if the probability of C equals two is greater than the probability of C equals one. So now I just write run this, I can rewrite both of these in terms of Bayes rule. So now just the probability of X given C times the probability of C is greater than the probability of the other class. So now all I need to know is the probability of the class and then the probability of the sample given the class, which I should have. I can factor out these because this is just some constant, right. So this is a constant on both sides, it's the same sample. And I can just say that I don't actually need to know what this value is because as long as it's constant, these two will be the proportion will be relative. OK, so using the assumption that our gender distribution for each class is a normal distribution. Now all I need to do is take this narrowly function times the probability of the class in question and this is evaluated for both classes. So running out of time here. So if I do OK, if I do this, then this will this will simplify to the version below. There are a bunch of multiplications and exponentials here so I can make that a bit simpler by using logarithms. Right. If I do the logarithm, it's going to bring the exponential down in front case in point. So if I just take the logarithm of the covariance matrix for the second class, this allows me to bring the one half down in front as a coefficient. I can get rid of the E, bring the one half down. Now I'm back in terms of the square distance and the inverse covariance matrix. So now all I need to do is calculate this, add the log of the log problem of that class and do the same thing for the other class and then compare which one is greater. So now we can define the last the each side of this last inequality as some discriminant function. It's called a delta for class K. And then the new sample, the class of a new sample X is going to be the argmax for all those discriminant functions. And so then the boundary class between the boundary between class one and class two is going to be the set of coins for which the discriminant function is equal for the two classes. Right. If I'm right along that boundary, it should be 50-50, which one I'm in. And this equation has to be quadratic in X, meaning that the boundary between class one and class two is quadratic. And so we just defined something you may have heard of called quadratic discriminant analysis, which is a linear way of doing discriminative or doing classification using generative models. So a lot of terms in the mix here and kind of counterintuitive. It's I'm looking for a discriminative function for a probability distribution, which I'm using to define generative models of different classes. And it is a linear way of doing classification, even though the function is quadratic. So I apologize for that. I, of course, did not invent these terms and their usages. So it's just a lot to kind of keep track of. OK, so we will do we'll go through QDA code and do linear discriminant analysis on Tuesday. Thanks. And I hope to see you at the event this evening or watch it afterwards.