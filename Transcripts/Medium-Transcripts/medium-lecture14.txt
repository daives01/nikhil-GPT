 Yeah. Yeah. Yeah. Yeah. 11 and 12. Try again I think I had to. I had to revoke access because they show the answer to a two. So I put I should have put that back. Try. Try reload the page also try a different browser because then as it saves it in your cash. All right, let's go ahead and get going guys. So let me just first brief announcement about the schedule. So it looks like I'm going to be gone. This, or is it these two days, so 28th and 30th of March. So I'm going to be in California to conference. What we probably I had to do this once before. Last time is because I went to Korea and I had no choice this time might be able to do remote lecture. So I think it's only be off by one, one hour I could, we could just do like all online those two days. The other option. And this would only really be the case if my paper presentation happens to fall at the class time, and I can also ask them to move that. But there's also possibility just like me recording the lecture and you're watching it at your convenience and then I'll put like a discussion board up for that lecture so you can post questions that worked pretty well in my grad class last semester when I had to go to Korea so one of those two things will be the case. So it depends like 30th I may be traveling so like possibility of me just giving lecture like from the lounge in the Denver Airport. It's gonna be interesting. And then 28th, I, there's a small chance that my presentation will will coincide with class up just a heads up so basically don't bother coming to the room on those days I suppose that's what you need to take away from that right now. I will, I will, I will, about what the plan is. So just luckily I'll be able to introduce reinforcement learning in person and we'll be able to do the the two clear learning in person as well so like what is these two, the two middle lectures will be online or asynchronous I'll just do it anyway. So, hopefully that works for everybody. Make sure that we get get through the content on time that I still have to darn I have to go give a conference presentation and get to travel. So that was the only real announcement right now so I will start convolutional neural nets if it's. Well, I don't care whether you want me to or not I'm going to start convolutional neural nets. I don't care about your opinion whether you want to or not this is what we're doing. Okay. So you probably heard the term convolutional neural nets. If that brings to mind computer vision tasks you're not wrong that's what these things are mostly used for release where they sort of first prove themselves so first I want to be able to introduce you to this data set called M list that you also may have heard of. So this is just like a bunch of handwritten digits that's because that's what it is. This is the modified National Institute of standards and technology database. So basically what it is, is, this is actually a variant on the original quote NIST database so MNIST modified NIST with it's basically just a bunch of handwritten samples of the digits zero through nine obviously. This is commonly used for training various kinds of image processing algorithms so the first kind of neural perceptron image processor that was developed I believe at MIT. I'm not sure if you use exactly MNIST digits but it was a digit recognition task. So this has been pretty much a common factor in neural machine learning from the very beginning like even when neural nets have to be run under giant mainframe and really weren't very powerful. And then when we reached about 2010 2012 and we got the hardware processing power and the data to make these things actually scale, then MNIST suddenly became sort of your go to go to set. So even now, you know, vision paper is well sometimes publish you know a demonstration on MNIST and then also it's very common for say, kind of the more like NURBS tile, like optimization papers will demonstrate on MNIST because it's very well known and simple problems you can kind of demonstrate your optimization on these sort of saddle optimization problems you try to find, you know, an optimal point in a highly nonlinear function. And then they'll go demonstrate things on MNIST clustering algorithms, they demonstrate an MNIST. So we will also be making quite a lot of use of MNIST in this class because it's a very nice introductory database. So the original MNIST database was taken from employees of the American Census Bureau. And then the testing data was taken from American high school students. So basically you have you train on, you know, government employees, and you test on high school students so you can you can kind of see like how the testing data like should probably resemble the training data. Because by that your handwriting is more or less similar than high school. And so then you have adult handwriting as the training data you evaluate on maybe young adult or youth handwriting, but not like, you know, kids handwriting where they're like drawing with crayons and holding the crayon and like a full fist. So what MNIST did is they basically took the black and white NIST images and they just normalized them to fit in this 28 by 28 window. So 28 by 28 is 784 I think. I didn't just calculate that in my head I have been memorized from before and I might be wrong. So we have seven basically 784 pixels, and then the anti aliasing did. So just to zoom in, what I mean by anti aliasing is you kind of see how the lines are not jagged right there's there's a little bit of blur there so the anti aliasing is basically just a thought smoothing technique. So, this turns a black and white image into a grayscale image where around the edge of the image there is some not fully black and not fully white pixels. And so then that and give us this, this modified MNIST data set. So, like I said MNIST is this kind of default teaching convolutional neural net data set various types of computer vision systems. In this particular lecture, we're not going to go so much into the training, but into the intuition behind the convolution, what that operation is, and how we can actually extract features using this type of operation. So, let me see. Talk about the ways I want to see if I can actually talk about sort of the fundamentals of the vision system in the notebook text looks like I don't. So I'm just going to riff on that for a little bit. So, the convolutional neural net of course is basically this kind of common computer vision backbones let's talk about how humans do vision, and how that intuition translates to a computational setting or how it doesn't so basically your, your eye does not have to be fixate on a single point. That's actually a very difficult thing to do. There are two common types of eye movements in humans and other like predators, and there's basically saccades and smooth pursuits. So saccade is basically if you're looking at a fixed frame effectively, then your eye tends to kind of make these involuntary movements across the frame to pick out relevant features. So if I look right ahead, my eye will probably naturally drift towards things that look like faces so every even though I'm staring at the wall right now. There's nothing really interesting there. My eyes naturally going to pick up, you know, your head your head your head your head because these are the things that show up in my periphery that are likely to be relevant. So let's imagine that I am instead a hunter on the African savanna, and I'm going after a gazelle right so what I would do then is, I basically fix the point but that point is moving. And so this is the smooth pursuit this is the other type of movement that you're that your eye tends to do so I don't necessarily have to move my head to follow this this thing that I want to track my eye will do it for me. So what that means is that for mammalian vision systems and things like birds are different with talk on mammals for the moment. The eye is naturally drifting across a scene, just involuntarily, trying to pick out relevant information. So just think of the optical flow of your eye there's just this whole firehose of information coming at you at any given moment, you can't pay equal to anything that's coming in even if you even if you were able to represent things as pixels. You automatically do this filtering, you try to you pick out things that are informative. And so the idea here in convolutional neural nets is if you want some mechanism that the computer will be able to use to pick out important information at a low resolution. So if we look at say, how many weights we would use to classify this using a fully connected net. So let's say if we have, you know, 20 hidden units in one layer and 10 units in the output layer, then each pixel is going to have a weight in the unit. And then each unit has 24 by 24. So that's so many for us or 20 20 by 20 is a 704 plus one for the bias that many weights. So the hidden layer is going to have 785 times 20 individual weights, and the output layer is going to have 21 by 10. So what does that equal? Well, it equals 15,910 when you add them all up. Of course, if I had a bigger network, that would be that would grow exponentially more. So this is a lot of weights to train to try and classify it, you know, 28 by 28 image. So you can imagine if I flatten this all out into a 784 plus one vector input vector. This is still that's like that's like a whole lot of weights to classify this one sample. So let's think about how this number can be reduced. So large numbers of weights require more samples to train. So if I have 15,000 weights, I'm going to need a lot of these 785 dimensional inputs to actually achieve reasonable performance. So what if we could follow some intuition from the vision system and basically just provide every hidden unit with a part of the image. So now imagine that behind my eyes, I have some my cortex. It's got a bunch of connected neurons in some fashion, we'll just assume that I'm going to use the brain and the computer is basically analogs at this point. So then I have all my optical information coming in, and I can have individual units that are effectively specialized to pick out different things. So for example, I could have things that are specialized to pick out like certain colors and maybe here I'll be able to recognize the contrast if you're wearing like a dark jacket against a light background, then I may have things that optimize like pick out human faces, right, because this is a common feature that I need to attend to. So in all the things that come at me, this would be my training data set, I need to have different specialized units to pick out those individual relevant features. And then I can combine them into things. So okay, I see a human face and then I see human limbs and legs and feet. So this is probably a human, I could have different neurons that effectively activate on seeing those different things. And then you put them all together, in this case, just using some sort of linear sum, and it's like, okay, well, this satisfies this checks all the boxes of features that I need to identify something as a human, so it's probably a human. So that's basically the intuition that we're trying to go forward the convolution neural net is trying to be more efficient than just processing the entire optical flow effectively one pixel at a time. So we can take part of the image, let's say a little patch, maybe 10 by 10. And then we can assign each unit a random patch. So then we would have 100 plus one by 20 plus 21 by 10. So that reduces our number of weights down to 2230. And so now we're about one seventh of the original number of weights. Now the problem is that if I slice my image image up saying to 10 by 10 patches, and I stack them side by side and they have 10 by 10 pixels that are being fed into one unit, and the adjacent 10 by 10 pixels that are fed into another unit. What if that split goes right down the middle of some interesting feature, and the left half and the right half. When viewed in isolation are not actually all that relevant, or you can't, you can't detect through all that relevant. Right. So it may be that just seeing the left half of something on the right half of something is not going to give me meaningful information I need to see both of them together so I need some sort of overlap I need my 10 by 10 pixel, or 10 by 10 patch my other 10 by 10 patch, I need some other patch that kind of straddles them. So instead of just stepping, you know, discreetly across my entire image. I really want to do kind of a slide. And so this is now replicating something closer to that psychotic movement that the eye does in that the eye is darting around but it's not just fixating on like an individual square in your visual field. So in that moment that it slides is taking in that information. Right. So if I'm looking at someone's face, and I'm kind of looking at one eye and then I moved to the other eye. At the same time also to get you know information about say the shape of their nose or their forehead or things like that. So now I want to try and replicate this. So, you know, you can add more units to cover more parts but that the way the number of weights starts growing again. So now we have this optimization problem of, I need to cover all parts of the image, and I want to do it without growing the number of weights. So we have the sort of dual constraints that we're trying to try to cover here. So, can we figure out a way to cover more parts of the image without increasing the number of weights in each unit, and the number of units. So obviously, should be no surprise the answer is yes. So what we can do is we can take a hidden unit that will receive these 10 by 10 image patches, and then apply this unit to all of those 10 by 10 patches from a given image. So now we have this lens that just seems like a 10 by 10 patch. So this can be shifted around the image sort of like looking through a pinhole camera, and I can move it around the image and pick out individual features as it as it's going. And each of these outputs can then be reassembled into another image, and that quote image should have some level of interesting features about the, about the original image. So if we have our 10 by 10 unit, this would result in basically the 28 by 28 digit image will have a two by two set of 10 by 10 non overlapping patches, and then eight pixels left over on the right, and at the bottom. So this is a hidden unit to these four patches and this produces four outputs that can be arranged into two by two matrix. And then each of these outputs represents how well that pattern matches the intensities in each patch so now I have basically this filter that has some weights in it that will optimize or that will activate at a certain level when it encounters certain types of features. And so if I see a part of the image that just really kind of aligns with whatever those weights signify, then it should have a high activation and if I see something that's the contrary to that, or just doesn't activate it then it's going to have a lower activation. So now we can see that I have weights that will then be effectively, I'll, I'll use the word designed for now so I'm not going to get into training in this notebook but I have weights that have values that are intended to activate on certain types of input features and non certain types of others. So, this process of shifting the focus is this convolution. So this convolution operator is basically this shift of focus across an input, and then using those weights to basically process the entire in the entire each segment of the entire input in the same way, and then determining which regions of that input are actually most relevant to this, this particular weight matrix or what we'll call a filter. So weights and hidden unit often called a kernel or a filter I'm going to use the term filter for the duration of this class just with the same consistency. But these two terms are used more or less interchangeably. So you often see the word kernel using place of this kind of in the analogy to like an SVM kernel or something like that. So in this case, we was filter just think of it this way. I'm trying to filter out the parts of the image that are relevant, right and my filter is going to be some numerical weight matrix that allows me to do that for certain types of features, I'm just going to have a bunch of different filters. And so, them all put together should allow me to select the different relevant types of features for a bunch of different classes. So I have 20 units in the hidden layer I'll have 20 much smaller images produced, and then the weights in one unit, they could be values that say result in a high value when that image patch has like a vertical edge, so that filter will be optimized to activate when it sees a vertical edge. Then the second unit might have like weights that optimize for another type of feature like a horizontal edge or a curve or a diagonal or something like that. So if you're using a lot of different calendars features like that it should then output a larger value, whereas that vertical edge filter if it sees like a curve like the bottom of a three. That's not very well aligned with what it is, what it is designed to activate on so it's not going to output a very high value. So, this allows me now to effectively say for different parts of the image this filter is designed to activate on certain things And so, I get a high value here so this is sort of a vertical edge filter. This is a simplified example because in reality with MNIST there's sort of limited numbers of features you can use to combine into images where you have curves you have straight lines horizontal lines diagonal lines, etc. So in actual image classification, you're not going to have individual filters that activate on like very specific say entity level features it's not like this is a dog filter. It's more like this is a filter that responds when it sees like things that are triangular right that that's going to be that's going to respond when it sees things that like the ears of certain kinds of dogs. Often you'll actually have filters that are optimized for multiple things you may very well end up with say a filter that will have a high activation when it sees a dog or part of a dog but also when it sees part of a car. And that just happens to be, it's optimized for multiple things and the other things other units that get activated are also very important for determining what the output of that, that network is. So in the simplified case with MNIST, we now have this network that has 2230 weights, it can process an entire image and really instead of increasing the number of weights we just have some slightly additional processing time for the convolution, and some impact on storage for storing those smaller outputs. So, this digital computation and storage costs is generally pretty small it's much more desirable to have this than to try and have a fully connected network where you have like one weight for every input, which would be every pixel. So now we come to the question of overlapping. So I want to like shift this lens, I want to instead of just like taking my pinhole camera, I actually want to move it instead of like pinpoint pinpoint pinpoint. I want to move it all the way so that I get a continuous picture of the entire input. So what I can do is I can take this thing called the stride is basically just a length, and this is like how much do I want to shift the lens of this unit. So I can do that when calculating my patches. So I might shift it by like one pixel or two pixels or something. But if I say a 10 by 10 image, I want that stride lights to be less than 10 so I capture some overlap in between the different patches. If I just do one with my 28 by 28 image and my 10 by 10 patch, then 28 minus 10 plus one that would be 19 patches left to right, similarly top to down, we'll assume all square images. And so then each unit will be applied 19 times 19 times to produce a new image that is 19 by 19 or 361 output features. So now each hidden unit will produce 361 values. And so when we weren't doing the convolution, each unit produced one value and the output layer received 20 values, one for each unit. So now the output layer receives 361 times 20 or 7220 values. So for this slightly elevated cost of computation storage, I'm basically able to get, you know, in this case, 361 times the information in that final output layer. So instead of trying to make my judgment about my category over 20 values, I now have over 7000 values, and those are bound to be much more informative. Maybe I should be able to find, get a lot more information out of those values. Okay, questions on the intuitions. All right, so let's move on to some actual examples. Effectively what we're going to be doing is we're working on the hand crafting the process of creating an image, making patches, performing convolutions and we'll see what types of activations or outputs we actually get. So, I'll import the MNIST data set. Let's look at the, look at the shapes here. So I printed out X train, T train, Xval, Tval, Xtest and Ttest shape. So please look at these numbers and kind of decode them for me. So how many samples are there? How are they represented? And what are the output classes? How many samples are there? Somebody said 70,000. So there's 70,000. So we have 50,000 in train, and then 10,000 in validation and 10,000 in test. So we're going to split them using this train val test. So how are they represented? Yeah. So they're 28 by 28 arrays. These are flattened now. So they're just, the way this is stored is basically just flattened the pixel values. So they're pixels. So what do you think? How are they numerically represented in those 78 element arrays? Yes. Or normal or a normalized version of that basically. Yeah. So these are, these are or 0 to 255 or 0 to 1. So these are grayscale values. So eventually they're either already standardized or they will be. So effectively they're going to be squished down to between 0 and 1, where 0 is fully black and 1 is fully white and everything else is some shade of gray. And then what are the output classes? Yes. So there was a nine, right? So this is the MNIST data set. So it's for digit classification. My goal is to take this thing, a bunch of these and see, you know, which of these classes is it. So we might look at a sample like this one here where my mouse is and say, well, that might actually be a nine, right? Even though it's in the row of four. So this would be labeled with four, but our classifier, you know, when we train it might actually not do very well. So, okay, so there's our data set. So these images are grayscale. So how do you think if they were RGB images, how do you think the representation might be different? So you have three different values. You have three different values, right? Yes. You basically, and they would still be 0 to 255 or 0 to 1. It's just one would be the red channel, one would be the blue channel, one would be the green channel. So now when you consider that, you just have to change the representation when feeding into the net slightly to account for basically that third dimension that is the color. So that's pretty simple with grayscale images for now. So if you look at t-train, the first 10 output samples. So what is what do you think this means? What's the what what what is the first image in the data set? The five, yeah. So these are not ordered, right? We don't have all these zeros, like at the front of the data set and then all the ones and all the twos. So these are in some sort of random order that they presumably shuffled it and they saved the data sets when you open the pickle files in this order. OK, so if we look at this, like the seventh sample, right, this is going to be an instance of a three. Yes. Yeah, these are the target samples. So t-t-t is what we're always using to denote the target output labels. This is a classification problem, so they're not scalar values, they're they're they're class labels. So in this case, I think the class labels correspond to the actual digits. But remember what we said about the orthogonality of classes. So the classifier could have all the threes under label five for some reason, right? Wouldn't be very intuitive, but you could do it because the computer doesn't have any interpretation over like the number three. It sees the number three and it's just a collection of pixels. It doesn't know that actually corresponds to the number three in its internal structure. The way we've ordered here is just happens to line up because that is, of course, a very intuitive way to do it. And why wouldn't we do it that way? OK. So let's now take this and plot it. So I'm going to use the IM show function. So this allows me to basically display data as an image. I can just put it in a pie plot. So I'm going to make this draw image function that will put in the image and label and it will draw the image and it will put the label above it. So I put three, then there's a three and it'll give me this class here. So the MNIST data set is natively like this with the background is all zeros that is fully black pixels and then it's basically white on black. That's not, again, not a very intuitive way to to view pencil drawing. So I'm going to create a function that will actually invert this and draw it like a pencil image. So I'm going to kind of draw the inverse image. So that looks like this. So now instead of 10 by 10 patches like we were talking about, let's use seven by seven patches, the 28 by 28 image. So this will divide in very nicely. So to do this, you can use two for loops and you step across the columns left to right and then an outer loop that steps down the columns top to bottom. And then I'll collect each patch into this patches list. So the 16 patches in total, right, 28 by 28, and that divides into or seven devices that very nicely. So with my seven by seven patches, I end up with something that looks like this. So first thing you might notice is like some of these patches are all black. And this is well, why do you think this patch is all black? It's all zeros. And actually the way that we've written this function is basically doing the negative of the image. Negative of all zeros is still zero. So there's basically this assume there's going to be a range there. And in this patch, because it's empty, there is no actual range. So I will modify this function to include Vmin and Vmax. So basically this is going to enforce a range from negative one to zero. So that way when I invert it, they're actually it does it does force a range on that and makes the all the all black patches all white because the inverse of negative one is one. And so that's going to draw us all. So draw that. OK, now it looks nice and uniform. So all intensities are zero. So I specify this min and max value. I can I can switch that to to all white. So if I want some overlap, then I'll want seven by seven patches that shift by, say, two columns and two rows. I'm going to use a stride length of two. So instead of shifting by seven each time, I want to get that overlap. So now what I'll do is I will then go through my patches and then add more patches that include those intervening intervening pixels. So now I have one hundred ninety six patches. So basically, 14 by 14. So if I plot this. Takes a minute and this is what it looks like. So this is sort of your your strided version of this three image, and it gives you a pretty nice sense of what it what your what your patch is focusing on as you move it across the entire image. Couple of issues with this, you notice here on the on the right side in the bottom, you have these odd sized patches because basically my stride has run off the side of the image. There's nothing left there. So we need all the patches to be the same size. Conversion neural nets like fully connected nets still require fixed size inputs. So so far, you basically have to specify your input size. If you deviate from that, then it yells at you. So we're still in that territory. We talk about recurrent nets later. We'll get some instances of when that is not the case. But for the moment, we're kind of stuck in this world. So what we're going to be doing is we're going to just discard the ones on the right in the bottom. This is just the most simple technique to use right here. Usually more desirable is to actually pad. So that is, I want to basically take this last one and pad it out so that when as I stride, I'm not I get to the end, and I'm never going to run off the edge. That's a little more complicated to implement. It doesn't really affect anything you want to do here. If you want to see that I have code that that does padding for you. But for now, let's use the copying solution. So basically, if the row where I'm at where my stride starts plus seven, my stride length is greater than the size of the image or the same for the column, I just stop here. So now instead of 196 patches, I have 121 patches. So 11 by 11. And so now it looks like this. So you can see that we still keep the entirety of the image that we're interested in, in this like there's no pixels being cut off. And we also have the advantage of having these nice square patches all the way through. Okay, so now just, we have this additional storage, but how much storage are we actually using so these patches. Are they actually using lots of storage that got my storing every single one of these as a separate place in memory, or are they just views on the original image race remember review is a shallow copy, where if I change the view I change the original. So hopefully they're just views and so I can test this by modifying the original image, and then seeing what I'm seeing redrawing the patch and seeing if it changes if it shows a modification. So let me look at the first four rows and columns, they're all zeros. So now I can take like this upper left column of that first patch. So if I change all those two one. And then I print the, the first five you can see that I've changed those first four to one and then there's all zeros. So now if I draw it again. So now that those first four pixels in the top left, have all turned to black. And so therefore these are just views. Right so I changed the original image I didn't change anything about that patches list. But then, having done that when I when I when I draw those patches, the changes can apply. Okay, so don't want this to screw up our actual processing so let's just reset those pixels. Okay. Well the the trimming here is what makes them uniform so they're all uniform by default. So all of these these are all uniform squares. Right. And so then once I once I get beyond that my stride starts running off the edge of the image and so I'm just like well let me just stop here. Yeah. Yeah, so this is, this is why padding is generally a slightly better solution, because if there is interesting information at the edge. You don't want to lose that on MNIST is such that all the images are designed to be nicely centered and so like, I can be pretty cavalier by sort of throwing things away at the edges because I am reasonably sure there's nothing interesting there. So just keep in mind that like, this is a nice task to demonstrate how this works and it's pretty intuitive but like, just because you demonstrate something on MNIST is no guarantee it's ever going to work at anything else. Other questions. All right. Okay, so now let's talk about weight matrices as a kernel or filter. So how would we apply. How do we create a unit as a filter and apply to all patches. So if I want to multiply something by these patches which is seven by seven weight matrix, I need to have something that is of a shape that can also multiply by those patches. So, it's just going to be another seven by seven weight matrix. So I'm going to have a patch size, and I'm going to have a filter size, as long as those two are compatible I can now do things to that patch with that filter. So, we'll start by just kind of hand crafting some patches and see how they react to different types of features. So, let's make a path that should detect diagonal edges from lower left to upper right. So, this is, I'm just going to kind of craft this filter we see that we have a bunch of negative ones here, and then a bunch of positive ones here. So, how this detects edges is basically filtering the image patch at the same size, so they will multiply together. I want to have some negative weights that should kind of deemphasize things on one side of this edge and some positive weights that should emphasize that, or these just sort of preserve the information on the other side of the edge. So, if you imagine an edge that's like black above and then white below or something like that. And then that quote negative side, you would want to maybe deemphasize some of that information on the positive side you might want to bump it up a little bit more. So, if these two were to align perfectly you'd basically say well this is a perfect match for this shape that I was looking for. And I should have, where I have like a bunch of ones, they're going to multiply by these ones, and they're all then you sum together you get a pretty high value. What about the other things? Well, if these are all zeros then these negative ones just all multiply by zero becomes zero. And so you basically have an output that almost looks almost exactly like the inputs. In this case, if it was a perfect match. So, what we'll do is we'll apply this patch to a bunch of different kind of handcrafted images, and we'll see how they respond. So, if the patch is all zero, and what I'll do is I'll store all these experiments for plottings. So then if the patches is all zero, remember this is a negative. So, this in reality it's actually black. So, in this case, what we're going to see is that the black pixels are the positive values and the white pixels are the negative values, just because we're using our our inverse drawing function. So, here's this patch. So, it's blank. Let me multiply them by the weights. And that's, it's still all blank. Right, because it's all zeros, everything multiplies to zero. No, and then it's it's it's matrix multiplication, but you see you saw everything, but everything is still zero so everything comes out of zero. So, seven, seven by seven array of zeros times seven by seven array of literally anything is going to give you a seven by seven array of zeros. Okay, so now if the patch is all ones. So here's our patch so now it's fully black. What do you think, what do you expect this might look like when I multiply that by my handcrafted edge filter. Yeah, might look something like the edge filter knows that's a reasonable supposition. What happened. A couple of things happened. One is that there is actually in reality there's some grayscale in these values but the the inverse drawing kind of eliminates that but you do see this sort of vertical line. Remember how matrix multiplication works. So I take every value multiplied by every other value in a row or column and sum them all together so basically a single value is incorporating information from the equivalent row or column in the other matrix. So, what I'm getting there is not necessarily the, the, the exact patch, but I'm actually getting effectively a graphical representation of kind of how much information is relevant to this patch. So what you're going to see is effectively, the more the greater the distribution of higher values doesn't necessarily match the actual physical configuration of the individual values, but the overall distribution should be higher. If the patch is a good match for the filter. It's pretty intuitive with this version because anything multiplied by all zeros becomes all zeros, but because of the the matrix operation and the sums across rows and columns. We're not going to necessarily reproduce the exact patch. So what this means is that when you multiply a patch by a filter, you end up with something called a feature map that is sort of, it's kind of like a low resolution low resolution like gist of the input, and that it's got things in the input that kind of resonate with this patch, and the output will reflect that numerically, but it's not necessarily going to reproduce exactly like something that's reminiscent what was in the input. So now it's had this checkerboard pattern. So, alternating zeros and ones. And so now when I any guesses what this might look like. I guess we scared you off of making these kinds of predictions. It look up it'll kind of I mean we'll have some features of this it'll look sort of like a chessboard in that it'll, you'll, you'll see you know some of these. Some of these filters here. I can just draw the draw image I think this will sort of show it in more light so here. Annotate this real quick so you saw these this part here was black. It's just because there's a threshold there about point five and so that negative drawing is where it's below point five it turns it all white if it's above point five it turns it all black. So that's just an oddity of the way that we're drawing things right now. So, okay, so what if the patch actually contains an edge from bottom left to top right, and I'll just use the, just imagine that the image is inverted. What do you think is going to look like. So this is a very plausible. Well, this is this is this is the actual image, so that this is the actual patch. So it looks something like this. So yeah you do you do see places where it like responds very well. Right where in this bottom in this bottom right here. So this is a value that's like pure white because when I saw them over the rows and columns I get a very high value there and then it's been it's normalized, but then all went on the on the left side, where it's basically not a good kind of match for that. For that filter, you're going to get a low, low output. So, what about the reverse. So I have this. So, predictions okay let's just see it. So we get something like that. So it sort of looks like we take this image, and this image it's sort of like, it's been rotated and inverted. Right. Again, and this is just because of that matrix multiplication. Alright, one more weird pattern. So I got this thing kind of looks like something on like a rug or something. And then, so it's feel it does have some of those features that I'm interested in right it's got this horizontal edge, but it's also got a bunch of stuff like above the edge that maybe I don't want to emphasize quite so much in my output. So that's going to look something like that. So now you can see that it's similar. This one here is similar to the one above, right visually, and that they have similar features. But this other information here is basically like this is part of this patch and this, this filter is only partially relevant to this patch so I don't want to have as high an output. So, again, so here's all of them, side by side. So here's this is sort of what was intended to show but without the negative image Oh, ignore, ignore this one is because I ran that cell twice. So, get rid of that. All right, so in this case, effectively we start by multiplying something. So remember, because we are now, this is not using the input imagine this one is all white. So, if this is all white this one on the right should be all white. This is all black, multiplied by my filter it gives me the sort of odd gradation. This is the checkerboard I see kind of some features reminiscent of that checkerboard like it now it has this combination of features of the input, and kind of the visualization of the pad of the filter if you were to do that. And then here we see some oddities and that the, the visual representation is like not very intuitive in terms of reflecting either the input or the filter, but it does if you combine to the entire thing. It will actually sort of correspond to how much relevant information was in this in this patch relevant to that filter. Any other questions. Yeah. So, So, I mean, for the whole training process is basically to automate this. So, what we've done here this this what I've tried to show here is, this is the operation that's happening inside a convolutional network once you've arrived at those filter weights. But it wouldn't make a lot of sense for me to sit around and hand craft all of my filters, I have to have a very deep knowledge of my data set it might be impossible. Because I don't know everything every type of visual feature I might encounter. Instead, what we would want to do and we'll get to this next time is we're going to want to take a bunch of data and basically train the weights in these filters to better optimize for that data to tease out those individual features. So this is how we end up with these convolutional filters that can be optimized for both parts of a dog and parts of a car. So if you think about image net which has 1000 different categories, much more complicated than MNIST only has 10. How much space do you need to store all the information about those about those different categories well it's less than you think if you're very efficient about packing information together. So I could have a filter that part of it you know it will respond to this dog type feature and also respond to this car type feature. And then I put it together with other things that some of which respond, some neurons respond to dog features, but not car features and those respond to car features not dog features, and that both neuron would be activated along with one set or the other depending on what the input would be. And so the precise values that are trained into those convolutional filters can be kind of opaque, and you don't really know exactly what they're responding to and that's actually an open topic of research. Yeah. So those common images we do you can use them in sequence tasks, because of the stride operator. So for example, you can, they're, they're not like necessarily great and natural language tasks with that you can use them in that if I want to compare going to learn something about context, I can have a window, it's going to look at say, my center word and say, one or two words on either side, and then I move my window and say okay now I'm focused on this word and these are the other things on either side. So it's pretty much anything that you can use this, this kind of striding operator where you can use a convolutional format for whether or not it's really optimal for that is a different question but it's possible. Yeah. Yeah, okay. Yeah, that makes sense. Yeah, so similarly, sequence type data, you know where you want to look at say, you know, a DNA base or something in context to things not either side of it, it could be useful there. Okay. Okay. All right, so that we've kind of denoted this is just like patch times weights. And so this is something that's called the feature map. So if you see that term it's basically saying these are the inputs times some weights is going to give me some representations. So basically the the feature map is not really meaningful in and of itself which hopefully you've seen here, when when the output is like not obviously you know kind of shares some echoes of both the input and the filter but it's like not, you know, it's not the element wise multiplication of either. So it provides information about how much response, do I the filter get out of this input that I just received. So basically you can leave it was like, how excited am I by this thing how much of a high number of me going to put out. So if I'm, if I'm a filter and I'm optimized for particular type of feature, I get really really excited when I see that feature not put a high number, and if I see something that's like irrelevant to me, then I will do much. So basically the more positive values in the matrix that's created in the feature map. So in this case, since I inverted the colors is actually the more white in the image in the right column, then the greater positive value that patch will will output when you multiply by that filter. So for example, if the filter defines a top to left, left to top bottom left top right edge. So then only one of these features this one I'll give you the answer. Looks like it's reduce a very positive value. And so that's actually this one. So, because these are normalized. It's maybe may not be entirely clear like what the, whether the top one of the bottom one would output a higher value. That's just because we're normalizing all the values into it is into a specific range. If I were to print out the actual numerical values. This one should have like objectively higher values. So, for that. Alright, so image. So, no, because there we go. So, this patch has this well defined edge and when you multiply by the edge filter, it will output a very positive value. So now let's look at our three patches again. This. Alright, so take a look at some of these patches and you know do you see patches that look like they might respond well to that edge filter that we handcrafted. So there are features in this that probably share some information so that maybe like this patch here perhaps perhaps this patch here this one there. So there may be some, some relevant patches in here that this filter would be able to detect. So we can apply this filter to all the patches. And so then to do this we just need to multiply the intensities in a patch by the corresponding weight and then sum it up. So, we can do that here this will just show me the outputs, the actual numerical outputs. This of course is hard to read. So let me make it into an image, and then drive. So, the dimensionality this image is going to be 11 by 11. So if you remember from before, the way we broke up our patches seven by seven and then discarding all those in the edge we went from 14 by 14 patches to 11 by 11 patches. And so then each of these. When I multiply that and then sum it up should give me a single value. So I have 11 by 11 patches. So now I can do this. So now what does that look like kind of looks like the three sort of looks like the three if like you squint a whole lot. Like if you do that is like yeah you can see the three. So basically what this is then is this is not the inverted one so just where black was white now white is black. So now we have those, say these high values here. These sort of fall kind of in like the crook of the three the arms of the three, which has something that sort of resembles that edge right so if you look up back up at the three image. We're talking like these, these parts here. Right, so that has kind of that that edge type feature that we're interested in. So now you can apply all of your weights to filter all the patches in a single multiplication, which makes things even faster. So we have my new images like 11 by 11. So 121 patches, and then a seven by seven patch. So, the shape of the weights then should be also seven by seven so they multiply together. So then to do this I'll just reshape my patches array past negative one. So what we'll then do is we'll take that seven by seven and turn it into 49 right so now I have 121 samples, each of which has 49 individual values. So what are those. So remember let's count up. Let's count these up so there should be 11 by 11 patches there's 121, you just flatten the image row by row. So now I have all of this row, all the all the second row and so on and so on. And so now each of those will basically have 49 values associated with it for each patch. Okay, so then if I reshape my weights into a 49 by one and from a seven by seven. This allows me to multiply this new reshaped array by those weights. So now this image is going to be 121 individual pixels with one value each. And so now this allows me to get the exact same thing. So this can be the sequence of events can be basically a neat way of optimize or speeding up your computation. So fewer for loops, fewer opportunities for mistakes. And then you also get to use the vspeed inherent in NumPy. Okay, so now the idea is to come up with a number of these weight matrices. So if I have my three. It's only so useful to have that certain type of edge right I also want to be able to detect say horizontal lines, vertical lines, curves, things like that. And so I need to have all of these things optimized in in my filters so think about the MNIST data set we see all these different types of features in there. So it stands to reason that if I have you know some some network, right, let's just say you know I've very bad neural network here. And just imagine that I'm not going to draw those lines between them. But you know for some input. You know if if this weight has a high activation and then this weight has a high activation and then you know this is great as an output layer. Then, if I see these right should correlate with another with a different class compared to if like I see this one, and that one. So again, think I use this metaphor before you can think of this as like one of those pachinko machines where you put your coin in at the top and it bounces off a bunch of different rods on the way to the bottom, and some some it comes out at say one of 10 slots at the bottom. So the goal is I'm putting in my input. And depending on those weights it should sort of effectively diverted its path through the network in terms of which nodes have the highest activation. They're all going to output something, but I'm going to be interested in those things that have you know that that really high activation value. And then that is going to kind of when I run the softmax function squish that into probability distribution, where it's got a peak at some class. And that's going to be sort of where your sample comes out at the bottom of the, the pachinko machine. So I need to have these different types of filters in my, in my neural network. And so each individual node is going to have some sort of different filter that lives inside of it. What is that right like you were saying we do not want to have to sit there and handcraft all of these for it maybe for MNIST like wouldn't be too infeasible to do that. There's a limited number of features and you probably actually could sort of handcraft an appropriate number of filters, and then have a neural network of the right size such as a different filter in each in each unit, and maybe you wouldn't get bad accuracy but does anybody really want to do that when you could just learn it all from the data. Doesn't seem like a very productive exercise to me. So then what we would do is we want to learn these weight matrices. So basically every, every unit in the hidden layer is going to be this convolutional unit with this n by n weight matrix, plus one weight for that constant bias, and then we just like back propagate some error from the output layer to the convolution layer, and I'll play the update those units weight so for example, same prediction applies, we have a softmax layer that will output in this case, 10 values that the probability that each of the sample the samples falls into those classes. And I can compare that to my one hot vector where one of those elements is effectively 100% and everything else is zero, and I can see exactly how wrong I am so if my sample is actually a seven, and by randomly initialize convolutional that predicts a four. Well then, among other things, the current weights are activating in a way that will output a four I need less of those and any more of the units that are going to activate in a way that's going to output a seven. And so then I can I can back prop this error, and then update those individual weights. So this is basically just what we've been doing all along, but it's kind of at a different scale, and it's a little bit tricky. So every unit is going to be applied multiple times to all the patches. So you think of my, my three image. Every single one of these patches is going to go through every unit. So I need to kind of get an error metric. That's that represents the overall picture. So, for example, if I have like a four and a seven, they both maybe share a diagonal line type features something like that maybe it's in a different place in digital got that feature. And so I want those. Maybe I want to preserve like those some of those diagonal line filters, but remove or do or like optimize away some of the other ones. And so, my back propagation operation demands an error. That's kind of this holistic view of how wrong were all of my patches, all of my filters and apply to all of my patches. So what we do then, and I'll get into the math later is we're basically going to sum up all the resulting weight changes that result from applying each unit, a unit to each patch, and then that some is going to be part of that error term that I use to update the weights. So before we get into the code for doing this, let's just revisit the method for dividing an image of into patches. We had to use these two nested for loops. So we all know that for loops are slow and they introduce opportunities for error so since convolution is this very common procedure numpy has this add stride as strided function that will kind of do this for us. So I'm going to use this stride tricks library that will import, and I'll define this make patches function. So, here what we do is we pass an access is my input, and then I specify what size patch that I want and what stride length that I want. So then I will flatten this. I'll take my square images flatten them into a continuous array. And then I'll just look at how many samples do I have the image size is going to be just the square root of the second dimension so that is the number of pixels. And so then I can compute the number of patches that I would have given this patch size, and this image size. So then I can this will just automatically use the stride tricks as strided function to reach to give me this array that has everything reshaped into the right number of patches. So if I pass in my X train, and I have my 50,000 samples each of which is a 28 by 28 images and 784 pixels passes through make patches I'll look at just the first two samples. And I'm going to have a patch size of seven so we'll assume square patches seven by seven, and a stride length of two, like we did before. So now, the number of patches is going to be to the number of images by 120 121 patches for each image, and then 49 pixels for each patch. And so now I can actually plot this so just running this make patches function will give me the following. So the first two images in the data center five and zero. And here are the fully strided versions of that. So, pretty neat trick that you can use to automatically patch your images. Okay. All right, so then the weights in my filter that I defined a seven by seven. So I'll reshape those into 49 by one. So, there's two filters. One of them is going to detect edges in one direction and one in the other direction. So then what I'll do is I'll take my weights I'll just copy that into two identical columns. And then if I print out the first one you can see these are still, there's that patch that we that filter redefine now redefine the second column to represent the new filter so you can see here that we now have negative ones kind of on the bottom left side and negative ones on the top right. So I have basically two kind of opposing edge filters. So now here's that second filter that we define to see now compare this one to this one and we have edges going in opposite directions. So then, I'll just take the entire all my entire set of patches, multiply it by my array of weights. So, I now have weights representing basically two columns representing the flattened version of my two different patches. So I can take my output, which is this case two images and multiply them by that all at once. So now this is going to give me two images by 121 patches and then basically two values for each right and then those two values should allow me to to to plot. So, here are my original images and then these are the, the feature maps for each of them, using those opposing filters so I've got my five, and I've got my zero, and you can see with one type of filter we're clearly getting a higher outputs on parts of the image, where we're getting the exact opposite output on using the other filters. So if you compare say the top line of the five using the first filter and the second one, they're pretty much inverses of each other. They're not going to be exactly be so, depending on the specific pixels in each individual patch, but clearly using an edge filter for like one direction versus an edge filter for another direction to basically get like these complimentary feature maps out of each stage. Questions. All right. So one thing that is not in this notebook that I want to talk about that it is actually in like the NLP version of this is pooling. So let me pull that up real quick and we'll just talk about that because it's one thing that I didn't mention in this. So, CNNs. Okay. Alright, so where's, here's the pooling party. So, we kind of do the same experiment that I showed that I showed you guys right here. So if we start with our feature map. In this case the the pack size is five by five both pretended seven by seven. So the feature map ends up with 49 individual values. I need to get some single representation out of that. And so I could just summit right that's sort of your, your default strategy. So this feature map is going to record information about the precise values and the input. And so basically small changes in the locations of these could really change the output. So for example, if I have my three, and I strided differently or the image is offset by like one pixel, given a fixed convolutional filter, then the output that's going to be pretty I may not make that much of a difference right I may not be all that interested in whether or not the part of the path that part of the images like shifted by one by one pixel or not so one of the key features of convolutional nets is there basically invariant to translation when trained properly. So that is it doesn't really matter where in the image. The thing I'm interested in is my convolution neural net shouldn't always kind of respond to that in more or less the same way for with MNIST we sort of allied to this because all the images are nicely centered, but for robust image processing you want something on this kind of more overall picture. So that is you want to preserve as much information as you can while reducing the size the input to subsequent layers. So we can obviously we can change the stride length to do this but also more robust approach to this thing called pooling. So pooling will also rely on the stride across the feature map. So basically it's going to take something like this, and then kind of break this up into its own patches and perform some operation over those patches. So common pooling operations are averaging or max you'll hear average pooling or max pooling. So if we take this image here, it might correspond to these numerical values. So now I can define this pool function that's going to take in the feature map and a specified method. And so then what it's going to do is it will take this end by end patch over the feature map, and then either compute either the average or the max of the values in that window. So for example if I were interested in this in this feature app and I had like a two by two stride, then it would look at say these four. These four values, and it would take either the average of the max of these four values and say this is kind of the gist of this part of the feature map, and then the next two values it would look at and then we take either the average or the max. So, if we take this feature map on the left and compare it to the average pool average pooled version on the right. Do you think that this is sort of like a reasonable approximation of the things that appear in the feature map. Yeah, right we see darker lower values in the bottom left on average lighter values on the right and maybe some middling values in the upper left. Right so adding this extra specific information might not get me a whole lot more than just looking at this. So, we can look at what average pooling versus max pooling often does. So pooling layers often occur after activation so you'll have some nonlinear function then you do the pooling. So, as I note here, CNNs usually use ReLU but when I wrote this function I felt lazy and I use 10h just because I didn't want to define a ReLU function. You know what, it happens. So if we have the following feature maps, and then we pull the features using average features or max features, you're going to get slightly different results. So in particular if you look at say, the two that the one that we were just focusing on, right, if the right side is average pooling and left side is max pooling, we maybe get a little more information in the average pooled version. So we're in the max pooled version. Effectively these two segments come out to roughly the same value. So these are different ways of further reducing the size of the input to the future layers. So we can also do say pulling over the sample image using that edge filter. And so that's going to, if we have like sample image in this case this is just a square, and I have the same edge filter defined here, and then I can, So basically here is the original feature map. So just the raw feature map. And then here is the average pooled version, and here is the max pooled version. And so all of these roughly represent the feature map in approximately the same way. But instead of having say 49 outputs, you have four. And so this allows us to further reduce the input size and maintain the speed of the convolutional operation, while still maintaining the information about the input. Okay, so that is kind of intro to convolutions in the large, alluding to how we actually need to train these things of course. And so we're going to get into that next Tuesday. So we'll actually do convolutional neural network training next week. All right. So if there are no questions, then I will let you go.