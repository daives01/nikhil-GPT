 or. Thanks. Alright. Alright guys, let's Alright. Alright guys, let's start. So I appreciate you start. So I appreciate you start. So I appreciate you hardy souls who came out today. I actually was not expecting this many people. I actually was not expecting this many people. I actually was not expecting this many people. I actually was not expecting this many people. I actually was not expecting a dozen to 20 people in the room which is twice as much as I would actually be here. So, yeah, thanks for, thanks for the rat and we wait for him to catch. There we go. All righty so what we're going to do today is I will just go ahead and do the assignment. So, assignment to is due So, assignment to is due today for most of you so if you haven't requested if you need an extension. And your default due date is today I'm sorry it is now too late. So, please make sure late. So, please make sure that you get your assignments submitted. So, so well, let's do today. So, so well, let's do today. We were assignment one if you We were assignment one if you choose to do that are going to be the last two days. So, we're going to do assignment three. So, I'd say around So, we're going to do assignment three. So, I'd say around now is probably when things start to get pretty, pretty intense to get pretty, pretty intense, as far as the workload for this class. So, it's about it So, it's about it as far as class announcements. So what we're going to do today is I'll finish up the, the classification with logistic regression lecture. And then I believe we are exactly on track for where we need to be is the 20 second, and then we'll get to the third. Okay, let me get through this and I'll see I think that notebook 11 is like pretty, pretty short. So, see if we can get through that today because we are now a little bit behind. Okay. No. All right. So, just a refresher on So, just a refresher on last time. So, we're doing So, we're doing classification of course and so we talked about that in terms of probabilities. So, we basically have a bunch of classes. And then for example we want to predict the probability that it falls into each of your any classes or your classes. And so then what we have to do is we basically have to maximize the likelihood of seeing the data that we actually have according to the class distribution that we've got so the data of course in this case is going to is going to include the samples and whatever numerical features define each of those samples and then a label for each for each class and remember the label is now going to be an indicator variable or a one hot vector where it's all zeros except for a single one where in the index corresponding to the class that the sample belongs to this can also be thought of as a probability right so it is a 0% chance the ground truth is basically you know sample sample and is K or is not K for whatever class K. In other words, for the correct class K it is 100% likely to be that class and it's not going to be anything else so think of this as probabilities everything is 0 or 1. I'm now trying to minimize the distance between my prediction and the ground truth which is now just a probability distribution right and being a probability distribution it has to sum to 1 so we have the data likelihood we want to maximize now so this is going to be we have some function G that's basically going to be my prediction function for my sample parameterized by the weights K for every class K and then divide that by the sum of the prediction values for that for every class M right. So we define the gradient with respect of the log likelihood with respect to W and we end up with update rules for each individual weight W. This should look kind of superficially familiar to what you're what you already know from doing progression with neural networks that is I'm trying to update an individual weight and so I need to take the previous value of that weight and then add something to it to move it in the right direction that something is going to be some constant learning rate let's say alpha and then this is going to be the sum of all my errors for my samples right so WJJ corresponds to class I want to better optimize the weights corresponding to that class I need to effectively figure out how wrong is my prediction for every sample in my data set for every sample N. So then what I'm going to do is I'm going to have some target value this is going to be that indicator variable where all zero is the single one in the right place for sample N for that class J so basically what this means is that now for this sample N at index J there should be a one and there should be all zeros other one. So then what I want to do is I want I want to subtract from that my prediction right so this should be this function G I'm going to technically have a different function G for every class J and this will tell me the like of my class I just need to do this for every class and then of course I have the other term there being that input so pretty standard optimization function at this point in that I have some learning rate I have some error term and I have some input right and these are the three things plus the weight that I need to use to optimize the value of that weight to the correct value. So of course we're doing this in going to be doing this in Python and so we want this to be some some level of optimized for speed so if we have my update rule for W sub J where J is some class as given above we can see and we check out the well the expected shapes of these arrays are going to be right so remember we're dealing with these weighted sums of inputs and so we're going to be adding this constant one to the front of each sample so what's the dimensionality of X sub n it's going to be D plus one where D is the number of measurements or features for that sample plus one which is that's my bias column and then we're going to add this that's my bias column and then by one because I'm really dealing with a single sample right now W sub J should also be D plus one by one which is just a single class and so then T sub n J minus G sub J of X sub n is going to be some scalar so again this is my this is my error measurement how wrong am I and so I need that to be expressed in terms of a scalar value. So this all works but you'll notice that the sum is over n and then each term in the product is going to have n components so that means we can rewrite these as matrices and then do it all as a dot product to basically do all my computations across the entire data set at once. So just to work this out we'll kind of do something similar to what we did when we worked out the update rules for neural networks using matrices so that is we can remove the sum and basically just replace the subscript n for all samples with just an arbitrary placeholder which is use the star so I can remove this sum right here and so now for whichever value of n I'm just going to be updating the associated value of T sub J for that sample. So consider remember we want to set up all our data as these big matrices where my rows are the samples and then my columns are going to be the targets. So in this case the target is going to be you know a n by k array for k classes. So what are the shapes of each piece here? So this is going to be T minus G this is going to be n minus one so or sorry n by one so for every sample I should get a scalar value saying how wrong this prediction is and so now X right likewise I remove the n and I'm just using this kind of placeholder star so this should be this is going to be basically my big X my big collection of all my inputs and so this should be n for n samples by d plus one where d is the number of features of the sample plus one for the bias and so then W sub J as above is going to be d plus one by one. So this is going to work now if we transpose X and then just pre-multiply it and then we just define G as a function that accepts X. So if you look at this and then here below they're basically just identical having removed the sum not only all I need to do is transpose my big X to make sure that my shapes align. So now what we've got is we basically have an arbitrary update rule for all of my X for a single class J. So now we've been able to effectively get rid of the summation over n and so now I want to get rid of my other subscript which is this J. So in other words we're going to try and make this expression work for all the W's. So now we've successfully gotten rid of n let's do the same thing to get rid of J we'll just replace J with our placeholder and so likewise every time I see a J here I end I just replace with the star so now I have nicely I end up with sort of T sub star star which if you remember how we handle this in the intro to neural network lecture this is basically just saying I'm going to replace this with a big matrix of all of my T's. So here the star star means I can account for any row and any column so now this is just going to be big T matrix of all my target values. So if W star which is basically now just W this is going to be D plus one times K. So now think about the what we're trying to map to is going to be probabilities over K classes right so this output should be of dimensionality K what are we trying to map to those K classes? Come keep your size D plus one for the bias so this is going to be D plus one by K. T sub star star is just big T you remember so this is going to be n by K. How many samples do I have? N. So this is going to be each of them could be one of K classes so n by K. And now G of X is going to be n by K minus one so this should this basically predict my samples and then for each of them is going to predict some class. So actually why is it pulled on? This is incorrect. My mistake. So this is going to be N by K. So times there we go. So G of X is n by K. So now T minus G of X should also be n by K of course we have you know n by K minus n by K equals n by K just element wise. And so T or sorry X transpose times T minus G is now going to be D plus one by K. And so now we have this T minus G plus one is D plus one by K. And so now X transpose times T minus G plus one is D plus one by K. And so now we have the update equation for all my W's being for any W I'm going to take the previous value of that weight plus learning rate times the input times the error return. So far so good. So A will be some constant or alpha constant. T is my n by K target indicator variables and then G of X is the prediction function over X or over the values X. So this what does what does G what does G look like basically? So we defined for K from one through big K. We said that this function will just take it to be the shape of E expi to the W times X. So we do the exponentiation because we basically need to end up with a subtraction where we need to be able to do T minus G. But previously, if you remember, we worked in the world of logarithms. And so if I can't necessarily easily subtract, there's nothing that I can take the logarithm of a subtraction very easily, right? If I take the logarithm of a division right now, I can I can subtract the logarithms, but actually want to be subtracting the scalar values itself. So to get around that, I'm going to do is just going to exponentiate both sides. Now I can actually do a subtraction. So if G sub K of X is going to be my prediction function for that class K divided by the sum of the prediction outputs for all of my classes, then I can change these to handle all the samples X. So now instead of for a particular class K and for a particular sample N, I'm just going to take F of X parameterized by W, right? So now I'm back in my familiar territory of I have some inputs X that I'm trying to map to the right outputs. My job is now to optimize for the right weights that will let me do that. So I can just say that F of X parameterized by W is E raised to the X times W. And so now G of X is going to be F of X parameterized by W divided by basically the sum over all the rows for X parameterized by W. So I basically have one one class and then I divide by the sum. So given training data X, which as you recall is N by D plus one and then class indicator variables T that are N by K, we can then perform the following expressions, perform the operations with the following code. So first what we need to do is we need to create this function to get indicator variables from the class labels because you might find that your class labels are not set up in these one hot vector representations. In fact, they're just like labeled ABC or 123. So you need to translate them into an appropriate indicator variable format. So basically I'm just trying to go from this, you know, 12213 to 100100101001 for however many classes you got. You might discord notifications. It keeps beeping at me. Oh god, you have to do that. Okay. So everybody just ignore that please. So we're going to do is we're going to find this make indicator variables function. This will take in the T column matrix. So this is just going to be like a two dimensional matrix. It's N samples by one. And then what I will do is I'm just going to pretty much just reshape this into the into the into a two dimensional matrix if it's not already and then just take those individual values and map them to the appropriate index in a vector that's otherwise all zeros. As a demonstration, let's take the above sample right here. Reshape it, right? And then I will run it through the make indicator variables. And this gives me the same thing. Right? So these are now the above indices as indicator variables. So now for how many things am I trying to predict? It's going to be however many classes I've got. And what I want to predict is now the probability that my sample falls into each of those classes. And by dividing by the sum of all predictions, I can ensure that I'm going to get a value that's a basically a percentage out of one. So I'll define G that does all of that here. So now I have G of X of W. And so I'll have my my F, which is going to be going to exponentiate X times W. Then I'm going to take the denominator and then I'll return my G's, my G values, which is going to be that F's value divided by the denominator. This G function is also sometimes called the softmax function, as you probably have heard of. So softmax function is this common final layer activation function. And now as you see, what it does is it takes your scalar values and maps them all into some probability distribution that sums to one. So like the sigmoid, right, which converts things into a value normalized between zero and one, the softmax does that too, except it does it makes sure that that that sum is normalized so that all values sum to one. So as you can see, the sigmoid is useful for binary classification, right? If everything is going to be between zero and one, then if I have a class of interest, all I need to do is sigmoid some value and it's going to tell me the probability between zero and one that it falls into that class of interest. So this is useful for two class problems because software because sigmoid squishes everything between zero and one, which means that to get the probability of the other class, I just do one minus the sigmoid. Softmax, of course, I can't use the sigmoid function for multi-class problems because if I have a class of interest, all that's going to do is tell me what's the probability that this is not a member of that class, which is a binary classification problem. If I actually want to know which class it falls into, I have to be able to normalize all the probabilities such that they sum to one. Turns out, maybe we'll review this after spring break, the softmax is a generalization of the sigmoid function for multi-class problems and it's a fairly simple derivation to show that. That is not, I believe, in this notebook, but we can get to that. So softmax function is basically as you may have seen, you know, you take your final output, aside, prior to the softmax, exponentiate it, then divide by the sum of all exponentiations. So now the updates to W can be formed with code such as this. So I'll make my indicator variables, I will define the weights in the appropriate shape, define some learning rate, and then for every step in my specified number of epochs, I will then take the softmax of X and W, right, in this function, we're multiplying X and W, and so then we can use my update rule to better optimize the weights. So this, in a nutshell, for a very simple linear classification problem is how I can use the softmax function. And so when I'm doing neural networks, what I'll do is I'll just have the appropriate insertion of hidden layers to handle non-linearity and then do backprop in a very similar way. Questions so far? I'm curious, why is everybody gravitating to this side of the room? Even when people are here, that side of the room is much sparser for some reason. Maybe you all come in through that door and it's just closer. Anyway, okay, we have an unbalanced distribution in this class, right, if I just want to predict like where someone's sitting in the class, it's almost always going to be on this side because, sorry, this is very empty over there. So here is code for applying linear logistic regression to the Parkinson's data. So we're still in the world of linear equations here because I don't have my hidden layers to allow me to insert non-linearities. So I can still use the softmax function, of course, to squish everything into the appropriate probability distribution, but what it does is just taking a matrix multiplication of X and W, which is still a kind of standard linear equation that we have been doing since the beginning of class. So we'll do the Parkinson's data set again. So let me read in the data. So this should look familiar, 195 samples with 24 features each. So then I will do the same type of data cleaning that I did before. So of course I don't want to include the status, which is my output label in my inputs, and neither do I want to include the names. It's not a helpful feature. So I drop those and then I slice off the status column. This becomes my targets. So now I can see that I've got 22 input features in X. These are the names of all those features. I have a single output that's status. Standardization function, as you've seen before, so I'm just going to compute some means and standard deviations and standardize the inputs. I'm now also going to import the QDA, LDA implementations that we've used in the previous lecture. This will allow me to compare the performance of logistic regression to the two previous methods that we use, namely QDA and LDA. Okay, so now to generate our training data and our validation and testing partitions, what we're going to do is we're going to partition data into folds on a class-by-class basis. So this is kind of similar to what we did earlier in that we're going to define how many folds I want, and then I'm going to use one of them to be testing data and then one of them to be validation and then the rest to be training data. The only wrinkle here is that I need to make sure that I have approximately the same proportion of samples from each class. So this function does this. This is called generate stratified partitions. So what this will do is it's going to generate my sets for training, validate, and test for both my inputs and my targets. Or if I don't want validation data, I can set validation to false, and it's going to give me a dictionary that's keyed by the class label. So if I want to retrieve, say, the training and the validation sets for class zero, it's going to be set up in a pretty intuitive way. So what we do here is I'm just going to basically shuffle all of my rows, and then I'm going to pull out the appropriate set for each fold, and then I'm going to partition my folds, or I'm going to, having partitioned my data in my folds, I'm going to segment off which collection of folds I want to use for training, validation, and test. So basically this just does all that. So in this part here, this is just going to make sure that I'm roughly balanced across all of my classes, and then this part will generate the test and optional validation fold, and then return the rest as training. So if you run this, what we'll do here is I will print, these are each of my folds, and then we'll see that the first number, this is like my number of training samples, and then my number of validation samples, and my number of testing samples, and then this will be the number here is the partition, or the percent of the partition that is class zero. So if you remember that we have the healthy samples and the Parkinson's samples, and it's not a balanced data set, but we want to see roughly the same proportion of each class across all of my folds. So we can do this in this trick using np.mean, right, so this is going to be computing the arithmetic mean across the specified axis, and what this does, t-train equals zero, is it's going to turn everything to either true or false, and so if this is true, it's going to be a one, if it's false, it's going to be a zero, and np.mean will handle that automatically. So these are the individual samples. So what that means then is that np.mean of A equals equals zero is going to be the proportion of samples in A whose values are equal to zero, and just replace this constant here with whatever thing you're interested in. So now what we can do is you can write a function that's going to iterate all the possible ways of making train validation and test sets from n partitions that I specify, and that'll also train my QDA, LDA, and now my logistic regression models over this data. Okay, so what I'm going to do is I'm going to save time just using the break statement to stop execution after just one run of cross validation, and then to the end of the run, I'm going to have to calculate the mean errors or accuracies over all cross validation runs. This is what you're going to be doing in A3. So does everybody know the term cross validation? Who's not familiar with this term? Okay, somebody that's funny. So basically cross validation, I have a data set, and maybe it's small, or maybe I just want to make sure that I'm not just getting a lucky split. What I'm going to do is I'm going to basically partition it into these folds, and I'm going to hold one out as the thing to be tested on. Now remember, maybe that particular split is lucky for some reason. We talked about how you can just sort of, depending on your random seed or other factors, you can get a particular testing split that just happens to perform well. If you hold that testing data out, the remaining training data is just very indicative of that testing data. There's nothing really in the testing data that is maybe unusual from the perspective of the training data. So in that case, it's quite likely that you would get a very good test performance. But we don't know that you just segmented out the 20% of the data that had that property. So if you stop there, you could report a really high test result, but then someone else who runs your code partitions a different split, or they have a different random seed, they get different rows in that split, and all of a sudden you're showing 96% test accuracy, and they get 68% or something. They're like, whoa, what's going on? This is way lower. So what you want to do, or one thing that you can do, is to take that test split and rotate a different one each time, and then average all those results. So for example, if you had some test split that was like, okay, you got 96% on this one, and then 76% on the next one, and then like 86% across the remaining three, just to make my mental math easier, you end up with an average test accuracy of 86%. And so that's much more realistic than either the high end, the 96, or the low end, the 76. And so in that way, you'll get a more accurate picture of the actual performance of your model trained over these different splits in the training data. So clear. Cross-validation can also be a useful technique when you have a small data set. So for example, it may take, let's take this sample. We have 190 samples. This is not huge. It will do fine for these linear methods, but let's say you have a more complicated problem that requires a neural network. Often it requires more samples to actually converge to an appropriate model, and so you need some, you know, you need like 90% of those training samples to get your model to converge and you hold that 10%, which in a sample that has 195 samples to begin with, you'd only, you'd have like 19 test samples, right? And the results might not be all that indicative, right? Again, you have the sparsity problem, so you could end up with a with a lucky split problem. And so one way you can kind of ameliorate that is to cycle through these different testing splits. So a number of reasons why you might want to use cross-validation. We will do that here. And so you're going to have to implement this in A3 using similar methods. So we'll have this this version of the runpark function. It's going to be called runpark logreg for logistic regression. We'll also include the outputs of QDA and LDA. So this will output the prediction accuracy using all three of those. So I'm going to run my generate stratified partitions. I specify how many folds I want. In this case, I'm not going to use validation, so I'm just going to generate train and test. And now I compute my means and my standard deviations. I standardize my X trains and my X test using those means and standard deviations. And then I will do all my preprocessing upfront. I'll append my column of ones. Now the new stuff for linear logistic regression is now I have this make indicator of ours function, right? So this t train and t test now gets transformed into t train i and t test i. So now I should have some one hot vector representation or indicator variable representation of all of my target samples. And now the rest is as we've seen. So here, to this point, ignore this likelihood list for the moment. We just initialize our weights, specify some learning rate, specify some training time. This is my forward pass because it's linear. I'm just running through the softmax. So you can imagine this being just a neural network with no hidden layers and a softmax on the output. It would be the same thing. And I do my backwards pass my weight of the weights and my backwards pass my weight update as we saw. What I'll do is now here I will convert the log likelihood of likelihood. So if you remember, we were calculating the log likelihood of the data to make the mathematics easier. We need to convert that back to the actual likelihood because that's what we're trying to maximize. And so then what I'll do is I will append this likelihood per sample to this list so I can plot it. So now what I can see is after I plot, I see the likelihood of the data as training proceeds. And then I will print the percent correct using the logistic regression method. Then I'll do the same thing using the QDA and LDA code that we did before. And the rest is plotting. And then I will define this percent correct to turn my results into a coherent accuracy. So let's run it. Okay. So this is the percent correct for each one. So the logistic regression is 89.2 train, 78.9 test. QDA and LDA have really high numbers. Or actually, wait a second. This is the same because, right, I forgot. This is the issue. So sorry. Sorry about that. Okay. So here we go. Run this again. Okay. This is more like it. So we see logistic regression and LDA have kind of similar performances, although we actually see the logistic regression has a higher test accuracy this time. QDA, we see the same thing that we saw last time. We received this really high training accuracy. And the test accuracy is not as impressive, suggesting that QDA, again, may be overfitting to this data. These are the weights. And you'll see that basically the weights of the second column are just the negative of the weights in the first column because this is a binary classification if it's not one, it's the other. We're using the softmax because that's generalization. We could recast this problem into one that uses the sigmoid because, again, one minus sigmoid of X will give me the probability that the sigmoid of X is not in the class of interest. Now if you look at a single sample, these are the individual values. These are standardized. So now the samples times the weights, right, this is going to be that first sample times W. This gives me some scalar value, right, 1.23 and negative 1.23. We can already see what the output's going to be. We already know which class it's going to be in. But just to run everything to completion, I'm going to exponentiate this. So now this is basically e raised to these values. So now they're no longer negatives, right, 3.45 and 0.3. I sum the exponentials. This is now 3.7. So now I normalize these. 0.92 and 0.8. So now these sum to one. I just do a sanity check here, right, this does sum to one. And the argmax is going to be the first element or class zero. So this is a, the sample belongs to class zero. Once we got to this point, you could probably tell what the answer is going to be just looking at the scalar value because this is a binary classification problem. It's going to be one or the other if it were a multi-class classification problem, it would not be necessarily so easy to see that. And also the softmax has the nice properties of making everything consistent with your error term relative to a probability value that you do need. So you can sort of see what the answer is by the time we get here, but you actually need to softmax in order to get a meaningful error term that I can use to do things like weight updates or back propagation in neural networks. So finally our charts here, this is the likelihood of the data according to logistic regression. We can see that we end up with just like from 0.5, you go up to probably about 0.8, most of that. And now here this plot, you can see the outputs for each individual classification. What we will find is in most cases, LDA and logistic regression are performing the same. There are probably only two samples where LDA says one thing and logistic regression says the other. QDA, again, we have some overfitting in the training data so it's not performing quite as well. So we can run this again one more time. So we can run this a few more times. You see similar trends. And generally what we find here is that at least for this data, logistic regression can usually achieve a higher test accuracy than LDA even though maybe the training accuracy is the same or slightly lower. So this may be a problem where logistic regression is more suited. So run this a few more times and you can see similar trends as we have seen before. Okay. So now the code above, this is using SGD in the gradient of the log likelihood. So do we have a better way of doing gradient descent? And remember, gradient space ascent, not descent yet. So we can try Adam. We've demonstrated that Adam often converges faster with fewer training epochs. But first you need to define your error function to be minimized and as gradient function. So previously we were trying to ascend the gradient, but the function to be optimized, we actually want to try to minimize the gradient so we can use a general solution using any kind of gradient descent algorithm. So in this case, what we're going to do is we're going to create this function as the negative of the log likelihood. So this allows us to minimize the function and then the gradient function also needs to include the negative. So here's the definition of log likelihood above and its gradient below, also written as matrices. Oh, did I stop? Oh boy, I hope I didn't interrupt something. Just stop the girl. See what happens. Okay, didn't break. Sorry about that. I think I stopped accidentally. So I'll define this second version of the function that will be using Adam. I'm going to import the optimizers class that I have before so I can specify which optimizer I want. And now what I've done here additionally is below the softmax function is I've now defined the negative log likelihood function and then its gradient. So now I can use these to get the output of my gradient descent operation at each step. And so now this is the thing that I'm trying to minimize. And then what you may want to do is convert this back into the likelihood of the data. So here what I'll do is I will take this to likelihood. So I have the negative log likelihood. What I'm going to do is it's going to take the negative of NLL or the negative log likelihood, exponentiate it. This gets me back into regular likelihood. So now I can define this likelihood trace that is going to be, that's going to instantiate an instance of Adam that takes the negative log likelihood as an input and its gradient and the number of training epochs learning rate. And then I just define this error convert F function to likelihood. And so this error convert F is a member function of the Adam optimizer. And so it's just going to call this here that I've defined. So it's automatically going to convert the negative log likelihood back to the just plain likelihood. Then the rest is the same. Right. We just do just a regression QDA and LDA. Print this. And so now here this is written error is a little bit misleading just because of the way that the neural network class is written. This class is written to originally based off of the regression problems and trying to minimize error. Here all I've done is I'm now printing the likelihood of the data instead. So just a note when you see error here, you'll notice that it actually increases sometimes. And that's because we're trying to maximize the likelihood of the data, which is what's being printed out here. OK, so maybe a more we could rewrite this to actually print the negative log like we didn't see it decline. But then it would sort of go opposite what the chart actually shows. So here we see the error or likelihood climb to some threshold of about point eight nine. And then we also see the the training and test accuracy. And you can see here that using Adam, at least in this case, train accuracy is only eighty eight point nine or test accuracy is actually ninety six, which is significantly better than the test accuracy for either QDA or LDA. So run this again. And then we see similar different split here, but we're still we're still getting basically better test accuracy than the train accuracy using using logistic regression with Adam. So just throw that out there to some of you. What would you do to change this to run SGD instead of Adam for the linear logistic regression? Just think about how the code is set up. We'll go back to where we instantiate that, highlight it a little bit. So what would you do to use SGD instead? And the answer was probably a lot simpler than you think. Well, so I always use softmax for all classification problems. Right now I'm just I'm asking this is this is a lead into the next lecture, just just to sort of telegraph where I'm going a little bit. I need to change a word, a single word here. What do you think? What do you think of the back? You agree. Yeah. OK. So basically, we have this optimizer class that that contains implementations of Adam and SGD. So to do that, all we need to do is change SGD to Adam and then change the change the type signature of the function in whichever way it demands. So which brings me to the next lecture, code reuse for the next lecture, code reuse through inheritance. Before I do that, any questions on logistic regression? All right. So OK, 11 code reuse by class inheritance. This will be pretty useful to you, and I think this is relatively short, so I'll let you go early, which would be cool. So what you have done, say, in A2 so far, what we had you do, for example, is you complete a neural network implementation and then we say here's a cell, copy your whole neural network implementation into that cell and make the following changes. Right. And some of you doing that are probably thinking this seems like a very inefficient way of doing things. Why am I doing this? Well, it is not a great way of doing things, obviously. Instead, what we can do is you want to make relatively small modifications to the existing classes and functions. So what you can do is you can add arguments to select for the new behaviors, like what type of activation function you want to use or what type of optimizer you want to use. Or you can use class inheritance to extend the original class. So I assume that all of you are familiar with doing class inheritance in some way, right? Maybe in Python, maybe not, but presumably you've at least taken Java here and so you've done class inheritance. So we'll do just an example here that uses the neural network class. So to allow, say, the tanh or the relu activation functions, you had used this keyword to specify tanh or relu, and then in that copy of your neural network class, you had to add the additional relu behaviors and the gradients and everything like that. So instead, let's use class inheritance to do the same thing. So first, I'm just going to write out the optimizers file. So this is right here. And then I will put it in its own Python script. So now to use this class, you need to import it like we've done. And here is a... I'm thinking about giving away the answer to 82. Maybe I should just end class here. What do you guys think? Do you want to get like 30 minutes back? I'm going to stop here. No. I feel like I shouldn't let this like... Yeah. What's that? We were right where we need to be. And I think the due dates slip every year a little bit. I have to calibrate them appropriately. So I'm going to call it a day here, actually. So I don't give away the answer. Okay. So you all get 30 minutes back. So I will see you next week.