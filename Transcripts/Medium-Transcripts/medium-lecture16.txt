 Okay, let's start. All right, so messages again to people who aren't here if I excuse you from one day doesn't necessarily mean your excuse from the next day or if you're still sick or whatever you gotta let me know. I understand the weather is kind of crazy right now and this seemed like people do get sick with the sort of weather change so just please make sure that if you remain sick, you let me know that you remain sick. So if you're are not here one day and you're still not there the next day I'm going to start wondering why. So, it's just, where are we right now I'm going to do today is I will do convolutional neural network training with pytorch. So we're getting pretty far ahead in the content of like what compared to where you are with the homeworks right now right now you're doing a cross validation, and then we're going to do and this is going to be the fully connected that you won't do convolutional network training for an assignment or assignment five. So now, sort of apply versus what is what your class is going to start to start to diverge a little bit on we are back on schedule what I'm going to do today is I'll go through this notebook which should be on the shorter side. And so then, after spring break. What you usually have is kind of kind of like a review session. So I'll take any extra time today and just answer questions about CNN softmax some of the various components that we've had to put together, you know, in the last couple of weeks. So I will have office hours, we end early for some reason I'll just go over there. And so, now would be the time to ask me questions about assignment three. And also if you started thinking about project ideas, I want to discuss that. You can also do that during office hours today. So after the break, I will formally announced the project. This proposal, you see if to write down what you propose to do and I'll either approve it or ask you for revisions, then you have about six weeks or so to complete the actual project. Yeah. You are allowed to you don't have to you can work in groups up to four. The catch is that if you work in a group project I expect to see a commensurate amount of effort. So like if you work in a group before it better look like it took four people to do your project. Other questions. So, this is going to be in terms of the amount of work there would be compared to be a lower bar to clear, because it's one person worked for six weeks as opposed to four people. I do expect to sort of see quality work but it's going to be you know I'm going to qualify that based on like how many people are on the project is not going to be like, you know, you have to run every exhaustive evaluation on your own, you have four people you can probably run a lot more evaluations. That's the sort of thing I'm looking for. Anything else. I guess the point of that is that it would behoove you to start thinking seriously about your projects. And I'll go over this again after the break but if you already have something you're working on. Maybe start thinking about how you might be able to apply that to like put a machine learning spin on that. That would be an acceptable thing to do for this class so my goal is not to give you extra work. If you already have some active line of research and you want to make it so it applies to this class that's perfectly acceptable you have to convince me that that's what you're going to do and you're not just like completely piggybacking on something else you have to do some dedicated effort for this class, but you're allowed to use existing resources and things you know your, the goals for you to use your expertise in in a way that's relevant to the course material. Okay. So to share screen. All right, can you can everybody on zoom you can see my, my notebook window. Sorry, can you can you see the zoom. All right. So, you've done intro to convolutional neural nets. So basically the core mechanic of a convolutional neural net is this filter. This is just a weight matrix that effectively needs to be optimized to output a higher value when it encounters things that quote match it better. Those things of course can be multiple different combinations of features. So these are handcrafted filters just to illustrate how the concept works in terms of what type of activation and scalar value you're going to get out of performance linear some operation from a filter to a patch of the image. Of course you want to be able to actually learn these filters from real data so pretty much the training process of a CNN is trying to optimize the weights in these filters you have to specify things like what the size of filters are, and then how how you patch up your images. And so that's going to learn weights that are optimized for whatever types of data encounters assuming those hyper parameters. We did the NumPy version. And now of course you want to make use of as much speed as you can because as your network start to grow, they start to take more compute. So let's go through effectively similar operations, except using PyTorch this time. So, just a point, if you were going to use PyTorch on the workstations in the CS department, you want to execute this command. So basically what this does is it's going to add the site packages library where PyTorch should live to your Python patch. Sorry, Python path. Can't speak today. So you can run this every time that you log in but a better solution would be to add it to the start of script in .bashrc. So that way it's always there when you log in, and you don't have to worry about this. But if you have things like, if you run import torch and you can't find it, very likely it's missing from your path. So just a point of fact. So this is taken from this example of implementing a CNN PyTorch. So this is kind of stripped down version of that. You can look into. Okay. So we're going to do our usual sets of imports, right, so we're going to import NumPy. Of course we still want to use NumPy to do things like you know processing and plotting our outputs, because remember PyTorch operates as a tensor, and you can't do certain operations for things like next visualization directly on tensors. Nonetheless, the main thing we're going to be using is torch and from torch we're going to import the autograd module. And then we of course going to keep track of time and Jesus and Pickle are to to open our data. So in our NumPy version of a CNN, this looked, our constructor looks something like this. Hold on. So we specify our patch size, and then we initialize an instance of this neural network classifier CNN so 28 by 28 that's going to be our input size. So remember, fixed size inputs, and we're going to assume square images. And then this two and two this is going to be our list of that defines the hidden layer architecture and output size would be the number of unique labels you have right so for MNIST this should be 10. So you're going to have unique values in T or whatever your class labels are. I set my patch size remember that this is a square, so I say patch size five. This is effectively a five by five. Right. And then stride to similarly that's also a square I'm going to take two step two pixels over, and then two pixels down and I reached the end of the row. So this is also like a two by two in both directions in the PyTorch version you're going to see a lot of the same things, but it's slightly slightly slightly different. So we're going to use of some of the PyTorch functionality to more automatically define things like our network architecture. So, first of all, just like take a look at these two definitions, and what do you notice that is similar about them what do you notice that is different. Start with the easy one what's the same about these definitions. First parameter right both 20 by 28. That's the input size. Yeah. Yeah, so the output size too so it's the same data, right so it's going to have this is MNIST they're going to be 10 classes still. And our inputs are going to be 28 by 28 images. Okay, so now what's what looks like it's different about this. Yeah. Yeah, yeah, right so I'm separating out, you know, the convolutional layers and the fully connected layers. So here. Remember when we did the version last time we basically said okay I'm going to just decide there's a single convolutional layer, everything after that is a fully connected layer. Right, so I could write the NumPy version so that is more like this. The PyTorch basically does this will allow you to do this automatically, as we'll see so right now, like, what you see here that I'm highlighting. This is not a PyTorch command specifically this is just a Python call that instead shades an instance of this conduct class. The net the conduct class will be written using PyTorch. And so now I can take these, these parameters and use that to basically, you know, construct the individual PyTorch layers one by one, exactly as I want them. What else do you notice that is different about the PyTorch definition. Yeah, so this is well this is the stride. It's, it's too. What's the name of the argument that's there. Per layer right so I can do, I can do something I can have a little more control here in that I can specify different size patches for a convolutional layer and also different size strides for convolutional layer. Let's think a little bit about why you might want to do that. So remember, in our sort of single convolutional layer example, we have let's say 28 by 28 image, and I might specify a seven by seven patch. And then I stride over it say two pixels at a time this is going to give me, you know some number to be like 11 I think 11 by 11 patches using cropping. So I'm taking a 28 by 28 image turning it into an 11 by 11 little sub images, each of them gets multiplied by a filter that should be the same size of the patch so also seven by seven. From that I get a linear sum right or I get some sort of value so the easiest one is just get a linear sum so single value. I can also do pooling, right, we're just going to take those initially seven by seven values, or whatever the size of the patch times And then I can down sample that. So I did like a two by two examples I take the seven by seven down sample it to a two by two, or just take a linear sum and down sample it to basically just a single value. Let's take the pooling example, because that makes it friendlier to more convolutional layers. Let's say I take my seven by seven feature map down sample it to a bunch of two by two feature maps. All of those let's say there's also 11 of them. So 11 by 11 of them 121. All of those, we have 121, but effectively like four pixel images that way. Those go into the next, the next layer. So if I have a bunch of input this effectively like pre patched already there two by two, and I have a patch size or stride length, that is three, or let's say even two, what's going to happen. I have a bunch of two by two images. I have like 11 by 11, and I stride over them with a stride length of two. So I'm not going to be able to really overlap between my patches. No. Let's say my stride length is three. No, I'm also going to miss things. So effectively, the stride length is going to be dependent upon what's the resolution of the thing that's actually going into the layer. If I have a patch size that is five and a stride length of two, this is going to down sample my five by five and I pull my down sell my five by five feature maps into some lower resolutions. This makes it easier to to compute right it's faster this list less information. Also if I do my pooling right I might get a pretty nice representation of what's really important about those feature maps at with less computational cost and less memory cost. But I'm also going to be feeding in a smaller quote image into the next layer. And so, if my stride length is too big I won't be able to patch my images effectively. So it's going to basically just throw an error. So, it can be, it's usually desirable in fact to have different patch size per convolution layers because you want to get important information from maybe a very complex image down sample it something that's like you think is truly important about that image and then see that in the next layer so we can do a similar operation over those what we'll call gisted feature maps in order to get what it then thinks is really important using filters that are probably optimized for somewhat more complicated features in the end. So when you're using your convolutional neural nets you want to be careful that you're not an image that's of too small resolution to be useful. So, for task like MNIST, one convolutional neural net or one convolutional layer is probably enough because it's very simple task for more complicated things real images, you know, multiple convolutions, multiple convolution layers, but you got to be careful about these things like stride length. Okay. So, that all all clear so far. So first just let's look at a backdrop from our previously existing neural net lectures. So if I'm trying to optimize the weight or the value of the single weight. So remember we're going to have some weights that just sort of reside inside these hidden units all apply some activation to them. And we can't we showed how we can optimize the weights one at a time so if I have this is a layer V, and I have an individual weight I'm trying to optimize the value that this gets optimized to is going to depend on among other things. The error that results from every unit that is connected to this. So, just consider there's like one neuron right here, and it's connected to in this case three other neurons so whatever value comes out of this first hidden neuron is going to have some bearing on the output that comes out of those three other neurons. If those are my predictions, then it's going to be some level of wrong compared to what the actual truth is. And so I want to optimize this weight in this quote parent neuron over these are all graphs in a way that's going to take into account how, how wrong all the children are. So, it's like you know if you, if your children misbehave you punish the parent. I guess that's what people do. My children are very well behaved so not an issue. One of those even born yesterday hasn't had a chance to be hasn't had a chance to misbehave. Anyway, what we are trying to do is like if this is slightly wrong right if y y one minus t one is like a little bit wrong. y one minus t two is like really wrong, and then why t y three minus t three is, you know, also really wrong. It might be more beneficial to perform a greater optimization on this weight because two out of the three things that it connects to have very large errors, right so we can we consider all of these. So basically in plain English what this will show is that we've got k output nodes in this case three. And so this should output, you know, k number of values for any input sample. So for this regression example it might be continuous values, so I want to predict properties of a car from other properties of a car. And to classify this is an output probabilities. So again the error term is really the same it's just that I have to make sure that my quote, not units but kind of units are the same right you'd be subtracting MPG from MPG horsepower from horsepower probability from probability, and just modulo, whatever standardization you have to do. So, all these are, there's, these should all be apples to apples comparisons at this output layer so then the way to update will be follow these red lines. So if, depending on how wrong these things are, this is going to be updated based on values that are in part derived from these, from these values. So I see parsley dried here just in the colloquial sense, not in terms of the partial derivative. So, the partial derivative is kind of the key component of the actual mathematical operation of backpropagation. In this case I'm just saying, a portion of this error is going to inform how much the connected way it gets updated. So, we updates in the hidden layer are going to depend upon the way updates, to some extent in the output layer or any other layers that are connected subsequently to that hidden layer. So, we're going to have about like say two units and W, and the upper layer and just a single hidden unit and be so we'll just have you know one hidden unit and then two output units connected to it. So these are the derivatives that we use that you may remember from, from the first year of network lecture. Right, so the partial derivative of the error with respect to weight one is going to be negative two, or in this case to constants you can factor it out, times the input disease is going to be the input that that that output layer, and then the partial derivative of the error with respect to the hidden layer weight is going to be similar except because there are two output weights, I have to sum the, the error times the value of that weight. Right, and then, because I'm also going to be applying this activation function before the output of layer V goes into layer W, I have to account for that so if my activation function is 10 H, this is going to be one minus z squared. And then the last thing I need to do is I need to multiply it by the input to the layer, and in the hidden layer that it would be x. So, the update on w one is going to be proportional to the product of the error and Z, which is the input that layer, and then the update to V will depend on the sum of both of those errors in output layer W times the derivative of the activation function with respect to Z, and times the input to the hidden layer. So, same thing is happening in my classifier network. So, what was the point of all this you know if you've been paying attention you understand how weight updates work. So let's actually see what happens when we apply it in convolutional neural nets so if you just remember from Tuesday. I have the same filter that's going to get multiplied by a bunch of different patches. And that's going to contribute to that final prediction. And so, I need to best optimize this particular filter to account for it. And so, that's going to be a good prediction for everything that it might encounter. Right and good prediction in this case might mean that high activation for for certain things at a low activation for other things is just trying to best optimize what those things are that when this filter encounters it, it's going to best produce the most predictive values and over all these working conjunction there are a bunch of different hidden units, and just trying to do this all at the same time. So, in order to out to perform the actual back propagation operation in a hidden layer. What I need to do is I need to collect all those delta values from all applications of all filters to all patches. And then let's turn this into a big weight matrix, as you can do, do my multiplication operation all at once, just to make sure that it reshaped the inputs into a compatible shape. Okay, so we'll do our kind of square and diamond task again. So here I'll define my diamond. So there's my diamond. And so now what I will do is I will define a patch size and a stride length and I'll patch it appropriately. And then this is going to give me something like this. So there are 64 patches according to this patch size and stride length that I specified, and this patches look like this. So now, this filter would be applied to each of these patches. So as you're going to generate a feature map that is the product of the filter and the patch where in this case C is going to be the weights of that of that filter. So this is the sum of all those values. So if prod here is basically just a list of the values that result when you apply this filter dispatch, a sum of all those, this becomes a scalar value representing how responsive this weight is to the values in the patch. And then this value thing gets propagated through the network. So right now I'm taking a single value, I could do pooling and just down sample to a lower resolution image. So what our standard neural network operation looks like now is very similar. So the only thing here is that if z equals h of xv, v are now our convolutional units, and there is an application of each one of these to every patch. So I can remember my patches are now just considered them to be input samples. I just flatten them and I feed them into my, well, I guess I flatten the image. And then what we do is we just reshape everything so it's a nice square that multiplies together to get our value. Then y is my prediction. This is just going to be z with my bias times my output layer weights. And then we have the softmax operation. So what I do is I exponentiate y. So y is just some scalar value. So I need to turn this into something that can be turned into a probability distribution. So I'll exponentiate that. I'll then sum over all values of f for each class. So this is basically summing across the columns. And so then I will take that and then take the f for each individual value class divided by the sum. And this is going to give me a probability distribution. So now what I'll do is I'll take the sum of all my indicator variables. So now my indicator variables are just these, again, one hot vectors. So think of them as a probability that is all zero except for one case where it's 100. And then times the log of the actual probability that I predicted, which is g. So if I do that, then what I can do is I can take the gradient in v. And then what I'll do is I'll take my inputs, x input in layer, and then multiply them by t minus y, except now these are as probabilities. So I have the indicator variable version minus the probability distribution g. And then multiply that by the weights and then multiply that by the derivative of the activation function. So these parts are pretty much the same. It's just that I've got the indicator variables minus the predicted probabilities rather than a scalar minus a scalar. So now what I need to do is for the convolutions, I need to sum all of the partial derivatives of the error for every delta value in the gradient. So then for every convolutional unit, we need to find the delta for every scalar value. And that's going to come out from every feature map when that filter is applied to each patch. So let's review the back propagation of weights in the CNN. So we'll assume this is kind of a recap of the one from last time. So we'll assume this only has a single convolutional layer, and that's the first one. So what I'll do then is I'll reshape this backed up delta matrix into the right form. And then I'll reshape the convolutional input matrix to something that's compatible. Now I can take that input times the delta. So this is just doing this part here. And then what I will do is I'll just calculate the derivative of the error with respect to the weights for the convolutional layer with simple matrix multiplication. And then in the fully connected layer, it's the same as we've been doing. So the trick is basically just collecting all of my values for every application of every filter, every patch into a big matrix. This allows me to effectively perform the same operation as long as I can ensure that my matrices are in the right shape. Okay, so this sounds like a lot of work, right? So you got to remember, okay, I've got to collect all of my delta values into a matrix. I have to make sure that my inputs are appropriately shaped. Then I can do a matrix multiplication. But let's check the PyTorch definition again, right? This is a single convolutional layer. In the PyTorch definition that we had, we can have one multiple convolutional layers. Each one has a different patch size and each layer has a different stride length. Here they're the same, but I could very easily change them to be different. So you can see that this is rapidly going to grow out of control if I'm trying to do this all manually using NumPy operations. There is a very good reason why we limited the discussion on Tuesday to having a single convolutional layer. So it seems like a pain. If only there was like something we could use to automatically calculate the gradients. So if you're thinking, boy, I wish I had something like that. I have good news for you. Let's remember lecture eight. We got this thing called autograd in PyTorch. So recall what autograd does in brief. We construct our neural network as a computation graph where everything can calculate its own value and the partial derivative of its own value. And then you connect those things into a graphical structure such that operations, the outputs from a single, from one node can then be fed into as arguments into any sort of child node. And so then I can calculate everything. Let's say the partial derivative of every leaf and then back propagate them with a simple, with a single one line call. So this definition of ConvNet using this is basically what we're going to execute that Python call and instantiate. So first you'll observe we're inheriting from torch.nn.module. This allows me to make use of all of the PyTorch functionality for doing things like creating neural networks with simple commands. So now I can specify those things that I want, right, the input size, number of convolutional layers, and new stride lengths, patch sizes, etc. I'm going to do some things to make sure that we can use the GPU, but I specify the activation function. Now that I've done this, I can now create all of the convolutional layers. So here I'm going to create this thing called module list, and then I'll add things to that list that represent those individual layers and their properties that I want. So first, this argument is going to be the number of channels for each pixel, this first argument here. So the input height and width is going to be the square root of the number of inputs. So again, if it's 784, its height is 28 with this 28, so the square root of 784 is 28. And so now I can create this module list for the convolutional layers, and I will just kind of zip up my number, desired number of hidden layers, patch size for that layer, stride length for that layer, and then every trip through the loop I'll add something to the convolutional layers list that contains those properties. What that is, is this is what you're probably going to see when you read other people's PyTorch code, is you instantiate these items, it's basically like torch.nn.layertype. And this could be .dense or .conf2d or .conf1d or .lstm or whatever type of network you're trying to write. And then you specify exactly those things that we just talked about, the input size, the number of units in this layer, and then other in this case for convolutional nets, we have the kernel size, I have stride length for other types of layers, these would be slightly different properties. So you'll see, I'll create nnet equals torch.sequential, and then I'll add these things one by one. So what I'm doing here is I'm feeding in a list that automatically specifies what I want the properties of this layer to be. And then because I'm assuming this is all convolutional except for the things that I specify like number of hidden and convolutional layers, I'll create the convolutional layers that way. And then I'll also create these linear layers for the fully connected. Okay, so this is just your standard way of just creating your your pytorch neural network is effectively just a list of what layers I want. And then when I do like, you know, model of input, it'll run my input through all those layers in sequence, I can specify very cleanly. I want a convolutional layer here, I want another convolutional layer, I want to do some pooling, then I want a linear layer, then I want to do like dropout, and everything that I could possibly want to try. Okay, so now forward all outputs is going to be basically just like one pass through my network with inputs x. And so this forward, the forward pass will basically call this. So you can see that it's handling the convolutional layer and the fully connected layer. So what I can do is I can just say for conv layer in self.conv layer, so for every element in that list, which is an object that represents the layer itself, I can actually just use that object as a function over the input. And so now I can just say that, okay, specify the convolutional layer, it's got some number of hidden units, each of them have x weights in them. If I just call a conv layer of inputs, it'll run that convolutional layer of over the other elements. Train is pretty much the same as we've done before, so I specify what method I want to use all this should look fairly familiar so I just remember set requires grad, I specify the type of loss function that I want to use in this case because we're doing categorization I'll use across entropy loss and then the rest of this stuff is pretty much just for plotting and training. So to calculate the loss, I'll actually run my CE loss function that I just specified over y, which is my predictions and the targets for this batch. And then loss.backward will actually perform back propagation and then call your optimizer.step will actually perform the update of the weights. And then lastly remember that we have to zero out the gradient every time otherwise it's going to be accumulating the gradients because that's just how PyTorch works. Now we have the softmax function in PyTorch. This is the same as we've seen before, I'm just using PyTorch tensors. So here is a trick here to avoid overflow or actually calculate the max. But then I exponentiate and then I take the denominator, I divide the exponentiated version by the denominator, that's our softmax operation. And then use function is use function. This is pretty much the same I'm just using the Torch version instead of the NumPy version. Last thing, especially if you encounter problems with the GPU, I recommend you refer to this notebook here. So you need to detach things from the computation graph, move them back onto the CPU and then optionally convert them back into NumPy to do processing with them. So if you leave things on the GPU, you will encounter difficulties. All right, so I will set the device that I want to use. In this case I'm running on my lab machine so I have access to the GPU, I will hit yes. So it's now running on CUDA. I can check nvidia.smi, I forgot to tell my lab that I'm using the machine for class. However, it does not appear that anyone's really using it. It's fine. So you can actually see with this nvidia.smi command the usage of a given machine. And so here I can see like, I don't know, someone's, I'm assuming this is probably me running Python 3.8, no one else really appears beyond this right now. All right, so now that I specified my network, I can actually try to run it. So I'm going to open up MNIST. Right, so this should look pretty familiar. So what I'm doing, I'm just splitting it into train and test. We must reshape the input matrices for convolutional networks in PyTorch. So what I'm going to do is here I'm just going to take the number of samples, one dimensional sample, 28 by 28. This actually requires a two dimensional input to be fed into the convolutional layer. It will do whatever flattening is required. Yes. The parameters of torch.module. So did you see that somewhere? Oh, here? Like these? Yeah. So basically this is going to say what needs to be updated. So remember self is an instance of a neural network. So it inherits from torch.nm.module.parameters is going to be one of the actual weights of this. So this is W. So what am I updating? This is my collection of like W, and V, and V prime, and whatever. It's basically, you remember how we would collect all our weights into like a single array to do say atom optimization back in Lecture 6. This is just a version of that. So I'm going to have a neural network that is a convolutional net of a certain size with a certain number of layers and a certain number of units per layer. So this is going to in turn mean that I have N weights that have to optimize. This self.parameter stores basically the address of those actual weights in memory. So when I call torch.optim.optimizer, I pass in, what's the address in memory of the things I'm actually going to update whenever I make this call? Okay. So this is never explicitly set because this is a member of torch.nn.module. So because my ConvNet class inherits from torch.nn.module, it has access to this. So when I call self.parameters, because self is that instance of ConvNet, which then inherits from module, it has access to that member variable of module. So when I create the convolutional net using that single line call that in my case, it's stuck inside of a for loop, it's doing things like assigning, you know, it's initializing values into self.parameters. Any other questions? All right. So, all right. So we open up MNIST. We can see that it's pretty much the same. So you can see that I've got by default, the data, the numerical representation of MNIST is flattened. So it has just single 50,000 rows, 784 values. So PyTorch requires that we actually reshape it to that two dimensional shape to feed it into a 2D ConvLayer. So you'll see that before reshaping, we got 50,000, 10,000, et cetera, by 784. And now I have 50,000 by 1 by 28 by 28. So I've got 50,000 samples. And then each of them is basically a single dimension. And then each of that is 28 by 28. So now I can have things like I can specify a batch size that I might want for that second argument. So I can say, OK, I want to feed in 100 of these samples at a time because it's slightly faster than just feeding them in 1 by 1, 100 times faster than feeding them in 1 by 1 and optimizing the weights for every individual sample. OK, so then what I will do is I'll instantiate my instance of my convolutional net. So now I call ConvNet and it is instantiated using 10 hidden units and 20 hidden units and then a final fully connected layer of five units, output size of 10 because there's 10 classes. And I have a five pixel stride or five pixel patch in my first layer, a seven pixel patch in my second layer. And I stride two for each one. And then we train and we can see that we achieve our loss after just 10 epochs is going down significantly. And then finally, one thing you can do is you actually just print the network structure. So it actually will print out this nice little display of what your network looks like. So you can say that I've got a conv layer one and two. And then it takes in, say, a one sample and 10 hidden units with a kernel size of five by five and a stride length of two by two. The second one is going to basically map from 10 to 20. And then it's going to have a kernel size of seven by seven, striding two by two. Fully connected layers. So you're wondering, OK, I have to flatten the output of my convolutional layer to feed into the fully connected layer. How many actually dimensions is that? So we can just print this out and see exactly it's 180. So this fully connected layer will take in 180 inputs and then map them down to basically five dimensions. So in features, number of dimensions on the input, out features number of dimensions of the output. So that's five. And then this last thing, this is the softmax layer. This is going to take basically a five dimensional representation of every sample and then distribute it into probability over those 10 classes. It's going to say, OK, we've got you me five numbers and I will tell you what the probability that it falls into any one of these 10 classes is. All right, so we trained. Looks like it went pretty well. So then I will use my use function. So I have my test set and what it will print out is going to give me the classes and then I'll use that to calculate the percentage that's correct. 90.58 percent correct. So we can try a few things like we can we can train for longer, for example, if I train for twice as long, then. So you get 95 percent accuracy. Right. And so you can see that just by just by increasing the training size or the training length, we get a significant boost. You can also try, you know, messing around with the batch size, adding more hidden layers. You got to be careful when making sure that your your patch size and your stripe length are appropriate for the layer. But you can see what kind of accuracy we get. So let's examine the effects of one of the layers weights over on an input. So first, let's get the hyper parameters of that first layer. So that first convolutional layer. Let's just take a look at what those values mean. Right. So it's got 10 hidden units. This is the kernel size five by five and the stride length two and two. So now let's view the outputs of each convolutional layer. So I'll take one sample and this here's how to take the 20th sample from my test set. So I will then turn that into a torch tensor and now I'll just run that directly through the forward all output. So I have a single sample. What I want to do is I want to grab every the output of every layer and accumulate them so I can see what's happening to the sample in the first layer, the second layer, et cetera, et cetera. So now I'll have like layer one weights. This will give me the weights data for this list or for this for this layer. So if you look at like CNN net dot children, this is going to say in this graph structure, I can find the children. So these are like the first child to be this conglayers set. Second child to be the FC layer set and then inside conglayers I can get like, OK, the first convolutional layer. That's what the zero zero is. And then in that I can do dot weight and actually get dot data, which is going to give me the actual weight values. So now what I can do is I can plot this. So I'll take the layer one weights. And then I will multiply this input by those weights and we'll see what each layer is actually what each unit is actually predicting for actually outputting. So this is the output. So take a look at these. So what you'll see here, that's the original image. This nine on the left, you will see these are just the plotted values of the weights. Right. So these are the filters. And then this on the right, this is going to be what happens when I apply this filter to every patch of this, this nine. So take a look at this. And what do you what do you see? What are these different filters? What do they appear to be doing to this image? Yeah, I mean, ultimately, yes, but let's take let's let's look like one at a time. Let's take this. Maybe compare like this one and this one. Yeah. Yeah. Right. So and they can remember what these numbers with these colors mean. Right. So if it's, I think we're using the negative version here. So if it's darker, it's more positive. And if it's lighter, it's less positive. Right. So this filter, for example, this first one is probably activating more on like the outer edges of the nine. So maybe it's like responding more to like lower pixel values or something. Whereas this one, it seems like it's mostly not it's sort of it's gray on the outside. So sort of ambivalent. Right. Maybe it doesn't have very strong, either positive or negative correlation either way. Inside the interior of the nine, it seems to be mostly negative or low values. But then all those edges, it seems to be having higher activation. So this one, this filter, at least for this sample appears to be kind of optimized to detect this type of this type of feature. And in fact, you know, this is sort of one case where this actually might be reflected in the plotting of the weights itself. So you notice like, take a look at where you see the darker values in this filter and compare that to where we're seeing the lighter values in in the feature map. Right. So sometimes it's not so obvious this one kind of seems like there might be some sort of correlation there. So, what can we say kind of generalizably about this? We have different filters that are optimized to kind of pick out different features of an image. Right. So some of these are maybe optimized for things more like certain types of edges. Maybe these two kind of seem to result in a similar feature map. So maybe these two, even though they look quite different, are maybe optimized to pick out similar features. And these are all normalized. So there's no guarantee these values are actually the same. But within that normalized range, they appear to be similar. And so does that one. Then if you look at like these two here, they also, you know, again, the filters themselves have pretty different weights. When you apply them to different patches, you'll find that they're maybe selecting for like the outer portion of the image as opposed to like the inside. All right. Questions. Right. So what do we observe here? We just did that. So basically what we can take away from this is that these different features, we have 10 different filters, and each of them is optimized to pick out a slightly different part of the image. So you put all that together, you can imagine that for the different things that occur in MNIST, like, okay, yeah, these filters can pick out like the edges that I'm interested in, the curves that I'm interested in, maybe like the outline of certain shapes. And so these are all useful things to identify in hand drawn digits. So this 95, 90 to 95% performance we get on MNIST seems to make sense. So now let's take a look at the second convolution layer. Right. So again, I do index at one. I'll get the second layer. So this then takes in 10 inputs, has 20 hidden units, kernel size is 7 by 7, stride length is 2 by 2. So layer two weights is going to be the weights from this, from the second layer. And so now I'll see what the outputs are when I actually input some samples. So I'll take the same X, that same nine image, and then plot the outputs of this. And this looks like that. So what's going on here? Right. This is not very interpretable at all. So you're taking layer one as an input and then saying something. It's saying those feature maps that come out of layer one. So basically the inputs to this layer two is all this stuff. Right. So, okay, we can see that this sort of, you know, this is reminiscent of a nine, especially if you know that a nine is what you're looking for. But then I take these values and those are a quote image. Right. This has already been down sampled. This is no longer 28 by 28. This is, you know, I don't know what it is, five by or seven by seven or something, whatever. So this is already lower resolution. And so then I put that into the second layer. It gets chopped up into patches. And then a bunch of these weight matrices that, you know, if you plot them look something kind of like these as well, random looking patterns and then you multiply that by those patched, chopped up, patched low res versions of the nine that have already been kind of processed by some of these to such for some of these features. And it comes out looking like pretty incomprehensible. Right. So this you can't really impute any real meaning to what these things are actually detecting. It's more useful to see what those outputs look like numerically. So this is going to look something like this. And you can see that maybe this first row here is a kind of all the same for this, that one output. So it's hard to say which one that is. So basically every seven by seven filters going to apply to a 12 by 12 input. And that results in a three by three output. So this is, these are the actual these are the inputs for layer one. And then these are the outputs for layer two. Right. So this is three by three. So this is the second layer we get. In this case, I think maybe there's one fully connected layer. So this case, this is this would be flattened into a single array, and then fed into effectively just like a nonlinear classifier. But then then you get your final output, then you softmax, then you get your final output. And once again, that fully connected layer is like not required. It just often is used. But, you know, sometimes it just like adds extra compute for no real reason. So we're just sort of doing it here to show the difference between convolutional layers and fully connected layers. Okay, so we've got this, it's a 12 by 12 input, not 11 by 11. That gets fed into the convolutional layer. It gets chopped up and patched. That's seven by seven filters applied to each one. And then that ends up resulting in a three by three output. So, what this will do is I will then look at the indices that 12 by 12 image. And so what I'll see like, how many intervals of size seven can fit an interval of 12. And you'll see that it's three, right, we've cropped one pixel, because we run off the edge. But this is how we get from seven by seven applied to a 12 by 12 input to get a three by three output. Okay. So just think about like, how many times can I apply a filter of size n to an input of size m. And that will tell you. And so then you assume either crop or padding to make sure that everything fits. All right, so now what I'll do is I'll grab the first 10 samples from some random place in the test set, and I will plot them. And so, this looks like what this will show is this will show the sample and this was going to show the probability that it is a member of any given class. So for example, here's a one, and it's got a pretty high probability, probably close to 100 that it is a member of class one. There's also kind of a low probability that's a member of class seven. Right. Here's a seven, and we see the reverse. Right. That makes sense. Right, because sometimes depending on how you write a one that can look a bit like a seven. If this if this had like a little line there, you could it could be somewhat ambiguous so yeah. Yeah. Not necessarily I mean in this case it does just because we think about think about the number the digits you know zero zero through nine. The one that looks most like a one is seven out of all the other choices that you've got. So like, it's likely that like the second place choice for a very obvious one like this is obviously a one no reasonable person even, you know, confuses for a seven. Yeah. It's likely it's not necessarily globally true so there's probably some samples in here where it has like the European seven with us with a stroke across. And in that case, it might be really, really evident that it's like this is seven like nothing else even comes close but it's just check something out real quick so. Okay. High probability seven non zero probability of one. What's that. So, this network even though this doesn't really look like a nine in any real way and you know you don't really confuse sevens and nines. Our network has determined that there is maybe a slight probability that this is actually a nine. Right, so I could rank all the output probabilities in order. This one is even like a very small probability that thinks this one is a five. So a lot of these really small values come out from just doing the optimization usually from randomly initialized weights. It's like, it does pretty good in predicting like the top one accuracy because the choices are pretty obvious if you go a little bit further down it's like yeah the third place choices thing is making is like really make a lot of sense. At that point it's just almost like residual distribution of the probabilities. So we can find examples maybe that are somewhat more ambiguous here's another. So here's like one of those F sevens right we actually sees a similar thing where we see almost identical plot, actually. So in this case the network has seems to have optimized you for this type of distribution, maybe it makes sense with the nine because like you could you know if you close this gap it might look a bit like a nine. So let's see if you can find some other examples that are maybe more ambiguous may have to run this couple of times. Let me try a different set. So, okay here's here's one. Right, so here's a nine. And it's pretty obviously a nine but the actual value here might be like it's 50% likely to be a nine and that just happens to be the highest of all of my choices. But, you know, it also thinks there's at least probably a 20% chance that it's a four, and also a slightly less than 20% chance that it's a seven. So, our network right if we were to plot the distribution of our test samples, we'd probably see our sevens and our nines kind of close, because for whatever reason, it seems to have noticed that like one of the nearest neighbors to a nine is probably some instances of class seven. So, in this case, this is one where maybe it's not quite so confident that this that this is a nine, even though that is the highest probability. So the individual samples they the probabilities have to all sum to one obviously, and you usually see similar distributions like if you look at these two threes. Right, there's roughly similar probabilities of it being an eight and maybe even a zero. So, there's kind of similar distributions across classes, but there are cases like if we look at this nine and this nine, you know, the distribution is roughly the same in terms of like you you can sort of like linearly scale one to the other. But this one, this particular nine seems to have a much lower confidence on the probability of being nine. So, you can compare it to like this other one, like this nine looks a whole lot more like this last one. Right. Other observations other questions. Okay, so just briefly review. This is going to be important for assignment four so good to start thinking about this now. What's the key difference between calculating your error for a regression problem versus a classification problem. Yeah. Probabilities is the key part I mean not necessarily calculating the probability that you're incorrect you're calculating. What, yeah. That's what you output that's what the softmax is out there so we talked about the error so if I have my prediction, minus my, or my target minus my prediction. So, if I have my prediction, minus my target, I would say, in assignment one for regression problems. What's different about classification problems to represent this probabilities and therefore So, the target is zero and one right and so the targets are represented as like the ground truth values are going to be represented as what one of them is going to be one. So it's a what it's a what kind of variable. So we have the indicator variables right indicator variables are a bunch of zeros, except in the correct class it's a one. So if that if these, if you think of these as probabilities instead of numbers. What does that one represent 100%. So this is the ground truth. I'm saying, I know that this sample is a member of class nine. Okay. In other words, there's 100% probability that this, this sample is a member of class nine. If these are probabilities, they all sum to one. What is my network outputting in terms of probabilities for classification problem. So they also all sum to one right, their probabilities. And so, if my network is is predicting that for some sample. It is 30% likely that it is a member of class nine, and my target value saying it's 100% likely this is a member of class nine, then what am I subtracting Yeah, so you're directing the different the probabilities right so and then you're doing that at a scale that includes all the classes. Right. So if I have a free class problem. Let's see if people can see on zoom will try and draw this. Okay, so basically if I have a three class problem, and my sample is a member of class to and this work doesn't work. This might be better. Okay, so if my indicator variable is basically 010 saying that my sample is a member of class two and I can barely read that. Then my, my network will be output like okay point 1.7 and then okay point two. So, this is my target T. Wow. We try the red one. That's even worse. Whatever. This one is T. This one is why, okay, put a minus sign there, we're going to end up with is zero minus point one, one minus point seven, zero minus point two. This gives me an error for every probability I can then use that to optimize my weights. So what I want to get it I want to, I want to approach this thing. And I'm going to get this thing I want to minimize the distance between this thing, and this thing. The only difference here is that these are probabilities whereas in regression as predicting scalar values. So the quote units here are just like percents, whereas in a regression problem they actually represent things like whatever the units are the values you're trying to predict. Other questions, comments, thoughts. Would you need to have more data. Do we need to add more layers to it. Yeah, so as a general rule of thumb yes more convolutional layers will allow you to get better accuracy or get a better performance, but also a bigger network. You may be more likely to overfit. So, and this is not a difficult problem. So you know, throwing four convolutional layers at it. You're not going to get a whole lot of extra, like extra mileage out of that. For more, for harder problems, you know, image net action recognition problems. You know, type of video classification, whatever, the more complicated your problem is, the more likely you are to benefit from, you know, additional convolutional layers as long as you're not just kind of throwing them at the wall, you know, randomly. So how many layers we can have? How do we know like, at this point it's for the whole of it. I mean it's going to depend entirely on the data set. So there will come a point, if you know your data set there will come a point at which adding more convolutional layers is just not going to give you anything besides taking longer to compute. And usually that point is pretty apparent. You have to empirically try and verify it. So you know, you try like three layers and like, okay, I get 97% train accuracy and 80% vowel accuracy or whatever. You add another convolutional layer. It's like, I'm not getting much better than this. And it's just taking me a lot longer to train. So you're going to try and find that sweet spot between where you can actually train it in reasonable time versus getting accuracy. And as you add more, you basically approach the law of diminishing returns. You get to a point where it's like, I'm just not getting any more from adding this extra compute tower. One more question. When we get train and test results accuracy and train has higher accuracy than the test, what does, I mean, you might have explained this but what does that tell us about it? Does that mean that our train went well but when we actually tested it, it didn't perform well? It depends on like what the gap is. If it's like my train accuracy was 97 and my test accuracy was 94, that's fine. You're actually doing very well. If your train accuracy is 97 and your test accuracy is 79, that's much more of a problem. Something like that would suggest that there are probably peculiarities of the training data that it might be overfitting to. And that might be because like you have too many convolutional layers or something. And so it's like it's able to, if you got a train set that contains a disproportionate number of nines that sort of look like this one, right, it might be learning something about this like weird hook at the end or other features that are kind of spurious correlations to something in the training data that doesn't appear as much in the testing data. So I see this and it doesn't necessarily match anything that I train on very well so I don't know. Is it because, is it kind of to be also because of how we did separate the whole set into 80 and 20? It can be, yeah. So remember this is also why if you do, it's important to do stratified cross validation. So you can say like I'm not just getting a lucky split or something and my test data just happens to be very nice and perform like my training data, my test split or such that. They train very well and they test very well. Whereas if you try a different 20% all of a sudden it falls apart, right. So what you really want to do is you want to try these averages and see like if I sort of hold out a different portion of my data and then train on the remainder and validate and test on this portion and then rotate that portion I can see how well is my network can that be expected to perform on a random new sample. And so that can take more time obviously is like it's hard to you know train a big network like this. So often most of like the modern tasks will have a curated train validation and test set. So you say like I evaluated on like this, the test set of this data set and everyone knows that they can go and get the same test set. So presumably the test set is assumed to be kind of curated such that it's friendlies to network but not too friendly if you train on that training data. All right. Anything else. All right. So I'll call it a day there I will head back to my office. And you know you guys can just stop by anytime between now and then for 30, and I will have a good break. And I will you in a week.