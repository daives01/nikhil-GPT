 Okay. Can someone confirm you can see and hear me okay. I'm like, you can okay great. Yeah, sorry the ironically I've been San Francisco and the Wi Fi here is like, terrible. So hopefully this will hold out it seems to be okay up here in my room. Give me a few minutes to let everybody in. Let me get started. Okay. Yeah, good enough. I'll get started. So, I'm ready for logging in this should be pretty short. I was just doing the next lecture, and it is pretty brief and one after that is relatively long so what I'll do is I will just do. I'll do it on Thursday, and then we'll just take Thursday off, and then tomorrow or next week, we'll resume and we'll be back on the expected schedule. Should the Wi Fi crash for some reason and you lose me. What I will do is I will just record this and I'll post it of course, as normal. Yes, only just a couple of announcements today. So we are working on assignment three grading and we will hopefully get that back to you by the end of the week or so. And I know some of you had submitted some questions on assignment for I will do my best to respond to those in timely fashion. I'm going to be traveling tomorrow I'm going from San Francisco to the east. I will have like I mentioned before, like the layover in Denver so maybe I can spend some time responding to your messages. So, one announcement to just send out to, to all the CS undergraduate through here as you might be, as you may have heard, we are in the process of recruiting a new chair of the department and our first candidate, Bruce Draper is going to be doing his interview tomorrow and Thursday. And as part of the chair search we are. We want to basically get feedback from every constituent group on campus. So, are in the departments that includes, of course, faculty and staff and then people at the college and university level and graduate students but also undergraduates. So, if you are a computer science undergraduate I know there are a number of people from other departments and forgive me for just sending this out to the CS folks. If you are a computer science undergraduate and you have some time around lunchtime on Thursday, you will have an opportunity to meet with Bruce. And once you get free food, and you also get a chance to talk to him about his vision for the department how he sees it evolving in the next five years during an ostensible term as chair, and make your any of your concerns, or known or get any questions answered. So, at the very least this is an opportunity for you to get some free food. This would be in room 305 of the computer science building. So if you're interested in that, you have an opportunity to talk to our incoming chair candidate, and we'll have one more after that, probably early next week, where the same opportunity would be extended to you. So, if you're interested in that I would encourage you to take part in that we would like to see I would say, maybe a dozen or so folks at least attend this lunch. If you have any more questions about that feel free to reach out to me I can give you some more information if you're, if you want. I believe that's all I had to, to announce right now. Anybody got any questions about anything. Let me screen share up. Okay. All right. Sure screen. I'm gonna go ahead and turn the water real quick. Okay. All right, so this will just take as long as it takes and after that I'm going to let you go. Okay. So, basically what we're going to talk about today is going to be reinforcement learning in continuous spaces and this is going to be just a slight evolution of the quote 1D maze example that we use last time so if you remember, 1D maze I suppose So, we're just we have some determined goal position on the number line and our goal is to try and move our agent or our, our object whatever it is into that position. So that is if my goal position is five on the number line from one to 10. Then what I want is if I'm to the left of index five, and then I'm going to move to the right, I'm to the right of it then I'm going to move to the left and if I am exactly at five I want to stay still. So the way we model this is basically we have a discrete action space, which is negative 101. And then we will take these steps in those given directions. And I have basically a cost associated with taking a move. So that is, I get a reward of negative one for taking a move unless that move lands me in my goal state, in which case I get a reward of zero. And in this case we have a cost function that we are basically trying to to maximize the reward by virtue of taking the fewest steps possible or the steps that are most likely to land me in my goal because I get a non negative reward for that. So, that's kind of our deterministic version or discrete version. Let's think about how we can model this in a in a continuous space. So that is, let's, I'm going to say we have a marble. So now I have basically a flat flat plane. And I can move horizontally along this, and we're going to do that by say picking up and moving my marble but not going to do it by exerting forces on that marble. And so we can think about how this is actually going to work in say a real physical space and you can imagine that if I have a rolling object which is Sam can constrain the motion from into a single dimension, then my, the state can actually be modeled using the position of the object and its velocity. And then my actions are going to be forces that exert upon that marble so I can actually push it to the left or I can push it to the right or I can do nothing. So you imagine this is going to be a slightly different scenario that I wish I'd brought a marble with me and personally didn't think about that. But if you imagine if the marble is already moving in a direction, right, if I, if I may take no action is going to continue moving in that direction will assume basically I'm not going to have things like friction. So I'm going to say it's a frictionless space. You could model this as a as an environment that has friction if you had a more realistic simulator. In this case we're just going to be doing it kind of text wise and Python. But let's imagine that we have a marble, and I can push it in one direction or another, or I can do nothing. So of course if I'm already moving in a direction, I do nothing it's going to continue moving in that direction. So let's imagine that if it's got a high velocity say moving toward the right. If I push it toward the left if I exerted force and toward the left it might not slow it down, it may not move it start start moving toward the left, and it will slow it down. And it may in fact slow it down enough that it will. So, my goal now is that if I have a goal location. I'm going to have a slightly different policy, right so you imagine now that instead of taking these discrete steps left or right. I'm exerting forces, and those forces are going to have effects on the continuous motion of this object and so the policy is going to be quite So, unfortunately I can't really see any of your reactions I hope that was a clear explanation if it's not what I recommend you do in this remote scenario is basically just like raise your hand, or put a question in the chat, or even just like speak up and interrupt so honestly speaking up and interrupting is probably the best way to get my attention in this in this scenario. So our goal is now to get the marble in a particular position on the track. And so we're going to be doing this using a similar method. Basically I'm just going to be using my neural network as a regressor, and we'll see how this is different from the discrete case that we examined last time. So import my neural network with regression so I do my standard imports with NumPy and pandas and pipeline and also import my optimizers and my neural network class. So, now we're going to define our reinforcement function initial state function and next state function, but they're going to be defined to basically model this dynamic marble problem. So if you compare the functions as you specify them here to the ones in the notebook from last time you'll be able to see the differences. So, what are our variables. So we'll have first x sub t, this will be position, and what is called the unit meters for now, it's gonna be whatever it's a big marble right it's going to be moving in terms of meters, but this might as well be centimeters, and then prime is going to be the velocity in meters per second. Okay. So, if you look at what is my position at time step t plus k that is x sub t plus k. This should be my current position right at time step t, plus the integral over time current t to t plus k of the velocity. Right so that makes sense if I'm moving in a constant velocity over n seconds I need to calculate how far I moved over n seconds and then add that to my current position that's just going to be the interval of those values. So you know we all should be familiar with with how this is done. You know we've probably all done, I hope you've all done, integrals and your calculus classes. And so we know that velocity is the derivative of position acceleration is the derivative of velocity, and what's the derivative of acceleration it's actually we call it jerk. And then beyond that I don't think they have turned for that. So, we're trying to now calculate the integral the anti derivative of velocity with respect to time. So of course this is what this year. So I have going to be calculating with respect to t, I'm going to be taking the, the integral of x prime of sub t. So, that is the kth position for now is going to be the current position, plus the total of all velocities over k time steps. Of course this is a continuous continuous calculation. So for any time step t calculating change of position is basically going to be the change in t delta t is going to be the current position, plus the change in t however many time steps have elapsed, times the velocity for for those time steps. And then of course calculating velocity is going to be the same I'm just going to be adding the acceleration right so x double prime is going to be the acceleration at time t and this of course would be in meters per second squared or whatever unit of distances per second squared. Now, this is a continuous problem as mentioned, but of course we have this issue that I'm sure you have encountered before whenever you try to calculate integrals and calculus class we have computers being discrete machines. So, we know that the exact integral curve is unknown, but we know that its starting point is a particular value. So, if we have my starting point of integral curve as a sub zero here, we can use the Euler integration method to approximate it so that is I take a small time step, right, some discrete time step, and then I approximate the value of this curve over those time steps and of course here, right, we can see that because the size of this time step is large enough that my approximate time step is going to be the same. And so, this is going to be if you remember when if you studied calculus like I did, one of the first things you probably did was break the curve in the area under the curve into basically boxes, right, discrete time steps. So, you might draw something like a terrible drawing. Let's say I have a time step, you know, something like that and then the next time step I might approximate it like this much and then so on and so on. And then basically the integral is taken to be the sum of all these and then of course if I have some time step, you know, t, if I decrease this value of t, I'm going to get a better and better approximation, right. So, if I subdivide this suddenly I have these small boxes that are basically giving me a finer grained approximation of the area under the curve, right. So, that's the Euler integration method and then of course we can show that as the limit of this, that subdivision approaches, you know, infinity, then this is going to be an accurate approximation of the actual area under the curve. So, in this case, whatever delta t is where we define that as is going to be the Euler time step. So, here might be a fairly large value. So, maybe over a second, this is an imprecise approximation, but over say, tenths of a second, it's going to be better. So, how do we model this in actual code? So, first let's define our functions. So, our valid actions, right, remember these are forces now. So, I'm going to be exerting a force of negative one or one or zero on my moving object. So, now my functions have to be defined somewhat differently. So, for the reinforcement one, right, I take an s and s and of course this is my current state on my next state. So, my goal is state five, but of course I know that the likelihood of landing at exactly state five is pretty small. So, I want to basically optimize my policy to approach state five as close as possible and I want that the reward for that to be zero when I kind of get within an approximate integral of that value. So, what I'll do here is I'll define this as basically if my next state is within index of one, within one unit of my goal, then I will get that zero reward. Otherwise, it's going to be negative one for every action that I take. Okay, initial state is very similar, except I have I'm going to be sampling a continuous value instead of just some value between like zero and 10. So, I will take, you know, don't add some random noise to my to my starting position. So, the next state is kind of the continuous analog of the previous version that we had. So, element zero is the position element one is the velocity. So, now this has broken up my state down to two distinct values. So, not just the position, but now this is the position. And then my action is going to be one, either negative one, zero or one. So, what we do different here now is I'm going to define my order integration time step. So, here I'm going to say it's a tenth of a second. And so, now given my velocity that is s one, I'll be able to add that value. So, this is per seconds, we're going to multiply that by how many seconds you're in my time steps in this case point one, and I add that to my position. So, force is the action that I take. And so, then we need to do the mass of the object. So, here I'll just define the mass to be a particular number. And then we can, here we have say a little bit of friction here that we can, so I'm sorry, I guess I was incorrect in saying that we're going to model this completely frictionless environment. Sorry, maybe we're going to have a small amount of friction. And so, then given this, I'm going to take that delta and then multiply it by the force divided by the mass minus 0.05 to account for some friction and then times my velocity value. So, this will be my new velocity. So, now I'm going to have some balance on it, of course, so I don't want to roll off the edge. So, if I hit the boundary, then I'm going to set the velocity to zero, I'm going to stop moving. So, my initial state function above that's going to sample initial state, so it'll be a two element array. And the first element is the position of the marble. And second element is the starting velocity. So, the starting velocity is always going to start at zero. So, if I end up, if I start at seven points something, then it's going to be sitting there. And I want to figure out what action I have to take, and that might be exerting a force to the left or to the right, that's going to move me to a different place on my line. So, in this case, I sample my initial state, it gives me 6.13. So, we can say, okay, my goal is five, I started 6.13 and kind of to the right of this. So, given this, I'm going to randomly choose from our three valid actions. So, here it says, okay, I'm going to take an action of one that is exerting a force to the right. So, I'm going to do this, you know, a number of times. And so, then I'm going to sample these actions, and then calculate the reinforcements accordingly. So, here I'm going to do this for like 1000 time steps. What's going on here? Well, I'm going to make a random choice from my valid actions. I'm going to use that action to calculate what the next state is. I'm going to use the value of that state to calculate my reinforcements. And then I'm going to append that to a list and I'll plot it. So, run this and we can see what happens. So, after 1000 time steps, I started at, in this case, it's starting me at 1.1 and it is getting to 5.56. I'll try it again. So, here we start at 5.8 and we end up at 6.6. So, I'll try it again. I started at 9, I ended up just hitting the wall at 10. So, briefly, let's try and gloss this graph. It's a little bit hard to read, but we can see this is actually like a bad, let me try one that's a little more indicative. So, here, okay. So, we start at 8.8 and we can see that the, we can see how the position kind of moves to move toward the left and then it goes back toward the right. Then it takes kind of, it's just toward the left pretty consistently. And then it starts to go back toward the right and eventually it ends up pretty much not far from where it started. The other thing we can see here is that where we look at the reward here, it's the red graph, you can see that it's usually negative one, except for when my position is kind of within one unit of being at five. In that case, the reward is zero. So, we can see the modeling of the position and the velocity is sort of hard to read. It's the orange line there, but you can see how, you know, when it's exceeding two, for example, moving very, very swiftly toward the right and so on. So, you might have noticed something kind of weird about this and we'll get to that in a moment. Let me plot my last few values. So, if I look at the last 10 positions and the last 10 velocities, sorry, I read that again, I guess. So, in this sample, now this one starts at 3.5 and ends at 4.5. So, look at the last 10 positions, right, 4.46, kind of moving toward 4.5. Last 10 velocities is, you know, 0.02, 0.15, 0.02 again. And then the last actions you can see here to go from time step, let's see, t minus eight to t minus seven. You can order, let's say, maybe t minus seven to t minus six, for example. We can see that it's got a, no, sorry, I'm reading this backwards. My mistake. Sorry, I'm pretty tired. So, this compares zero and one. So, here we have an action that is zero. And then it was moving in a given direction. And so then the velocity is like 0.02. We have to take this action of one and the speed increases. We take an action of negative one, the speed decreases, take an action of zero, the speed decreases just a little bit due to friction. Take an action of one again, it starts to increase. So, did the marble ever get to the goal position? Well, not really. But we see some places where it approaches the goal position and that those are where we see this reward as being zero. But it never stays there and it doesn't ever seem to learn anything about how it's supposed to approach that position and stay there. So, what went wrong here? Well, there's a couple of things. One is basically we're just replacing the state with a state plus action and then just calculating the reinforcement actually just based on like two instances of the same state. Right. And so what we need is we actually need to have some way of approximating what the best action is according to the state action pair inputs. So that is we need to be using our network as the Q function. So we need to define our epsilon greedy function. Remember what the epsilon greedy function is. This is going to be there is some probability that I'm going to take a random action. Why do I take that random action? Well, it's because I might find some sort of strategy that does actually get me to the goal that I want, but it does it in a very suboptimal way. And so once I discover that strategy, I do not want to keep repeating that strategy in case it's very securitous or very cumbersome or something. I have to allow for the possibility that there is in fact a possibly better option there. And I might be able to find it by instead of exploiting my my best current strategy. I'm going to just take a random action. So the epsilon greedy function is going to specify some value of epsilon, say, point two. And so if I sample a random number that's less than that value, I'm going to take a random action. Other than that, I'm going to take the greedy move that is going to exploit my current best my best strategy according to my my Q function. So remember, the neural network is approximating as basically a function approximator that's allowing us to learn what the current best move is according to the state action pairs that I have sampled from the environment previously. So this is the epsilon greedy function that's basically the same as it was before. So I have the only difference here is that the state instead of being a single value, it actually be two values. Now it's going to be position and velocity instead of just position. But the function is written the same way. And all I need to do is just pass in that two element tuple or list or whatever into my state. So now I have my my Q net that will allow me to just like use that that function. So now I need to be able to train that function, of course. So here we can see just the epsilon greedy part is not actually training the function. This is just using it when it's already been optimized. So what I'm going to do is I'm going to generate some samples that represent my my experience from conducting various trials in this environment. So I've defined my environment is now having a model of a position and velocity representing my state and then a set of actions that I can take that are these are discrete actions, but they're going to have kind of continuous response within this space. So in terms of I'm not going to be picking up and moving my object, I'm going to be exerting a force on it, which means it's velocity will speed up or slow down in whichever direction. So make samples is written generally to accommodate for whatever whatever my representation of my state is. So here I define these function kind of placeholders where I can define what my initial state next state reinforcement functions are. So here, you know, I just I just call these functions. So next state F is going to take in my state and my action. Next state Rn is my next reinforcement. So that this should give me the resulting reinforcement from the previous state and the next state. And then epsilon greedy will just use the QNAT to choose the next action. So let's try this right. So if I create an S, this is going to be the leftover S from my my sample kind of randomized run here. So you can see that I've got four point five seven. That's my position zero point three nine. This is my my velocity. So basically at the end of this, that last kind of randomized thousand trials, I have my marble that's a position four point five seven. And it's moving relatively slowly toward the right. So this actually would eventually get to the goal, but it probably would keep going and overshooting it. So this state consists of position your velocity. So now this is a two element ray as shown. And so this two S is actually going to be three elements. So therefore, we're going to modify this make samples function just to allow our state variables to contain multiple values. So here's that. So what I will do is I'm going to update the state SN from S and A. And so then here, the rest of it is pretty much the same. So this is a very small, small adjustment. So if we can compare this version here, there is no next state. There's no call to the next state function here. So what this does is this is going to take my next state. I'm going to assume that my starting action is always zero. And so my next day when I take no action is going to be the same as the current state. OK, so now I'm ready to trade. So before we do this, we're going to make this plot status function to show how we're doing. And we can look at the various different things that we're going to plot. So like last time, we're going to show a bunch of different graphs and we can go through all of them. So what I want to plot is one, the probability of taking a random action. So that is my exploration over the number of trials. So if I'm doing epsilon decay, then I would expect that as my model gets better and trains more, then I'm going to have a lower probability of taking a random action because I'm fairly confident that my best strategy so far is actually relatively close to optimal, given the amount of data and training that I have. So the second thing, starting position of every sample. Remember, we're creating these samples. And each time I create new batch of samples, it's going to reinitialize my environment and start my marble in a new position. So I want to see how my training evolves, given that one time I might start my episode at 2.6, and then the next time I might start it at 7.3, and then one time I might start it at 10, the next time I'm going to start it at 4.9. And so as I train, I should have a better idea of how to get to the goal from my starting position. So here I'm going to mark in this graph, I'm going to mark the goal position at 5 so you can see how things kind of evolve. Next thing I'm going to plot is the latest policy shown as the action for each state. So we should have some graphs showing that given my state, this would be the best action, right, and this should be some sort of continuous manifold at this point. So this should be mean reinforcement versus the trial. So we'll actually smooth this and we'll show the mean reinforcement every 20 trials. And then five is going to be velocity versus position, right. So this should be in a well-trained policy relatively intuitive. So if I'm close to five, I should be moving kind of slowly in the right direction so they get as close to five as possible and don't overshoot it. So this fill between function is kind of cool. You can we'll see what that does in a moment. And then plots, I guess this should be six through 10. I don't know why I've numbered seven through 11. This would be the max Q value versus the state and then versus the actions versus the position of velocity. And then we're going to show some top down and some 3D plots so we can see kind of how this looks and what influences we can make from the 2D versus the 3D. So that's the top down and then the rest is just drawn. So let's write a function to test it. We'll plot the marble down at a number of positions and we'll try to control it and we'll plot the results. So for end trials and starting positions, I'm going to run a bunch of simulations for a certain number of trials and then plot that. Okay, so now we set up the standardization. This is pretty much the same as before, right, we have our, our means for my inputs and my targets. I'm going to use that to standardize my values when I'm using them to train the QNET. And so then I'll plot, you know, my, my standardization. So then here, but this is basically my training and my plotting. So I'm going to have, this is gamma is my discount factor, that is how much do I discount projected reinforcements like far in the future. I'm not very confident of those. So I maybe don't want them to have a huge bearing on the actual training of my model, because, you know, they're, they're, they're so far in the future, they probably don't matter all that much. And then number repetitions of the Q update loop, and then the number of steps between new random initial states, that's a learning rate, specify my final epsilon so I can calculate my decay rate. And then I create the QNET I'm going to use just use a fairly simple one that has 10 hidden units as a single layer. So here's my QNET structure. So three inputs. So for the position, the velocity and the action, and one output. This should just be the best predicted action. Where this ready best predicted the best predicted reward. So then I run setup standardization, and I'm going to specify some initial epsilon value. This is one, because when I start, I don't have any best strategy. Right. So I think this to one. I'm just gonna say, start by taking a random action and then I'll burn some that then they can decay my epsilon value. So then I collect all my samples, run train, and the rest is for plotting. So let's run this real quick. And there we go. So, we can see here for example. This is the random action probability starts at one. And you can see it sort of rapidly decaying towards zero because this should be learning something. Well what exactly is it learning, let's take a look at the mean reinforcement. So starts out, like, kind of point eight five. I'm not sure what the initial. The like the very first initial state that selected was might have been somewhat close to to five, but it pretty quickly drops towards negative one. And then so sort of just a bombs along here for about 300 time steps. And then it starts to actually learn something right so we can see here that it's at this point, right about 300 time steps, it sort of started to figure out what the best policy was. And so now here, if you look at the maximum of the, the Q values here and in 2d and 3d. And you can see that, you know, for example, what's the Q value, when the position is very close to 10 and then the velocity is negative right is negative point three, and then kind of the converse is true. And it's, when it's like close to five and the velocity of zero, the actions. This is pretty interesting so you can see that when my position is five. There's sort of the sharp boundary, given the, the actions that I have available to me. So if we look at the state trajectories for epsilon equals zero. So, this is my desired position. Right, this is five. And then you can see that if I have, say, a position of five the state trajectories for the velocity tend towards zero. Whereas if I have a lot of positions that are say closer to 10, then my state trajectories, you know tend toward the native numbers where if there's my positions are zero my state trajectories tend toward more positive numbers. So this is the policy for zero velocity, so you can see the kind of how well this maybe has trained. So, this is maybe not quite as optimal as I would like so I would expect that, you know, my best policy would basically be RRRR 0LLLL. For if I'm at zero velocities that is if I'm not moving I'm in one of these, one of these states, where should I be moving well what this has learned so far is that if I'm in states eight nine or 10 I should move left, which is good. But it's also learned that if I'm in states zero through seven I should move right, which is not quite right. So you can try to train this again to see what happens. So this is a good example of how this is actually taking up to like 15 seconds or so. You kind of see similar patterns starting to evolve right this this curve starts to look familiar. So does this one. And again I think this is, yeah. So, actually you got a little bit worse that time, right so now it's basically learning that if I'm at 10 I should move left. And if I'm at anything else I should move right. So try one more time, and then we'll move on. So you can kind of see things changing here over time right now it's basically like learning a strong policy just moving to the right. Okay, now things are starting to happen. We're getting some, some indications we should start moving left there. Okay, so this is done again. So, maybe this is not like a terribly successful policy and try one more time to see if I can get it to kind of demonstrate something nicer. One thing we're observing here is that reinforcement learning has a tendency to be unstable. And so often you start to learn something and maybe your X, your epsilon is decaying too fast and you kind of find some suboptimal strategy and stick with it. Or, I'm not liking what this is doing. But you can see that you know there's, it sort of finds some sort of degenerate strategy and it's kind of sticks there for a little bit. So sometimes you need more complicated strategies maybe we need a bigger neural network that would take longer to train, maybe we need more, more samples per, per episode that would allow us to, to kind of get, get more experience. So, let's, let's just sort of accept that this is sort of learn to kind of some kind of suboptimal policy and actually seems to be doing worse than the previous like five times that I tried it. So let's plot the or let's print out the last, last 10 rewards. So what this has learned here is that if I over time, the last 10 rewards, rewards that we got were negative one. So this was basically somewhat unsuccessful in training by, you know, a couple of different things like we can try maybe adding a adding more hidden, hidden units. Try this again. So you can see this taking longer to train this time. But now where are we at. Yeah. So, starting to learn a little bit more a little bit earlier. So, now, the policy for zero velocity in this case kind of seems to be just sort of bottoming out with always moving to the right. So what we're seeing here in this example is a strong tendency for the instability of reinforcement learning. So in this case for this, for these continuous problems, basically what we what we find is that say Q learning is not necessarily always the best choice. And so often we encounter these problems with stability when trying to use a sort of deep Q network. So we have other policies that I alluded to earlier in class, such as the actor critic methods. So you have say things like an EDPG or a soft actor critic or an A2C. And what that does, we have these two neural networks where one is the actor that is to choose the action. One is the critic that tries to predict how good or bad the action is going to be. Right. And so then the actor tries to get better at predicting good actions and the critic tries to get better at predicting the quality of the action. And then these two networks are updated in tandem. So that's one way of kind of solving some of these or at least addressing some of these instability problems that we encounter with continuous spaces. OK, anybody have any questions? This again, just for fun. Now we're seeing something desirable here. So now we have a policy that's kind of approximating what we'd expect. So this was now that we're done here, let me show you a case that we've been moderately successful. So if you look at my my policy for zero velocity, this is where my mouse is that state five. This is where we want to end up. And so what this policy has learned is that when I'm less than five, I want to move to the right. When I'm greater than six, I want to move to the left. And when I'm at either five or six, I stay still. So this is actually this is a moderately successful policy. And we actually managed to get one in that it's approximating when I get close to five, at least knows that I should probably try to slow my marble down. Whereas if I'm far off to the right, I need to be accelerating toward the left and far off to the left and you be celebrating toward the right. My action policy. So you can see here that where I have this last sample actually appear to be quite successful where my my marble position is within this red bar that has been one unit of five and my velocity is very close to zero. And so similarly, if you look at the actions here, it's learned that if my position is a ten and my velocity is like negative four, then the action, this is really probably don't want to do a whole lot here, for example, because I'm moving in the right direction. I don't want to accelerate too much. Whereas the opposite is true if I am say close to zero and my action and my velocity is four. Right. So I might want to accelerate a little bit. But I'm probably not too much. Whereas if I am at ten and my velocity or close to zero velocity is four, I'm moving fast in the wrong direction. And then I need to kind of accelerate or stop my motion and start moving back in the other direction. So now we look at the state trajectories and now this is the pattern that we would expect. As I am, if I am at zero and my action is my velocity is zero, then we can see that what I'm what I'm doing here is like as I'm moving in toward five, I kind of want to increase my velocity. But then I also want to start by crossover five. I need to be kind of moving back towards decreasing my velocity moving back towards the left. Okay. Questions. All right. So, in conclusion, I suppose, continuous spaces are challenging for reinforcement learning. And so you need to have sometimes more sophisticated techniques to try and solve these. We find these cases in, in scenarios like robotic manipulation, where you're trying to control some say some some movement of a joint and continuous space may to manipulate an object. So this is a problem that can be solved with a lot of continuous sampling and learning from experience, but it often takes some fairly sophisticated hardware and the ability to sample, you know, lots of lots of experience from my environment. Okay. So, next time so basically Thursday, just going to take off because I'm going to be traveling, and then we will reconvene next Tuesday. What we will do on Tuesday is going to be the reinforcement learning for two player games that is learning to play tic tac toe. That's going to be the subject of assignment six, and we see if I do have the schedule ups and schedules here. And then I will also assign assignment five, then. And so that's this is at present the day that assignment for is due so assignment four will be due. I will assign assignment five to have two weeks for assignment five, and then I'm going to assign assignment six before assignment five is due but you should have plenty of time for this. Also hope you're all working on your project proposals. That's going to be due next Thursday question in the chat. Very interesting. Okay. Thank you. Okay. So I guess there are no other questions or comments, then I have no problem with calling this a short day, and I will see when I get back home. Do I have all I will not have office hours today I have to go back to the conference. But if you have questions. Again, feel free to email me, and I will respond as quickly as I can. Thank you. Bye bye.