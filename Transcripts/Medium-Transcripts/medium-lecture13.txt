 All right, you already going. Okay, so everybody should have turned in assignment to, which is good, I can pick up on the notebook that I left off on last time. We will try to get this back to you within about a week. So today I'm going to go finish this code reuse notebook. This is short. Then we'll start classification using neural networks and then I will assign assignment three. So I'm going to assign assignment three today. You have like a solid three weeks to work on it. So I didn't want to give you something to do during spring break. So this is due the Tuesday after spring break. So you have time. I do recommend that you start early, especially if you want to have some some spring breaks. I'm not going to be here. And so of course, I'm not going to be holding office hours. So recommend you start early and to give yourself time to to work on on this. So I guess without further ado, let's do this one for real. I hope you enjoyed the extra 30 minutes I gave you on Thursday. So these these notebooks are now available again. Note that we're not going to do a reread on assignment two because basically the answer is now given to you or an answer is not given to you. So You know this this one is not going to be eligible for if you do want to resubmit assignment one reminder that those are That's a hard deadline. So we will not be accepting anything else on assignment one after midnight tonight. So As you recall, you know, if you want to modify your neural network implementation or any other implementations. You can either add arguments to parameterize the different behaviors or you can replicate code from the original class using class inheritance. So first, let's just show our neural network implementation not going to go through the code because you've already seen most of this before. Here's optimizers and has an admin implementation there. So now we will import optimizers and then here's a version of the neural network code, you'll notice the familiar functions that these have all been filled out. So you're this is basically your implementation or something close to it. If you if you got full credit on assignment to That what we had before was we had a division function argument and it you have to change this to specify whether you want to use 10 H array loop. That was that was part of your assignment and this had to be added in the neural network instructor calls in that in it function. So now what I can do using this is I can now define two different instances of the neural network a 10 H version and radio version. That have the same architecture. Otherwise, simply by just passing things activation function. To the to the constructor. So now if I just print this, it will show me that I've got an instance of neural network that has An input size of 10 100 hidden units followed by 50 hidden units and an output size of one uses the 10 H activation function. The other one is the same just uses the radio activation function so The same two kinds of neural networks could actually be implemented just using two separate classes. Right. So we can say we can have two classes, one called neural network 10 H and one called neural network radio, but You would observe already. I think that most of the code in these two classes is going to be identical. And so if I duplicate all this code in two separate classes, you are effectively by yourself a future headache because one day you may decide that, oh, there is a bug or I want to change some functionality and you do it in the 10 H version. You have to replicate it exactly in the radio version or your code is going to work differently. So you make one mistake. And all of a sudden you have diversion implementation. And so, you know, one of them might not work or may not work the same and suddenly your results not directly comparable. So instead what you Inherit most of the functions for one class to create the second one and only change those things in the second class that would directly override things in the first class. So this would be going through these in real time, or I can find my typos. So this would be readable if you just define two functions. So we'll call the activation function activation function gradient. Of course, the things that I want to change about this, this new version of neural network is going to be the activation function. So If I just find these in the first class, then I can overwrite them in the second class with a different functionality and everything else can remain the same. So it makes sense to define the second class first and we'll just see how this looks in terms of code implementation. So if we assume that neural network 10 H is already defined, then I can just define my neural network ReLU by passing the neural network 10 H class as an argument into the class definition. So now we have neural network ReLU Will inherit everything from neural network 10 H except now I have these two functions for the ReLU activation and the radio network. Gradient that have the really specific behaviors here. And so presumably they would just overwrite any equivalent implementation in 10 H. So we run this. And it didn't give me an error because I sorry there was an error there previously, but then I ran the rest of the notebook. So if I if I cleared the kernel and run this get an error. That's because the neural network 10 H function. The first time you run this notebook has not been defined because I ran the first time. The first time you run this notebook has not been defined because I've already run the notebook completion once it has been defined. So it doesn't give me that error. But if you got that error, what you need to do is you need to find the original neural network class redefined to use this activation function and activation function gradient functions and then I'm going to rename that to neural network 10 H that it matches the definition that the neural network ReLU implementation is expecting. So if I define neural network 10 H you will observe that it looks pretty much the same as the current neural network implementation, except when I get down to here, I now have instead of say the 10 H function or the ReLU function as you defined it, I just have a generic activation function and activation function gradient. Now, of course, these have to be when I define this, these basically have to be initialized in memory with some behavior. Right, I could just say pass, for example, I could just ask you were here that would basically say do nothing. But then, of course, if I tried to run this then I would not have that that requisite behavior. So I'm going to define some default behavior. And for the purposes of this demonstration, I would just assume that my default behavior is going to be using the 10 H function. Right. So if if not defined your activation function is 10 H. So you'll see that we now return the 10 H and then we return one minus y squared, which is the same as the gradient 10 H. So now forward pass same functions as before. So now I can define my second class. So now I have an instance of neural network ReLU that inherits from that 10 H version but overrides the activation function as gradient. So now you observe that this version here, instead of having this activation function argument that specifies in a string form what activation function I want to use, I now can use this one. Right, let's compare these. So here's neural network using the activation function argument. And then here is the differences, different classes. Where I don't want to specify that argument. So there are different advantages and disadvantages to this. So like one advantage that in the small example is not that significant, but might be if you're using this at a large scale is that like I don't have to specify the activation function name in a string. This prevents me from say making a typo in that string or something. So in that sense, it's a little bit cleaner, but there are some obvious disadvantages and in general, we'll see one of these techniques is preferred. Yes. So you mean both the 10 H? Well, that's what we're doing in this first version here, right? Because then you have, if I have all these functions available, whenever I run the neural network, I have to specify which one of these I want to use. Right. So that's what this activation function argument is doing. That's what you did in A2 effectively, right? So like there's both of these are valid strategies and we'll see at the end that in fact the strategy that we were using is probably on balance preferable. But it's important to understand how you can do this. So when we do, I think it's assignment three, you have to basically take your neural network implementation or A neural network implementation for regression and redefine it to classification, which is you if you've been paying attention, there are some subtle differences there, but you don't have to change the entire neural network architecture. So it makes sense to kind of inherit from a generic regressor neural network and then overwrite certain functions to make it classifier. Yes. Like an example of what we might do, but this is a very serious for us. No, this is not necessarily for assignment. This is a this is an example of how to use class inheritance in an example that's simple enough to illustrate in a few slides, but it's probably not what you actually want to use this for. Nonetheless, let's assume that I'm just going to be using this to define different activation functions. So now if I print instances of this, you can see that now I've got these two separate instances of what are different classes. Although this one neural network relu inherits from the first one, they have the same architecture. So now you can see that this approach, you know, worked pretty well. But now imagine if I have like a bunch of different activation functions, right? We talked about tanh, we talked about relu. Some of you may have implemented the switch activation for A2, right? There are a bunch of other different activation functions talked about, you know, jlu, elu, etc. So I don't necessarily want to define like a different class for every single one of them when I'm just overriding like two functions. So, you know, you have to implement, you know, let's say if I have three variations with each with two choices, right, I'd have you know, two to the three, eight different classes. So even this bit here, just doing it for these two, these two variants seems a little bit excessive. So you'd have to remember like all the class names. So on balance for this for defining the same type of neural network for this for a similar task where I'm trying to do, say, change the hyperparameters. On balance for that approach, this type of thing where I have an argument that I can specify is probably the better approach that allows me to implement the different functions that I need all within the same class. I don't remember different class names. And I just have to specify which arguments I want, because here we can effectively define a neural network with these four arguments where NH is a list. And it is to specify the input size, the network architecture and the output size and the activation function for arguments for this function doesn't seem excessive. It might be more so like if I were trying to define radically different neural network architectures in the same function and instead of specifying, you know, feed forward versus convolutional versus recurrent in the actual constructor itself, I have a like type argument where it's like CNN and then I specify all the other things I have to set up my neural network as a convolutional neural net based just on this argument. That's not necessarily the best way forward for that. So when deciding whether I want to use class inheritance or arguments for specifying neural network parameters, it's kind of important to consider what exactly you're going to be editing. So for activation functions, I would argue that probably using this type of argument based method is preferable for different types of network architecture or for different types of network classes entirely. So feed forward, regressors, classifiers, the convolutional nets, recurrent nets, etc. Then it makes more sense maybe to have some sort of base class that has like all the functionality, you know, is common to all these things. And then I just have to modify certain functions. That's what we're going to do next. So we can take, you know, create different types of neural networks and then use say the same implementation of like your optimizer, your training function, those things that maintain their same basic shape and don't change a whole lot even when you may radically change the particulars of the network architecture. Using class inheritance is probably the better way to go. And so, for example, that torch.nn module that we used briefly kind of provides you with all of this basic functionality. So when you do assignment three, for example, you're going to be using PyTorch and you need to refer back to the autograd notebook, notebook eight. And so if you see things like, you know, loss.backwards, right, single function called it basically does all the backdrop for you. That's part of that neural network module. And so I wouldn't have access to that function if I didn't inherit from the neural network module. So these lower level things with regard to the functioning of the neural net is probably a better use for class inheritance, whereas some of the more hyperparameter tuning like things that you're going to be doing in most of your neural network usage probably can be specified in argument structures. Alright, questions on this? Alright. Let's start classification and I will try to get through all of this. So I have time to talk about assignment three. If we have to leave a little bit of this to Thursday, it's probably okay. So previously we talked about regression, right, so we did linear regression, nonlinear regression, all of these are predicting continuous values, right. So we can use the neural network as a way of introducing arbitrary nonlinearity to effectively construct a universal function approximator for predicting continuous values. So the principles are that we have some nonlinearity that's applied to the output of some hidden layer. So that would be say I have X times W gives me some set of scalars, I apply some tanh to them and this then the output layer is going to be linear in W and that just gives me my my prediction. So now when I'm optimizing my neural network, the values of W are going to be optimized effectively with the assumption that there's going to be a tanh or just some other nonlinearity applied to it. So of course, if I optimize the weights for a tanh network and this like clone them into a ReLU network, we're going to get drastically different performance of course because the actual outputs are going to be different. So if we have this scalar output, then we want to classify categories instead, right, what we did is we basically want to classify these things into probabilities that they fall into one of a set of K classes. For that we used in the previous notebook number number 10, we used logistic regression. This included the softmax calculation given here. So what this does, you know, just exponentiate basically the output of the output layer and then sum up the output layer and then sum up the output layer. So this is going to take the i-th prediction of X, it's going to turn this into a probability and that probability represents the probability that sample X sub i falls into whatever whichever category is denoted by these weights or more properly, since all our weights are going to be stored in a big matrix, each column is going to be associated with a particular category. And so weight sub k of that multi-class weight matrix W. So the softmax, this part here, right, this is the prediction, this is the output layer in the sense that this is the output of that final multiplication by those output layer weights. So this is going to be again still some scalar value. Then by applying the softmax function, this allows me to turn it into a probability and then turn the linear model into a logistic model. So when we previously introduced neural networks as this way of arbitrarily introducing nonlinearities into regression problems, so now we want to do classification. So what might be some thoughts you might have about how to do logistic regression, as you may remember from notebook 10, but in a nonlinear way. So this is the first thing that we're going to do. Yes. You can have neural networks that have multiple layers and activation functions. That's our way of arbitrarily introducing nonlinearities, right? So if we just had that, which you end up with would be a nonlinear regression function, and then we discussed that how we can turn a regression function into a classification function is the application of the softmax. So this is going to turn those k predictions that I want to make about some sample into k classes and the probability that it falls into each one. Right. So basically what we're doing is we're now just putting together the pieces of things we've already learned. We talked about softmax last week, and we've been doing neural networks now for a couple of weeks. And so we're just going to apply the principles of turning scalar values into probabilities using softmax to the use of neural networks as universal nonlinear function approximators. So let's just review some of the math. So remember when you're doing classification, we're basically trying to arrive at the set of weights that would allow we to maximize the log likelihood of the training data. We're trying to maximize the likelihood of the training data, but we work in logarithms because of these nice properties that allow us to add and so multiply, so we don't get these very infinitesimally small probabilities. Instead what we get is just a set of negative numbers, and you're basically trying to figure out which of these numbers is the least negative. It allows me to maximize the log likelihood. So just recall we're going to be using more or less the standard variable definition. So big W is going to be the whole weight matrix, and then X is going to be all the inputs where big N is the number of samples, therefore X of N is the Nth sample. Big K will be the number of classes, and therefore little k will be an individual class index. T, these are going to be the target values, so remember that the target values are now class indices. And so these are indicator variables where I have a string of zeros and then a one at the index that represents the appropriate class, and so therefore T sub Nk is going to be whether or not sample N falls into class K. So you can just think of this as being basically binary for K columns, right? So if it's a member of, it either is or is not a member of class zero, it either is or is not a member of class one, so on and so on until class K, and then by N rows for each sample. So now I basically just have an N by K matrix representing my outputs. So now P of C equals K given X sub N is going to be the probability of class K, right on the left side of the bar, given sample X of N. For simplicity's sake, we usually just say P of K given X of N. We can also rewrite this as some function G sub K of X of N, and in fact we'll just shorten it further to G sub Nk. And then we talked about how this arbitrary function G that we arrived at is actually the softmax function. I went through a bunch of math involving exponentiation and logarithms to basically show that if we perform a certain set of transformations, we can then derive the probability for a set of classes. So if we have the likelihood of W, this is going to be the probability for all N for all K of the probabilities of some class K given some sample, exponentiated to the indicator variable. So what that does is that this value is basically only exists, is only not one when T sub Nk is one. So if I raise anything to the zero, it becomes one, and then the product of course that just cancels out, just multiplies out. So similarly, if I then take the log likelihood, all I'm doing is I'm now just turning all my products into sums, I'm bringing my exponent down in front, and so now the logarithm of one is of course zero, and so the same is true if this T sub Nk is zero, then that also goes away. And so therefore, the log likelihood is going to be the sum for all N over the sum for all K of the indicator variable times the log probability basically. So now the gradient log likelihood will be given as follows. So if this is our log likelihood function, and then we have G being realized as its softmax function, and let's just define Y sub Nk as being that prediction for the output of sample N for class K. So in other words, this is going to be the weights of that class multiplied by that sample. So therefore, we've now arrived at the same definition of softmax function we had before. And so now if I take the partial derivative of both sides, I end up with the following. So now I'm just taking the sum for all N overall K of T sub Nk divided by G sub Nk, this is the actual target, and this is going to be the actual the prediction value, and then I multiply that by the partial derivative of G with respect to W. So if the general gradient looks something like this, so now we have effectively the partial derivative of the prediction with respect to the weights times T sub Nk minus G sub Nk, so this is going to be my ground truth target minus my prediction. So once again, this part is my error term, right? So this is the same as T minus Y in the linear regression problem, this is basically what's my ground truth minus what did I predict? How wrong am I? So again, T minus softmax is going to be how wrong am I, it's just going to be how off are my probability distributions, right? So if I'm very correct, then for that ground truth, it should be very, very close to one, because in the target values, my only choices are 0 and 1. And then I just do that, and then I take the sum for all classes, and then for all samples. So for linear logistic regression, Y sub Nj, that is the product of the weights times the input. So therefore, the derivative of that will only exist when J is the class of interest. So I'm going to do this for each of my classes, but for all of my classes, except for the one that actually is the truth value, I'm going to get 0. So I just want to optimize away from those and toward the thing that I actually have a value for. So therefore, the partial derivative of the log likelihood of the weights with respect to the weights is going to be equal to the sum of your error terms times X sub dJ, where d is going to be that particular input. So the nonlinear version is in some ways a little bit simpler if you remember how our neural network operations work, because effectively, all I'm going to be doing is the same neural network operation that we've been doing just with the softmax at the end. So let's remember the general form here. So we just have sum weights W. So to include the nonlinearities, we have to have a hidden layer. And so we'll just call this V. And so then the log probabilities of the K classes given X are going to be H, some activation function, over X times V times W. So remember the quantity here, X times V, that's that hidden layer output before the activation function. So previously, we call that Z or maybe A. And then I'll apply H over A. Maybe I'll call that Z. That will be the output, the final output of my hidden layer. Then I take that and multiply it by my output layer. And that gives me my pre softmax prediction. So because we're not still dealing in the world of logarithms, so we're going to take the top part of the softmax function, E, exponentiated to Y sub nK. The log of that, of course, this will just cancel it out. So we'll assume that all logs here are natural logs. So now I just have Y sub nK. So now Y sub nJ, where J is just whichever class index I'm kind of focused on at the moment, is going to depend on V and W. So that's going to depend on V and W. So that's going to depend on V and W. So therefore, the log likelihood of V and W with respect to V is going to be just the same as above, except now I have the partial derivative of Y sub nK with respect to individual weights in V. And I do the same thing just with respect to W. So now here at the end, the only thing that changes is going to be just what happens in the denominator of the partial derivative. And so now the log And so now the log likelihood of V and W with respect to W is going to be just the sum for the error terms times the partial derivative of Y sub nJ with respect to W. So we've already calculated these two things, it turns out, in the previous neural network lecture. So we already went through the derivation of how we calculate the derivative in order to do backprop. So this is in the training by gradient descent section if you need to review. Again, eventually we'll see this in Python, so this is all getting a bit sorry for you, I don't think you need to worry. So when you compare the above with the derivatives of the mean squared error in regression problems, you can start to see the parallels. So here E, this is going to be our error term, this is the thing we want to minimize in a regression problem. The distinction between that and log likelihood is we're trying to maximize the log likelihood, but the operations are broadly the same. So this is the formula for squared error. And so then below that we see the derivative, the partial derivative of the squared error with respect to each of those weights. So you now start to see the parallels between these. So here we have my absolute error term, and I square that to get my root mean squared error. And then here I just have sort of the class relative error term, I'm not sure that's, you don't really call it that, but this is the error term in terms of my probability, and then I take that and subtract myself next. Yes. Is there a reason why we are not creating some of all the classifications of the large partial derivative of the- In this one? Yeah. The partial derivative of that is pretty much the same than- Right, yeah. So what we do here is we're basically taking for, this is going to be some class, specific class K. This is just going to be for a arbitrary class of J. So I'm just trying to compute the partial derivative of Y sub nJ for this particular class, which may not be the actual class of the ground truth class of the output. And so I'm just trying to compute the derivative of the log likelihood with respect to the weights that define this class. Same for the regression. So we can just compare these to the actual likelihood, and we basically see that there's a strong parallel between the regression version and the classification version, just by recasting the error in terms of log likelihood, where now my prediction is basically the softmax distribution over different classes. So the previously derived matrix expressions for neural networks, we can just use those as we have been. All we need to modify is the output calculation. So this is squared error, right? So standard first two lines are my kind of standard neural network operations. So I take my inputs times hidden layer weights, apply some activation function, take the output of that times the output layer weights, it gives me Y. So now I have all my T's minus all my Y's. I'll use that to compute my error term. And then the derivative of the gradient of the hidden layer weights is going to be the error term times the weights. Then I'm going to multiply that by the derivative of the activation function from that layer, then multiply that by the input. And then the gradient for the output layer weights is basically much more straightforward. All I have to be concerned about is the actual error. So now the changes needed for nonlinear logistic regression follows. So T, I is going to be the indicator variable version of T. So remember, this is going to be some n by k matrix where there are all these one hot vectors. So the first two things are identical. So take inputs X, multiply them by hidden layers, take an activation of that, then take the result of that, multiply them by the output layer. This gives me some scalar prediction. So far the same. So now what I do is to exponentiate that scalar prediction. This is going to give me some other scalar value. And then I just sum across all the columns and then divide by that sum. So these three lines, that didn't work out well. F, S, and G, that's the softmax calculation. So I take the exponentiation and I divide that by the sum of all the exponentiations. And so then the log likelihood is going to be the sum of the indicator variables times the log of the softmax. And now these two, or these two lines that are partially highlighted are effectively the same as above. The only difference here is instead of the scalar prediction Y, I have a softmax prediction G that is a probability distribution. And I still use the same activation function derivative to calculate the gradient. And then the only difference with the output layer is I just have my softmax probabilities and subtract those from the indicator variable. So yes. Just in the very last layer. So you can think of it as like neural network is proceeding as normal. And then at the very end, I just decide, well, I don't want a continuous value. I actually want a probability distribution. So I perform this softmax operation that turns that into a probability distribution. So now what that means is that the error term is literally just, it's still a distance metric. It's just now for every, the ground truth for every indicator variable is basically going to be a bunch of zeros and a one. Think of that as probabilities instead. It's basically 0% except in one case where it's 100% because we know this is the class for that sample. And so now I'm just trying to maximize or trying to minimize the distance between my predictions and that ground truth. And that only makes sense if I'm also predicting probabilities. I need to have some functions going to turn my output into a probability. When you graduate, they will call you begging for money. Okay. So how do we do this for two dimensional data? So let's just try to create some two dimensional data and then try to separate the distinct segments using a nonlinear logistic regression. So I'm going to use kind of a similar example to this one. So if you remember from when we did regular activation, I kind of had this chart where we had these sort of two curves. And I gave it as an example where you can combine some nonlinear activation functions to fit a curve to it. So if you get a tanh function, you kind of want something that rises and then peaks and starts to fall. For ReLU, you're going to get a more piecewise curve, but with enough ReLUs, you can try to fit this pretty approximately. So let's do something similar except not trying to fit a curve to this data. I'm trying to fit a curve between sections of data. And this curve would be an example of what kind of function. Decision that's not a bad choice, but wasn't quite what I was going for. But when you make a decision, what are you doing between choices? Getting closer. Like the first five letters are correct. Discrete, yes. We call this a discriminator function. Yeah. So basically what I'm trying to do is I'm trying to find a line that's going to keep as much of the blue dots on one side and as much of the red dots on the other side. So I'm trying to find a line that's going to keep as much of the blue dots on one side and as much of the orange dots on another side. And it's not going to necessarily be perfect, but you can see sort of as I just trace my mouse kind of like that, this might be an example of like a suitable discriminator function. So I'm going to make some two dimensional data this time that has similar properties. So we have this thing, looks like a Tide pod. Don't eat it. And so now obviously just by looking at this, you can tell that like there's no way that I can fit a linear function between these areas. There's just no such linear function unless I add a third dimension where each of these are like distinct along some z-axis or something. And in fact, it would be pretty difficult to just fit an average kind of deformed linear curve to this. What single curve can I draw between regions? It's going to separate all my points, nothing that works very particularly well. So what we'll try to do is we'll try to classify this data using a five hidden unit neural network with nonlinear logistic regression. And my goal here is to basically separate the portions of the Tide pod. All these individually colored points are taking the instances of a different class. And so I need to be able to classify them accordingly. So you can think of this now as clearly a problem that has to happen in multiple dimensions. I've got three classes. Just by looking at the data, we can see that if I'm restricted to working in this plane that you see here, I cannot do this, even using like the most nonlinear function I can compute. So we need to have multi-dimensional data. So what I will do is I will now define a new class called neural network classifier. And I'm going to do this by subclassing the existing neural network class and making relevant changes. So this is where code inheritance becomes very useful. I'm no longer trying to just change the activation function of something. I'm actually trying to change the purpose of the network from a regressor to a classifier. And that's going to require more than just specifying some arguments. I could do this. I could say like neural network and then pass type classifier, but that it would end up probably duplicating more code that is necessary. When in reality, as we've observed, kind of all I need to do is change what happens at the very end of the network. So I'm going to need to define a softmax function. I'm going to need to define my back propagation functions to use the output of that. So I will import a neural network class. And then what I will do is I'll define an implementation that it's already in this neural networks implementation here. Neural network classifier, the subclasses from neural network. This allows me to specify the input size and then the hidden layer size in this case, just a single hidden unit with five or can layer with five units and then three for the classes. And so now I train, I specify my number of epochs and my learning rate, and then I can plot my outputs and then I can plot my likelihood function and my training over time, et cetera. So let me just run this and it'll take a few seconds to run. Like last time, because I'm subclassing from neural network and I changed the functions necessary to turn this from a regressor into a classifier, I didn't change the print functions. So it's still printing error, but you notice that number goes up when in fact, as we've done before, we're just actually printing the likelihood. So this is the likelihood of the data. I'm getting a pretty good number. And so here is my, this is my training line. So I'm going to use the number. And so here is my, this is my training likelihood plotted versus training epochs. You can start, you can see how we started below 0.7 and end up getting quite close to a 0.95. And here is the data again. And here's what it predicts. So you can see that it's doing a pretty good job. It's fine. It seems to have identified that that outside region belongs to the blue class. And this kind of top region belongs to the red class and the bottom region belongs to the green class. And it's also doing it in a way that is effectively capturing kind of the shapes in this data. Not too bad. Obviously it's going to make some mistakes like here. These green samples seem to be predicted as members of the red class, but overall it's probably doing a pretty good job. And then there's some white space here that is just sort of filling in somewhat arbitrarily, but it just, it doesn't have data for that. So what we can now do is we can plot this in multiple dimensions to get a better look at it. So this is the distribution, probability distribution for the red class. Right? So you see when I have samples that have these XY coordinates, the probability of being a member of the red class is given by this function here in three dimensions. Similarly for the green class, you can kind of see how this one and this one sort of fit together. Right. And then there's the blue class, which has this big ring on the outside. And then it's the, it descends to zero probability in the center. Okay. So now you can see how with this two dimensional data, we are basically fitting a three dimensional function to model this highly nonlinear class distribution. So if you want to plot the outputs of the hidden units, you can actually see what it is learning. So you can actually see like for a given input, what one unit is going to output. And you'll kind of see how those curves, which also could be visualized in three dimensions when combined, should allow you to predict this probability distribution. Okay. Questions so far? Yes. There is a reason why you just show this one hit in there. Yeah. In this case, this problem is like simple enough that that can solve it. So you can, well, you can't really run this. Let's, you know what, let's just do it. Right. Let's just change the hidden size to just try 10 for now. And let's see if it does better. Right. We can already see that we're doing slightly better at maximizing the log likelihood of the data. So here's my prediction. Maybe it's like a little more symmetric. Okay. And here's that. Right. So same things. We can try, one thing that might be interesting to note here is actually, you see there's a bit of a dip there. It's kind of hard to see, but we've basically got some place where the probability of falling into the green class is not as certain as it is in some other regions, but it's still higher than anything else. So if you look at this part here, it doesn't, this chart doesn't show the probability, but it's probably, it's at least greater than 0.3, 0.33, but maybe it's not that much bigger than that. We can try, you know, making this a bigger network. And this might end up, you know, fitting even better to it. So it looks like it is so far, right. Getting pretty close to fully maximizing the likelihood. So again, this is probably the best fit we're going to get to this data. And then we look at our shapes, right. And this is like close to as exact a fit as you're going to get. And so in this case, that single hidden layer was like just fast enough to train on this machine, even on the CPU. You can have a slightly bigger network and it's, you know, fitting slightly better to the data. So that was just a function that is nonlinear enough to capture this effectively, but also just really fast to run. Okay. Any other questions? Okay. So let's try some actual data. So let's finish the toy problem, move on to some real data. So we have this human activity data given in from accelerometers. So basically, there's a bunch of data where people had some accelerometer, it might've been like a smartphone or a smartwatch or something. And then based on how people are moving over time, the goal is to classify what kind of activity they were participating in. So am I climbing stairs? Am I playing tennis? Am I running? Am I jogging? Am I, you know, just resting? Am I eating dinner? So basically, this, the name of the files, just accelerometers, it sort of obscures what's being done. But basically, based on how I am moving, can you tell what I was doing? So the classes include things like walking, playing with Legos, playing Nintendo Wii, climbing stairs. So X is the motion data and then T are some class labels corresponding to the activities. So what, how do we define class labels for the data? How would you do that? Yeah. Each class can have a different integer value, right? Yeah. You have to encode that in some way. Does it, what's your intuition about, like, does it matter if I put like more similar activities, like numerically closer together or not? Does it, yes? Who thinks yes? Okay. Who thinks no? Okay. So generally, at least for these, for classes, classes problems like this, generally it doesn't matter that much, because you think of them as vectors. The vectors are kind of orthogonal to each other. Say I've got 12 classes, you can think of just like 12 dimensional unit vectors, each is orthogonal to each other. And so basically what it is, is I am, if I'm not doing one thing, I'm doing something else. There's no real overlap. So that is, I can just decide arbitrarily, like walking is zero, playing with Legos is one, eating dinner is six. And it doesn't really matter because the probability distribution, the ways should optimize for that label set. Of course, if I change the label set, then those ways are going to be completely wrong, right? So it's going to be very dependent on the label set that you choose when you train. You have to make sure that you're evaluating against the same label set in the same order. Where this falls apart, of course, is problems where similarity matters in the final output. So in particular, in like language problems, it doesn't make a lot of sense to have the word puppy be equally orthogonal to the word dog as it is to the word truck, right? Because obviously two of these things are much more similar to each other than either of them is the other thing. So in problems like that, you have to have more sophisticated ways of representing your classes. But in this case, we can just assume that they're arbitrarily chosen. So here we have 225,000 samples, each of them has four outputs. And so then data looks like this. So we can see what do these look like inputs and which of these look like outputs? Yes. First column looks like an output. We just talked about how these are integer class labels. So this looks like integers. So I'm going to guess these are the class labels. And then these are continuous values. So it seems like I can take my first column, turn that into my T, and then the remainder will be my inputs. So here's a function to generate k-fold cross validation sets where we talked about cross validation. Anyone remember what that is? If you do, it's going to help you with assignment three. For each of the part of your sample size, you can actually test the model that you created on the right. Yeah. So I'm going to do this for k times. So I'm going to have each time going to hold out a different set and then rotate that through so that each time I'm training on k minus one fold is evaluating on the k-th fold. And this will tell me a decent average picture of how my model can be expected to perform on arbitrary unseen data. So here's a function that does that. I will randomly order x and t, partition them into folds, and then I will basically return each one for x train validate and test. So if I do x dot shape, I end up with this number of samples times three, three dimensions for each one. I'm using this yield keyword at the end. What is that? So this is basically something that suspends execution and then returns the current state back to the caller, but it will retain the state information so I can continue what I left off. So the return keyword will just exit the function entirely at that point. Yield will basically say, here's what I've got right now. I'm going to return you some values, but if you want to keep executing, I'm going to maintain my state information. This is kind of a functional-like operator in Python. So it's sort of like the continuation operator in Haskell, for example, if you know anything about that. Just a demonstration of how that works. I can have some function here that is just a times two function. It's going to return i times two for, you know, in range of i. So now if I turn, if I print out list of times two, it's going to give me i times two for zero through nine. But if I just return the result of the times two function, actually gives me this generator object. So what's that? So the generator object allows me to basically call this next function over it to prompt it to return whatever it's going to yield at the next step. So remember, we're keeping the state information. So it's basically, was the last thing that I returned, and then I'm going to continue where I left off. So if I call, if I return my generator function, it's basically an instance of the output of times two. And so now it actually, to actually get into that, I have to call next over it. So if I call next z the first time, it's going to give me the first item that times two would return, which is zero. Then if I call next z again, same thing, right? I did not change the syntax of this call at all, but now it's returning two, because z is an instance of this function that preserved the state information because I use the yield keyword. So using this in my K4 cross validation sets means that I can then just call next to basically get the next cross validation set. It's going to segment everything just right, return my train val and test partitions. So now the size of these things, you can see I have 75,000 samples by three and then by one. And then these are equally partitioned right now. So I now have a train validation and test partitions, all of equal size partitioned into different folds. So now I'm going to call the NP, we're going to use this unique function. This is going to find the unique elements of an array. So if I run unique over t train and with this return counts keyword, what this is going to tell me is what are the different elements of t train and how many times does each of them exist? So now we can see that I've got 10 classes, they've got labels on them one through 10, and I have roughly 7,500 of each. So this is a decently balanced sample. So there are 10 classes for each class K and then there are K prime instances of that instance in our data. We can also control how many digits after the decimal point are printed for a NumPy array. This will be important for doing the confusion matrix. So I can set my precision to five. And so now if I just divide the counts by the total shape of t train, so basically this is accounts for class divided by the number of samples, we can get the percentage of the data that falls into each class. And so you can see that's roughly 10%. So this is a well-balanced sample. We can do this also for tval and ttest. And we can see we have roughly the same distribution across all of them, which means that for cross validation, we can expect that this result is going to be relatively accurate. So all these steps are very important to do if you are performing an evaluation on an neural network. So let's see where we at right now. So what I will do now is I'm going to construct a neural network classifier where the input layer is of size n, where n is the number of parameters in every input sample. In this case, that's three. At step two, a miracle occurs. And at step three, we get the answer. So what we do here is I'll just define my neural network classifier that we did before. I'll call train on it. All that stuff happens. And then I will just see how long it took and I'll plot my training likelihood versus the training time. Any questions on this while it's training? Yep. So the reason why we're not using one-hot encoding is because we're using the neural network. Is that right? Sorry, I guess I sort of elided this because I didn't show you the neural network classifier code because you have to write that yourself. One of the things that it does that you will have to do is it takes those integers and turns them into one-hot vectors. So you'll see that if we look at the definition of the class, so what's the input size is going to be that second dimension of x dot shape. So in this case, three values for each input. This is the hidden layer size. And then n classes, this is the number of outputs. This is the number of things I want to predict about each sample. In a regression problem, these would be n scalar values pertaining to something about that sample. For a classification problem, the thing I want to predict is for every class, I'm going to predict the probability that it falls into this class. So in this case, this would actually have an output size of 10. So that did that. So it looks like it is pretty, it's decently maximized the training data likelihood. So for classification problems, you want to see the percent of samples that are classified correctly. And then it's also interesting to see which ones are misclassified. So you can see if maybe there are some classes that are being confused for other classes more often than not. So you think of this as basically a grid where you have along either the rows or the columns, you have your predicted classes. And then on the other one, you have your true classes. And so those things along the diagonal, which for a perfect model, you basically have 100% along the diagonal, as we'll see. So this is called a confusion matrix. And so this is just like a table of classification percentages for all the target classes and predicted classes. So in our version, the row is going to be the target class, the columns are the predicted class. This is not universally true. I have seen cases where people have flipped it. So just be careful when reading confusion matrices. This is typical. I believe this is more common. And so then the diagonal is going to show the percentage of samples that are correctly classified for each target class. And you want it to be as close to 100 as possible. Yes. So this training, it seems like it's been a lot longer than what we've seen in the measures before. Is that due to just how many samples we have or is it more due to the neural network? In this case, it's probably due to the number of samples because there are 75,000. So the data here, it's three dimensional data. So yeah, it's like an extra dimension than say the Tide pod example we did before. So it takes a little bit longer there, but mostly it's going to be the number of passes through the data. So you have to pass through the entire data set to 75,000 samples versus, I don't know, a couple of hundred probably for most things we've done before. So you wouldn't expect it to be if the samples were the same sample size. Yeah, no. I mean, if you're dealing, yeah, so the thing about the number of numbers that it has to pass through, it's going to be n by d. So the longer the input dimensionality is, of course, the longer it's going to take to go through each sample. And then the more samples you have, the longer it's going to take to go through the entire data set. Yeah. Okay. So we can then just run the use function and then we'll end up with a basically an n dimensional array showing all the percentages. So this will convert the predictions into percentages. I'll just spend that to table. I'll print the table. Of course, this is not very useful to look at. It's just a bunch of numbers. If you can mentally conceptualize how it's being arranged, you can see there seems to be a good percentage for that first class and maybe a less good percentage for that second class. But let's arrange it in a way that's actually easy to read. So if I put in a pandas data frame, I'll put some headers on it. So now we can have the class names and their indices. So now you can see the class one is rest coloring, Legos, we tennis, we boxing movement at various speeds and climbing stairs. So now we put these into a data frame. I can print this out into a nice grid. It's now you can see by stepping down along the diagonal, which samples are, which classes are well classified and which ones are not. But of course, this is still not like the best way to look at it, right? It'd be really nice if I could get an immediately intuitive grasp of what this is showing you rather than having to analyze all the numbers one by one. So first of all, let's convert things to percentages. So you can look at this tutorial on pandas styling to help you with your data presentation. If I can convert things to percentages, now all of a sudden it is quite a bit easier to read. I'm no longer staring at so many decimals. Now the above function call doesn't save the style and the confusion matrix. So if I run this again, it's going to give me the decimal version. So I can add colored backgrounds to provide a quick visual comparison. Using CMAP equals blues is going to give me this nice, this color scheme where I can immediately look for the dark cells. Now, one thing you'll notice here is that it basically the darkest cell in each column is the darkest one. And then everything is graded relative to that. So what that means is that this one, we tennis, where it's only classified correctly 3.5% or 3.8% of the time, but that is the highest value in this column. So everything's normalized relative to this. So this is the same color as rest being classified correctly 96% of the time. So now we can combine these two styles in an object-oriented fashion. So now if I run this one, now we get the percentages. So one thing that we will want to figure out is like how to normalize this. There's some things you can do in Python to do that. So now I'm going to try a bigger network. I'm not going to run this live because it'll take a long time. So I'm just going to start from here. I'm going to kind of just skip through this. But you can see over a bigger network, I now have two layers with 100 units and 50 units. This is the data likelihood. So you can see that as we train this bigger unit has some fluctuation in that likelihood. So if I'd stopped training here, maybe it wouldn't perform as better, but eventually it kind of stabilizes. So now if I run this again, pretty confusing matrix, you can now see that at least the correct class is the one that is classified correctly most of the time for all classes. They're not all equally classified correctly. For example, stairs is only correct 30% of the time, whereas rest is still correct. 96% of the time, coloring is 85. So now let's check the validation and the test sets. So the validation percent correct is about 57%. But we see kind of a similar distribution for all the different classes. And this way you can see which ones are commonly confused. And this makes quite a bit of sense. It may be difficult to tell from accelerometer data whether I'm moving at 1.75 meters per second or 2.25. So I'm going to check the validation and the test five. Blah, this is going to depend on my height, my stride length, maybe the type of shoes that I'm wearing. So by looking at a confusion matrix, you can see which classes are commonly confused and maybe consider ways that you might process the data to make that easier. So confusion matrix is kind of the most common way of presenting multi-class evaluation for most machine learning problems. Okay, so then same for the test percent. Basically, we're seeing very similar trends in terms of overall accuracy and also prediction between different classes. So we've got some issues here. So for example, obviously some of the really easy ones like rest and coloring and even Legos, it's getting right, you know, most of the time. Wee boxing, there's probably like a relatively distinct movement pattern that is picked up in the data. But kind of the walking, jogging ones and even climbing stairs, the overall accuracy is just not that great, particularly for like class eight and class nine. So what if there's a different data representation that we could use to represent movement? So we can use this thing called a continuous wavelet transform. And here is some code that's going to apply a CWT to this accelerometer data. I'm not going to go into this, but basically what it's doing is just converting this into a waveform that's going to approximate the frequency and amplitude of the motion. So imagine if I'm walking really fast, right, imagine that accelerometer like hitting the floor or the sidewalk, you know, every time my foot touches down, there could be like a pulse every time I take a step, right? And so the frequency of that pulse is going to be correlated to say like how fast I'm running or whatever my pace is. And then the amplitude is going to be like how hard my foot is hitting the pavement. Whereas if I'm sitting there playing Legos, you know, sitting on the ground, I'm going to get a different pattern of movement that's going to have a different frequency and a different amplitude. This is a useful technique for transforming this raw accelerometer data into something that's maybe a little bit more intuitive. So if you are familiar with CWTs, you can look at this code and apply it maybe to some of your own data. Okay, so if we just look at some of the different properties, basically we can see, you know, we can define a max frequency and then we can see for the different samples, you know, what some of the properties are. Might be easier to look at this in terms of a visual chart. So now what I can do is I can actually plot some of this and you'll only see like three distinct regions here because there's actually 10, but it keeps repeating the same colors and they're kind of all plotted on top of each other. So if we look like real close, you might be able to see some other ones, but not really. So if we now look at the individual samples, luckily they're ordered. We have about 7,500 of each one. So you can see, okay, these are the samples of class one and class two and so on and so on. And so now I can plot the frequency for these three dimensions for each of these. So if you look at instances of like class, what, five, I guess, I think this is like one, two, three, four, five. So this is we boxing, right? And so these, you can just go down below this and you can see the amplitudes for that. So this particular class seems to have quite a bit of distinct frequency and amplitude compared to the other things, right? And now let's look at the ones that got confused, right? So these were sort of fast movement, maybe running or jogging. If you look at these segments here, there's not a lot of difference between them. So it kind of makes sense maybe that these were getting confused. You'll notice that like there is a slight difference, maybe the amplitude in Y for this class is slightly less than that for the next class. So maybe this representation will allow us to tease that out a bit more. So just think about how you would move in terms of doing each of these activities. And that's reflected in the frequency and amplitude of the signal. Okay. So now I'm going to take this representation and I call it my CWT net. I'm not going to train again because it takes like a minute. And then I will run a prediction and then I'll see how well this is doing. So already you can see, if I look at test percent correct, 92% compared to like 58. So clearly this data representation is a good way of representing this for this task. Now look at the confusion matrix. Wow. So really nice numbers there. One thing that is maybe not coming through very closely is coloring is often misclassified as rest, right? Because if you look at it, it makes sense because of if I'm converting my data to frequency and amplitude and maybe it's going off of like the accelerometer data from my phone in my pocket, there's no real distinction in frequency or amplitude of motion associated with coloring versus rest that would be picked up by an accelerometer. And that CWT transformation may be squeezing out data that was useful for that. So we had some cases where like coloring, that percentage actually goes down right now. It's like 50 something before it was 64, right? So maybe we lost some information here. Okay. So that was classification. Basically key points. Everything gets converted to a probability distribution, but when using neural networks, that all happens at that very end of the operation. But also the way you represent your data, it can make a big difference. And this is true for all kinds of neural networks from just how you're encoding the classes to how the inputs are represented. And so you have to think pretty hard about how you want to represent your data for a neural network classification operation. Okay. Questions. Okay. So before we go, let me go through assignment three. So the purpose of this assignment is to build an implementation of a neural network in PyTorch. So you're going to be doing a lot of the similar things that you did to assignment two, but you're going to be doing it the PyTorch way. So what you will do is once you've done that, you're then going to conduct some training experiments on some data, and you're going to be doing this using cross-validation. So notebooks, you're going to want to look at eight for the PyTorch stuff. The cross-validation notebook whose number I don't recall, I think maybe it's seven, though it might be nine. And that should cover most of it, I think, so basically you're going to want to review PyTorch implementations and cross-validation. So we have this N net class. What you're going to need to do is you're going to complete the train and use functions the PyTorch way. So you cannot just copy and paste your A2 code, although the principles remain the same. So basically you've done it in NumPy, but NumPy is limited by the CPU. So let's move to PyTorch so that you can make use of the GPU for more substantial operations. So you're given definitions of, say, the activation functions. You can do tan h array that's done for you. What you need to do is you need to complete the train. So you need to calculate the forward pass, calculate the mean squared error, take the gradient. Remember in the PyTorch method, most of all of these things just happen in single lines. So you'll find, if you find yourself working out the math for things, you're probably on the wrong track. You've done that already. So now you need to do this using PyTorch's built-in functions. So what you need to be careful are things like zeroing the gradient, making sure that you're detaching things from the computation graph, et cetera. Same for the use function. So same operations, right? You need to standardize x through the forward pass and un-standardize, but again, you're going to need to do this the PyTorch way. This thing here, if you run into errors about the computation graph, just review that line, save you a lot of trouble. So for an example, we'll give some data like we did before, run it through your implementation of NNET, and then calculate the RMSE. If you do it correctly, your plot should look like this. So then you then need to perform experiments over actual data after performing stratified cross-validation. So we give you the complete code for generate k-fold cross-validation. So you don't need to change anything with that. And then here's some example data using just some dummy data using this function. So you can compare your outputs to this. And now you need to train, create a function that will train the neural nets and then average the RMSE over all the ways of partitioning. So you have to do all your cross-validation, take the average, and then report that. So you have to find this function that's going to do things like define an instance of neural net and then generate k-fold cross-validation sets, and then retrieve the output and report it. So basically, this is the same as you did in assignment two, except you're calling the cross-validation function and you need to call your PyTorch neural net instead of the NumPy one. So the application is going to be to this airfoil data. You can go to this website and download it. And then it gives you these parameters like frequency, angle, chord, pressure. So you need to apply your run k-fold cross-validation function to the airfoil data. X will be the first five columns and T will be the last column pressure. So basically, trying to predict pressure using these other five things. And below, you'll find an example run over some real data. So you need to, again, collect your outputs into a data frame where you report the architecture and the RMSE for train, val, and test. And then you can plot the results. Okay. So this is a much more coding-heavy assignment. So you'll score most of the credit if the train use and run k-fold cross-validation functions are defined correctly. And then you can test this with the greater, same as before. So unlike this one, you need to complete this one individually. So A2 in the final project, the only thing you can work with a partner on. You can earn 10 extra credit points in this assignment. So there's significant advantage to doing that. One is to add a keyword argument that will allow you to use the GPU. You have free access to any of the CS machines. Then if you need to use those, if you don't have a GPU in your own. And then also you can find another data set and apply this to that data set and report on your conclusions. So you will get five points for doing one and five points for doing two. You can elect to do one, but not the other, if you have the time or lack thereof, or you can do both. And this is due March 21st. All right. Questions? Okay. All right. We'll see you Thursday.