 All right, let's get started guys please. Right, sorry about the delay I couldn't. My computer is not connecting to the Wi Fi for some reasons I can log into zoom. So, humans behind schedule. Just a reminder for attendance if I grant you a leave for like if you're if you're usually attending person. If I say it's okay to attend online for a specific class that is for that session only right if you want to attend, if you're, if you continue to be sick for example or whatever. Let me know that you're still sick so I do not expect you back. Right. But if I said it was okay on Tuesday. I need to know that you still plan to attend remotely on Thursday, and so on. All right, so today I will finish up the linear regression notebook. And then what I'm going to do is I'm actually going to assign the first assignment, and then go into notebook for because pretty much what you need to know in the assignments occurs in notebook three. Let me share my screen first. So, I guess, before I begin. Are there any questions anybody want to review anything from notebook three, the last part of this for the first 1015 minutes maybe of class. So many questions about the content so far. Okay. No. Oh, and also, so sorry I did the tutorial on Jupiter notebooks. So hopefully those of you who needed were there. It is recorded, and we will put the recording up on canvas in case you need to review any of the material. I also have a version from last year's TA that I think covered more or less the same thing but maybe presented slightly differently or may cover some different things so you'll basically get two takes on how to use the department machines. If you care to, if you care to view them. So, the expectation at this point now is that you are at least conversant and logging onto the department machines and you know testing your notebooks and stuff like that and making use of the resources there. So in particular, let's say you don't have a GPU, the work that requires that you can log on to, you know, some of the department machines. I will let me get there I will send around a list of GPU enabled department machines because not all of them have GPUs, but you should have access to be some of the do. Okay, so if there are no questions about the material so far just to recap we're just basically solving an optimization function with multiple inputs using matrix multiplications right and the way we do this is through stochastic gradient descent effectively So, you're trying to figure out with a set of weights that are arbitrarily poorly or well optimized at any point. For a sample you take that sample multiply it by the weights, and that gives you an output, and you measure the incorrectness of that output compared to some ground truth label, and then that error is used to update the weights in the correct direction. So, the descent of the gradient to the gradient here is basically this is a high dimensional derivative so just a slope in multiple dimensions. If it's easier to think about it this way you can just think of it as basically just a slope in three dimensions and you have basically two components of the derivative. And so I'm trying to minimize my position on that gradient, right so I'm trying to find as close to the global minimum of the gradient as I can. I don't know which way to go along the gradient but I do know where I just came from. So it's sort of like, instead of looking down a hill trying to see where the descent the great the direction of steepest descent is actually looking up the hill, seeing where the steepest direction I just came from was and walking backwards. Right and then I can see this should get me closer to the minimum how close did actually get me to the minimum and I am I going in the right direction, right am I moving along the right dimension. So previously, we did examples where we have an arbitrary number of inputs. So remember for every sample if you have an n by D matrix representing your inputs, you have n samples those the rows, you have D dimensions to each row that is the number of things you measure about that sample. Right. And for the moment, we'll just consider everything to just be tabular numerical data. So this is how we can solve for, you know, D inputs to predict one output. So if I want to predict the miles per gallon of a car from a bunch of different, a bunch of a bunch of different other parameters, I can do so, we can then also do things like figure out which of those other inputs are most important for predicting the that output and which of them are positively or inversely correlated with that just by looking at the weights in a linear model. So if you can turn something into a linear problem, it makes it much more explainable because then you can see exactly which input is correlated with the output. When we move to nonlinear problems neural networks this becomes difficult to impossible because of the number of operations that have to be should be performed at every step. But for the moment, this is a neat technique that you can use to basically see what's most predictive about my model. So the cool thing is now that if I have all of these input parameters, if all these inputs, I should really stop saying parameters because that means I have to wait. So I've all these inputs, I can use the inputs to try and predict any of the other values. Right. So for example, we were using these values to predict miles per gallon, but some of these values may also be correlated with each other. And some of them may be predictive of other things. So this allows us to have multiple target components. And so what I can do now is I can do things like use the values that I have for every sample to predict more than one thing about that sample. So, and what this means then is that we can basically use the exact same matrix operations and all you have to do is adjust for the size of your inputs and the size of your outputs. So if we regard T the targets as an n by one matrix of output values one per sample, all I need to do is add an additional column. So in this case, if I want to predict miles per gallon and horsepower, then every row will still represent the input values pertaining to each sample of car and the output values now be two columns, which are n by two where the two values are miles per gallon horsepower and n is the number of samples. So all we need to do is add those additional columns of target values. So now the T will become an n by k matrix where k is the number of things that you want to predict. So D is still the number of things that I measure about each sample, k is the number of things I want to predict. It doesn't matter if D or k is one or more. Right. It's you can still solve this using a multi input or multi output linear operation. So the operations are fundamentally the same in that you assemble the data for predicting miles per gallon horsepower. So let's just make sure that I didn't clear the kernel. So this is just going to pick up where we left off last time. So if you look at, if you recall these indices, so these numbers refer to indices in the data frame where the target data in this case zero and three, those are the columns that correspond to miles per gallon horsepower. So you arrange your data and pandas, you just look at your data and say, okay, these are the columns that I want to select for the input, the output, and you can just slice through the data set accordingly. We already established the names of each of these so I can print things nicely. So I will, as long as I have the order that I want my targets to be in, I'll just, you know, pluck out those those names from the names variable that I previously created and then do the same thing with the X names for those values I'm still leaving as inputs. So now that I have my entire data, I'm going to take columns one, two, four, five, six, and seven to be my inputs and columns zero and three to be my targets. So if I print these. Now, my shape, the shapes, my inputs and outputs are slightly different. So it used to be 392 by seven. Now it's 392 by six, of course, because if I take something out of the input and put in the output, you know, I can't stay in the input so I can't be predicting the horsepower from the horsepower. That would be too easy. So now my inputs are 392 by six and my outputs are 392 by two, and we can just Sandy check and make sure that we've got the right values by printing the names for each column so now using cylinders displacement weight acceleration year and origin, I'm going to try and predict miles per gallon horsepower. So, we talked about splitting into train and test so of course, just like you can have horsepower in both your input and your output. I don't want to be testing on the same samples that I trained on, because I'm going to overfit to that data, right and there's going to be no guarantee that the model that I train on this is going to perform well on other data that I haven't seen. So what I do is I approximate this by splitting this into a training test set with the assumption that the train the train set and the test are going to at least more or less resemble each other. So, what this means that I, if I use, if I train this, this model on cars on car data predicting miles per gallon horsepower. It's not going to do me any good, trying to predict whether right because even if I set up the matrices in the right shape. So, if I use the same number of inputs and same number of outputs, those inputs mean entirely different things as outputs mean entirely different things and unless there is this website called like spurious correlations that you can like check out, you know, number of like, I don't know, number of RSV infections compared to number of jaywalking fatalities or something and you find like that there's actually a relationship but it's completely arbitrary. So that aside, unless you find a case like that, you're not going to have a case where the inputs are predictive of the numbers, looking at the numerical values because they mean entirely different things. And so we'll see in the assignment will be working with weather data and you can see exactly you know how those values are being used there. But having done that, let's see, I'll just split my my train and test samples I'll use the same 8020 split that I used before so I have 314 training samples, 70 testing samples. So they are the are six and two elements for each sample respectively. So we talked about standardization. I want to standardize that I am not subject to the range of the values where I have one parameter one one input that has a really large range. Don't even remember why we don't want these really large ranges. Yes. So you saw that graph where like the the weight ranges in the thousands and other all the values are in like the hundreds or less. So first of all, it makes it really difficult to interpret. And, and if I'm trying to look at what the relationships are between different values that becomes difficult to do when I'm performing the gradient descent update doesn't even remember why a large values in your inputs are not desirable. Yeah. Right, so if I have, if I'm just trying to optimize you know linear function in one dimension I effectively have some sort of quadratic curve, where we know that there should be a global minimum somewhere. And I'm taking these little steps down the gradient approaching that curve. But remember the steps are calculated not only by the error, right, the distance between your output and your target but you also multiply that by the input that produce that output. And so if those values are really big. You're also going to be taking that step that error, scale it by the learning rate or some small value but then you're going to scale it up again by like 3000 or something. And so if you're close to that gradient you could basically just jump over to the other side. Right. And so you will never approach the minimum that will allow you to optimize the function. So, this does a number of things one it makes it difficult to converge, as we saw, when we're not standardizing if you remember the lowest our root mean squared error value got was about nine for this data, whereas after we standardized it got closer to three in the same amount or less training. And so one thing that we want to do is want to basically restrict to the ranges of our values to between, you know, zero and one or between some known, known constraints. So one thing that I will do then is I'll calculate the means and the standard deviations, and then I can standardize my value by by taking the raw values retracting the mean and then taking the quantity divided by the standard deviation. Key point, I only want to standardize using values means and standard deviations to calculate it from the training data. Why. Yeah. Right, I could have something in the testing data, let's say, let's take the weight of the car. And I have ranges between 2000 and 5000 pounds and I standardized all my weights and I get this nice distribution. And then in my testing data. I, one of my samples is like a semi truck or something and it's like okay this is now 30,000 pounds or something like that. You don't want to standardize the range of your testing data such that the upper bound of your, of your, your standardized ranges correspond to 30,000 in your testing data and only 5000 in your training data, because then for that for that value for that input. That 5000 pound sample is effectively can be treated similarly to that 30,000 pound sample but in fact, that much larger value that much larger rate value has some implication for the output value you're trying to predict. You standardized using your train your train date training data and use the same means of standard deviations to standardize your test data mean that any outliers in the test data will appear as outliers once standardized respective to the training data so if I were to combine all my data, it would look approximately the same. So we do that here, we insert the concept column of ones of course that's to insert that that coefficient that can be we can tune this bias weight to, and then similarly we add bias to our, to our names we can interpret it later. So now I have my 314 by seven and 314 by two training data. So now I can train this linear model to predict both miles per gallon horsepower. So, one thing I will do here is effectively this is the same function as, as we did before. All I need to do is change here is now I have my way to realization has two columns, because I need to, when I multiply by these weights, I need to get two columns out. So now this is going to be an inputs by two. And so then training is pretty much business as usual, I will take the predicted value and compute the error and then I'm going to update that using a friendship fraction the negative derivative of the square with respect to the weights. So now I can take the back of my squared error some so I can report it. And then I will print it out so let's run this. And you can see that we now can within about 50 training epochs, we end up with an RMSC of about 6.11. So, remember what the, when we were only predicting one value what the final RMSC was after 50 epochs, approximately nine, that was the first time before standardization after standardization it was about three something like that. So this is also standardized now the comparison we want to make is RMSC after 50 epochs of 6.1 when predicting two values RMSC of three point something when predicting one value that intuitively makes sense to you. Okay. It's, it's double I'm predicting two things when we expect the RMSC value necessarily to be to scale linearly. Not really. But it does make sense it's bigger just intuitively because I'm predicting two things I'm predicting two things using how many things. So, previously I was predicting one thing using seven things right so it may be that effectively, if I take horsepower out of my input. It might be correlated with miles per gallon so it's somehow a good predictor of miles per gallon in some way. I take that out of the input so it's less able to predict miles per gallon because it doesn't have this one key indicator right so I'm predicting more things or fewer things. So this is the reason it would take longer to converge so I might try training for longer I could conceivably approach a similar RMSC value, but also we may find that it sort of bottoms out somewhere, and we never quite get there so this is just pretty intuitive kind of data science if I have fewer inputs trying to predict more outputs it's going to be a noisier model. It might still be quite good but it's generally going to be noisier. So, now we can see that the, I put the weights here. And we can see that I've got two columns here. So right and I've got now it's a two by seven. And so we have the the bias weights up at the top and then the weights associated with every input below. And so we can, we can see a couple of things by just examining these weights so if we look at the the weights for the miles per gallon target that's going to be the first column. And then we can print the weight to the horsepower target that's going to be the second column. So, take a look at this and see what what you observe. So what things are positively correlated with miles per gallon. So, there is right we saw that you know there's a clear because the way the data is ordering is a clear correlation. And we have, you know, miles per gallon typically got better over over the year. Anything else origin, right, yeah there's there was this slight origin where because because we had American cars at index zero European cars and then Japanese cars, they happen to be ordered in a way that was kind of friendly to British miles per gallon. So, what things are positively correlated with horsepower weight. Right. So we need more horsepower typically to move a heavier car for example. What else. Yeah, so acceleration is pretty strongly negatively correlated right whereas displacement is positively correlated to about the same, about the same degree. And then the other side cylinders you know more cylinders more horsepower. So you know, just think about what we know about cars, some of these things make sense. Let's take a look at the biases right the bias is what your whispers speak up. It's the y intercept, it's this it's it's the, it's the some access intercept depending on how many, how many inputs you've got but it's the thing that I will default to if I have no other information right. So, we just look at these values. Let's pretend all the for we have some sample where every other value is zero. 18 seems like kind of a reasonable value for for miles per gallon it falls within that range of known miles per gallon values. Same for horsepower right 83 horsepower is like not very much but it's a reasonable value for horsepower. And so if I don't have any other information. I'm not as good as starting place as any 83 for horsepower is a better starting place than 18 for horsepower thinking about cars that were built in the 70s or the 80s. Right, so just when you're doing these linear problems. Your intuition kind of plays a lot of a big role in to serve, looking at your errors, and this becomes more and more difficult as we explore more, more complicated problems. And so the operation of optimization kind of remains the same. So, this foundation of linear regression is going to be the set of operations that we will basically keep keep modifying slightly each time as we get more and more complicated until we have arrived at things like neural networks and reinforcement learning so just kind of keep that in mind. And so the operation of inputs times weights equals outputs is going to be the core of pretty much everything that we do in this class. So now let's use our model to predict both of these values so I want to predict. Take for some my testing data predict miles per gallon horsepower. So my prediction shape should be 78 samples by two, and I will plot these actual versus predicted for both of these. So here are my two charts. This is predicted miles per gallon, and this is predicted horsepower. So, one thing we observed is that it didn't converge quite as well as the the single model predict the one value. So, if I remember to think back to I can scroll up the notebook I want to see it to what that plot looked like we were trying to predict just the miles per gallon. What do you remember about that compared to these how to how do these do compared to your memory of that, that plot standardized one after we standardized. So that line after we standardized that line is like, more or less dead on right it kind of cut nicely through those data points were here, we see both of them are a little bit on the low side right this also attests to the increased difficulty of optimizing this, this function, when I have only six inputs random predict two outputs, and because of the nature of horsepower versus miles per gallon. We were basically removing some information from each classification by taking that information out of the input. Yeah. Generally speaking, yes, it depends on the nature of the inputs, it could be that like it could be different use case where you might have three inputs that are just very information rich for some reason and really good at predicting those four things. There is like a strong correlation between those three inputs and those four outputs. And so in that case you wouldn't really see something like this, but generally speaking we have sort of a bunch of data that you've gathered for a bunch of different samples. Yeah. Yes, right. Yeah, so there could be like two columns in this data that are not really correlated right I could maybe, you know, acceleration is not maybe a strong predictor of miles per gallon or something, or it's at least a weak one. So maybe I could remove acceleration and predict miles per gallon acceleration at the same time it wouldn't see as much penalty, right, you can experiment with this and see. Yes. Are the non informative inputs hurting the results. Are the non informative in supporting the results. You, you could empirically verify this by removing those things that have low weights and see if you get a higher correlation with the other weights, or with the other inputs. Usually the non informative ones don't affect it very much that's why they're not informative so this these weights that are close to zero. Effectively what it's saying is that this thing given the data that I have it's not a strong predictor of this one way the other if I remove it I wouldn't see a whole lot of effect. But you might, right, you might see that actually this is just noisier. The reason that is the reason that it's poorly correlated is basically if I plot my out my desired output versus this input I just see things that are all over the place. And then I'm just adding noise in which case, removing that input might actually increase your performance. Okay, so what you'll find is that like machine learning is just like a really empirical field and a lot of times the answer to. Does this happen if I do this is do the experiment and find out and this may be some people doing really deep theory of machine learning they can have some like proof of this. But in most cases which you're going to find is we did an experiment we change the following conditions and the output was this and therefore we've evidence just that this is the case or not the case. And so likewise this is going to be a very experimentally driven course you're not going to be doing any mathematical proofs. So, if you're worried about that don't worry if you're really hoping to do that I am sorry. So let's see quantitatively how well we did in terms of the root mean squared error so in this case, what we get is for the training data, I will have for remember we get error values for both of those outputs. And so, for the miles per gallon I have a root mean squared error over the training data of six point six point odd. And for the horsepower it's 25.6 and then for the testing data. It is also quite similar right we see numbers that are really close. This tells me a couple of things one is that my training data is a good does resemble my testing data in a reasonable sense. And also, we can see that you know maybe in in raw in terms of raw value. It's harder to predict horsepower, then, then miles per gallon. So, we can just put this out in a more friendly form here they are again for review. What is this star doing here. Anyone know this particular intricacy of Python. Yeah. Right yeah so I've got you know to two arguments in this list, rather than having to just iterate through this list and print them out one by one. I can shove it all into a single line right and just split this out. So that these. This is basically being treated as an art as a list argument into the format statement. And so then it's being is generate the output, just nicely. So, this is just a little demonstration of this if I have this function foo, or I have three arguments So if I define a list that is 123. Okay, there's three things here this function needs three things. This should work. But it, of course, it doesn't because really this function is expecting three separate arguments, and this has a single argument that happens to be in three elements. But if I put the star here, then I can break this down into the three arguments and it'll that function will work. So, that was an aside, but it looks like we have a bigger error for horsepower, right, in our in our model. And so maybe you know one way of thinking about this is maybe this is due to the larger range of horsepower right so miles per gallon is pretty constrained between 13 and 25 or something horsepower is bigger, the values are bigger but also the range is larger. So, if I just put the min and max of the each target in the testing data, then I get something like this, right so for the, for the miles per gallon I have a min of 11 actually max of 48. Whereas for the horsepower I have a min of 44.3 and a max of 255. Yeah. So, this is going to be after. So, what this will do is this is going to be before normalization where I've unstandardized the outputs. So these are these are the actual these are the actual values and these are the raw, the raw data. Okay. Any questions about this content or the, basically the previous two days worth of content on linear regression. All right, so let me then go into the assignment. So, this was in the way. Okay, so the assignment, if you go to the first assignment, someone linear regression with SGD. You, first of all, it directs you to download the a one notebook from the public facing page so just as a note all the assignments are going to be posted here. If you click on this, you get this, you can download it using this. If you click on this often it's going to happen is you get a bunch of JSON, just save this as in the dot IP Y and V format and open it up in Jupiter notebooks and it'll let you actually get the interface. Remember, you need to basically install Jupiter and then start the Jupiter client through the, through the command line, and then you're getting going to get this window that I've got here. And this will turn this into this. So, those of you who have accommodations through STC these are already put in. So, for those of you who got those canvas handles this automatically so the late penalty. You remember 1010 points off every day that's late. This is factoring in any blanket accommodation that you may have, you know, it's the submission box closes 10 days after the assignment is due because if you submitted 10 days late doesn't matter if you did perfect it's still zero. So, I will just advise you that it is strongly to your advantage to get things in on time even if they're incomplete. So, for various reasons pertaining to how I calculate your grades. It's really better for you. If you get a 70% assignment in by the due date, rather than you know an 80% assignment in two days after with the late penalty. So that would just something to motivate you to manage your time. Of course things happen if you do need an extension reminder to let me know 24 hours before the due date. You're going to get a automated grader I'll talk about how to use that this downloaded here. Question. That was it okay. Yes it's a one grader zip, I'll show you how to how to use that in a minute. And then your regression of SGD. So, you're given some starter code that has the sort of fill in the blanks bits here. And so you're going to need to fill in the train use and RMSE functions and then apply them to some other data so basically the, the skeleton So this is given for you, we give you the inputs. We do part of this like the the printouts and things like this and the returns for you but what you need to do then is fill in each of these steps. Everything that you have to do here is contained in notebook three in some form which are going to need to do is look through, find that piece and maybe change the code slightly to accommodate either the way the data is formatted, or what we're asking you to do here. So you're going to need to have a list of all assignments in this class. And the steps are effectively written out for you in sufficient detail you just need to, you know, figure out how to implement these steps. So, the model is going to take in the inputs, the targets, learning rate, the number of epochs and then an optional argument verbose. And so that's just going to be, you know, whether or not you trade you reprint out during the training process. So you take your, basically at the end of train right you return this model used to take in that model and then for some input x predict what the value is according to your train model. So the shapes of the each component are given to you. And then RMS E is similar. You take in a matrix of predictions and a matrix of target values, and then you return the root mean squared error between those predictions and those targets. So, generally you the assignments are going to be split between coding and discussion. So, in this case you're going to get 60 points for writing the code. And then the remainder is going to be experimentation and discussion. So here's the train right calculated means standard deviations. We've done all this in some form, use function, and then RMS see right this I don't really need to spell that out in the code. So what you're going to be given then is basically some simple dummy data that you can verify whether or not you're at your limitations are correct. So here's a simple example. If we just have, you know, some, some function that generates x and t, then you can verify whether or not you're your implementation produces the same result. So, in a working train implementation or I'm not going to run this because train isn't complete break, but a working train implementation would give you a result like this. So, a working train implementation should look like this. So a strategy you might use is basically copy this so that you can refer back to it see whether your results look the same both qualitatively and then quantitatively by containing by looking at the numbers, and then the automated radar will run a set of unit tests for you. So, you know, here's just the plotting for for these tests. Then the real data that you're going to be working with is this weather data come coming from the CSU weather station. So this is the data file here it's just a text file right now so just save this and it looks pretty messy. Of course, so the first thing you're going to need to do is loaded up in pandas. So you get five points for reading it into pandas, and then check for missing values. Remove samples that contain missing guys so pretty standard data cleanup. Once you've done that then you need to create a linear model that will take the data here that you have like you know days, it's a year's worth of data, you have the days of the summums and things about like the wind the barometric pressure and stuff like that. Your goal then is to predict the next day's average temperature. If you look in this data set you will notice there is no column saying next day's average temperature. Right, so you have to manipulate the data set a little bit to extract the next day's average temperature. It is a year's worth of data so you will be able to do this for almost the full year. So this is going to be one, one sample that you will not be able to predict that for. So, you just focus on like some of these features, you, there are other features in there if you find them to be useful you can do that experimentation and discuss it. So if you focus on like these features, then what you're going to need to do is you need to modify the data file to add this next T av. And so you know here's a here's how you can, you know, hint is it is a hint on the naming convention you can use. So then you select these eight columns up here from the data frame assign them to an umpire. So you assign X to those columns for all but the last row, there are reasons for that. And then assign T to be the first column for all but the first sample. And so now the first sample is associated with the first row in X. And then that's the, the T, the T av for the following day. So use the train function to train a model over X and then and the T data and then try different combinations of learning rates and number of epochs. Use the use function to predict and make plots of the, of the, the target versus the predictions, and then do some discussion of your observations, and then you can use the value of RMSE as your metric here. So for a linear problem RMSE is a decent metric for other types of problems. It tends not to be all that useful. Then do some discussion about like which, which inputs are most predictive of the desired output. So we just did that kind of sort and print the weights. And then you can discuss what's positively correlated with inversely correlated. Okay, so this automated grader will allow you to effectively make sure that you get the 60 points for the code. By passing the unit test that will be enough, and then the remainder is just running what should be functional code and discussing your results. So, if you know how to run, you know, the, the Python script in Jupyter Notebook. Basically, if you download this, you'll extract a one grader dot py make sure it's in the same folder as the notebook and make sure what I recommend doing is really just putting them alone together in the same folder. So don't have any other documents in there. It's looking for a Jupyter Notebook that has the name formatted in a particular way so if you have like two copies in there it's going to be a little confused. So run this, run the cell, and it will basically print out you know the outputs of your unit tests and what the score is for each of those. So we're going to use a similar grading script. That's basically the same things which is different specific values right so that you can't get away with just hard coding the answer in or something. We will look at your code of course right so we're not going to just take the the auto grader on blind faith. So, you know, don't you, you look at the automated grader code to understand how your inputs and outputs are supposed to be formatted. Don't look at the auto grader code to see what the answer did unit test is and hard code that into the notebook right so you're provided this, you should look at it but you need to be using it in the appropriate way. Finally, you can get one point of extra credit on this assignment so this assignment is worth 100 points so if you get the extra point you get 101%. Problem is that if you think about weather data, the previous day's temperature is probably pretty close to the next day's temperature. There have been times like shortly after we moved here we had that one day where it was like 30 or 90 degrees one day with like a forest fire and then 32 degrees next day with snow. So that was an outlier in most cases right it's 30 something degrees today and it's going to be kind of similar to that tomorrow. In a time series the best solution may often be to just predict the previous solution. And that is the predicted value in this case will look a lot like the TF shifted to one time step. So to do better you can try to predict the change. So rather than the raw value predict change where your target is now the next day's temperature, minus the current day's temperature. So if you've set up your target data like this run the previous experiments no need to train to change the training function just retrain the model, then you can do other experiments with different hyper parameters, and then report on your results. Okay, any questions on this. Okay. So this is going to be due February 9, and this, you have an extension already. And so I recommend that you start early. Yes. Yeah, so you'll get, you'll get a number out of the, of the unit test that you pass you get like x out of 60 for the code. So basically what you'll get is you'll get a number for the code number for the discussion. Usually, these try to say you know great work or something if you get 100% basically means there are no, no glaring errors. If you notice something interesting you know sometimes I'll comment on it like you had a particularly good discussion. I will mention you know, for these points in the discussion. If you didn't do something and then how much you were deducted for that. So, we try to get things back within a week, although there are like 75 of you, and only one TA, so we asked for your patience, sorry particularly asked for your patience. Our goal is to get things back within a week after that for the deadline, give or take a day or two, but sometimes there are unforeseen circumstances. Okay, questions, comments. Yeah, you want to not change, not add any cells to the assignment. If you, if you do like a bunch of experiments you want to like show the results, you can just say you know, run train with this number of epochs in this learning rate or something, you can do that. You cannot change the skeleton code that you were given because the automated graders expect things to be formatted in a certain way you may break it. Yeah. Future assignments will kind of follow the same model of like you will look at the last week's lectures while we're doing it or will they kind of. They might separate a little bit depending on how things go but it's not it's, you will look at previous lectures within the last two weeks typically. So for example, if you look at the calendar assignment. So like assignment to mostly going to be contained within notebook five assignment. Is, and I'll just give you notice like for is the one that tends to tends to be the most difficult. That involves putting these together from a couple of different notebooks like 910 and 12. So, things get a bit more complicated as time goes on but generally yes. Okay. Yeah. Yes, I thought I put that in the notebook. So, oh yeah so you need to name your notebook has last name dash a one. So, in will download everything through canvas and rename everything is like, I don't care if two of you have the same last name that's not going to be an issue. So yeah, just do that. Okay. So let's move on to sort of starting with non linearity they're not in the way that we're necessarily going to do it for the rest of the course. So these models have been linear in the parameters and linear in the input features. So this is going to be not very useful for things where there's a nonlinear relationship between the input and the output. And of course, there are many things where this is the case. I think a linear model is like a really good first step for many things but it's often not the best model for most things. So, one thing you can do is basically screw around with the inputs by applying some function to them before they even go into your model to make the relationship nonlinear effectively what I'm trying to do is I'm trying to see if I have a linear model that's the only thing that I've got. But I suspect that there's a nonlinear relationship between some featuring the output. Can I do something to that feature to effectively make the relationship linear. Right. So, for example, if I can, if I observe the data is observing the data that does like a squared relationship between some input and the output. I can already tell that linear model is not going to do me very good but if I could just if I just squared the input, suddenly I would be able to do something with a linear model. So, with fixed nonlinear features you have a way of introducing nonlinearity as long as you know what kind of nonlinearity you're trying to introduce. So let's say you've got a single feature for each sample, and your data matrix will look just like this x zero through x n. You can try a bunch of other features like by by squaring it, or taking the cube or taking the fourth power. And now I have a bunch of other basically features that are that are derived from that first column that might be helpful. And this is simple to do in Python just using the H stack function. But we don't really know which of these powers of x is going to be useful. We can look at the magnitudes of the weights, right, we may find that maybe this fourth feature here is highly correlated with the output and that might tell us something. And as long as the input features are standardized we can do this but we can actually do some more things. So for example if you can actually build multiple models, we call bootstrap samples of the training data by trying different feature functions over these things. And then we can compute confidence integrals of the weight values. So, if a zero is not included in the range of weight values specified by the 90% lower and upper confidence limit, then we can say that you're 90% certain that the value of this weight is not zero. If the range does include zero then the corresponding feature is probably not one that's all that useful. So what is a bootstrap sample. A bootstrap sample is a smaller sample that's called bootstrap from a larger sample. This is a type of resampling where you take large numbers of smaller samples repeatedly from the single original sample with replacement. So here's a more in-depth explainer of that. Below is some code that will illustrate this process. So we will include this value lambda, which is going to be a penalty on weight magnitudes. We don't want things that are like overwhelmingly correlated or anti-correlated with this. We want to find things that are predictive, but not like hugely so. So, let me import NumPy and Pyplot, import random. So here's a train function. This uses the least squares function. So this is not the answer to assignment one. So if you try this, it's not going to work. And the use function is also not the answer directly to assignment one. Root mean squared error. This is very close to the answer, but I don't really feel that bad about giving away because it's like it's one line and it's not that difficult. Yeah. Little underrated. I was just reminded by the importance. In A101, are we able to just import pandas as PD? Yeah, you can, I think. I guess just. That's how I... Yes, I believe you can, because there should be no automated test on that. So it shouldn't fail that. And worst case scenario, I'm wrong and you try it. You run the autograder, right? And you'll find out if that was the mistake you made. That's easy to fix. Okay, so here we have some train use and RMSC functions. You will notice I'm using this NP dot I. So what is that? That is the identity matrix. So this is a 2D array with ones in the diagonal and zeros everywhere elsewhere. So NP dot I, I assume I equals I for the identity matrix. So let's make a simple function of X. So we'll just take this function here, negative one plus 0.1 X squared minus 0.02 X cubed plus 0.5 N. N is just going to be some noise drawn from a standard normal distribution. So I will take 40 training samples and 50% of them I'll use as training data. So now what I can, if I look at my X and T, what I'm doing here is I'm going to stack 40 samples from... Actually, so I'm taking 40 samples between 0 and 3 and then 40 samples between 6 and 10 for a total of 80 samples. And then I'm going to reshape them. So now if I plot my testing data versus my training data, I get something that looks like this. So if we do this just using dots, right, so we have this distribution. So now what we can do is we can add squared and cubed values of each feature and go up to the fifth power. So now I can have this XF, which is going to be my sort of my additional features. I'll divide my data into training and testing sets randomly. So if I've N rows, same as you did before, I will then just basically take all the row indices, shuffle them and then slice out those row indices from my training data and then make the remainder into my testing data. So what I will do is I'll take N rows times the training fraction around this, right, of course, because in case my training fraction doesn't divide neatly into my number of actual samples, in this case it's fine, I had 80. But if I had, you know, if I do like a 70% training test split and I had 315 samples that might not divide all that well. So it doesn't have to be exactly, you know, X and 1 minus X percent. It's just got to be pretty close. So if we do this, in this case we divide very cleanly we have 40 training 40 test samples. So now what I can do is I can color code my samples. I look at the train and test data so what you want to see if you have the ability to plot your data, you want to see something more or less like this where there's a neat overlap between the training data and the testing data. So there's a couple of things here so like there's no test sample that's really well represented in this area of the input. So it could be that maybe a model fit to this is not going to necessarily perform all that well and things that form the fall kind of within this narrow window. That's okay. This is, this is all generated randomly this is the sort of thing we expect. So now let's make models on bootstrap samples of training data, so models will just be a list of models one for each bootstrap sample. So, what I'll do is I'll create 100 models, and then I'll draw random samples with repetition from my, from my training data. So for example here I can specify that I want to draw these samples from a range of So here what I'll do is I'll just take n train minus one, and then draw that those those samples with repetition. So then I run the train function over this and now X train boot and t train boot will be bootstrapped training and and target samples. And so now I can basically create this list of models that are just weight matrices solved with the least squares function that should fit to this data. So, how many models do I have I have 100. Let's look at the first model. Right. And so now we see the outputs formatted as the weights. So these are the weights. And then these are the means and standard deviations of my inputs. So now apply all these models to the test data. This is pronounced why all I grew up in rural New Mexico where they think they're part of the south apparently so they said y'all a whole lot, which is a little bit weird. So I will take for every model, I'm going to use the use function, and I'll take the result of that and then I will append that to my why all which is going to be why is going to be our term for the for the predictions. And so why all will be all of my predictions. Of why all this is the list so what I will do I'll turn it into an umpire array. So we have 100 by 40 by one so we have 100 models. Right. Each of them is pretty making 40 predictions, and then one. Right, what's the one. So it's an extra dimension right so it is one column in the sense that like all of these things are being shoved in there and they have one prediction each but it's just, it's kind of just there I don't really need it all that much. So I can use the squeeze function, which will reshape this into and preserve the same content, and then effectively just get rid of this extra dimension that's that's ancillary. Okay, so I can turn my why all into the sheet that I need by applying the squeeze function then transposing it. And then I look at my why test. And I will just take this to be the, the mean of all of my predictions for every model. So now the RMSC test is going to be the mean if I take the mean of the error between the white test and the t test right the actual values, and the square it this is going to be my RMSC. So, my test RMSC is going to be 4.0 roughly. So, having bootstrapped all these models. I take the mean of all those predictions I can use that to compute a root mean squared error value. So now let's take a look at some methods are going to put to use next. There's two things here, NP dot linspace, this will return. And even these space numbers over an interval from A to B, and then NP dot reshape, which you may be familiar with, basically take some input and reshape it into new dimensionality as long as the shapes are compatible with preserving all the data. So, reminder that you can use the question mark to look at the doc string to figure out exactly what the input is to a function if you are countering errors. So, now what I'll do is I will create this even distribution that is choosing in this case 100 evenly spaced numbers between zero and 12.5. And so we end up with this. And so, same thing with reshape. So if I look at the shape of this, right, this even distribution should have 100 numbers in it because that's how many I created. I can reshape it into a column matrix, just by doing N plot and one, right, so 100 by one is still 100 so I preserved my 100 samples and now they're in a column like this. And if I look at the shape this is now 100 by one. And the only real trick is that the new shape has to be compatible with the old shape so I could reshape this into 50 by two and that works just fine. And even this would work right 25 by two by one by two. This will function, it will preserve the same data. It's just now I basically got a four dimensional array where things are maybe not very cleanly organized depending on what I want. But you will see things like this, like, if you're working with RGB images right every pixels represented by three values, but the convolutional net may require that everything be flattened into a single array or maybe three arrays for each of the channels. And so you have to reshape an N by N pixel image by three channels, right and then flatten out the individual pixels in each channel so you basically have an array for each one. So, the reshape gets more and more complicated as the, as the operations and the architectures get more and more complicated, but the real thing is I need to know how many numbers I'm working with. I need to make sure I'm not trying to cut numbers or add numbers. So, if I change if I do try to do a reshape over 101 by one over 100 samples that's not going to work right of course because there's no way that I can, I can turn 100 numbers into 101 numbers. Okay, so, um, what we can do then is I will generate some evenly distributed numbers all then each stack, basically the powers of those numbers together. And now I end up with this right of course zero the power of anything is zero. It's now we have all of my numbers from that original distribution up to the power of two powers of five. So now let me use the evaluate method. And what I can do that. So, I can use that here. This gives me 100 by 100 predictions. So remember this use method is being applied over every one of those hundred models in models. And so now I can plot those outputs. Right. So, for those models, each of them is a line. Right, we can see that they are all, you know, better or worse fits to this data. And so I can use that to measure the power of the numbers based on those, those power features that I've accumulated. And so you can see that for, for example, those odd powers are these ones here where it's probably dips here and then goes back up later where you have this quadratic or the quartic powers. You know from the positive value, and then hit our global minimum and then go back up. Right. And so you can see that this data can be modeled by a quadratic function or a cubic function or a quadratic function or a fifth power function. But once we get outside of distribution, this data is not fit to that. Right, so we have no idea whether the next sample is going to be down here or up here. Right. And depending on what other samples, you'll show up in this data set, we will find that either like the the cubic function is best, or the quartic function is best or so on. Okay, so now let's try some other data. So some real data this is going to be related to the design of holes on yachts so I guess like if any of you have a yacht. I'm sure there's a lot of other ones that normal people have. So let's take a look at this so we have this yacht hydrodynamics data maybe it's like this like the early sailing yacht, you know those are small not the megayachts. So, look at this data this doesn't really mean a whole lot to me certainly and it may not mean a whole lot to you if you don't know things about boats. So let's try and get some information about it so we'll just say that I happen to know what each of these columns means center of buoyancy prismatic coefficient length displacement ratio beam drop ratio length beam ratio and this thing called the fruit number. And then these are all my inputs and I'm trying to predict the resistance. Right. So I have these things I don't really know what they mean. So I'll go ahead and stick my header row on there with the names. Okay, so now I can see which of these values are associated with which of those parameters so we have you know a bunch of these similar values they're kind of all the same this fruit number is kind of slightly different. And then the resistance value changes. And the center of buoyancy, maybe the same for all of these boats in fact. So I'll just try to get the data to look at it another way, right, it can be messy to look at them. Look at the data, just in chart form. So, we look at center of buoyancy. And there's just, just kind of, there appears to be three clusters here but there's no clear relationship between this value and the resistance right. And the same kind of seems to be true for most of these things. Right, there's no monotonic relationship or near monotonic relationship between resistance and any of things, except for what this fruit number right I don't know what the fruit number is, but I can see that there is there's some sort of relationship on some kind of power curve with this right so I can at least say that in some universe the fruit number is predictive of resistance in a way that none of these other things really is. So this gives me something to look at. Another thing I can also do is just plot, all of the values, all the raw values. Right. And we also see that this curve appears repeatedly, if I just go through each of the samples so right, this appears to be the same thing as the fruit number, right, if I took all this pink curve and dispatch all those all those samples on top of each other I would get something that looks a bit like this. So what I can do is I'll then train a model to predict T from X. So I'll run my train function my use function again. And this gives me an RMSE of 8.8. Don't really know exactly what that means relative to other things until I tried different types of feature combinations, but I can plot the targets over the predictions, and we can see that they're not very well aligned. So this model is just a linear model, predicting this resistance from these from these inputs doesn't really seem to be fit very well to this data. So now, plot the predictions versus the actual values. And we get something like this okay clearly our predictions are the red line the actual values are the blue line, a linear model is not going to fit to this data very well, except for a couple of points here and there. So this last variable, the fruit number it looks like the square root or something might be linearly related to distance to resistance right if you look at this. Okay. If I take some root of this I can probably squish this down to a line in a way that I can't do for the other things. So let's give it a shot. If I plot the resistance versus the fruit number, I get this. So this is a nonlinear curve. So let me try and see if I can make this relationship linear. So the first thing I will try is taking the square. Right, this gives me this. It's a little bit better it's kind of hard to see the difference but this is something of a shallower curve at least so maybe I'm moving in the right direction. So, let me try the fourth power. Okay, this is getting better. Right, so this is clearly more linear than before even though it's not completely linear yet. Not quite there is try the eighth power. This looks like I'm, I'm almost there. It may not be perfect, but it seems like as I increase the power at least up until about eight. This is turning this highly nonlinear relationship into more linear one, one that I can predict using a linear model. So let me try it with the actual data. So I will take. Remember, we took x and we stacked on like a fourth power or something, or actually those the previous data my apologies, we just take x which is the raw numbers then we stack on the eighth power. So now I can train this. And now I get an RMSE that is much lower, right went from 8.8 to 1.5. And this will confirm that I'm doing much better job fitting to this data. Right, so I'm getting a lot more overlap seems like I'm on the right track. Other thing I can do is plot the predictions, first the line seems to be kind of a decent linear relationships there. So let's take a closer look let's zoom in on the first 50 samples. Let's see if we can do any better. Right, so this is doing pretty good but there's still some cases where maybe I could, I could fit better to this line. It seems like higher powers work better. We don't know, like, how much higher we need to go. And at what point I'm going to start getting a diminishing rate of return by increasing the exponent of x. But I can do that is I can visualize how the model does in terms of RMSE versus the exponent so what I can do is just run again run run a bunch of models, applying different powers to that input, and then look at the RMSE. So, here's that first one we saw as just the raw input has an RMSE of 8.8, 4.0, and now we start seeing these one point odd values. So, it seems like the RMSE if you look at this is bottoming out around about 13. And after that, it actually takes up slightly, but not enough to really be meaningful. So it seems like somewhere around 13 appears to be maybe my, my best value for the for the exponent. So, I can confirm this by just plotting the exponent of x versus the RMSE. And here we see this rapid decline, and then at about 13 or so you know it sort of bottoms out and receive this flat line there. Okay, so now finally what I can do is I can try the different exponents of x, 2, 4, 8, 13, and then 16 for good measure, and then use use that data, and then plot predictions. And this seems to be, you know, probably about as good as a predictor of this of this resistance value as I can get. So if I look at the predictions, these are the actual predictions, it doesn't mean a whole lot. But if I plot this against the line, I can see that now I've managed to turn this into a relatively nice linear fit right it's not perfect, as we, as the actual value increases the distance for the prediction also increases. But that is kind of to be expected. So, we've discovered something with this fruit number and the resistance value. Is this a reasonable supposition? Well, what is the fruit number in continuum mechanics the fruit number is denoted FR, and say dimensionless number defined as the ratio of flow inertia to the external field. It's a significant figure used to determine the resistance of a vessel through a liquid, so water or even air. In naval architecture it is explicitly used to determine the resistance of a partially submerged vessel. What's the relationship between fruit number and resistance? Well, I don't know. This paper does if you're interested to find out more, but we did empirically discover that there is a relationship using machine learning techniques. So, this is the point where you can, as a data scientist you can basically discover things without a lot of, you know, topic knowledge. You then want to talk to someone who knows something about that topic in order to figure out how to interpret your results. So, one thing that I will harp on is basically like where you have computer scientists kind of like rediscovering some field. So, like, since I worked in NLP and I actually originally a linguist in like 2013 there's a lot of papers saying like, oh yeah, we rediscovered linguistics using machine learning. It's like there's literally a whole field that's been doing this for like a thousand, thousands of years. So, you're not new. So, periodically, computer scientists will like discover something awesomely new using machine learning and it turns out there's like a whole field out there that has been doing that for a long time. So, it will serve you well to consult with experts. Nonetheless, you can discover relationships like this using machine learning techniques. This has been introduction non-linearity and on Tuesday we will throw all this away in order to introduce an arbitrary method of introducing non-linearity that is neural networks. So, good luck with the assignment and I hope you have a nice weekend and I'll see you on Tuesday.