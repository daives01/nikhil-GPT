 Okay, let's get started guys. Your screen share. Alright, so I'm gonna tell you a stupid joke that I just heard. So everybody here speak French. How do you say the letters GPT in French. Right. What does GPT mean in French. Means like farted. Okay, so basically if you do cat I farted in French, you get shot GPT. Thanks to Professor Louie de Lepouchet for that one. So anyway, those of you who have been to office hours, you see the meme on the cork board, right? So it's like the Drake meme and it's like chat GPT, no, cat GPT. So I didn't realize this. Another did my wife actually, even though she doesn't speak French, and has seen that meme. But then Louie de Lepouchet caught me on that. So that was my highlight of the morning. Alright, anyway, so now I'm going to tell you about talking about the dude named Adam. So let's go do that. So just a reminder that assignment one due on Thursday. So if you do need an extension, I will need your reason for an extension by the end of the day tomorrow. And then I can get back to you. You can send it to me earlier of course. But then I can get back to you with either the assent or a counter offer or denial if your reason is just bad. But so make sure that if you have that, you make sure you get that to me soon. Again, extending circumstances if you do get hit by a bus before class on Thursday. Right, so that obviously I can adjust accordingly. Again, once a reminder not to throw yourself in front of a bus to get an extension on a one I believe these the subsequent assignments are a good deal harder. So don't throw yourself in front of a bus for those either though. Alrighty. Any questions about hope you all have made some progress in the assignment. Yes. I have a question about this assignment in part 15 points. Yeah, I have like a certain amount of sentences. I have mine to be chunked out like two sentences. Okay, it's fine. So, what the reason I have things like that there is just because what I'm most concerned with is basically are you able to do meaningful analysis I don't really care whether it's long or short some people are really good at doing like very dense but dense analyses and like they have you know, five sentences but they really capture the gist of what they've done in those sentences. But some people if I just say that some people tend to flail there are certain students who really do need a kind of And so that's why that's there is like you know if you if you need a benchmark of where enough where you've done enough you know here's a reasonable benchmark, if you've got this many sentences, it suggests that you've at least been able to put reasonable thought into it, I will of course read it to make sure you didn't just feed it to charge a bet to give me your answer. And because it is also good as generating you know fluffy sentences on end. So make sure that you're of course writing your own sentences but that's what I'm really looking for is just strength of the analysis and whatever it takes to get there that's important. Okay. Other questions. All right, let's continue talking about Adam. So you remember Adam from last week, Adam, he is a gradient descent optimization algorithm. So remember that Adam is not not actually an acronym it's short for adaptive moment estimation. Yes. Oh yeah sure. Okay. Yeah, so Adam is short for adaptive moment estimation, and it has the benefits of being straightforward efficient with little memory requirements. And it's very friendly to this method that we've done, basically stacking all of your your weights together, so that you can perform optimization on all of them at once. Right, this helps in our in the efficiency of the computation. It's also appropriate for things like noisy or sparse gradients, and also the hyper parameters have an intuitive interpretation. What is that interpretation so moment in this case is moment as in momentum. So if you think about moving down the slope of your gradient if you're very far from the optimum, and you're trying to reach the bottom, it may be wiser to move fast. So if I can figure out that I probably have a long way to go I'm just going to move quickly, because I don't expect to reach the optimum any point soon and so I'm not likely to start skipping back and forth over it so what I'm trying to get to with SGD, I'm just taking these tiny tiny steps with the goal of reaching the optimum and not not going, you're not going back up past the optimum. So this allows you to be a little faster, so you can basically take larger steps. When you're far away from the optimum, and then slow down when you're closer to the optimum and do it in a smart way and not just having say a decay in learning rate where you start fast and are deterministically moving slowly. Remember that the two key variables in Adam are the first moment and the second moment so this is basically the average of the recent magnitude so that is, if I've taken big steps recently, then it may be more reasonable for me to take a relatively large step now. That second moment is also like what's my rate of change over these recent samples so if I've been if I've not been changing, probably not good and means I'm probably not approaching the optimum. But if I've been changing relatively little, it means that I can still take big steps and I don't need to really need to worry about slowing down anytime soon. If your first moment values are still large. Yeah. They don't, they don't, they can get stuck so this is why I'm so the question for those of you zoom how do how do these algorithms determine distinguish local from global minima, and the answer is they do not because this is a global minima. Because this is where the data set balance is really important so one thing that happens commonly, I'd assume asked me about this this morning is that if your data sample is way overbalanced. And you have like if you're a classification problem you have three classes but you have 10 times as many members of Class A compared to Class B or C. And then your criterion is just like, if you're in training is just accuracy or something. It's really easy to basically say well I can't get a better answer than just classifying everything as Class A is going to get me 96%. And I can't really do much better than 96% for a class for three way classification tasks so why not just classify everything is Class A. If I have 1000 samples of Class A and 20 symbols samples each of Class B and C. Okay, so this can happen, and gradient descent or Adam or any optimization algorithm will happily let you do that. Adam may have the benefit, as we'll see when you look at the loss charts of sometimes allowing you to basically skip out of a local minimum, and then climb that hill and then find basically a steeper path to descend down to a global minimum. And so there is some benefit to basically allowing it to sort of overshoot what might be a local optimum and SGD tends to, especially if your learning rate is very small, it'll find a local minimum to stay there because it can't go anywhere fast enough that lets you get out of that local minimum. So this leads me to sort of how do we, how do we determine the quote speed, at which I can move through the gradient at every optimization step so if I've been moving very very fast. I can probably continue moving very very fast that is I can take bigger steps. If I've been taking big steps over say the past four training iterations, but the steps have been slowing down. So my rate of change is now going, you know, my, like, the derivative of my, of my steps length basically across the gradient is declining. It may serve me to still take a relatively large step this time but maybe make it a little bit smaller. Right. So, what this suggests is that the gradient I'm moving along the gradient away this suggests that I'm still moving along the steep slope of the slope is getting slightly less steep. So maybe I am getting a little bit closer to my minimum and so then I should probably slow down a little bit so that I'm less likely to skip out of that, that bowl, that minimum. Okay. So naturally this is closely related to the concept of moment, along momentum and physics so basically we're looking at the distribution of values around, around a mean or a variance right so the probability distribution of these values that you're sampling is going to be the same as the rate of change that you're taking at each step. Okay, so what we have are these tunable parameters, these beta values. And so this is going to be a decay rate for the first moment estimates, and then beta two is going to be an exponential decay rate for the second moment estimates. And then we also have a constant here to make sure that we are not dividing, remaining division by zero. So this alpha or row, this is the learning rate that you specify that's a constant. So I'll just say, I'm going to do Adam with a learning rate of point 001 or something like that, and that value is not going to change. I'm like I can say some reinforcement learning algorithms. I think in a few months. But here I have these other values that will say, I can perform some calculations over my first moment and second moment estimates that is sampled from the previous end training iterations to determine how I need to adjust my step size so my step size here is not a rigid constant rather something that I can tune a little bit based on these values. Okay, so any any questions on the Adam recap I'm going to go into the implementation now. Everyone's cool with that. All right. So to show this. Let's do what we have done in all this kind of work on a dummy example. Okay, so I will make some training data this is going to use very much the same, the same formula that we use in notebook five, this is basically the same function so I create some, some evenly spaced values on the x axis apply some function this case the same function from notebook five. It's just a nonlinear function with some noise added. Right, so I have introduced nonlinearities here by trying to have it model some variant of a sine function, and then making it difficult by adding some noise. So I'm going to get my optimization to approximate this nonlinear function, I'll make some testing data, the important things that they resemble the training inputs. So now I'll create my add ones column, or my add ones function that this will do is just be a generic way of adding that constant column of ones. So I'm going to simplify my hype some of my hyper parameters for my neural network in this case, the thing that I'm looking at is just what's the number of hidden units in my aware so I'm just going to assume I have a single hidden layer, and I'm going to create a number of units in that that will be that will allow me to introduce nonlinearities. So now if I create 10 hidden. So I'm going to look at memory of two sets of weights, V and W right via the these hidden layer weights so I take my inputs, multiplied by those things, and this gives me some scalar values that have been applied on the new function to those now nonlinear eyes scalar values are then multiplied by the weights and W, the output layer weights to actually give me the output values, still a regression problems I'm just trying to fit your scalar values to scale the values. So now if I look at, you know, take the, the shape, basically just do prod of the V shape this will tell me you're to buy in this case 10. So, 20. What is NP prod. So this is the product of an array of elements over a given axis so in this case the array of elements was two and 10. It's just the shape. And of course there's only one axis. So now what I can do is just confirm this is correct by just checking these two are equal right one plus one times and hidden. Why is it one plus one. Well we have one input. Right, this is my input value and x, and then I have the constant column of ones. Right, so there's going to be two inputs one of which is going to be a bias, that's going to have a bias weight trained against that, and then there's the actual feature guy that's going to have some weight training against that. So I'm going to set the other parameters of my neural network so and hidden, I'll set my, my learning rate in this case to point one, then I can also just scale the learning rate in this case by the number of samples times the number of outputs. So you can do this to kind of optimize the, the learning rate before you start training to what you think might be at least a value in the appropriate range for the amount of data that you got. So now then I'll initialize the weights to uniformly distributed values between, you know, these are normally distributed between negative one and point one or negative point one and point one. So now let me print the current state of my neural network. Right, so I've got a learning rate in this case, point 005 seems like a normal learning rates kind of in that range like, you know, one one hundredths down to 10 to negative five or so. So that's typically typical learning rate value. And then here are my values currently of V and W right these was randomly initialized. So they don't really mean anything right now. And they're also certainly not optimized to this to this function. So if you remember the pack function from earlier in the notebook on Thursday what that does is it will take my different weights from the different layers and pack them all into a single vector, so that I can apply optimization operations over all of them at the same time. And then we have the inverse of that which is the unpack function, I just have to supply the appropriate shapes that I want to unpack those arrays into, and it will give me the actual arrays back. So now I'm going to set my number of epochs or train in this case for 100,000. So I'll take these that this many steepest descent steps in the mean squared error function. And then I'll do some sort of collection for plotting. So finally this is the meat of the operation here so this should look familiar to you already so we have our inputs x1 multiply that by V apply a 10 h function to that that gives me Z I need to append and put a column one to the front of that again. So I'll do that by W that's going to give me my final output, and then I calculate the error, right just my ground truth target minus my actual predicted value, and then use these in the backwards pass so just a reminder of the pieces of this operation. So Z is 10 h over the input, right, this is going to be the derivative of 10 h over the input. And so then the error. This is actually going to be the error term for this for the sample, I just have to transpose it to make sure that my weights are in the right in the right shape. So this is going to be the gradient in W is much simpler so this is just going to be the input Z input to that layer times the error. I can pack my gradient values into a single vector, using this, this function, and then W which are my packed weights, I can just optimize all of them at the same time by subtracting row times the gradient so what's happened here is that I've computed the layers separately. Right, so I've done a gradient of V ingredient of W. So, then what I do is I pack them into a shape that's going to be the same shape as all the ways, right, basically have a gradient for each weight, so the gradient arrays are going to be the same shape as the actual weight arrays when I pack them they end up in the same shape as well. And so now, now I have a single array I can do a single operation over them. So this allows me to be much more computationally efficient. It'll create error traces for plotting and the rest is just pipeline. So, take a look at this. And now we can see it start to converge. Right, so we can see the train and test RMS E. So the train has converged down to a pretty low value and test is slightly above that. And after 10,000 samples, basically you can see that we optimize entirely you know pretty early on. Right, and then there's not a whole lot of improvement going on there. You'll notice a couple of things that's pretty subtle in this but you can sort of see this little hook here at the bottom of this graph right. This is one of those cases where due to the atom. For a three year that I'm here. So, this is going to be. We didn't actually use Adam here. Sorry, my bad, we're going to get to that later. So now we look at ignore everything I said like past 30 seconds. Come back to that in a moment. This is actually just a weird, this is not nothing to do with the gradient is like just a weirdness in the plot looks like. So what's, what's the next thing here. So this is just going to be my actual predicted values and then according to the model. This is what it's predicted so it's maybe not very good. And then these are the hidden outputs for each unit right so each one of these represents the output of this unit. Depending on what the input is. Yes. This is just one layer. Yeah, this is currently was using a single layer with 10 hidden units. Alright, so now we're going to repeat this training loop using Adam this time. So, previously this is just sort of vanilla SGD as we've learned it already. And then the, the version with Adam so differences here. So same neural network right still 10 hidden layers are sort of 10 hidden 10 hidden units one layer. I'm not going to specify these other values so small epsilon just to prevent dividing by zero right tiny number. And then I have these two beta values. So let's just go back up refer to what beta one beta two are again. So beta two is the exponential decay rate for the first moment estimates and beta two is the exponential decay rate for the second moment estimates. So in this case, these values are set are set to be point nine and point 999. So we can see that effectively I'm going to have a kind of a 10% decay for the first moment estimates, and then a really really small decay for the second moment estimates. And then row here I'll just set 2.001. So, same thing as before, I'm just going to initialize my weights randomly. Now I'm going to specify these two other things, empty and BT and then beta one T and beta two T. What are those we'll see those in a moment. So these are going to be bias corrected moment estimates that I'll use to basically update the those beta one and beta two values. So, up until this point, look at the highlighted code. This is the same. Right so standard operations input times weight supply 10h add a column of ones, multiply that by output weights that gives me the value, then take that value subtract it from the target that gives me error, use those error values to make the backward pass. All right, so now I need to approximate the first and second moments right these are going to be estimates about how fast I've been going down the gradient, so far, so let's think of it as like, you know, momentum and acceleration or something like that. So what I'll do is I'm going to look at the gradient on this is going to look at the gradient, the error gradient with respect to W, because I want to look at how fast I'm kind of getting down. Sorry, this is all of them together. So maybe when we look at how fast I'm moving down my gradient defined by all of my weights. And so what I'm going to do is I'm going to take beta one times empty so the kind of existing movement estimate moment estimates, and then incorporate the decay value. So remember when the first moment the is kind of the, the mass, quote unquote, store the center of mass the center of the product distribution, and then V is going to be the second moment of the variance. So I'm just going to take the square of my gradient. And then I will use the beta two value to optimize that BT. So I'm going to use these beta one t and beta two t's to basically correct for bias. So this is going to be so beta one t is just going to multiply that value by the currently the pre-selected or the set value beta one, do the same for beta two, and then m hat and V hat are going to be these bias corrected estimates. So in this case I'm going to take empty divided by one minus beta two t, and then now obviously for for BT. So now how do I actually perform weight update so the formula is slightly different here. So you still have the same components, except instead of the error, I'm going to be updating based on these moment estimates. So these moment estimates are kind of not averages but derived from the previous end set of updates. So again if I've been moving really really fast my error is really really large, I can justify taking a bigger step. So if my error is pretty small, if it seems to be moving in that direction, I should take a smaller step or I should take a lesser less big step. And so what I'll do here is I'll take m hat divided by the square root of V hat, add an epsilon just in case V hat is zero for some reason. And then I'll multiply that by row, and that's going to be the amount by which I update all my weights. So, store in the error trace, and then I plot, I create some testing, test data and evaluate it using the forward pass just in a single line. So here, remember, so X test one, that's our inputs testing inputs multiplied by V that gives me Z, or let's just say A, apply 10H to that to get Z, add ones to Z so this is going to be the same as Z one above, multiply that by W that gives me Y. And then finally, just plotting as before. So, let's go. So what do you notice comparing this with Adam compared to SGD, by the loss curve? Yeah. Yeah. Yeah, I didn't really, they sort of, I mean, the individually converge, right, and they sort of, they kind of plateaued a value, but it's maybe not quite as neatly aligned, right, we see here, the test RMSC is actually lowered first in the train, and then around 10,000 epochs they tend to switch. What else do you notice? Yeah, the train here. So, I mean, we may, we might be working with the training data a little bit. But what, what did you just notice about the shape of the curves? The Adam one is much more, give me a word, abstract, or just I was going to say just like bouncy or something. It jumps around a whole lot. In particular, we see that it's not usually with SGD, you see a nice curve from a high value to a low value and it just sort of reaches some minimum value and tends to stay there. Right. With Adam, huge drop, slight drop, bigger drop. Oh, now we're going back up again. Go down again. Okay, now we start climbing. It sort of looks like, you know, when I'm running a marathon, I like to look at like the elevation map beforehand and this sort of right here looks like that looks like that part that was put it like mile 29 or mile 19. That's just like hell because I'm two hours into a run and maybe climb a hill. So basically what's going on here is that this is skipping out of the gradient. So there's some minimum that it's encountered here and it might be a it might be the global minimum, it might be a local minimum. And its moment estimations are such that you're multiplying that by the step that you're taking across the gradient. And we say move down the gradient because the goal is to reach this minimum, but the gradient is just some surface in multiple dimensions. And I just sort of keep moving in the steepest direction. If I'm moving the steepest direction from where I am now, if there is sort of, if I'm moving in this direction, there's suddenly a big hill, or maybe even just a little hill. So let's say my slope here is very steep. So I'm moving down it very fast. And then there's a slight upward hill in this direction. So my step size is sort of from the tips of my fingers, then, and I'm starting here, where my left hand is, then I'm going to be taking a big enough step that basically lands me over here. Even though the minimum is somewhere here around where my belly button is. So effectively by taking a step that big, I have kind of skipped over that minimum. What Adam is hoping to do effectively is that it's hoping that maybe this is a local minimum. And so by moving past it, I can get on a trajectory where somewhere further along, I will find another minimum that maybe will lead to the global minimum, if one exists. So here what seems to be happening with this data, and if I were to run this again, even with different numbers, this might well change, is that there's a minimum in the gradient of the training data that we are able to find. We get practically down to zero here. There's some fuzz here at the end that suggests like maybe we're taking a really small steps back and forth across some global minimum. For the testing data, remember that every the gradient defined by every data set is going to be different, even if they resemble each other. And so the minimum for the training data that we've calculated is maybe not the best one for the testing data. So perhaps the minimum, the best minimum for the testing data is back here somewhere and we actually kind of found it, but in the training data, there was still somewhere else to go. So perhaps it was not actually able to find that. So Adam has some advantages and disadvantages. But in this case, if you look at where our error curve is relative to the training data, it's doing really, really well. So questions. Yes. So if you wanted to hit, assuming that that dip is the global minimum, to kind of change this to better hit that dip and not bounce out of it, could you be changing the betas then? Yeah, so you can you can change the betas. You can change that exponential decay rate. You can change a number of things, right? You can change the betas here. You could change the learning rate itself, right? You can change the number of things, right? You can change the betas here. You could change the learning rate itself, right? How much am I scaling the whole thing by? Maybe I just, this data says that I should be taking smaller steps overall. I can even potentially just change the number of training epochs. So like, I can only see this after the fact, but it sort of seems like maybe I hit that minimum at 10,000. Maybe I should have stopped training there. You can use some techniques like early stopping or patience, where I can see if I use, say, my validation accuracy as my criterion, I can sort of say, if I don't see an improvement in my validation error in this case, if I don't see an improvement in my validation error for like 10 epochs or something, I'm just going to say I'll stop now because I'm probably not going to get any better than this. So the number of techniques you can use. The problem with this is that this dip here is in the gradient for the test, and I'm just plotting this for comparison. When I'm training, I do not have any notion of this, right? So I'm kind of trying blindly. I'm hoping that training against this data, I'm going to be encountering test data that resembles this closely enough that this will be a good model. And here, even though we kind of generate it from similar formulas, it seems to be a decent fit. It's not like the RMSI values are huge or anything, but maybe not as good as it could be. Other questions? Yeah. Yeah. Yeah. So basically here, we're looking at this. This is in case the hat ends up being zero. So that is your movement. This is the variance of your your second moment estimates. And so when would this be zero? This would be zero if the past and moment estimates that I'm sampling are all the same. Right. So this would this would indicate a couple of problems. One, you're not converging. You're sort of you're you've got you've gotten the exact same error value for the past and epochs. But that might be the case, you may sort of plateau for a bit and then and then sort of find your way out. So in the case that like you're going to be plus epsilon, you're going to be dividing by this. But let's assume this is zero. Right. So this was zero would mean that I am kind of getting I've been getting the same error for the past and epochs. So what I'll do is I'll add some epsilon and then divide by that to kind of hope that the first moment estimates can just be enough to sort of shake it out of this this run. This can allow this can cause us to kind of bounce out of that gradient. So, for example, if this value is too small, you might end up sort of taking a big leap. It's like I'm not moving. I'm just going to take a leap of faith and I'm going to jump way out there and hope I land on a favorable part of the gradient that allows me to do optimizing. So you don't want your epsilon here to be too, too small in this case, because you might have situations like that. You don't want to be too, too large, of course, because then you one don't want to just you cut your your move and estimate down to nothing because that also would stop training. But then you also don't want your step to be too small to be effective. So again, this is sort of one of those hyper parameters in the Adam paper. They found kind of what they found to be best values for the betas. And I don't recall if they found like a best value for the epsilon if I if they did, I should have written it down to get to the part of the notebook. Other questions. Yeah. So, finally, Let's take a look at this. If you remember the model for SGD was sort of just a straight line. Right. It's the green line is just kind of straight. So it sort of optimized. It didn't really make use of the nonlinearities available to it, to be quite honest, just sort of optimize the straight line through the data that approximated the correct slope. Here we can see that the blue line is the is the training data. The green line is the model that follows it very closely, which is also witnessed by this very low RMSE value at the end of training. So it's doing a really good job of fitting to the training data. And it's, it is getting the general shape of the curve of the testing data but not as well. Right. So you mentioned it we're fitting in here, this might be a case where we are fitting very, very closely to the training data and not fitting as well testing it's It's not like it's a bad model. This is a pretty simple case. It's still getting the overall shape pretty well. But if I'm looking at training versus testing, then it's definitely doing a much better job fitting to the training data than this to the testing data. And then finally here are the, the actual hidden outputs for this. For this sample so. Okay, questions on on Adam. Yeah. Um, yeah, so you as you want it. Well, I guess the obvious one is like it's simpler to to implement so because getting started. I would suggest mastering SGD before moving to Adam. Also, it does have SGD is a little bit more deterministic. So when you're in assignment to, for example, I believe you were asked to compare and contrast SGD and Adam and you'll see like the Adam loss curves you get these kind of more stochastic things where they bounce around a bit more, meaning that you may have these kind of spikes, where you are popping out of some minimum, and there is a small chance that you sort of get off on a wrong track, and your training actually you know will sort of collapse after that point. So SGD is like somewhat more reliable, assuming all other conditions with regard to the quality of your data are true. A bit more deterministic, easier to implement has fewer moving parts. So, you know, one of these things. These are things that you can all try, like when you have when you have a different, different types of data you can try you know different types of optimizers Adam and its variants are extraordinarily popular. And so basically, the everything is built on a backbone, but SGD is kind of, it's a bit pedestrian right everything goes really slow, you're just taking very slow deliberate movements along this gradient. So it's not good for really big tasks. Adam will allow you to converge faster at the cost of maybe a little less predictability and some risk of kind of going off into the woods and you're training failing once in a while. Yeah, other questions. So, you know, Adam variants are used everywhere. So most of the large language models are trained, usually using something called Adam W says Adam with weight decay so you actually specify a value where if a weight has been updated in the long time it's it's kind of assumed not to be important. And so then it's valuable kind of attenuate. So I guess the corollary of that is basically the larger model you have for a given task, the higher probability there is that some subset of those weights are just irrelevant. So if I have a trillion parameters in my modern model to do, you know, diffusion or language generation or something like that. Probably don't need all those billion parameters some 10 million of them might be incorrectly optimized or could just as easily be zero and really wouldn't change the performance. So this allows us to that property sort of allows us to do things like fine tuning where I can assume that for a different task, maybe I can use those weights that aren't really being used and better optimize them for task performance. So, these different optimization techniques allow you to kind of leverage different properties to neural networks. So, it's now 237. So I will start the next notebook which is going to be on finding good parameters. So, controls, go to number seven. All right. Let's start this. So, optimizers data partitioning and finding good parameters. This is a notebook on a couple of different topics, all of which are going to be important for doing assignment to which I'm currently planning on assigning Thursday. But basically optimizers are these fun, these operations like SGD and Adam, right, how am I actually managing my movement along with gradient in order to try and better fit to my data. So when I talk about the optimization function this is going to be, you know, I've used SGD or I use Adam or I used our mess proc or use Adam W etc etc. So, I guess when discussing Adam, we just discussed how to create the single vectors of weight values. And so we can also view parts of that vector and find the weight matrices for every layer of a neural network so I can say, I got my entire weight matrix and I just have my, my update vector, and then just by performing a single operation I'll be able to update all my ways at once, that's been much more computationally efficient. So, here is a function that will do that will create the views on a weight matrix automatically so let me just create a random sample. So I'm going to create it and I'm going to create this, this just this object. So in actually view the values I have to slice it. So, now I can see, instead of a three by three sample I just have nine values. So now I'll define this make weights and views function. So what this does is it creates the shapes of the different weight matrices let's say I've got you know weights V and weights W and maybe other hidden layer weights of pre specified shapes. And so then I will take, I'll take all of those and then stack them into a single vector. Right, so this gives me something like this array. So let's create this all weights vector. And then I will, in this case, I'm just initializing it with some uniformly distributed values. And then I'm going to build this list of views. Remember a view in Python is just a shallow copy. Right, so if I change the value in the view I actually change the value in the original, as opposed to a deep copy which creates an actual separate place in memory. Here what I'm going to do is by creating the views I can then change the value in the sort of you shallow copy, and that's actually going to change the original weights. So what I'll do is I'll reshape the corresponding elements from the vector of the all weights into the correct shape for each layer. Okay, so for every shape in this list that I passed in, I'll create basically a view onto the always vector in that appropriate shape. So now this allows me to treat V and W as separate objects but if I modify my V and W objects I'm actually modifying my all weight vector. So what this does that is that when I actually get to my training step, I can have my update vector, and then just apply everything every operation to every element of the weight vector at one time. So now this allows me to keep things organized in that I can just see like what the values of say V or W are, if that's all I'm interested in, while still maintaining the computational efficiency of a single operation at training time. And you know let's take a look what the shapes of the weight matrices would be if I had a neural network with two inputs, two hidden layers with 10 and 20 and 10 units respectively and a single output. So I'll build that. So what I'll do is I'll specify the number of inputs and in number of hidden units per layer, the number of outputs, and then I will initialize an empty list that will store this so for every, for every hidden layer. What I'll do is I'll append one plus the number of inputs, right, the one because of that bias column. And so it's gonna be one plus one plus number of inputs by the number of units in this layer. Right, so if I have two inputs, this is going to be one plus two so three by the number of units in layer. First one was 20 so we get three by 20. So then, the next thing I'm going to do is I'll. So then for each one right if I have three by 20, and then one plus 20 so 20 is what comes out of that first layer. One plus that is 21. And then we just have a single output. And so then what I will do is I will append you to one, and then plus one by an H, and then I'll set an into NH because then H is the number of inputs to the next layer. Right. So I can just do this. All right, so then I'll make weights and views. And so now these are like all of the, the weights right so you can see that this is, these are what's how much is this 20 by three by 20. And then 21 by 10 maybe, and then 11 by one. Right. So let's make some data with two inputs per sample and a single target output. So what we'll do now is we'll have our target values, the x and y coordinates, and then we'll make basically a kind of a train map, so we'll have. I just have like a square area. So I'll just make a coordinate and then I want to create some hills by specifying the z coordinate is the output. So, what I'm going to do is I'm just going to create like the surface, where my inputs will be say latitude and longitude, and my output would be like altitude or something like that. So, what I'll do here is, I will specify some centers. So this is like in my 2d plane this is where I want the centers of my hills to be, and then the heights for each hill. So, Judy coordinates so we can we can look at this as like my input would be two by two, and my output will be five. Right, so this is these are coordinates and then this is the height, whereas for an input of five and four, my output will be four. So you can see here just by looking at these, you can see that this is like a highly nonlinear in that I have the same output for or five for two different entirely different values like two and two and eight and two have the same output, and then five and four and three and seven also have the same output. Right, so very nonlinear function. So this should be something that I, you know, wouldn't require a neural net to to actually predict. So I'll define this calc heights. So why am I doing this, because I don't want to just have like is really sharp and don't have a completely flat plane and then like one point in the air, but I want to actually have a surface. So, what I'll do then is like for every point, I want to be able to take in a value that's not one of these inputs and calculate the appropriate height based on will assume you know a circular hill with an even drop off from the from the peak. So I'm going to plot these using this mesh grid function. So what this will do is it's going to take, you know, take coordinate vectors and return coordinate matrices is going to create on nd coordinate arrays for vectorized valuations of nd scalar vector fields over nd grids. So that is basically taking these numbers here and turning them into a nice even surface. So what I'll do is now I'll create my surface. So I'm going to have two evenly spaced lines. So I'll just have an x axis and my other x axis in this case just by two horizontal surfaces, and then I'll make a grid out of these two arrays. And then I will allow now printing x will allow me using the mesh grid function, allow me to show the coordinates of every point in the 2D grid. So if we take my 20 points, I have a point at 00 and also have a point to like point 50 and so on and so on until I get to 10 right so I've got you know zero evenly distributed numbers until 10, and then like the next row of zero through 10 next row zero through 10, and then a corresponding column at each of those evenly spaced values. So now for each of these and I have like 400 points for each of these I now want to apply the calc heights function that's going to give me, according to the hills and centers that I specified previously, what the what the values for each of these points would be. So this allows me to take now not just like five points, but 400 points and create a relatively smooth surface. Any questions about what I, what I'm going to be doing here. So now running the calc heights function, given the centers, right, this is going to be the height of each point in the grid. So, if I have this should be 400 by one. So remember this is H, right H is 400 by one, X was the 400 by two array here. So if I had a 10 H stack, if I stacked X and H together, what would that give me. First of all, how many columns do we have. If I stacked X and H so X has. So, so, this is this is a trade here. Right, this is X. So if I stack H and X side by side, how many columns do I have? Three. Right. So if I now have three columns, what do you think say a row of this stacked array represents? XYZ, right? Yeah, so this should basically be the 2D coordinates on the ground, and then the height of the hill at that point. Okay. So now you kind of get a sense of what probably this is going to look like that's actually visualize this. So I'll use, you know, just some visualization toolkits, just using axes 3D, I'll be able to plot these in three dimensions. And that gives me something that looks like this. Right, so I should have 400 points. Basically, these are my two horizontal axes, and then these these peaks, these are those five hills that I specified, and it's kind of smoothly interpolated the surface between those. So now you can see the highly nonlinear nature of this function. So, I know we're just going to play with lighting for a little bit so we can make this look a little bit cooler using light source. So this is a library that basically creates a light source from a specified point and it will render it, render the surface in sort of a really nice way. So now if I put a if I just kind of take a point light like over here and shine it on my on my surface, it looks like that. So it starts to look more like a landscape. All right, so now we can make some data. Right, so the whole point of this. You can use these cool visualization tools and it'll make your your projects and your assignments look really nice. But the whole point of this was to actually try to fit to this to this surface right. So let's make some data so the axis is going to be these points on our base plane, these are going to be the inputs, and then the target values for T is going to be the height of those points. So for every point in this on the surface, I should be able to calculate a height for that, because I've already created a smooth surface I've got this calc heights function that should be able to take in a number arbitrary number and give me the height of that point on the surface. So, what I'll then do is I've already calculated z right use those are the heights for each of these. So just make those into my target values. Now this is the thing I'm trying to predict. So, if you look at the shape of this data I now have 1600 points in my inputs so these are going to be 1600 by two right so this is 1600 x y coordinates, you can call them, and then 1600 associated z coordinates so now if I just have these stacked together y z coordinates if I slice off that last column and make this the thing I want to predict, I can set this up really nicely as input to a neural network that has two inputs, some a miracle occurs in the middle and then there's an output. Right, that's what we're after. So, we observed in the previous notebook that there may be a tendency towards some overfitting. So, who can define overfitting for me we've kind of alluded to this term and I'm sure many of you know what is overfitting. Yeah. Yeah, so we have overfitting is where the model is really good at optimizing into whatever patterns it finds in the training data, and it becomes so good at that that it's not good at anything else. Right, it's sort of like you you train for years for use the running metaphor again right you you saying bolt and you like train your whole life to run the hundred meter dash, but you fall apart when it comes to marathon, because although there's like a superficial it's really not at all like the thing that you've been training for so you say in both overfit to the marathon, and Elliot Kipchoge the guy who will like won the has the world record marathon overfits to marathon right and neither of them is going to be good at the other sport. There might be better than the average person at either, but they're not going to be particularly good at that other sport. So it's also like, you pick your metaphor it's like taking a football player and hope and assuming he's going to be like an Olympic League swimmer or something like that. So, how do we avoid this yes. So I said it again. You know, that was designed to like to detect one thing, but it's better detecting everything. So, just to repeat the question if you have a neural network that's like, was that you designed to do one thing but ends up being better at something else in that question. That would be a very weird case. Yeah, so you could argue that that is maybe a kind of overfitting, but it's like, it's sort of something must have gone longer in training in that point. So I guess over overfitting is not necessarily. You try to use a model of a design for one thing to do something completely different. Like, yeah, sure it overfit to the data compared to this other thing that doesn't resemble the training data at all. But you can't assume. So, that would be like trying to use a hammer in place of a saw just because they're both tools. Right. It's like, hey, you have different tools in your toolkit. I do not expect to be able to chop down a tree with a hammer. It's going to be very, very difficult. So, those are design choices that you can make and like if you assume if you design your network assuming that you're going to do one thing. It's not fair to apply it to a different thing. So you see like, you know, that kind of bad faith critiques of some some some papers like, well, your network doesn't do this. Well, it was never designed to do that. So, this is not the problem I was trying to solve. You're really not making a fair criticism of this right there. Probably other things you criticize it for, but that's not it. But that being said, generally the goal for neural networks if neural networks are universal function approximators that is if I'm assuming that there's a function that maps for my input to my output, and my job is just to find that function. The neural network is a universal function approximator meaning that with the right combinations of nonlinearities and layers you can approximate any function in principle, it just might be a really gnarly function that takes forever to approach. Nonetheless, you can do it. So the goal with neural networks is that always some level of generalizability. So what we do, we don't want to have an you don't spend all this blood sweat and tears in training this neural network that does just one thing on one data set once I want to be able to reuse this, at least somewhat. So, one thing that I can do is make sure that the data that I'm going to evaluate on it's never been exposed to before. Right, so this was something that we kind of have slipped under the radar with assignment one and that you don't have to do the train val test split. Because the data is friendly enough it's actually cyclical if you look at its temperature over a year so like the last date is very similar to the first date. So it works nicely there. But generally speaking you cannot assume that's going to be true. So what do I do there. I want to create these train validation and test sets. So what's the role of each one of these obviously we know what the training data is for this is what you actually fit your model to. And the test data is some unseen data that I've never, the model is not been exposed to. And this is what I want to actually perform well on this is where I'm going to basically prove that my model is a good enough approximator to this thing that it's not seen before that's what makes the argument that I've actually approximated that function that I'm searching for. So, these things involve a significant amount of computational power the examples that we're going to be using are not really all that big but we're all familiar, you know, with, you know, the large language models large vision models. Those take forever, and neural network training has taken forever for a long time one of the reasons that it didn't take off initially was that it took a mainframe the size of this room to do simple digit classification. And so I'm like okay this is a great party trick. I don't see what it's actually useful for. Well now we have the technology, the technology and the tools to speed it up, such that we can actually use them for real things, but it still often takes a while so again I don't want to sink all this effort into training this neural network and then run it on my test data and find out that it completely collapses. I want to have some reasonable, some reasonable assurance that I'm going to perform well on my test data. So I'm going to do for that. This is where the validation set comes in validation set is an extra set carved off of your training data that during training you continually test against right you're not training on this data. So you should not overfit to this data, but you can check your model against that to see on this other unseen data set that is not the test set. So this is what I can use to tune the model. Am I performing reasonably well. And so I can use this to find things like good hyper parameters I can kind of see like is my learning rate too high am I skipping over my my optimum the gradient is, is my neural network the right size. Do I have the right number and appropriate number of nonlinearities in it so these are the sorts of things I can, I want to be able to do. So I have a nice property of the training validation and test set to resemble each other. Right. So if I'm training on this hill date I need to get like a roughly uniform sample of points on this mesh. Right, I don't want to validate only on this corner, because this is not going to give me an accurate picture of whether or not my model is fitting to the rest of the data. So, effectively what I'm going to do is I'm going to train on some points on this mesh, I'm going to validate and other points on this mesh, I'm going to test on further points on this. So that's what I'm that's what I'm looking for. So what I'm going to do then is going to shovel the samples into a random order. Right, we did this before. I'm going to partition the data into endfolds and maybe say, I want to create and some partitions of this data. So I'm going to put the data in the first fold to the validation set. The second fold to the testing set and then the remaining folds into the training set. So, generally we want to have both your data in the training data, which will have a substantial enough sample in the validation and test that you can be reasonably assured that your model is going to perform okay on that data. So what I'm going to do here, this is just going to do the, the, the proceeding in code. I will shuffle the row indices we did this already in one of the previous notebooks, or to specify the number of folds here I'm going to do five. And then I will divide my number of samples by the number of folds round down to make sure that I always have an integer. And then I will accumulate those different folds into the different samples so like I'm going to in this case, just like I mentioned I'll take that first forward make it about set second hold make the test set, and then the remainder, make it the training set. So now let me print out the number of folds five, the number of the number of the first fold, and then how many samples in each one right so 320 by two, two inputs and 320 by one that's that output. So now what I can do is I can specify which one of these I want to use for which fold so the. So, the x x validate and t validate is going to be fold zero right there are two elements here. The first one of which is going to be the training the training and the second of which is going to be the targets. So, then I'll do the same thing for the first four this is going to be the test or the second for the test set, and then I'll just stack the rest together into the, the training data. So, now if I look at the shapes of each of these. The training data is 960 samples, each of which has two inputs and then the targets, and then I have 320 samples each in the validation the test. So, this is a pretty generously sized validation and test set in that the validation and test set or like one third the size of the training data. In this case that's okay, because there's like not that much noise in my in my data that I'm trying to predict. But the size of your validation and test set is another one of these things that you want to be judicious about choosing when you're when you're performing training. Okay, so now I'm going to basically run a solution to assignment to that you won't see because I've saved it off previously. But we'll see how we can actually use a neural network to fit to that that hill data. So, what you can do, you're not required to do for assignment to but once you complete the neural network class definition you can save it off into a file called neural network.py. So, assuming that you perform well in assignment to you now have a implementation of a neural network that you can then just reuse indefinitely. And then later, when we are through with assignment to I will actually just give you a neural network implementation to use and other things, just in case you know you're not so you're not relying on a potentially buggy implementation of a two. Okay, so I've got this neural network up you Wi Fi all import that. Now let me look at just the size of my dimensionality of inputs right two samples, and then a single output for each sample. So now what I'll define is the actual hyper parameters of my neural network the actual architecture, so the inputs the input layer is always going to be the dimensionality of the thing that you're measuring right so if I've got two samples or two inputs there should be two things right two nodes to inputs to the network. This list here this all to specify the hidden layers, so 10 units and five units. This could be as long as you want so basically the way that this, this neural network class is written is I can just add numbers to this list and will automatically create a new layer of that size. So finally I need to specify the outputs and of course the output is going to be how many things am I trying to measure right so in this case I've just got one thing that I'm trying to measure the height of the land at that point. So it's going to be single output so and then the default here, the is the 10 h activation function, because we have not talked about others just yet. Okay, so now if I just print the neural network. This is sort of in my my non pi neural network implementation. This is my architecture, so I've got two inputs, one layer of 10 units one layer five units one output of use the 10 inch activation function. When you're using libraries like tensorflow or pytorch, they do come with a handy print function where you can actually print out the number of architecture and see you know the sizes of the different layers, and all, all the fancy things that you can do there like different activation functions and residuals and whatnot. So, I find the train function that is much like what we have done previously. So again, a generic one that takes in the inputs the targets number of epochs I'm going to train for learning rate and then method is just the optimizer so here I've been using SGD. So, let me run it. So you can see it's working. It's pretty fast. This is not a very simple, not, there's a pretty simple problem. So my error after 10,000 20 epochs ends at 1.19. So I can experiment with a couple of other things right so I can try and see what happens if I use Adam instead. Right. So in my implementation I, I've got a way to basically just pass in which type of optimizer I want to use it will use that method. So let's explore using Adam instead. And we can see, you know, first of all here we can see firsthand one of the benefits of Adam so in both cases I've trained for 10,000 epochs. But I've even in the first thousand epochs of training with Adam I got lower error than the whole training for 10,000 epochs of SGD. Right. So this is one of kind of a tangible demonstration of the benefits of Adam training takes about as long as case it takes about 2.61 And these little spikes here, right, that's the sort of skipping across the bottom of the optimum so it's, it's getting there and maybe trying to find its way out finding that that's not a good way out going back down and eventually to sort of settles there and maybe is a little bit of movement And then we'll see when you you do assignment to the Adam optimization is a little more approximate. Basically there's a sort of a toy test function you can use just approximate the minimum of a parabola and SGD will get you there exactly. So we know that that that is the minimum of that parabola and Adam will get you close, but not quite close enough to be useful, but not exactly. But I would I guess close enough to be useful but not exact is like a pretty good motto for most of machine learning. Okay, last thing we'll define the RMSE function, you all are probably familiar with this already. And then I will have defined my use function and I'll just apply that over my training data, and then just print the shape so basically here this is giving me the actual output so this is being stored, and then I'd put the shape, and that's 960 by one. So over the x train, right 960 samples each of two inputs through the neural network, and then it gives me an output for each of those. So let me print some, some values. So what I'll do here is I will put the RMSE for the, for the, the training data the validation data and the testing data so basically what this is doing is, I'm just computing the RMSE for my chart my train targets, and then my prediction so and then dot us of X training that's going to give me all my predictions. And then you can see that I've done a pretty good job at splitting my data such that I'm training in a way that is allowing me to get reasonably close testing or validation and testing error. Yeah. So you you you can do that. This is this is something I guess I, I guess I admitted that here so this is something that you can set as a as a criterion in kind of TensorFlow or pytorch. Here we're not really doing this because this is sort of homebrew implementation. You can set it up so that you're. So you can set it up to do that and print out you know every and epochs, what's my validation accuracy or error or whatever metric I'm using based on the state of my network right now. So you can think of what we're doing here at the end is, I've just doing a check on the validation data before testing, so what I can do here is, if I am not sure, right, so you're not supposed to touch the testing data until you're ready to apply. So what what often happens is, you know, evaluators will keep like a hidden testing set, and it's just like you don't get to see this, you have to write your own network the best you can. And then you send it to me and then I will apply and I'll tell you how you did. Here we're not doing that but you can imagine that we did. So testing data is often this. I want to know how I'm doing, so I will check against the validation data and if this value is lousy. So if I just print these two, and it's like okay instead of point 1138 it's like 10. So testing data is often this. I want to know how I'm doing, so I will check against the validation data and if this value is lousy. So if I just print these two, and it's like okay instead of point 1138 it's like 10. Like okay, this neural network was wildly bad. Something's wrong. I need to like add more hidden layers I need to add use a different activation function, whatever it is. So then I can try and change the hyper parameters of my neural networks so I get a lower value. Okay. Yeah. So, it varies in the so if I remember correctly in assignment to you're given it's like a similar when you're given template code, and you have to fill out like train and use function and something in, you have to change the optimizer to change from SGD to Adam. And then in assignment three you have to do some sort of grid search. Okay. All right, let me, let me get through this in the remainder of the time I think we're almost there. Okay, so we trained right it seems like we're doing a pretty good job. Let's actually try to visualize this so now what I want is I want to be able to take my, my surface. Show all my training points show my validation points and show my testing points. So let's take a look at that. And this is what we get. So, the blue points are trained the yellow points of our Val and then the green points are test. And so we can see that we seem to be doing a decent job. Right, so the training points. It's kind of it's a little bit difficult I don't think I have this no it's not set up to rotate. But you can see the training points are like very closely fit to the surface, and the validation points and testing points are mostly to there's a few like here's a testing point that's kind of maybe not so close, but it seems to be doing a pretty good job of being able to predict the height of the surface from the, the x y coordinates. All right, and then finally, let's see if we can visualize what the hidden units that actually learned. So previously, we had these lines saying okay for this x value, my kid and you and is outputting this value. Let's see if we can do something for for this data. One of the things to notice like as we get close to the edge here you'll see how we kind of see these these dips here. So, this suggests that basically it's not very good at optimizing for this local neighborhood probably because of a lack of data off the off the surface right we have no points here. And so it's kind of continuing transit may have observed from this direction, leading it to be a little bit lower than the actual value. Right. This might be a reasonable prediction of like what might happen if we extend to the terrain in this direction, but we don't know. So this is, I'm not going to go through this code in any real, real depth this is just for visualizing the outputs of the hidden layers three dimensions. So, this is what we get. And so you can see now how this is not necessarily very interpretable per se. But we will tell you for each input what the unit and every hidden layer is putting out. And this might be useful for, you know, if you want to trace the path of like a single point, right, we want to figure out like why is this testing point down here, then you could actually calculate what each hidden layer is outputting with a network of this size So this is like reasonably tractable like you could probably do this math if you were motivated enough. The black box nature of neural networks just comes from having it happen at scale. Right. If I have 10 or more, you know, dozens of hidden layers each with thousands of units. It becomes really hard to trace you know what's going on with a single input. All right, last thing we need to examine the effects of various hyper parameters so we can very different things right we can try different lengths of training different hidden layer structures. Different, you know, different optimizers etc so you can try all these things. And each of these are called hyper parameters right the parameters are the weights. My neural network is a function or combination of functions parameterized by weights. So what I'm trying to solve for are those parameters. So those are the coefficients, the hyper parameters are the things I actually have direct control over. I don't go in my neural network and tune all 1 billion of my weights. What I want is a function is going to let me do that automatically. And in order to achieve best performance, I'm going to be looking at those things that actually have control over such as the learning rate training time model architecture optimizer etc etc. So the hyper parameters are just the property of the architecture that you actually have direct control over. So what we can do now is grid search, but I can say that for different combinations of learning rates training durations layer sizes and optimizers and whatever else you chair, you care to examine. I'm going to instantiate a version of the neural network with those hyper parameters, train it, see how it's doing against my validation or my test data. And then from that I can decide which one I want to use. So, the role of the validation data is really, is really key and I'm trying to find the right hyper parameter combination so if you assume the test data is not to be seen until testing time. I don't want to tune my model on my test data that's cheating. So instead I'm going to tune on the validation data that I'm just assuming is reasonably resemblant of the test data. So in this case, this is similar to what you're going to be doing I think in assignment three, except that's classification, I think, maybe. So what you're going to do is you're going to try a bunch of different hyper parameter combinations and then try to observe trends like what happens as the learning rate decreases or what happens as I train for longer, or what is SGD doing worse than Adam and by how much. So due to time I've already run this I'm not going to go through this again. But what I want to look at are which parameter values are best. So, I can plot them. Right, I can just plot the RMSE, but that's not really helpful but I want to look at is every plot with respect to hyper parameter values. So, oh, what happened there. Well, a bug happened. But if we go to the one on the calendar should give us a version that we can look at. Come on. Two minutes. All right. So, all right, so now we can check out the different hyper parameter combination so for example, if I want to look at what happens that to the training validation and testing when I use the SGD optimizer, we can clearly see that this is a larger architecture train for longer, and that seems to work best. Right. Whereas for Adam. I can see that I still get that with the architecture size seems to matter more. Right. So here I have 1000 epochs and 5000 epochs with the same architecture, and I don't get that much improvement when training for 5000 epochs so this suggests that if like I'm trying to optimize for compute time maybe Adam is going to give me the best bang for my buck which we observed already in that previous notebook. So what I can look at is just put it on a pandas data frame, and then we can actually compare each one so I can see you know, where do I see the low RMSE number as well. I see them with Adam more than SGD, and I see them with larger architecture so it seems like the things that are most important are going to be the architecture size and the choice of optimizer. Once I've got that, I don't get much benefit from training for much longer. Right, so I get a minimal benefit from training for 5000 epochs versus 1000 epochs. Overall, yes, the best thing to do is just to train the hell out of it. 5000 epochs with Adam on this big architecture. Yes. This is true right yeah and so that is true what's the reason for that so if you look what's on the y axis RMSE. Right. So we also see that these two, like the the best SGD performance is not at all comparable to the best Adam performance. The best Adam performance is like way way better than the best SGD performance so if you were to plot both of these on top of each other we basically see like SGD kind of up here. And then Adam is really kind of showing how strong it is. So, this is sort of a method of deciding like what are the best type of parameters for me to use for this data. That is all for today. So, assignment one, two, on Thursday if you need an extension, remember reminder to get that in by your request in by tomorrow for my consideration. All right, we'll see you Thursday.