 I'm not going to let you know. I'm not going to let you know. I'm not going to let you know. All right, let's get started guys please. Right, sorry about the delay. I couldn't. My computer is not connecting to the Wi-Fi for some reasons. I couldn't blog with my zoom. So humans buying schedule. Just a reminder. For attendance. If I grant you a leave for like if you're usually 10 in person. If I say it's okay to attend online for a specific class that is for that session only. Right, if you want to attend if you're if you continue to be sick, for example, or whatever, let me know that you're still sick. So I do not expect you back. Right. But if I said it was okay on Tuesday, I need to know that you still plan to attend remotely on Thursday and so on. All right, so today I will finish up the linear regression notebook. And then what I'm going to do is I'm actually going to. Assign the first assignment and then go into notebook for. Because. Pretty much what you need to know in the assignment. It occurs in notebook three. Let me show my screen first. So. I guess before I begin. Are there any questions anybody want to review anything. From notebook three, because it's the last part of this for the first 10, 15 minutes, maybe of class. So there may be questions about the content so far. Okay. Exciences and no. Oh, and also. So sorry to the tutorial on Jupyter notebooks. So hopefully those of you who needed were there. It is recorded and we will put the recording up on canvas in case me to review any of the material. I also have a version from last year's TA that I think covered more list the same thing, but. Maybe presented slightly differently or may cover some different things. So you'll basically get two takes on how to use the department machines. If you care to care to be them. So the expectation at this point now is that you are at least conversant and logging on to the department machines and. You know, testing your notebooks and stuff like that and making use of the resources there. So in particular, let's say you don't have a GPU. The work that requires that you can log on to some of the department machines. I will, when we get there, I will send around a list of GPU enabled department machines because not all of them have GPUs, but you should have access to be some of the do. Okay. So if there are no questions about the material so far, just to recap, we're just basically solving an optimization function with multiple. Inputs using matrix multiplication. And the way we do this is through stochastic gradient descent effectively by trying to figure out with a set of weights. That are arbitrarily poorly or well optimized at any point. For a sample, you take that sample and multiply it by the weights. And they gives you an output and you measure the incorrectness of that output compared to some ground truth label. And then that error is used to update the weights in the correct direction. Right. And so the descent of the gradient to the gradient here is basically, this is a high dimensional derivative. So just a slope in multiple dimensions. If it's easier to think about it this way, you can just think of it as basically just a slope in three dimensions and you have basically two components of the derivative. And so I'm trying to minimize my position on that gradient. I'm trying to find as close to the global minimum of that gradient as I can. I don't know which way to go along the gradient, but I do know where I just came from. So it's sort of like instead of looking down a hill trying to see where the direction of steepest descent is actually looking up the hill. Seeing where the steepest direction I just came from was I'm walking backwards. Right. And then I can see this should get me closer to the minimum. How close did it actually get me to the minimum? Am I going in the right direction? Right. Am I moving along the right dimension? Previously, we did examples where we have an arbitrary number of inputs. So remember for every sample, if you have an N by D matrix representing your inputs, you have N samples, those are the rows. You have D dimensions to each row. That is the number of things you measure about that sample. Right. And for the moment, we'll just consider everything to just be tagged to where we can miracle data. So we saw how we can solve for D inputs to predict one output. So if I want to predict the miles per gallon of a car from a bunch of different, a bunch of a bunch of different other parameters, I can do so. We can then also do things like figure out which of those other inputs are most important for predicting that output and which of them are positively or inversely correlated with that just by looking at the weights in a linear model. So if you can turn something into a linear problem, it makes it much more explainable because then you can see exactly which input is correlated with the output. When we move to nonlinear problems, it's neural networks. This becomes difficult to impossible because of the number of operations that have to be. It should be performed in every time step. But for the moment, this is a neat technique that you can see. What's most predictive about my model. So the cool thing is now that if I have all of these input parameters, if all these inputs, I should really stop saying parameters because that means the weight. So I've all these inputs. I can use the inputs to try and predict any of the other values. Right. So for example, we were using these values to predict miles per gallon, but some of these values may also be correlated with each other. Right. Some of them may be predictive of other things. So this allows us to have multiple target components. And so what I can do now is I can do things like use the values that I have for every sample to predict more than one thing about that sample. So, and what this means then is that we can basically use the exact same matrix operations and all you have to do is adjust for the size of your model. And so if you regard T the targets as an end by one matrix of output values one per sample, all I need to do is add an additional column. So in this case, if I want to predict miles per gallon and horsepower, then every row will still represent the input values pertaining to each sample of car. And then every row will still represent the input values pertaining to each sample of car. And the output values will now be two columns, which are n by two, where the two values are miles per gallon and horsepower and n is the number of samples. So all we need to do is add those additional columns of target value. So now the T will become an n by K matrix, where K is the number of things that you want to predict. So D is still the number of things that I measure about each sample. K is the number of things I want to predict. It doesn't matter if D or K is one or more. Right. It's even still solve this using a multi input or multi output linear operation. So the operations are fundamentally the same in that you assemble the data for predicting miles per gallon and horsepower. So let's just make sure that I didn't clear the kernel. So this is just going to pick up a re-left off last time. So if you look at, we, if you recall these, these indices, so these numbers refer to indices in the, in the data frame, where the target data in this case, zero and three, those are the columns that correspond to miles per gallon and horsepower. So you arrange your data and tend as you just look at your data and say, okay, these are the columns that I want to select for either the input, the output. And you can just slice through the data set accordingly. We already established the names of each of these so I can print things nicely. So I will, as long as I have the order that I want my targets to be in, I'll just, you know, pluck out those, those names from the names variable that I previously created, and then do the same thing with the x names for those values I'm still leaving as inputs. So we have my entire data. I'm going to take columns one, two, four, five, six, and seven of you, my inputs and columns zero and three of my targets. So if I print these now my shape, the shape of my inputs and outputs are slightly different. So it used to be 392 by seven. Now it's 392 by six, of course, because of I take something out of the input and put in the output, you know, it can't stay in the inputs. So I can't be predicting the horsepower from the horsepower. That would be too easy. So now my inputs are 392 by six and my outputs are 392 by two, and we can just sandy check and make sure that we've got the right values by printing the names for each column. So now using cylinders displacement weight acceleration year in origin. I'm going to try and predict miles per gallon in horsepower. So we talked about splitting into train and test. So of course, just like you can have horsepower in both your input and your output. I don't want to be testing on the same samples that I trained on, because I'm going to overfit to that data, right, and there's going to be no guarantee that the model that this is going to perform well on other data that I haven't seen. So what I do is I approximate this by splitting this into a train and test set with the assumption that the train, the train set and the test set are going to at least more or less resemble each other. So what this means is that I, if I use, if I train this, this model on cars on car data predicting miles per gallon in horsepower. It's not going to do me any good trying to predict weather. So I'm going to try and figure out what the difference is. Even if I set up the matrices in the right shape. And I have the same number of inputs and same number of outputs. Those inputs mean entirely different things. Those outputs mean entirely different things. And unless some, there is this website called like spurious correlations that you can like check out, you know, number of like, different functions compared to number of J walking fatalities or something and you find like that there's actually a relationship but it's completely arbitrary. So that aside unless you find a case like that, you're not going to have a case where the inputs are predictive of the output just looking at the numerical values because they mean entirely different things. And so we'll see in the assignment will be working with weather data and you can see exactly, you know, how those values are being used there. So I've done that. Let's see. I'll just split my train and test samples. I'll use the same 80 20 split that I used before. So I have 314 training samples 78 testing samples. Now they are the are six and two elements for each sample respectively. Well, we talked about standardization. I want to standardize that I am not subject to the range of the values where I have one parameter one one input that has a really large range. Don't even remember why we don't want these really large ranges. Yes. Right. So we saw that graph where like the weight ranges in the thousands and other all the values are in like the hundreds or less. So first of all, it makes it really difficult to interpret. And, and if I'm trying to look at what the relationships are between different values that becomes difficult to do. When I'm performing the gradient descent update does anyone remember why large values in your inputs are not desirable. Yeah. Right. So if I have, if I'm just trying to optimize, you know, a linear function in one dimension, I effectively have some sort of quadratic curve, where we know that there should be a global minimum somewhere. And I'm taking these little steps down the gradient approaching that curve. But remember, the steps are calculated not only by the error, right, the distance between your output and your target, but you also multiply that by the input that produced that. And so if those values are really big, you're also going to be taking that step that error, scaling it by the learning rate, which some small value, but then you're going to scale it up again by like 3000 or something. And so if you're close to that gradient, you could basically just jump over to the other side. Right. And so you will never approach the, the minimum that will allow you to optimize the function. So this does a number of things. One, it makes it difficult to converge. As we saw when we're not standardizing, if you remember the lowest our root mean squared error value got was about nine for this data, whereas after we standardized it got closer to three in the same amount or less training. And so one thing that we want to do is want to basically restrict the ranges of our values to between, you know, zero and one or between some known some known constraints. So one thing that I will do then is I'll calculate the means and the standard deviations. And then I can standardize my value by taking the raw values, subtracting the mean and then taking the quantity divided by the standard deviation. So, the key point, I only want to standardize using values means and standard deviations are calculated from the training data. Why. Yeah. Yeah. Yeah. So, I don't think in the testing data, let's say if we, let's take the weight of the car. And I have ranges between 2000 and 5000 pounds and I standardized all my weights and I get this nice distribution. And then in my testing data. I, one of my samples is like a semi truck or something and it's like, okay, this is now 30,000 pounds or something like that. So, I'm going to use this to standardize the range of your testing data such that the upper bound of your, of your, your standardized ranges correspond to 30,000 and you're testing data and only 5000 year training data, because then for that, for that value for that input. That 5000 pound sample is effectively going to be treated similarly to that 30,000 pound sample but in fact, that much larger value that much larger weight value has some implication for the output value you're trying to predict. So, you're doing your train, your train date training data and use those same means and standard deviations and standardized or test data mean that any outliers in the test data will appear as outliers once standardized respected for the training data so if I were to combine all my data, it would look approximately the same. So, we do that here, we insert the concept column of ones of course that's to insert that that coefficient that can be we can tune this bias weight to, and then similarly we add bias to our, to our name so we can interpret it later. So now I have my 314 by seven and 314 by two training data. So now I can train this linear model to predict both miles per gallon horsepower. So, one thing I will do here is, eventually this is the same function. As, as we did before, all I need to do the only thing I can change here is now I have my way initialization has two columns, because I need to, when I multiply by these weights, I need to get two columns out. So now this is going to be an inputs by two. And so then training is pretty much business as usual. I will date the predicted value, and you can keep the error and then I'm going to update that using a fraction of the negative derivative of the square error with respect to the weights. So I'm going to bring back of my squared error sums like in reported, and then I will print it out so let's run this. And you can see that we now can within about 50 train epochs we end up with an RMS of about 6.11. Does any of you remember what the when we were only predicting one value what the final RMS was after 50 epochs approximately nine. That was the first time it was before standardization after standardization it was. So this is also standardized now the comparison we want to make is RMS the after 50 epochs of 6.1 when predicting two values, RMS the of three points something when predicting one value. Does that intuitively make sense to you. I see some nodding why. Double I'm predicting two things will we expect the RMS value necessarily to be to scale linearly. Not really. But it does make sense it's bigger just intuitively because I'm predicting two things and predicting two things using how many things. So it may be that effectively, if I take horse power out of my input, it might be correlated with miles per gallon so it's somehow a good predictor of miles per gallon in some way. I take that out of the input so it's less able to predict miles per gallon because it doesn't have this one key indicator right so I'm predicting more things a fewer things. So I might try training for longer I could conceivably approach a similar RMS value, but also we may find that it sort of bottoms out somewhere. I mean, never quite get there so this is just pretty intuitive kind of data science if I have fewer inputs trying to predict more outputs going to be a noiseier model. It might still be quite good but it's generally going to be no easier. So I see that the I print the weights here and we can see that I've got two columns here. So right now it's a two by seven. And so we have the bias weights up at the top and then the weights associated with every input below. And so we can we can see a couple of things by just examining these weights so if we look at the weights for the miles per gallon target that's going to be the first column. So the horsepower target is going to be the second column. So take a look at this and see what what you observe. So what things are positively correlated with miles per gallon. Years, right. We saw that, you know, there's a clear because the way the data is ordering is a clear correlation. And we have, you know, miles per gallon typically got better over over the year. Anything else. Origin right yeah there's there is this slight origin where because because we had American cars at index zero European cars in the Japanese cars. And so we ordered in a way that was kind of friendly to British miles per gallon. And so what things are positively correlated with horsepower. Wait, right. So we need more horsepower typically to move a heavier car, for example. What else. Yeah, so acceleration is pretty strongly negatively correlated, right, whereas displacement is positive correlated to about the same about the same degree. And then you know it's like cylinders, you know, more cylinders more horsepower. So, you know, we just think about what we know about cars. Some of these things make sense. Let's take a look at the biases right the bias is what. See, your whispers. It's the y intercepts it's this it's the, it's the, you, it's the some axis intercept depending on how many, how many inputs you've got but it's the thing that I will default to if I have no other information right. And so we just look at these values. Let's pretend all the, for we have some sample where every other value is zero. Eighteen seems like kind of a reasonable value for for miles per gallon. It falls within that range of known miles per gallon values. Same for horsepower right 83 horsepower is like not very much but it's a reasonable value for horsepower. And so if I don't have any other information. That's as good as starting places any 83 for horsepower is a better starting place than 18 for horsepower thinking about cars are built in 70s or the eighties. Right. So, just when you're doing these linear problems. Your intuition kind of plays a lot of a big role and just sort of looking at your errors. And this becomes more and more difficult as we explore more complicated problems. But the fundamental operation of optimization kind of remains the same so this foundation of linear regression is going to be the set of operations that we will basically keep keep modifying slightly each time as we get more and more complicated. Until we have arrived at things like neural networks and reinforcement learning so just kind of keep that in mind. The same basic operation of inputs times weights equals outputs is going to be the core of pretty much everything that we do in this class. power. So my prediction shape should be 78 samples by two. And I will plot these actual versus predicted for both of these. So here are my two charts. This is predicted miles per gallon. And this is predicted horsepower. So one thing we observed is that it didn't converge quite as well as the single model to predict the one value. If you remember to think back to I can scroll up in the notebook if I want to see it to what that plot looked like we were trying to predict just the miles per gallon. What do you remember about that compared to these how do these do compared to your memory of that, that plot, standardized one after we standardize the lines. The lines, maybe you better fit. Right. Yeah. So that line after we standardized that line is like more or less dead on right it kind of cut nicely through those data points were here. We see both of them are a little bit on the low side. Right. And this also attests to the increased difficulty of optimizing this, this function. So there's only six inputs trying to predict two outputs. And because of the nature of horsepower versus miles per gallon. We were basically removing some information from each classification by taking that information out the input. Yeah. So does the difficulty increase the number of people who are working with the opportunity at three. I mean, generally speaking, yes, it depends on the nature of the inputs. It could be that like. There's a different use case where you might have three inputs that are just very information rich for some reason and really good at predicting those four things. There is like a strong correlation between those three inputs and those four outputs. And so in that case you wouldn't really see something like this. But generally speaking, we have sort of a bunch of data that you've gathered for a bunch of different samples. Yeah. Yeah. Yeah. So there could be like two columns in this data that are not really correlated right now. You could maybe, you know, Acceleration is not maybe a strong predictor of miles per gallon or something, or it's at least a weak one. So maybe I could remove acceleration and predict miles per gallon acceleration at the same time. I wouldn't see as much penalty. Right. You can experiment with this and see. Yes. So, you can see that the non informative form is. In place. By the non informative and turning the results. You, you can empirically verify this by removing those things that have low weights and see if you get a higher correlation with the other weights. Or get with the other inputs. Usually the non informative ones don't affect it very much. That's why they're not informed. So this these weights that are close to zero. Eventually what it's saying is that this thing, given the data that I have, it's not a strong predictor of this one ray, the other if I had to remove it, I wouldn't see a whole lot of effect. But you might, right. You might see that actually this is just noisier. The reason that it's the reason that it's poorly correlated is basically if I plot my out my desired output versus this input, I just see things that are all over the place. Right. Maybe that's adding noise in which case removing that input might actually increase your performance. So what you'll find is that like machine learning is just like a really empirical field and a lot of the times the answer to. Does this happen if I do this is do the experiment and find out and this may be some people doing really deep theory of machine learning they can have some like proof of this. But in most cases, which you're going to find is we didn't experiment we changed the following conditions and the output was this and therefore we've evidence suggests that this is the case or not the case. And so likewise, this is going to be a very experimentally driven course you're not going to be doing any mathematical proofs. So if you're worried about that don't worry if you're really hoping to do that I am sorry. So let's see, quantitatively how well we did in terms of the root mean squared error so in this case, what we get is for the training data. I will have for the member we get error values for both of those outputs. And so for the miles per gallon I have root mean squared error over the training data six point six point odd. And for the horsepower it's 25 point six. And then for the testing data. It is also quite similar. Right. We see numbers that are really close. This tells me a couple of things. One is that my training data is. It does resemble my testing data in a reasonable sense. And also we can see that you know maybe in raw in terms of raw value. It's harder to predict course power than than miles per gallon. So we can just put this out in a more friendly form here they are again for review. What is this star doing here. Anyone know this particular intricacy of Python. Yeah. Right. Yeah. So I've got, you know, two, two arguments in this list, rather than having to just iterate through this list and print them out one by one. I can shove it all into a single line right and just split this out so that these. This is basically being treated as an art as a list argument into the format statement. So then it's been is generating the output just nicely. So, just a little demonstration of this if I have this function foo, or I have three arguments XYZ. If I define a list that is one, two, three. Okay, there's three things here this function needs three things. This should work. But it of course it doesn't because really this function is expecting three separate arguments. And this has a single argument that happens to get in three elements. But if I put the star here, then I can break this down into those three arguments and it'll that function will work. So that was in the side, but it looks like we have a bigger error for horsepower, right, in our, in our model. And so maybe, you know, one way of thinking about this is maybe this is due to the larger range of horsepower. Right. So miles per gallon is pretty constrained between 13 and 25 or something horsepower is bigger. The values are bigger, but also the range is larger. So if I just put the minimax of the each target in the testing data. Then I get something like this. Right. So for the, for the miles per gallon I have a min of 11 actually max of 48, whereas for the horse power I have a min of 44.3 and a max of 255. Yeah. So this is going to be after. So what this will do is this is going to be before normalization where I've unstandardized the outputs. So these, these are the actual, these are the actual values and these are the raw, the raw. Okay. Any questions about this content or the basically the previous two days worth of, of content on linear regression. All right. So, let me then go into the assignment. So, this isn't in the way. Okay. So the assignment, if you go to the first assignment, sign one linear regression with sgd. You, first of all, it directs you to download the one notebook from the public facing page so just as a note, all these assignments are going to be posted here. If you click on this, you get this, you can download it using this. If you click on this often it's going to happen is you get a bunch of Jason. Just save this as in the dot IPY and V format and open it up in Jupiter notebooks and it will let you actually get the interface. Remember, you need to basically install Jupiter and then start the Jupiter client through the, through the command line, and then you're going to get this window that I've got here. And this will turn this into this. Okay. Also, those of you who have accommodations through SDC these are already put in. So, for those of you who got those, it kind of his handles is automatically so the late penalty. You remember 10 10 points off every day that's late. This is factoring in any blank of accommodation that you may have. You know, it's the submission box closes 10 days after the assignment is due because if you submit it 10 days late doesn't matter if you did perfect it's still zero. So, I will just advise you that it is strongly to your advantage to get things in on time, even if they're incomplete. So, for various reasons pertaining to how I calculate your grades. So, it's really better for you. If you get a 70% assignment in by the due date, rather than, you know, an 80% assignment in two days after with the late penalty. So that would just something to motivate you to manage your time. Of course, things happen if you do need an extension reminder to let me know. Twenty four hours before the new date. You're going to get a automated greater. I'll talk about how to use that this downloaded here. Question that was it. Okay. Yeah, since a one greater.zip. I'll show you how to how to use that in a minute. Okay. So the first assignment linear regression with SGD. So you're given some starter code that has these sort of fill in the blanks. And so you're going to need to fill in the train, use an RMSE functions and then apply them to some other data. So, basically, the, the skeleton code is given for you. We give you the inputs. We do part of this, like the printouts and things like this and the returns for you, but what you need to do then is fill in each of these steps. So everything that you're asked to do here is contained in notebook three in some form, which are going to need to do is look through, find that piece and maybe change the code slightly to accommodate either the way the data is formatted, or what we're asking you to do here. So this is going to be the standard form of all assignments in this class. And the steps are effectively written out for you in sufficient detail. You just need to figure out how to implement these steps. So the model is going to take in the inputs, the targets, the learning rate, the number of epochs and then an optional argument verbose. And so that's just going to be, you know, whether or not you trade you print out during the training process. Use is going to take your basically at the end of train right you return this model, use to take in that model and then for some input x predict what the value is according to your train model. So the shapes of the of each component given to you. And then RMS E is similar. You take in a matrix of predictions and a matrix of target values. And then you return the root mean squared error between those predictions and those targets. So, um, generally you'll the assignments are going to be split between coding and discussion. So in this case, you're going to get 60 points for writing the code. And then the remainder is going to be experimentation and discussion. So here's the train right calculate the means of standard deviations. We've done all this in some form. And then RMS E, right, this I don't really just spell that out in the code. What you're going to be given then is basically some simple dummy data that you can verify whether or not your your limitations are correct. So here's a simple example. If we just have, you know, some, some function, the generates x and T, then you can verify whether or not your your implementation produces the same results. So, you know, we're working train implementation or I'm not going to run this because train is incomplete break, but a working train implementation would give you a result like this. And it working using the implementation should look like this. So a strategy you might use is basically copy this so that you can refer back to it see whether your results look the same, both qualitatively and then quantitatively by continuing by looking at the numbers, and then the automated rate will run a set of unit tests for you. So, you know, here is just the plotting for for these tests. Then the real data that you're going to be working with is this weather data coming from the CSU weather station. You can get the data file here. It's just a text file right now. So just save this. And it looks pretty messy, of course. So the first thing you're going to need to do is load it up in pandas. So you get five points for reading it in to pandas and then check for missing values. Remove samples that contain missing values. So you're pretty standard data cleanup. Once you've done that, then you need to create a linear model that will take the data here that you have like, you know, days. It's a year's worth of data. You have the days average temperature and maximum minimums and things about like the wind, the barometric pressure and stuff like that. Your goal then is to predict the next day's average temperature. If you look in this data set, you will notice there is no column saying next day's average temperature. Right. So you have to manipulate the data set a little bit to extract the next day's average temperature. It is a year's worth of data. So you will be able to do this for almost the full year. Think about it. There's going to be one. One sample that you will not be able to predict that for. So you can just focus on like some of these features. There are other features in there. If you find them to be useful, you can do that experimentation and discuss it. But if you're focused on like these features, then what you're going to need to do is you need to modify the data file to add this next T. And so, you know, here's a, here's how you can, you know, hint. There's a hint on the naming convention you can use. So then you select these eight columns up here from the data frame assigned into an umpire a and then you assign X to those columns for all but the last row. There are reasons for that. And then assign T to be the first column for all but the first sample. And so now the first sample is associated with the first row in X, and then that's the T, the T app for the following day. So then use the train function to train a model over X and then the T data and then try different combinations of learning rates and number of epochs use the use function to predict and make plots of the target versus the predictions. So you can do some discussion of your observations, and then you can use the value of RME as your metric here. So for a linear problem RME is a decent metric for other types of problems. It tends not to be all that useful. Then do some discussion about like which, which inputs are most predictive of the desired output. So we just did that kind of sort and print the weight. And then you can discuss what's positively correlated with the university correlated. So this automated greater will allow you to effectively make sure that you get the 60 points for the code. By passing the unit test that will be enough. And then the remainder is just running what should be functional code and discussing your results. So, if you know how to run, you know, the, the Python script in a Jupy notebook. Basically, if you download this to look straight to a one greater dot py make sure it's in the same folder is the notebook and make sure what I recommend doing is really just putting them alone together in the same folder. So don't have any other documents in there. It's looking for a. A Jupy notebook that has the name formatted in a particular way so if you have like two copies in there it's going to be a little confused. So run this, run the cell, and it will basically print out, you know, the outputs of your unit test and what the score is for each of those. So you're going to use a similar, uh, grading script. That's basically the same things which is different specific values right so that you can't get away with just hard coding the answer in or something. We will look at your code of course right so we're not going to just take the the autograder on blind faith. So, you know, don't, you're, you look at the automated greater code to understand how your inputs and outputs are supposed to be formatted. So you're going to use the autograder code to see what the answer to the unit test is and hard coded in the notebook right. So you're provided this, you should look at it, but you need to be using it in the appropriate way. Finally, you can get one point of extra credit on this assignment so this assignment is worth 100 points so if you get the extra point you get 100 1%. So, problem is that if you think about whether data, the previous days temperature is probably pretty close to the next days temperature. So, we're going to make sure the after we moved here we had that one day we were like 30 or 90 degrees one day with like a forest fire and then 32 degrees next day with the snow. So that was an outlier in most cases right it's 30 something degrees today and it's going to be kind of similar to that tomorrow. So, in a time series, the best solution may often be to just predict the previous solution. So, you can actually value in this case work a lot like the TF shifted to one time step. So to do better you can try to predict the change. So rather than the raw value predict change where your target is now the next days temperature minus the current So, if you set up your target data like this run the previous experiments no need to train to change the training function just retrain the model. Then you can do other experiments you with different hyper parameters and then report on your results. Okay. Any questions on this. Okay. So this is going to be due February 9th. And this you have any extension already. And so I recommend that you start early. Yes. Oh, good feedback. Yeah, so you'll get you'll get a number out of the of the unit test that you pass you get like X out of 60 for the code. So basically what what you'll get is you'll get a number for the code and number for the discussion. Usually these try to say, you know, great workers, I mean, if you get 100% basically means there are no, no glaring errors. If you notice something interesting, you know, sometimes I'll comment on it like you had a particularly good discussion. I will mention, you know, for these points in the discussion. If you didn't do something and then how much you were deducted for that. So you try to get things back within a week, although there are like 75 of you and only one TA. So we asked for your patience. Sorry, particularly ask for your patience. Our goal is to get things back within a week after that for the deadline. Give or take a day or two, but sometimes there are unforeseen circumstances. Okay, questions comments. Yeah. Not change, but not add any cells to use on you. You can you can add cells. If you if you do like a bunch of experiments, you want to like show the results, you can just say, you know, run train with this number of epochs in this learning rate or something. You can do that. You cannot change the skeleton code that you were given because the automated graders expected things to be formatted in a certain way you may break it. Yeah. The future assignments kind of follow the same model of like, you know, look at the last week's lectures while we're doing it or will they have a. They might separate a little bit depending on how things go, but it's, it's not, it's, you will look at previous lectures within the last two weeks typically. So, for example, if you look at the calendar. Assignment. So like assignment to mostly going to be contained within notebook five assignment. I think four is, and I'll just give you notice like four is the one that tends to, it tends to be the most difficult. That involves putting these together from a couple of different notebooks like nine 10 and 12. So things get a bit more complicated as time goes on, but generally yes. Okay. Yeah. Is there anything that was. Yes, I thought I put that in the notebook. So. Oh, yeah. So you need to name your notebook as last name dash a one. So in, we'll download everything through canvas and renames everything. He's like, I don't care if two of you have the same last name. That's not going to be an issue. So yeah, just do that. Okay. All righty. So let's move on to sort of starting with nonlinearity, they're not in the way that we're necessarily going to do it for the rest of the course. So these models have been linear in the parameters and linear in the input features. So this is going to be not very useful for things where there's a nonlinear relationship between the input and the output. And of course, there are many things where this is the case. I think a linear model is like a really good first step for many things, but it's often not the best model for most things. So one thing you can do is basically screw around with the inputs by applying some function to them before they even go into your model to make the relationship nonlinear effectively what I'm trying to do is I'm trying to see if I have a linear model. That's the only thing that I've got. But I suspect that there's a nonlinear relationship between some featuring the output. Can I do something to that feature to eventually make the relationship linear. Right. So, for example, if I can, if I observe the data, if it is observing the data that does like a squared relationship between some input in the output. I can already tell that linear model is not going to do me very good, but if I could just if I just squared the input, suddenly I would be able to do something with a linear model. So, with fixing nonlinear features you have a way of introducing nonlinearity as long as you know what kind of nonlinearity you're trying to introduce. So let's say you've got a single feature for each sample and your data matrix will look just like this x zero through x n. You can try a bunch of other features like by by squaring it, or taking the cube or taking the fourth power. And now I have a bunch of other basically features that are that are derived from that first column that might be helpful. And this is simple to do and Python just using the h stack function. But we don't really know which of these powers and x is going to be useful. So we can look at the magnitudes of the weights. Right, we may find that maybe this fourth feature here is highly correlated with the output and that might tell us something. And as long as the input features are standardized we can do this but we can actually do some more things. So for example, if you can actually build multiple models. So we can do some of the different graph samples of the training data by trying different feature functions over these things. And then we can compute confidence intervals of the weight values. So if a zero is not included in the range of weight values specified by the 90% lower and upper confidence limit, then we can say that you're 90% certain that the value of this weight is not zero. If the range does include zero then the corresponding feature is probably not one that's all that useful. A booster sample is a smaller sample that's quote bootstrapped from a larger sample. This is a type of resampling where you take large numbers of smaller samples, repeatedly from from the single original sample with replacement. So here's a more in depth explainer of that below is some code that will illustrate this process so we will include this value lambda, which is going to be a penalty on weight magnitude we don't want things that are like. Over one the correlated or anti correlated with this, we want to find things that are predictive, but not like hugely so. So, let me import numpy and pie plot import random. So, here's some here's a train function. This notice, this uses the least squares function so this is this is not the answer to assignment one. So if you try this it's not going to work. And the, the use function is also not the answer directly to assignment one. Root mean squared error. This is very close to the answer but I don't really feel that bad about giving away because it's like it's one line and it's not that difficult. Yeah. Yeah, you can I think I guess just. Yes, I believe you can because there should be no automated test on that so it shouldn't it shouldn't fail that and worst case scenario I'm wrong and you try it, you run the autograder right and you'll find out if you if that was the mistake you made that's easy, easy to fix. Okay, so here we have some train use and or messy functions. You will notice I'm using this NP.I. So what is that that is. The identity matrix this is a 2d array with ones in the diagonal and zeros everywhere elsewhere so NP.I. I assume I equals I for the identity matrix. So let's make a simple function of x. So we'll just take this function here negative one plus point one x squared minus point zero to execute plus point five and and is just going to be a noise drawn from a standard normal distribution. So I will take 40 training samples and 50% of them all uses training data. So now what I can if I look at my X and T. What I'm doing here is I'm going to stack 40 samples from actually some day 40 samples between zero and three and 40 samples between six and 10 for total 80 samples. And then I'm going to reshape them. So now if I plot my 10 my testing data versus my training data, I get something that looks like this. And so if we do this just using dots. Right, so we have this distribution. So now what we can do is we can add squared and cubed values of each feature and go up to the fifth hour. So now I can have this X F, which is going to be my sort of my additional features. So I'm going to take a data into training and testing sets randomly. So if I've N rows. Same as you did before. I will then just basically take all the row indices shuffle them and then slice out those row indices from my, my training data and then make the remainder into my testing data. So what I will do is I'll take, you know, N rows times the training fraction, what around this right of course, because in case my training fraction doesn't divide neatly into my number of actual samples this case it's fine. So I'm not going to do it. But if I had, you know, I'm going to do like a 70% training test split and I had 315 samples that might not divide all that well. So does that to be exactly, you know, X and one minus X percent. It's just got to be pretty close. So if we do this in this case we divide very cleanly we have 40 training 40 test samples. I can color code my samples. I look at the train and test data so what you want to see. If you have the ability to plot your data, you want to see something more or less like this where it is a neat overlap between the training data and the testing data. We can observe a couple of things here so like there's no test sample that's really well represented in this area of the input. So it could be that maybe a model fit to this is not going to necessarily perform all that well on things that form the fall kind of within this narrow window. That's okay. This is this is all generated randomly this is the sort of thing we expect. So we have to make models on a bootstrap samples of training data so models will just be a list of models one for each bootstrap sample. So what I'll do is I'll create 100 models, and then I'll draw random samples with repetition from my from my training data. So for example here I can specify that I want to draw these samples from a range of. So here what I'll do is I'll take n train minus one, and then draw that those those samples with repetition. So then I run the train function over this and now extreme boot and T train boot you know I bootstrapped training and target samples. And so now I can basically create this list of models that are just weight matrices solved with the least squares function that should fit to this data. So how many models do I have I have 100. Let's look at the first model. Right. And so now we see the outputs formatted as the weights. So these are the weights. And then these are the means and standard deviations of my inputs. So I'll add all these models to the test data. This is pronounced why all I grew up in rural New Mexico, where they think they're part of the South apparently so they said y'all a whole lot which is a little bit weird. So I will take for every model, I'm going to use the use function, and I'll take the result to that and then I will append that to my why all which is going to be why is going to be our term for the for the predictions. So what is the shape of why all this is the list. So what I will do I'll turn it into an empire. So we have 100 by 40 by one. So we have 100 models. Each of them is pretty making 40 predictions, and then one. Right, what's the one. It's an extra dimension right so it is one column in the sense that like all of these things are being shoved in there and they have one prediction each but it's just, it's kind of just there I don't really need it all that much. So I can use the squeeze function, which will reshape this into and preserve the same content, and then effectively just get rid of this extra dimension that's that's ancillary. Okay, so I can turn my why all into the shape that I need by applying the squeeze function then transposing it. And then I look at my why test. And I will just take this to be the, the mean of all of my predictions for every model. This gives me 40 values. So now the RMSE test is going to be the mean if I take the mean of the error between the why test and the t test right the actual value. And the square this is going to be my RMSE. So my test RMSE is going to be 4.0 roughly. So having boots strapped all these models. I take the mean of all those predictions I can use that to compute a root mean squared error value. So now let's take a look at some methods we're going to put to use next. So now let's take a look at some methods we're going to put to use next. There's two things here. Np dot linspace, this will return. And evenly spaced numbers over an interval from a to b, and then NP dot reshape, which you may be familiar with, we basically take some input and reshape it into new dimensionality as long as those shapes are compatible with preserving all of the data. So you can use the question mark to look at the doc string to figure out exactly what the input is to a function if you are countering errors. So now what I'll do is I will create this even distribution that is choosing in this case 100 evenly spaced numbers between zero and 12.5. And so we end up with this. Same thing with reshape. So if I look at the shape of this, right, this even distribution should have 100 numbers in it because that's how many I created. I can reshape it into a column matrix, just by doing n plot n one, right, so 100 by one is still 100 so I preserve my hundred samples now there's there in a column like this. And if I look at the shape this is now 100 by one. And the only real trick is that the new shape has to be compatible with the old shapes like could reshape this into 50 by two and that works just fine. Right. And even this would work right 25 by two by one by two. This will function, it'll preserve the same data. It's just now I basically got a four dimensional array, where things are maybe not very clean the organized depending on what I want. But you will see things like this, like, if you're working with RGB images right every pixels represented by three values. But you're the convolutional net may require that everything be flattened into a single array or maybe three arrays for each of the channels. And so you have to reshape and end by end pixel image by three channels, right, and then flatten out the individual pixels in each channel so you basically have an array for each one. So the reshape gets more and more complicated as the, as the operations in the architectures get more and more complicated. But the real thing is, I need to know how many numbers I'm working with. I need to make sure I'm not trying to cut numbers or add numbers. So, if I change if I do try to do a reshape over a hundred and one by one over a hundred samples that's not going to work. Right, of course, because there's no way that I can, I can turn 100 numbers into 100 and one numbers. So, what we can do then is I will generate some evenly distributed numbers all then each stack basically the powers of those numbers together. And now I end up with this, right, of course, zero, and power anything is zero. It's now we have all of my numbers from that original distribution up to the power of the powers of five. So now let me use the evaluate method. And what I can do that, use that here. This gives me 100 by 100 predictions. So remember this use method is being applied over every one of those hundred models in models. And so now I can plot those outputs. Right. So, for those models, each of them is a line. Right, and we can see that they are all, you know, better or worse fits to this data. And so, you know, based on those, those power features that I've accumulated. And so you can see that for, for example, those odd powers are these ones here where it's, you know, probably dips here and then goes back up later where you have this quadratic or the quarter powers. This is going to come down, you know, from the positive value, and then hit our global minimum and then go back up. And so you can see that this data can be modeled by a quadratic function or a cubic function or a quarter function or a fifth power function. But once we get outside of distribution, this data is not fit to that. Right. So we have no idea whether the next sample is going to be down here or up here. Right. And depending on what other samples, you know, show up in this data set, we will find that either like the the cubic function is best, or the quarter function is best or so on. Okay. So now let's try some other data. So some real data, this is going to be related to the design of holes on yachts. So I guess like if any of you have a yacht. Normal people have. So let's take a look at this. So we have this yacht hydro dynamics data. Maybe it's like the early sailing yacht. You know, those are small, not the mega yachts. So look at this data. This doesn't really mean a whole lot to me, certainly, and it may not mean a whole lot to you if you don't know things about boats. So let's try and get some information about it. So I'll just say that I happen to know what each of these columns means. Center of buoyancy prismatic coefficient length displacement ratio beam draw ratio, length beam ratio and this thing called the fruit number. And then these are all my inputs and I'm trying to predict the resistance. Right. So I have these things. I don't really know what they mean. So I'm going to go ahead and take my head around there with the names. Okay, so now I can see which of these values are associated with which of those parameters. So we have, you know, a bunch of these similar values. They're kind of all the same. This fruit number is kind of slightly different. And then the resistance value changes and the center of buoyancy may be the same for all of these boats. We can plot the data to look at it another way. It can be messy to look at them. Look at the data, just in chart form. So we look at center of buoyancy. And there's just, just kind of, there appears to be three clusters here, but there's no clear relationship between this value and the resistance right. The same kind of seems to be true for most of these things. Right. There's no monotonic relationship or near monotonic relationship between resistance and any of things, except for what this fruit number right I don't know what the fruit number is. But I can see that there is there's some sort of relationship on some kind of power curve with this right so I can at least say that in some universe, the fruit number is predictive of resistance in a way that none of these other things really is. But this gives me something to look at. Another thing I can also do is just plot all of the values, all the raw values. Right. And we also see that this curve appears repeatedly. If I just go through each of the samples so right. This appears to be the same thing as the fruit number. If I took all this pink curve and to snatch all those, all those samples on top of each other, I would get something that looks a bit like this. And what I can do is I'll then train a model to predict T from x. So I'll run my train function my use function again. And this gives me an RMSC of 8.8. Don't really know exactly what that means relative to other things until I tried different types of feature combinations. So I can plot the targets over the predictions, and we can see that they're not very well aligned. Right. So this model is just a linear model predicting this resistance from these from these inputs doesn't really seem to be fit very well to this data. So now plot the predictions versus the actual values. So I'm going to look at this like this. Okay, clearly, our predictions are the red line. The actual values are the blue line. A linear model is not going to fit to this data very well, except for a couple of points here and there. So this last variable, the fruit number, it looks like the square root or something might be linearly related to distance to resistance. Right. If you look at this. Okay. So if I'm some root of this, I can probably squish this down to a line in a way that I can't do for the other things. So let's give it a shot. If I plot the resistance versus the fruit number, I get this. This is a steep nonlinear curve. So let me try and see if I can make this relationship linear. So the first thing I will try is taking the square. It's a little bit better. It's kind of hard to see the difference, but this is something of a shallower curve at least so maybe I'm moving in the right direction. So let me try the fourth power. Okay, this is getting better. Right. So this is clearly more linear than before, even though it's not completely linear yet. So not quite there as try the eighth power. This looks like I'm, I'm almost there. It may not be perfect, but it seems like as I increase the power at least up until about eight. This is turning this highly nonlinear relationship into more linear one, one that I can predict using a linear model. So let me try it with the actual data. So I will take, remember, we took X and we stacked on like the fourth, fifth power or something. Or actually does the previous data. My apologies. We just take X, which is the raw numbers, then we stack on the eighth power. So now I can train this. And now I get an RMS E that is much lower, right, went from 8.8 to 1.5. And the plot will confirm that I'm doing a much better job fitting to this data. Right. So I'm getting a lot more overlap. Seems like I'm on the right track. Other thing I can do is plot the predictions versus the line seems to be kind of a decent linear relationship there. So let's take a closer look with zoom in on the first 50 samples. Let's see if we can do any better. Right. So this is doing pretty good, but there's still some cases where maybe I could, I could fit better to this line. So it seems like higher powers work better. We don't know, like how much higher we need to go. And at what point I'm going to start getting a diminishing rate of return by increasing the exponent of X. But I can do then as I can visualize how the model does in terms of RMS E versus the exponent. So what I can do is just run, again, run, run a bunch of models, and then look at the RMS E. So here's that first one we saw as just the raw input has an RMS E of 8.8 4.0. And now we start seeing these 1.0 values. So it seems like the RMS E, if you look at this, is bottoming out around about 13. And then after that, it actually takes up slightly, but not enough to really be meaningful. So it seems like somewhere around 13 appears to be maybe my, my best value for the for the exponent. So I can confirm this by just plotting the exponent of X versus the RMS E. And here we see this rapid decline. And then at about 13 or so, you know, it sort of bottoms out and receive this flat line there. Okay, so now finally what I can do is I can try the different exponents of X to for a 13 and then 16 for good measure, and then use, use that data and then plot predictions. And this seems to be, you know, probably about as good as a predictor of this, of this resistance value as I can get. So if I look at the predictions, these are the actual predictions. It doesn't mean a whole lot. But if I plot this. Again, the line, I can see that now I've managed to turn this into a relatively nice linear fit right it's not perfect as we as the actual value increases, the distance for the prediction also increases. But that is kind of to be expected. So we've discovered something with this fruit number and the resistance value. Is this a reasonable supposition. Well, what is the fruit number in continuum mechanics, the number is denoted FR and say dimensionless number defined as the ratio of. Flow and inertia to the external field. In other words, it's a significant figure used to determine the resistance of a vessel through a liquid. So water or even air. Ennevel architecture is explicitly used to determine the resistance of a partially submerged vessel. What's the relationship between fruit number and resistance? Well, I don't know. This paper does, if you're interested to find out more, but we did empirically discover that there is a relationship using machine learning techniques. So this is the point where you can, as a data scientist, you can basically discover things without a lot of, you know, topic knowledge. You then want to talk to someone who knows something about that topic in order to figure out how to interpret your results. So one thing that I will harp on is basically like where you have computer scientists kind of like rediscovering some field. So like, since I worked in NLP and actually originally a linguist, in like 2013 there's a lot of papers saying, Oh yeah, we discovered linguistics using machine learning is like there's literally a whole field. It's been doing this for like 1000, 1000s of years. So you're not new. So periodically, computer scientists would like discover something awesomely new using machine learning and it turns out there's like a whole field out there that has been doing that for a long time. So it will serve you well to consult with experts. Nonetheless, you can discover relationships like this using machine learning techniques. This has been an introduction on the narrative and on Tuesday, we will throw all this away in order to introduce an arbitrary method of introducing on the narrative. That is neural networks. So, good luck with the assignment and I hope you have a nice weekend and I will see you on Tuesday.