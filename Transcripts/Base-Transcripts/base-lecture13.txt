 Yeah. And. Yeah. Yeah. I'll. I'll be. I'll maybe. Yeah. I'll be. I'll be. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. You can do. Okay. I don't think about. I'm trying to do that. I can do that. I can do that. No, we're doing the things that are going to be happening. I'm trying to do this. You're trying to do this. I'm trying to do this. I'm trying to do this. I'm trying to do this. I'm trying to do this. I'll continue to do that. I'm trying to do this. I'm trying to do that. I am trying to do this. I'm trying to do this. That could be good. All right, you're all ready to start? All right, you're all ready. Okay, so everybody should have turned in assignment two, which is good because I can pick up on the notebook that I left off on last time. We will try to get this back to you within about a week. I'm going to go finish this code reuse notebook is a short. Then we'll start classification using neural networks and then I will assign assignment three. So I'm going to assign assignment three today. You have like a solid three least work on it because I didn't want to give you something during spring breaks. This is due to Tuesday after spring break. So you have time. I do recommend that you start early, especially if you want to have some, some of some spring breaks. I'm not going to be here. And so of course, I'm not going to be holding office hours. So recommend you start early and give yourself time to work on on this. So I guess without further ado, let's do this one for real. I hope you enjoyed the extra 30 minutes I gave you on Thursday. So these notebooks are now available again. Note that we're not going to do a reread on assignment two because basically the answer is now given to you or an answer is not given to you. So, you know, this, this one is not going to be eligible for. If you do want to read some assignment one reminder to those are. That's a hard deadline. So we will not be accepting anything else on assignment one after midnight tonight. So, as you recall, you know, if you want to modify your network implementation or any other implementations, you can either add arguments to parameterize the different behaviors, or you can replicate code from the original class using class inheritance. So first, let's just show our network implementation. I'm not going to go through the code because you've already seen most of this before. Here's optimizers and has an admin implementation there. So now we will import optimizers and then here's a version of the neural network code. You'll notice the familiar functions that these have all been filled out. So you're this is basically your implementation or something close to it if you got full credit on assignment to. So that what we had before was we had activation function argument, and it, you have to change this to specify whether you want to use 10 H or Rayleigh. And so that was, that was part of your assignment, and this had to be added in the neural network instructor calls in that in it function. So now what I can do using this is like an out define two different instances of the neural network at 10 H version and Rayleigh version that have the same architecture otherwise simply by just passing the activation function to the to the constructor. So now if I just print this, it will show me that I've got an instance of neural network that has an input input size of 10 hundred hidden units, followed by 58 units, and then I'll put size of one to use the 10 H activation function. So this is the right with activation function. So, the same two kinds of neural networks could actually be implemented just using two separate classes. Right. So we can say we can have two classes, one called neural network 10 H and one called neural network really, but you would observe already I think that most of the code in these two classes is going to be identical. And so if I duplicate all this code into separate classes, you are effectively buying yourself a future headache because one day you may decide that, oh, there is a bug or I want to change some functionality, and you do it in the 10 H version, you have to replicate it exactly in the version or your code is going to work differently. So you make one mistake. And all of a sudden you have diversion and limitations. So, you know, one of them might not work or may not work the same and suddenly your results are not directly comparable. So instead what you inherit most of the functions from one class to create the second one and only change those things in the second class that would directly override things in the first class. So this would be going through using real time where I can find my typos. So this would be readable if you just defined two functions. So we'll call the activation function activation function gradient. Of course, the things that I want to change about this, this new version of neural network is going to be the activation function. So, if I just define these in the first class, then I can overwrite them in the second class with a different functionality and everything else can remain the same. So it makes sense to define the second class first, and we'll just see how this looks in terms of code implementation. So if we assume that neural network 10 H is already defined, I think just define my neural network relu by passing the neural network as an argument into the class definition. So now we have neural network relu will inherit everything from neural network 10 H, except now I have these two functions for the regular activation and the real gradient that have the real specific behaviors here. And so presumably they were just to overwrite any equivalent implementation in 10 H. So we run this. And it didn't give me an error because I sorry, there's an error there previously, but then I ran the rest of the notebook. So if I, if I cleared the kernel and run this, get an error. Okay. That's because the neural network 10 H function. The first time you run this notebook has not been defined because I've already run the notebook to completion once it has been defined so it doesn't give me that error. But if you got that error, what you need to do is you need to find the original neural network class. Redefine to use this activation function and activation function gradient functions, and then I'm going to rename that to neural network 10 H that it matches the definition that the neural network relu implementation is expecting. So if I define neural network 10 H will observe that it looks pretty much the same as the current neural network implementation, except when I get down to here, and now have instead of, say, the 10 H function, or the relu function as you defined it I just have generic activation function and activation function gradient. Now, of course, these have to be when I define this, these basically have to be initialized in memory of some behavior. Right, I could just say pass, for example, I could just ask you were here, that would basically say do nothing, but then, of course, if I tried to run this, then I would not have that that requisite behavior so I'm going to define some default behavior. And for the purposes of this demonstration, I would just assume that my default behavior is going to be using the 10 H function right so if, if not defined your activation function is 10 H. So you'll see that we now return the 10 H and then we return one minus Y squared, which is the same as the gradient 10 H. So now forward pass same functions as before. So now I can define my second class. So now I have an instance of neural network relu that inherits from that 10 H version, but over writes the activation function as gradient. So now you'll observe that this version here instead of having this activation function argument that specifies in a string form. What activation function I want to use. I now can use this one. So here's neural network using the activation function argument and then here is the differences, different classes, where I don't want to specify that argument. So there are different advantages and disadvantages to this so like one advantage that in this small example is not that significant but might be if you're using this at a large scale is that like I don't have to specify the activation function name and a string. So that's what it's preventing me from saying making a typo and that string or something. So, in that sense, it's a little bit cleaner, but there are some obvious disadvantages. And in general, we'll see one of these techniques is preferred. Yes. Both you mean both the 10 H. Well, that's what we're doing in this first version here right because then you have if you have if I have all these functions available, whenever I run the neural network, I have to specify which one of these I want to use. Right. So that's what this activation function argument is doing. That's what you did in a two effectively right. So like there's. Both of these are valid strategies. And we'll, we'll see at the end that in fact the strategy that we were using is probably on balance preferable, but it's important to understand how you can do this. So when we do. I think it's assignment three, you have to basically take your neural network implementation or a neural network implementation for regression and redefine it to do classification, which is you, if you've been paying attention, there are some subtle differences there but you don't have to say, there's no differences there but you don't have to change the entire neural network architecture. So it makes sense to kind of inherit from a generic regression neural network and then overwrite certain functions to make it classifier. Yes. No, this is not necessarily for assignment. This is a, this is an example of how to use class inheritance. In an example that's simple enough to illustrate in a few slides but it's probably not what you actually want to use this for. Okay. Nonetheless, let's assume that you know, I'm just going to be using this to define different activation function so now if I print instances of this you can see that now I've got these two separate instances of what are different classes, although this one neural network really inherits from the first one, they have the same architecture. So now you can see that this approach you know worked pretty well, but now imagine if I have like a bunch of different activation functions right you talked about 10 H to talk about Rayloo. Some of you may have implemented the switch activation for a two right there are a bunch of other different activation functions talked about, you know, J. Lou, you would, etc. So I don't necessarily want to define like a different class for every single one of them when I'm just overriding like two functions. So you have to implement you know, let's say if I have three variations with each with two choices, right, I'd have, you know, two to the three different classes. So even this bit here just doing it for these two, these two variants seems a little bit excessive. So you have to remember like all the class name so on balance for this for defining the same type of neural network for this for a similar task where I'm trying to do say change the hyper parameters on balance for that approach. So I have an argument that I can specify is probably the better approach that allows me to implement the different functions that I need all within the same class I don't remember different class names. And it is to specify which arguments I want, because here we can eventually define a neural network with these four arguments where n H is a list. And it is a specify the input size the network architecture and the output size and the activation function for arguments for this function doesn't seem excessive. And so we need to be more so like if I were trying to define radically different neural network architectures in the same function. And instead of specifying, you know, feed forward versus convolutional versus recurrent in the actual constructor itself. I have a like type argument, where it's like CNN and then I specify all the other things I have to set up my neural network as a convolutional neural net based just on this argument. So that's not necessarily the best way forward for that so when deciding whether I want to use class inheritance or arguments for specifying neural network parameters. It's kind of important to consider what exactly you're going to be editing right so for activation functions. I would argue that probably using this this type of argument based method is preferable for different types of network architecture or different types of network, like classes entirely. And so it's like the first thing that's called the regular classifier is the, you know, the kind of functional nets recurrent net, etc. Then it makes more sense maybe to have some sort of base class that has like all the functionality you know is common to all these things. And then I just have to modify certain functions. That's what we're going to do next. So we can take a, you know, create different types of neural networks and then use, say the same implementation of like your optimizer your training function. Those, those things that maintain their same basic shape. And then change radically change the particulars of the art network architecture. Using class inheritance is probably the better way to go. And so for example that torch.nn module that we used briefly kind of provides you with all of this basic functionality is right so when you, when you do assign in three, for example, you're going to be using the network network network network software, which is like a machine network network network network. And so if you see things like, you know, lost not backwards. Right. Single function called it basically does all the back drop for you. That's part of that neural network module. And so I wouldn't have access to that function of I didn't inherit from the neural network module. So the more to the functioning of the neural net is probably a better use for class inheritance, whereas some of the more hyper parameter tuning like things that you're going to be doing in most of your neural network usage probably can be specified in in argument structures. All right, questions on this. Alright. And so I'm going to talk about this. I'm going to talk about this in a little bit about the animation and I will try to get through all of this so I have time to talk about assignment three. If we believe a little bit of this to Thursday, it's probably okay. So previously we talked about regression rights we did linear regression non linear regression all of these are predicting continuous values. Right. So you use the neural network as a way of introducing arbitrary nonlinearity to effectively construct a universal functional approximator for predicting continuous values. So the principles are that we have some non linearity that's applied to the output of some hidden layer. So that would be a say I have x times w gives me some set of scalars I apply some 10 inch to them. And this, then the output layer is going to be linear in w and that gives me my prediction. So now when I'm optimizing my neural network, the values of W are going to be optimized effectively with the assumption that there's going to be a 10 inch or just some other non linearity applied to it. And of course, if I optimize the weights for a 10 inch network, and this like clone them into a reloot network going to get drastically different performance works because the actual outputs are going to be different. So we have this scalar output, then what a classified categories instead, right what we did is we basically want to classify these things into probabilities that they fall into one of a set of K classes. For that we used in the previous notebook number number 10 we use logistic regression, this included this soft facts calculation given here. So what this does you know just exponentiate basically the output of the the output layer and then some overall possible outputs. So this is going to take the it prediction of x is going to turn this into a probability and that probability represents the probability that sample x of i falls into whatever whichever category is denoted by these weights or more properly, since all our weights are going to be sorted in a big matrix each column is going to be associated with a particular category. And so weights sub K of that multi class weight matrix w So the softmax on this this part here right this is the prediction this is the output layer in the sense that this is the output of that final multiplication by those output layer weights. So it's going to be again still some scalar value, then by applying the softmax function this allows me to turn it into a probability and then turn the linear model into a logistic model. So when we previously introduced neural networks as this way of arbitrarily introducing non linearities into regression problems. So now we want to do classification so what might be some thoughts you might have about how to do logistic regression as you may remember from notebook 10, but in a non linear way. You can have you enough neural networks that have multiple layers and activation functions that's our way of arbitrarily introducing non linearities right so if we just had that which would be a non linear regression function. And then we discussed that how we can turn a regression function into a classification function is the application of the softmax. So this is going to turn those K predictions that I want to make about some sample into K classes and the probability that it falls into each one. Right, so basically what we're doing is we're now just putting together the pieces of things you already were and you talked about softmax last week. And we've been doing neural networks now for a couple of weeks. And so we're just going to apply the principles of turning scalar values into probabilities using softmax to the use of neural networks as universal non linear function approximators. So let's just review some of the math so remember when you're doing classification, we're basically trying to arrive at the set of weights that would allow each maximize the log likelihood of the training data. We're trying to maximize the likelihood of the training data but we work in logarithms because of these nice properties that allows to add and so multiply. So we don't get these very infinitely small probabilities. Instead what we get is just a set of negative numbers and you're basically trying to figure out which of these numbers is the least negative. So that's a lot of the max amount of like the good. So just recall we're going to be using more or less the standard variable definition so dub big w is going to be the whole weight matrix. And then x is going to be all the inputs where big N is the number of samples, therefore x event is the nth sample. Big K will be the number of classes and therefore the little K will be an individual class index. So these are going to be the target values. So remember that the target values are now class indices. And so these are indicated variables. We have a string of zeros and then a one at the index index that represents the appropriate class. And so therefore T sub T sub n K is going to be whether or not sample N falls into class K. So you just think of this as being basically binary for K columns. And either is or is not a member of class zero and either is or is not a member of class one so on and so on until class K. And then by N rows for example. So now I basically just have an N by K matrix representing my outputs. So now P of C equals K given x sub N is going to be the probability of class K right on the left side of the bar given sample x of N. For simplicity's sake, we just say P of K given x of N. We can also rewrite this as some function G sub K of x of N. And in fact, we'll shorten it further to G sub N K. And then we talked about how this arbitrary function G that we arrived at is actually the softmax function. I went through a bunch of math involving exponentiations and logarithms to basically show that if we perform a certain set of transformations, we can then drive the probability for a set of classes. So if we have the likelihood of W, this is going to be the probability for all N for all K of the probabilities of some class K given some sample exponentiated to the indicator variable. So what that does is that this value is basically only, only exists only not is only not one when T sub N K is one. So if I raise anything to the zero, it becomes one, and then the product of course that just cancels out to just multiply it out. So similarly, if I then take the log likelihood on doing it is I'm now just turning on my products into sums I'm bringing my exponent down in front. And so now the logarithm of one is of course zero. And so the same is true if this T sub N K is zero, then that also goes away. And so therefore the log likelihood is going to be the sum for all N over the sum for all K of the indicator variable times the block probability basically. So now the gradient of log likelihood will be given as follows. So if this is our log likelihood function. And then we have G being realized is the softmax function. And let's just define why so the N K is being that prediction for the output of sample N for class K. So in other words, this is going to be the weights of that class multiplied by that sample. So therefore, we've now arrived at the same definition softmax function we had before. So now if I take the partial derivative of both sides, I end up with the following so I'm just taking the sum for all N overall K of T sub N K divided by G sub N K right this is the actual target. And this is going to be the actual prediction value. And then I multiply that by the partial derivative of G with respect W. So the general gradient looks something like this right so now we have effectively the partial derivative of the prediction with respect to the weights times T sub N K minus G sub N K. So this is going to be my ground truth target and minus by prediction. So once again, this part is my error term. Right. This is the same as T minus Y in the linear regression problem. This is basically what's my ground truth minus one that I predict. So again, T minus softmax is going to be how wrong am I just going to be how off are my probability distributions. Right. So if I'm very correct, then that for that ground truth, it should be very, very close to one, because in the target values, my only choices are zero at one. And then I just do that. And then I take the sum for all classes and then for all samples. For linear logistic regression, why is the NJ that is the product of the weights time the input. So therefore the derivative that will only exist when J is the class of interest. So I'm going to do this for each of my classes. But for all of my classes, except for the one that actually is the truth value, I'm going to get zero. So I just want to optimize away from those and toward the thing that I actually have a value for. So therefore, the partial derivative of the log likelihood of the weight with respect to the weights is going to be equal to the sum of your error terms times x sub DJ, where D is going to be that particular input. So the nonlinear version is in some ways a little bit simpler if you remember how our neural network operations work, because the fed to the all I'm going to be doing is the same neural network operation that we've been doing just with the softmax at the end. So let's remember the general form here. So we just have some weights w. So to include the nonlinearities, we have to have a hidden layer. And so we'll just call this V. And so then the log probabilities of the K classes given x are going to be H some activation function over x times V times W. So remember the quantity here, x times V that's that hidden layer output before the activation function. So previously we call that Z. Or maybe a, and then I'll apply H over a, maybe I'll call that zero to the output the final output of my hidden layer. Then I take that multiply it by my output layer, and that gives me my pre softmax prediction. So, because we're going to have to go doing in the world of logarithms. So I'm going to take the top part of the softmax function, right, he exponentiated to Y sub n K. The log of that of course this will just cancel it out so we'll assume that all logs here are natural logs. So now I just have Y sub n K. So now Y sub n J, where J is just whichever class index I'm kind of focused on at the moment is going to depend on V and W. So, therefore, the log likelihood of V and W with respect to V is going to be just the same as as above I have the partial derivative of Y sub n K with respect to an individual weights in V. And then do the same thing, just with respect to W. So now here at the end, the only thing that changes is going to be just what happens in the, in the denominator of the partial derivative. And so now the log likelihood of V and W with respect to W is going to be just the sum for the error terms times the partial derivative of Y sub n J with respect to W. So we've already calculated these, these two things, it turns out in the previous neural network lecture. Right, so we already went through the derivation of how we calculate the, the derivatives in order to, in order to do back prop. So this is in the training by gradient descent section if you need to review. Again, eventually we'll see this in Python so this is all getting a bit sorry for you I don't think you need to worry. So, when you compare the above with the derivatives of the mean squared error in regression problems you can start to see the parallels. So here, this is going to be our error term this is the thing we want to minimize in a regression problem. The distinction between that and log likelihood has been trying to maximize the log likelihood. So, this is the formula for squared error. And so then below that we see the derivative the partial derivative of the squared error with respect to each of those weights. So you now start to see the parallels between these right so here we have my absolute error term, right, and I square that to get my my root means squared error. So I just had sort of the class, the class relative error term, I'm not sure that's you really call it that but this is the error term in terms of my probability, and then I take that and subtract myself next. Yes. Is there a reason why we are not creating a system for all the data to be given to the large part of the derivative of the. And in this one. Yeah. Right. Yeah, so what we do here is we're basically taking for this is going to be some class specific class K. This is just going to be for a arbitrary class of J. So I'm just trying to I'm trying to, I'm trying to compute the partial derivative of Y sub n J for this particular class which may not be the actual class of the out the ground true class of the output. I'm just trying to compute the log likelihood the derivative of the log with respect to the weights that define this class. Same for the regression. So we can compare these to the actual likelihood, and we basically see that there's a strong parallel between the regression version and the classification version, just by recasting the error in terms of log likelihood we're now my prediction is basically the soft max distribution over different classes. So, the previously drive matrix expressions for neural networks. We can just use those in as we have been all we need to modify is the output calculation so this is squared error rights of standard first two lines are my kind of standard neural network operations. So I take my inputs times hidden layer weight supplies an activation function. Take the output of that times the output layer weights gives me why. I have all my T is minus all my why I'll use that to compute my error term. And then the derivative of the gradient of the hidden hidden layer weights is going to be the error term times the times the weights. And then I'm going to multiply that by the derivative of the activation function from that from that layer, then multiply that by the input. And then the gradient for the output layer rates is basically much more straightforward. All I have to be concerned about is the actual error. Now the changes needed for linear, sorry, nonlinear, legit progression follows. So, T is going to be the indicator variable version of T. So remember, this is going to be some n by K matrix, where there are all these one hot vectors. So the first two things are identical. So, like inputs x multiply them by hidden layers. Take an activation of that, then take the result of that multiply them by the output layer this gives me some scalar prediction so far the same. So now what I do is an exponential that scalar prediction. This is going to give me, you know, some, some other scalar value, and then this, I just some across all the columns, and then divide by that some. So these three lines. Okay. F s and G. That's the softmax calculation. So I take the exponentiation, and I divide that by the sum of all the exponentiations. And so then the log like that is going to be the sum of the variable times the log of the softmax. And now these two are having these two lines that are partially highlighted are effectively the same as above. The only difference here is instead of a scalar prediction why I have a softmax prediction G that is a broad distribution. And I still have, I can still use the same activation function, the derivative to calculate the gradient. And then the only difference with the output layer is I just have my softmax probabilities and subtract those from the indicator. So, yes. Justin the very last layer so you can think of it as like. The neural network is proceeding as normal. And then at the very end, I just decide. Well, I don't want to continuous value actually want a probability distribution so I perform the softmax operation that turns that into a probability distribution. So now what that means is that the error term is literally just, it's still a distance metric. And then the first one is just now for every, the ground truth for every indicator variable is basically going to be a bunch of zeros and a one. Think of that as probabilities instead it's basically zero percent except in one case worth 100% because we know this is the class for that sample. And so now I'm just trying to maximize or try to minimize the distance between my predictions and that ground truth. And that only makes sense if I'm also predicting probabilities I need to have some functions going to turn my output into a probability. When you graduate, they will call you begging for money. Okay, so how do we do this for two dimensional data. So, let's just try to create some two dimensional data and then try to separate the distinct segments using using a non linear regression. So I'm going to use kind of a similar example to this one. So if you remember from when we did regular activation I kind of had this this chart where we had these sort of two curves. And I gave it as an example where you can combine some non linear activation functions to fit a curve to it right so if you get a 10 h function, you kind of want something that rises and then peaks and starts to fall for Rayloo. You're going to get a more piece wise curve, but with enough Ray lose, you can try to fit this pretty approximately. So let's do something similar except now I'm trying to not trying to fit a curve to this data. I'm trying to fit a curve between sections of data. And this curve would be an example of what kind of function. A decision that's not a bad choice but it wasn't quite what I was going for but what else when you make a decision, what are you doing between choices. Getting closer, like the first five letters are correct. This is great. Yes, so we call this a discriminator function. Yeah. So basically what I'm trying to do is I'm trying to find a line that's going to keep as much of the blue dots on one side and as much of the orange dots and another side and it's not going to necessarily be perfect. But you can see sort of I just trace my mouse, kind of like that. This might be an example of like a suitable discriminator function. So I'm going to make some two dimensional data this time that has similar properties. It looks like a tide pod. Don't eat it. And so now obviously just by looking at this you can tell that like, there's no way that I can fit a linear function between these areas. Right. There's just, there's no, no such linear function, unless I add a third dimension where each of these are like this, or like distinct along some z axis or something right. In fact, it would be pretty difficult to just fit, you know, an average kind of deformed linear curve to this. Right. What's single curve. Can I draw between regions going to separate all my points, not, not one. Nothing that works very particularly well. So what we'll try to do is we'll try to classify this data using a five hidden unit neural network nonlinear logistic regression. And my goal here is to basically separate the portions of the tide pod right all these individually colored points are taking the instances of a different class. And so I need to be able to classify them accordingly so you can think of this now as clearly a problem that has to happen in multiple dimensions. I've got three classes, just by looking at the data we can see that if I'm restricted to working in this plane that you see here. I cannot do this, even using like the most non linear function I can compute. So we need to have multi dimensional data. So what I will do is I will now define a new class. We called neural network classifier, and we're going to do this by subclassing the existing neural network class and making relevant changes. So this is where code inheritance becomes very useful. I'm no longer trying to change the activation function or something I'm actually trying to change the purpose of the network from a recipe to a classifier. And that's going to require more than just specifying some arguments I could do this. I could say like you know neural network and then pass like type classifier. But that would end up probably duplicating more code that is necessary when in reality as we've observed kind of all I need to do is change what happens at the very end of the network so I'm going to need to define a softmax function. And we need to find my back propagation functions to use the output of that. So I will import a neural network class. And then I what I will do is I'll define and implementation that I've already in this neural networks implementation here. Neural network classifier the sub classes from neural network. This allows me to specify the input size. And then the hit him layer size in this case just a single hidden unit with five or can layer with five units. And then three for the classes. And so now I train my specify my number of epochs and my learning rate. And then I can plot my outputs and then I can plot, you know, my, my likelihood function and my training over time etc. So let me just run this. And it'll take a few seconds to run. Like last time, because I'm subclassing from neural network, and I didn't, I changed the functions necessary to turn this from a aggressor into a classifier. I didn't change the print functions. So it's still printing error but you notice that number goes up when in fact, as we've done before, we're just actually printing the likelihood. So this is the likelihood of the data. I'm getting a pretty good number. And so here is my, this is my training likelihood plotted versus training epochs you can start you can see how we started below point seven and end up getting close to point nine five. And here is the data again. And here's what it predicts. So you can see that he's doing a pretty good job. And this is what I've identified that that outside region belongs to the blue class. And this kind of top region belongs to the red class and the bottom region belongs to the green class. And it's also doing it in a way that is effectively capturing kind of the shapes in this data, not, not too bad. Obviously it's going to make some mistakes like here. So, you know, I'm not sure if I'm going to be a good example seem to be pretty good as members of the red class. But overall, it's probably doing a pretty good job. And then there's some white space here that is just sort of filling in someone arbitrarily, but it doesn't have data for that. And this is the distribution probability distribution for the red class, right, CC. When I have samples that have these x y coordinates, the probability of being a member of the red class is given by this function here in three dimensions. Similarly for the green class you can kind of see how this one, and this one sort of fit together. So, this is the blue class, which has this big ring on the outside, and then it's the, it descends to zero probability in the center. So now you can see how with this two dimensional data we're basically fitting a three dimensional function to model this highly nonlinear class distribution. So if you want to plot the outputs of the hidden units, you can actually see what it is learning. So you can actually see like for a given input to what one unit is going to output, you'll kind of see how those curves which would also be a little bit more. So, you know, the curves which would also could be visualizing three dimensions. When combined should allow you to predict this probability distribution. Okay, questions so far. Yes. There was a reason why you just wanted to find it. Yeah, in this, in this case this this problem is like simple enough that that can solve it. So you can. You can't really run this. Let's be a lot. Let's just do it. Right. Let's just change the hidden size to just try 10 now. And let's see if it if it does better. Right. We can already see that we're doing slightly better and maximizing the logarithm of the data. So here's my prediction. Maybe it's like a little more symmetric. And here's that. So same things. We can try one thing that might be interesting note here is actually, you see this is a bit of a dip there. It's kind of hard to see. But we basically got some place where the following the dream class is not as certain as it is in some of the regions but it's still higher than anything else. So this is the first part here. It doesn't this chart doesn't show the probability, but it's probably at least greater than then point three point three three, but maybe it's not that much. We can try, you know, making this a bigger network. And this might end up, you know, fitting even better to it. So it looks like it is so far. Right. Getting pretty close to fully maximizing the likelihood. So again, you know, this is probably the best fit we're going to get to this data. And then we look at our shapes. Right. This is like close to as exact fit as you're going to get. And so in this case, that single, that single hidden layer is like just fast enough to train on this machine, even on the CPU. You can have a slightly bigger network and it's, you know, fitting slightly better to the data. So that was just a function that is non linear enough to capture this effectively. But also just really fast run. Okay. Any other questions. Okay. So I'll try some actual data. So let's, let's finish the toy problem move on to some real, real data. So we have this human activity data given in from accelerometers. So basically, there's a bunch of data where people had some accelerometer might have been like a smartphone or a smartwatch or something. And then based on how people are moving over time, the goal is to classify what kind of activity they were participating in. So am I climbing stairs, am I playing tennis, am I running, am I jogging, am I, you know, just resting, am I eating dinner. So basically this, the name of the files, just accelerometers are sort of obscures what's being, what's being done. But basically, based on how I'm moving, can you tell what I was doing. So I'm, I'm like, the classes include things like walking and playing with Legos playing into we climbing stairs. So X is the motion data and then T are some class labels corresponding to the activities. So what, how do we define class labels for the data. How would you do that. Yeah. Class can have a different integer value. Right. Yeah. You have to encode that in some way. Does it. What's your intuition about like does it matter. If I put like more similar activities like numerically closer together or not. Does it, yes, who thinks yes. Okay, who thinks no. Okay. So, I'm gonna let you know that most of you know, at least for these for classes classes, class of problems like this generally it doesn't matter that much. Because you think of them as, as vectors. So, I'm not doing one thing, I'm doing something else. There's no real overlap. So that is I can just decide arbitrarily like walking is zero playing with Legos is one eating dinner is is six. And it doesn't really matter because the probability distribution the waste should optimize for that label set. So if I change the label set, then those ways are going to be completely wrong. Right, so it's going to be very dependent on the label set that you choose when you train it to make sure that you're evaluating against the same label set in the same order, where this falls apart of course is problems where similarity matters in the final output so in particular in like language problems. It doesn't make a lot of sense to have the word puppy be equally orthogonal to the word dog as it is to the word truck. Right, because obviously two of these things are much more similar to each other than either of them is the other thing. So, in problems like that you have to have more sophisticated ways of representing your classes. But in this case we can just assume that you know they're arbitrarily chosen. So, here we have, you know, 225,000 samples. And then it has four outputs. And so then data looks like this so we can see what do these look like inputs in which of these look like outputs. Yes. First column looks like an output it's we just talked about how these are in your class label so this looks like integers so I'm going to guess these are the class labels. So, seems like I can take my first column turn that into my tea and then the remainder will be my inputs. So here's a function to generate careful cross validation sets where we talked about cross validation. Anyone remember what that is. If you do it can help you with assignment three. So, I'm going to do this for like, K time. So I'm going to have each each time and hold out a difference, a different set and then rotate that through so that each time I'm training on K minus one folds evaluating on the case fold. And this will tell me a decent average picture like how I model can be expected to perform on arbitrary unseen data. So here's function that does that I will randomly order X and T partition them into folds. And then I will basically return each one for extreme valid age and test. So, I do X dot shape. I end up with this number of samples times three three dimensions for each one. I'm using this yield keyword at the end. What is that. So this is basically something that suspends execution and then returns the current state back to the caller, but it will retain state information so I can continue where I left off. So return keyword will just exit the function entirely at that point. You will basically say here that I've got right now. I'm going to return you some values. But if you want to keep executing, I'm going to maintain my state information. This is kind of a functional like operator in Python. So it's sort of like the continuation operator in Haskell, for example, if you know anything about that. Just a demonstration of how that works. I can have some function here. That is just a times two function. It's going to return. I times two for, you know, in range of I. So now if I turn, if I print out list of times two is going to give me, you know, I times two for zero through nine. But if I just return the result of the times two function actually gives me this generator object. So what's that. So the generator object allows me to basically call this next function over it to prompt it to return whatever it wanted. It's going to yield at the next step. So remember we're keeping the state information. So it's basically was the last thing that I returned. So I'm going to continue where I left off. So if I call if I return my generator function, it's basically an instance of the output of of times two. And so now actually to actually get into that I have to call it next over it. So if I call next the first time, so you can give me the first item that times to return, which is zero. And then I use the Z again, same thing, right, I did not change the syntax of this call at all. But now it's returning to, because Z is an instance of this function that preserve the state information because I use the yield keyboard. So using this in my careful cross validation sets means that I can then just call next to basically get the next cross validation set it's going to segment everything just right return my train val and test partitions. So these things you can see, I have 75,000 samples by three and then by one. And then these are equally partitioned right now. So I now have a train validation and test partitions, all of equal size partitioned into different faults. So now I could call the NP, we're going to use this unique function, this is going to find the unique elements of an array. So if I run unique over T train and with this return accounts keyword. What this is going to tell me is, what are the different elements of T train and how many times do we does each them exist. So now we can see that I've got 10 classes, they got labels on them one through 10. So we can see roughly 7500 of each. So this is a decently balanced sample. So there are 10 classes for each class K and then their K prime instances of that instance in our data. We can also control how many digits after the decimal point are printed for an on pi arrays this will be important for doing the confusion matrix. So I can set my precision to five. So now if I just divide the counts by the total shape of T train. So basically this is accounts for class divided by the number of samples we can get the percentage of the data that falls in each class. And so you can see that's roughly 10%. So, this is a well balanced sample. We can do this also for T val and T test. And you can see they have roughly the same distribution across all of them, which means that for cross validation, we can expect that this result is going to be accurate. So all these steps are very important to do if you are performing an evaluation on neural network. So, see where we at right now. Okay. So what I will do now is I'm going to construction or never classifier, where the input layers of size n ran as number parameters in every input sample. In this case, that's three. And step two, a miracle occurs. And it's step three, we get the answer. What we do here is I'll just define my neural network classifier that we did before. I'll call train on it. All that stuff happens. And then I will just see how long it took and I'll plot my training likelihood versus the versus the training time. All right. Any questions on this while it's training. Yep. So, I guess I, I sort of elided this because I didn't show you the neural network classifier code because you have to write that yourself. One of the things that you're that it does that you will have to do is it takes those integers and turns them into one hot vectors. So, so you'll see that if you look at the, the definition of the class. So, what's the input size is going to be this that second dimension of x dot shape so in this case three, prem, three values for each input. This is the hidden layer size, and then, and classes, this is the, this is the number of outputs. This is the number of things I want to predict about each sample in a regression problem. These would be N scalar values pertaining to something about that sample for a classification problem. The thing I want to predict is I, for every class, I want to predict the probability that falls into this class. So, this would actually have an output size of 10. Okay. So, that did that so it looks like it is a pretty, it's decently maximize the training data like the hood. So for classification problems you want to see the percent of samples that are classified correctly, and then it's also interesting to see which ones are misclassified so you can see if maybe there are some classes that are being confused for other classes more often than not. So, you think of this as basically a grid where you have along the rows of the columns you have your predicted classes and then on the other one you have your true classes. And so those things along the diagonal, which for a perfect model, you basically have 100% along the diagonal as we'll see. So this is called a confusion matrix. And so this is just like a table classification percentages for all the target classes and predicted classes. So in our version, the row is going to be the target class the columns of the predicted class. This is not universally true I have seen cases where people have flipped it so just be careful when reading to the user matrices. This is typical, I believe this is more common. And so then the diagonal is going to show the percentages samples that are correctly classified for each target class, and you want it to be as close to under as possible. Yes. So, in this case is probably due to the number of samples because there are 75,000. So the data here it's three dimensional data so yes, like an extra dimension and say the tide pod example we did before. So it takes a little bit longer there but mostly it's going to be the number of passes through the data at for and so you have to pass with the entire data for a thousand samples versus, I don't know, a couple hundred probably for most things we've done before. Yeah, no, I mean if you're dealing, yeah so the thing about the number of numbers that it has to pass through it's going to be N by D. So the longer the longer the input dimensionality is of course the longest going to take to go through each sample. And then the more samples you have the longest going to take to go through the entire data set. So we can then just run the use function and then we'll end up with a basically an end dimensional array showing all the percentages. So this will convert the predictions into into percentages. I'll just spend that to table. I'll print the table. Of course this is not very useful to look at it's just a bunch of numbers. If you can mentally conceptualize how it's being arranged you can see there seems to be a good percentage for that first class and maybe a less good percentage for that second class, but let's arrange it in a way that's actually easy to read. So if I put in a pandas data frame I'll put some some headers on it. So now we can have the class names and their indices you can see the class one is rest coloring legos we tennis we boxing movement at various speeds and climbing stairs. So now I put these into a data frame I can put this out into a nice grid. And so now you can see by stepping down along the diagonal, which samples are which classes are well classified in which one they're not. So first this is still not like the best way to look at it right be really nice if I could get an immediately intuitive grasp of what this is showing you rather than having to analyze all the numbers one by one. So first of all let's convert things to percentages. So you can look at this tutorial on pandas styling to, to help you with your data presentation. If I can convert things to percentages now all of a sudden it is quite a bit easier to read. I'm not no longer staring at so many decimals. So I'm going to put a little function call doesn't save the style and the confusion matrix so if I run this again it's going to give me the decimal version. So I can add colored backgrounds to provide a quick, a quick visual comparison. Using CMAP equals blues is going to give me this nice this color scheme, where I can immediately look for the dark cells. Now one thing you'll notice here is that it basically the darkest cell in each column is the darkest one, and then everything is graded relative to that so what that means is that when we tennis, when it's only classified correctly 3.5% or 3.8% of the time, but that is the highest value in this column so everything's normalized relative to this. So this is the same color as rest being classified correctly 96% of the time. So now we can combine these two styles in an object oriented fashion. So now if I run this one. So I'm going to put the percentages. So one thing that we will want to figure out is like how to how to normalize this is there's some things you can do in Python to do that. So now I'm going to try a bigger network. I'm not going to run this live, because it'll take a long time so it's going to start from here I'm going to kind of just skip through this. But you can see on the bigger network I now have two layers with 100 units and 50 units. So you can see that as we train this bigger unit has some some fluctuation in that in that likelihood so if I stop training here maybe wouldn't perform is better but eventually it kind of stabilizes. So now if I run this again, pretty confusion matrix you can now see that at least the correct class is the one that is classified correctly most of the time for all classes. So it will equally classified correctly for example stairs is only correct 30% of the time, whereas rest is still correct 96% of the time and coloring is 85. So now let's check the validation and the test sets. So the validation percent correct is about 57%. But we see kind of a similar distribution for all the different classes. So you can see like which ones are commonly confused, and this makes quite a bit of sense right it may be difficult to tell from accelerometer data whether I'm moving at 1.75 meters per second or 2.25. Right, wow, this is going to depend on you know, my height my stride length maybe the type of shoes that I'm wearing. So by looking at confusion matrix you can see, which, which classes are commonly confused maybe consider ways you might process the data to make that easier so confusion matrix is kind of the most common way of presenting. Multi class evaluation for most machine learning problems. Okay, so then same for the test percent basically received very similar trends in terms of overall accuracy and also prediction between different classes. So we've got some issues here. So for example, obviously some of the really easy ones like rest and coloring and even Legos it's getting right, you know, most of the time, we boxing. There's probably like a relatively distinct movement pattern that is picked up in the data, but kind of the walking jogging ones and even climbing stairs. The overall accuracy is just not that great, particularly for like class eight and class nine. So what if there's a different data representation that we could use to represent movement so we can use this thing called continuous wavelet transform. And here is some code that's going to apply a C W to this accelerometer data. I'm not going to go into this but basically what it's doing is just converting this into a waveform. That's going to approximate the frequency and amplitude of the motion. So if they imagine if I'm walking really fast, right, imagine that accelerometer like hitting the floor or the sidewalk. You know, every time my foot touches down. There could be like a pulse every time I take a step. Right. And so the frequency of that pulse is going to be correlated to say like how fast I'm running or whatever my pace is. And then the amplitude is going to be like how hard my foot is hitting the pavement. Whereas if I'm sitting there playing Legos, you know, sitting on the ground. I'm not going to get a different pattern of movement that's going to have a different frequency in a different amplitude. This is a useful technique for transforming this rock cell or other data into something that's maybe a little bit more intuitive. So if you are familiar with C W cheese, you can look at this code and apply it maybe just some of your own data. Okay, so if we just looked at some of the different the different properties. Basically, we can see, you know, we can define a max frequency and then we can see for the different samples, you know, what what some of the properties are might be easier to look at this in terms of a visual chart. And then what we can do is I can actually plot some of this. And you only see like three distinct regions here because there's actually 10, but it keeps repeating the same colors. They're kind of all ploughed on top of each other. So if we look like real close to be able to see some other ones but not really. So if we now look at the individual samples. Luckily they're ordered we have a 7500 of each one so you can see okay this these are the samples of class one and class two and so on and so on. And then we can plot the frequency for these three dimensions for each of these so if you look at instances of like class. What five I guess this is 12345. So this is we boxing. Right. And so these you can just go down below this. So this particular class seems to have quite a bit of distinct frequency and amplitude compared to the other things. Right. And now let's look at the ones that got confused rights these are sort of fast movement of you running or jogging. If you look at these segments here. There's not a lot of difference between them. So it kind of makes sense maybe that that these were getting confused. You'll notice that like there is a slight difference maybe the amplitude in why for this class is slightly less than that for the next class. So this representation will allow us to tease that out a bit more. So just think about you know how you would move in terms of doing each of these each of these activities and that's reflected in the frequency and amplitude of the signal. Okay, so now I'm going to take this representation. I'm going to call my CWT net I'm not going to train again because it takes like a minute. I will run a prediction and then I'll see how well this is doing. So already you can see for look at test percent correct 92% compared to like 58. So clearly this data representation is a good way of representing this for this task. Now look at the confusion matrix. Wow. So really nice numbers there. One thing that is maybe not coming through very closely is coloring is often misclassified as rest. Right. Because if you're looking at it, it makes sense because of you're converting my data to frequency and amplitude and maybe it's going off of like the accelerometer data for my rocket. It's there's no real distinction in frequency or amplitude of motion associated with coloring versus rest that would be picked up by an accelerometer and that CWT transformation may be squeezing out data that was useful for that. So we had some cases where like coloring that percentage actually goes down right now it's like 50 something forward 64 right so maybe you lost some information here. So that was classification basically key points. Everything is converted to a probability distribution but when using neural networks that all happens at that very end of the operation. But also the way you represent your data, you can make a big difference. And this is true for all kinds of neural networks from just how you're encoding the classes to how the inputs represented. And so you have to think pretty hard about how you want to represent your data for neural network classification operation. Okay, questions. Okay. So before we go let me go through assignment three. The purpose of this assignment is to build an implementation of a neural network in PyTorch so you're going to be doing a lot of the similar things that you did to assignment to, but you're going to be doing it the pie, the pie torch way. So what what you will do is once you've done that you're then going to conduct some training experiments on some some data and you're going to be doing this using cross validation. Notebooks you're going to look at eight for the PyTorch stuff. The cross validation notebook whose number I don't recall I think maybe it's seven though it might be nine. And that should cover most of it I think space you're going to want to review. PyTorch implementations and cross validation. So we have this end that class what you're going to need to do is you're going to complete the train and use functions, the pie torch way so you cannot just copy and paste your a to code, although the principles remain the same. So basically you've done it in NumPy, but NumPy is limited by the CPU so let's move to PyTorch so that you can make use the GPU for for more substantial operations. So given definitions of like say the activation functions you can do, you know, 10 H array that's done for you. What you need to do is you need to complete the train. So you need to calculate the do the forward pass calculate the mean squared error take the gradient. Remember in the PyTorch method, most of all these things just happened in single lines. So you'll find if you find yourself working out the math for things, you're probably on the wrong track you've done that already. So you need to do this using PyTorch is built in functions. So what you need to be careful are things like zeroing the gradient, making sure that you're detaching things from the computation graph, etc. Same for the use function. So same operations, right, you need to standardize X to the do the forward pass and on standardized but again, you're going to need to do this the PyTorch way. So this thing here, if you run into errors about the computation graph just like review that line, save you a lot of a lot of trouble. So for an example, we'll give some data like we do before, run it through your implementation of end that and then calculate the RMS E. So basically your plots should look like this. So then you then need to perform experiments over actual data after performing stratified cross validation. So we give you the complete code for generate capable cross validation. So you don't need to change anything with that. And then here's some example data using just some dummy data using this function. So you can pay your outputs to this. And now you need to train create a function that will train the neural nets and then average the RMS E overall the ways of partitioning. So you have to do all your, your cross validation, take the average and then report that. So you just have to find this function that's going to do things like define an instance of neural net, and then call generate careful cross validation sets, and then, and then get retrieve the output and report it. So basically this is the same as you did in assignment to accept your column across validation function, and you need to call your pie torch barrel that instead of the non pie one. So the application is going to be to this airfoil data you can go to this website and download it. And then it's, it gives you these parameters like frequency angle cord pressure. So you need to apply your run capable cross validation function to the airfoil data. Actually the first five columns and T you be the last column pressure. So basically trying to predict pressure and using the other five things. And below you'll find an example run over some real data. So you need to, again, collect your outputs into a data frame, where you report the architecture and the RMS E for train valent test. And then you can plot the results. So this is a much more coding heavy assignment. So you'll score most of the credit. If the train use and run capable cross validation functions are defined correctly. And then you can test this with the greater same as before. So unlike this one you need to complete this one individually. So, a two in the final project. The only thing you can work with a partner on. So you can have a different number of points of this assignment. So there's significant, you know, advantage to to doing that. One is to add a keyword argument though light use the GPU. You have free access to any of the CS machines. Then if you need to use those if you don't have a GPU in your own. And then also you can find another data set and apply this to to that data set and report on your on your conclusions. So there's five points for doing one and five points for doing to you can elect to do one but not the other. If you have the time or lack thereof, or you can do both. And this is due March 21. Questions. Okay, all right. We'll see you Thursday.