 Okay. I want to be able to address my options. I just need to get a few of them ready to go. So, every time I'm trying to get a trip in town, I'm going to go to the city. I don't know. I have no idea. I'm not having my time to put in my last time. I think my goal is to get out of here. I think I'm going to go to the city. I've been to Mountaintown. I've been to Mountaintown. I've been to Mountaintown. I've been to Mountaintown. I've been to Mountaintown. I've been to Mountaintown. I've been to Mountaintown. Okay. Start. Messages again to people who aren't here. If I excuse you from one day, it doesn't necessarily mean you're excused from the next day. I'm going to go with the weather change. Please make sure that if you remain sick, you let me know that you remain sick. If you are not here one day and you're still out there the next day, I'm going to start wondering why. So, where are we right now? I'm going to do today is I will do a convolutional neural network training with PyTorch. So, we're getting pretty far ahead in the content of where you are with the homework right now. You're doing a cross validation and then we're going to do MNIS, but it's going to be the fully connected that you won't do convolutional nets for an assignment until assignment five. So, now sort of you apply versus what is, what your class is going to start to, start to diverge a little bit. We are back on schedule. What I'm going to do today is I'll go through this notebook 15, which should be on the shorter side. And so then after spring break, what you usually have is kind of like a review session. So, take any extra time today and just, you know, answer questions about CNN, softmax, some of the various components that we've had to put together in the last couple of weeks. So, I will have office hours if we end early for some reason. I'll just go over there. So, now will be the time to ask me questions about assignment three. And also, if you started thinking about project ideas, I want to discuss that. You can also do that during office hours today. So, after the break, I will formally announce the project. This proposal, you may see it to write down what you proposed to do and I'll either approve it or ask you for revisions. And then you have about six weeks or so to complete the actual project. Yeah. You are allowed to you don't have to you can work in the groups up to four. The catch is that if you work in a group project, I expect to see a convention amount of effort. So like if you work in a group before, it better look like it took four people to do your project. Other questions. There is going to be in terms of the amount of work there would be compared to the lower bar to clear because it's one person working for six weeks as opposed to four people. I do expect sort of see quality work, but it's going to be, you know, I'm going to qualify that based on like how many people are on the project is not going to be like, you know, you have to run every exhaustive evaluation on your own. If you have four people, you can probably run a lot more evaluations. That's the sort of thing that we're looking for. Anything else. Okay. So I guess the point of that is that it would behoove you to start thinking seriously about your projects. And I'll go with this again after the break, but if you already have something you're working on. Maybe start thinking about how you might be able to apply that to like a pretty machine learning spin on that. That would be an acceptable thing to do for this class. So my goal is not to give you extra work. If you already have some active line of research and you want to make it so it applies to this class. That's perfectly acceptable. You just have to convince me that that's what you're going to do. And you're not just like completely piggybacking on something else. You have to do some dedicated effort for this class, but you're allowed to use existing resources and things. You know, you're the goals for you to use your expertise in, in a way that's relevant to the course material. Okay. So to share screen. All right. Can you can everybody on zoom. You can see my, my different notebook window. Okay. Sorry, can you can you see the, the two begin zoom. Okay. All right. So. We've done intro to convolutional neural nets. Basically the core mechanic of a convolutional neural ed is a filter. This is just a weight matrix that's that really needs to be optimized to output a higher value when it encounters things that quote match it better. And those things, of course, can be multiple different combinations of features. We did our handcrafted filters just to illustrate how the concept works in terms of what type of activation and scalar value you're going to get out of performance linear sum operation from a filter to a patch of the image. Of course, you want to be able to actually learn these filters from real data. So pretty much the training process of a CNN is trying to optimize the weights in these filters. And then how you pass up your images. So that's going to learn weights that are optimized for whatever types of data and counters, assuming those hyper parameters. We did the non pie version. And now, of course, you want to make use of as much speed as you can, because as your network start to grow, they start to take more compute. So let's go through effectively similar operations, except using pie towards this time. So. Just a point, if you were going to use pie torch on the workstations in the CS department, you want to execute this command. So basically what this does is it's going to add the site packages library where pie torch should live to your Python patch batch. Sorry, Python path. Can't speak today. So you can run this every time that you log in, but a better solution to be to add it to the startup script in dot bash RC. So that way it's always there you log in and you don't have to worry about this. But if you have things like if you run import torch and you can't find it very likely it's missing through your path. So just a point of fact. So this is taken from this example of implementing CNN pie torch. So this is kind of stripped down version of that. You can look and. Okay. So we're going to do our usual sets of imports rights, we're going to import an un pie of course we still want to use NumPy to do things like, you know, processing and plotting our outputs, because remember pie torch operates as a tensor, and you can't do certain operations for things like this visualization directly on tensors. Nonetheless, the main thing we're going to be using is torch and from torture going to import the, the autograph module. And then we of course going to keep track of time and Jesus and pickle are to to open our data. So in our NumPy version of a CNN. This looked our constructor looks something like this. Hold on, let me see if I can. So we specify our patch size, and then we initialize a instance of this neural network classifier CNN so 28 by 20 that's going to be our input size. So remember, fix size inputs, and we're going to assume square images. And then this two and two, this is going to be our list of the defines the hidden layer architecture and output size would be the number of unique labels you have. So for MNIS, this should be 10, but you know, unique values into whatever your class labels are. I set my patch size. Remember that this is a square. So I say patch size five. This is effectively a five by five. Right. And then stride to similarly that's also a square. I'm going to take to step two pixels over, and then two pixels down when I reach the end of the row. And then I'll also like a two by two in both directions in the pie torch version, you're going to see a lot of the same things, but it's slightly slightly different. So we're going to make use of some of the pie torch functionality to more automatically define things like our network architecture. So, first of all, just like take a look at these two definitions. And what do you notice that is similar about them? What do you notice that is different. And then the same about these definitions. First parameter right books 20 by 28. That's the input size. Well, Yeah. Yeah, so the output size too so it's the same data right so it's going to have this is MNIS to they're going to be 10 classes still. And our inputs are going to be 28 by 28 images. Okay, so now what's what looks like it's different about this. Yeah. Yeah, the political is right so I'm separating out, you know, the convolutional layers and the fully connected layers. So here, remember when we did the version last time we basically said, okay, I'm going to just decide. There's a single convolutional layer, everything after that is fully connected layer. Right. So I could write the non pie version so that it is more like this. But pie torch basically does this will allow you to do this automatically. As we'll see. So right now, like what you see here that I'm highlighting. This is not a pie torch command specifically this is just a Python call that instantiates an instance of this con to that class, the net the con class will be written using pie torch. And so now I can take these these parameters and use that to basically, you know, construct the individual pie torch layers one by one, exactly as I want them. What else do you notice it is different about the pie torch definition. Yeah, so this is the stride. It's it's two. What's the name of the argument that's there. Per layer right so I can do I can do something I can have a little more control here in that I can specify different size patches for a convolutional layer and also different size strides per convolutional layer. Let's think a little bit of why you might want to do that. So remember, you know, sort of single convolutional layer example. We have, let's say, 28 by 28 image, and we might specify a seven by seven patch. And then I stride over it say two pixels at a time this is going to give me, you know, some number of them to be like 11 I think 11 by 11 patches using cropping. So I'm taking a 28 by 28 image turning it into an. So the 11 by 11 little sub images, each of them gets multiplied by a filter that should be the same size of the patch to also seven by seven. From that I get a linear sum. Right, or I get some sort of value so the easiest one is to get a linear some sort of single value. I can also do pooling, which is going to take those initially seven by seven values, or whatever the size of the patch times the filter is. So I'm going to do a down sample that. So I did like a two by two examples I take seven by seven down sample it to a two by two, or just take a linear sum and down sample it to basically just a single value. Let's take the pooling example, because that makes it friendlier to more convolutional layers. Let's say I take my seven by seven feature map down sample it to a bunch of two by two feature maps. And then also 11 of them. So 11 by 11 to them 121 all of those, now we have 121, but effectively like four pixel images that way. Those go into the next, the next layer. So if I have a bunch of input, this, essentially like pre patched already, they're two by two, and I have a patch size, or so stride length that is three, or let's say even two, what's going to happen. I have a bunch of two by two images. I have like 11 by 11, and I stride over them with a stride length of two. So I'm going to be able to overlap between my patches. No. Let's say my stride length is three. No, and I'm also going to miss things. So effectively, the stride length is going to be dependent upon what's the resolution of the thing that's actually going into the layer. So, if I have a patch size that is five and a stride length of two, this is going to down sample. So I have five by five and I pull my downside with my five by five feature maps into some little resolution. This makes it easier to, to compute right it's faster this less like less information. Also, if I do my pooling right I might get a pretty nice representation of what's really important about those feature maps at with less computational cost and less memory cost. But I'm also going to be feeding in a smaller quote image into the next layer. And so, if my stride length is too big, I won't be able to patch my images effectively. So it's going to basically just throw an error. So it can be, it's usually desirable in fact to have different patch size per convolutional layers because you want to get important information from maybe a very complex image down sampling. Something that's like you think is truly important about that image and then speed that into the next layer so we can do a similar operation over those what we call just did feature maps in order to get what it then thinks is really important using filters that are probably optimized for somewhat more complicated features in the end. So when you're using your convolutional neural nets you want to be careful that you're not using an image that's of too small resolution to be useful. The next layer. So, for tasks like n-nist one convolutional, sorry, one convolutional neural net, one convolutional layer is probably enough. So, I think it's a very simple task for more complicated things, real images, you know, multiple convolutions, multiple convolutional layers, but you've got to be careful about these things like the stride length. Okay. So, that all all clear so far. So first just let's look at back prop from our previously existing neural net lectures. So, we've got to optimize the weight or the value of the single weight. So remember we're going to have some weights that just sort of reside inside these hidden units all apply some activation to them. And we can't we showed how we can optimize the weights one at a time so if I have this is a layer V and I have an individual way and trying to optimize the value that this gets optimized to is going to depend on among other things. So, whatever that that results of from every unit that is connected to this. So, just consider there's like one neuron right here. And it's connected to in this case three other neurons so whatever value comes out of this first hidden neuron is going to have some bearing on the output that comes out of those three other neurons. So, if I have some particular prediction, then it's going to be some level of wrong compared to what the actual truth is. And so I want to optimize this weight in this quote parent neuron over these raw graphs in a way that's going to take into account how how wrong all the children are. So, it's like, you know, if you, if your children misbehaved you punch the parent. My children are very, very well behaved so I'm not an issue. But one of those even born yes, we haven't had a chance to be hasn't a chance to misbehave anyway. What we are trying to do is like if this is slightly wrong right why why one minus T one is like a little bit wrong. Why two minus T two is like really wrong. And then why to why three minus T three is, you know, also really wrong. It might be more beneficial to perform a greater optimization on this weight because two out of the three things that it connects to have very large errors. Right, so we can reconsider all of these. So basically in plain English what this will show is that we've got K output nodes in this case three. And so this should output, you know, K number of values for any input sample. So for this regression example it might be continuous values. If I want to predict properties of a car from other properties of a car. In a classifier this is going to output probabilities. So again, the error term is really the same. It's just that I have to make sure that my quote, not units, but kind of units are the same right. So I'm going to be subtracting mpg from mpg horse power from horsepower. Probability from probability and just module, whatever standardization you have to do. So all these are there's this should all be apples apples comparisons at this output layer so then the weight update will be followed these web lines. So if depending on how wrong these things are, this is going to be updated based on. values that are in part derived from these from these values so I see partially drive here just in the colloquial sense, not in terms of the partial derivative. Although the partial derivative is kind of the key component of the actual mathematical operation of that application. In this case I'm just saying a portion of this error is going to inform how much the connected way it gets updated. So weight updates in the hidden layer are going to depend upon the weight updates to some extent in the output layer or any other layers that are connected subsequently to that hidden layer. So let's assume that we've got like say two units in W. And the upper layer and just a single hidden unit in B. So we'll just have one hidden unit and the two output units connected to it. So these are the derivatives that we use that you may remember from from the first network lecture. Right, so the partial derivative of the error with respect to weight one is going to be negative two, or in this case, just a constant, you can factor it out. Times the error times the inputs is going to be the input that that output layer. And then the partial derivative of the error with respect to the hidden layer weight is going to be similar except because there are two output weights, I have to sum the, the error times the value of that weight. Right, and then because I'm also going to be applying this activation function before the output of layer V goes into layer W. I have to account for that so my activation function is 10 H is going to be one minus z squared. And then the last thing I need to do is I need to multiply it by the input to the layer and in the hidden layer that it would be X. So the update on W one is going to be proportional to the product of the error and Z, which is the input of that layer. And then the update to V will depend on the sum of both of those errors in output layer W times the derivative of the activation function with respect to Z, and times the input to the hidden layer. Same thing is happening in my classifier network and my my regression network so what was the point of all this you know if you've been paying attention you understand how we'd update work. So let's actually see what happens when we apply it in convolutional nets so if you just remember from Tuesday. So I have the same filter that's going to get multiplied by a bunch of different patches. And all these are going to contribute to that final prediction. And so I need to best optimize this particular filter to account. To make good predictions for everything that it might encounter. Right, and good prediction in this case might mean that as a high activation for certain things and a low activation for other things is just trying to best optimize what those things are that when this filter encounters it, it's going to best produce the most predictive values. And over all these working conjunction there are a bunch of different hidden units. And just trying to do this all at the same time. And so then in order to out to perform the actual bad propagation operation in a hidden layer, what do you need to do is I need to collect all those delta values from all applications of all filters to all patches. And then as turn this into a big weight matrix, as you can do do my multiplication operation all at once just to make sure that reshape the inputs into a compatible shape. Okay, so we'll do our kind of square and diamond. We'll task again. So here I'll define my diamond. So there's my diamond. And so now what I will do is I will define a patch size in a stride length and I'll patch it appropriately. And then this is going to give me something like this. And there are 64 patches according to this patch size and stride length that I specified, and this patches look like this. So now this filter would be applied to each of these patches. So as you're going to generate a feature map that is the product of the filter and the patch where in this case C is going to be the weights of that of that filter. So, the sum of all those values. So if prod here is basically just a list of the values that resolve when you apply this photo dispatch. Some of all some of those, this becomes the scalar value representing how responsive this weight is to the values in the patch. And then this value thing it's propagated through the network so right now I'm taking a single value. I could do pulling and just down sample to a lower resolution image. So what are our standard neural network operation looks like now is very similar so all the only thing here is that is the equals H of the x v, V are now our convolutional units, and there is an application of each one of these to every patch. So remember my patches are now just consider them to be input samples. I just flattened them, and I feed them into into my wealth item. I was like flat and the, I flattened the image. And then what we do is we just reshape everything so it's a nice square that multiplies together to get our value. Then why is my prediction this is just going to be Z with my bias times my output layer weights. And then we have the softmax operation. Right, so what I do is I exponentiate why is why it's just some scalar value. I need to turn this into something that can be turned into a probability distribution. So exponentiate that I then some over all values of F for each for each class. This is basically some way across the columns. And so then I will take that and then take the F for each individual value class divided by the sum. This is going to give me a probability distribution. So now the, what I'll do is I'll take the sum of all my indicator variables. Right. So now my indicator variables are just these. Again, one hot vector. So think of them as a probability that is all zero except for one case where it's 100. And then times the log of the actual probability that I predict it. So if I do that, then what I can do is I can take the gradient in V. And then what I'll do is I'll take my inputs x input to in layer, and then multiply them by T minus y, except now these are as these are as probabilities. So I have the indicated variable version minus the probability distribution G. And then multiply that by the weights and then multiply that by the derivative of the activation function. These parts are pretty much the same is just that I've got the indicator variables minus the predicted probabilities rather than a scalar minus a scalar. So now what I need to do is for the convolutions I need to sum all of the partial derivatives of the error for every delta value in the gradient so then for every convolutional unit. We need to find the delta for every scale of value and that's going to come out from every feature map when that filters apply to each patch. So let's let's review the back propagation of weights in a CNN so we'll assume this is kind of recap of the one from last time. So we'll assume this only has a single convolutional layer and that's the first one. So what I'll do then is I'll take, I'll reshape this backed up delta matrix into the right form. And then I'll reshape the convolutional input matrix to something that's compatible. Now I can take the, that input times the delta. So this is just doing, you know, this part here. And then what I will do is I'll just calculate the derivative of the error with respect to the weights for the convolutional layer with the simple multiplication. And then in the fully connected layer it's the same as we've been doing. Okay. So the trick is basically just collecting all of my values for every application of every filter, every patch into a big matrix. This allows me to effectively perform the same operation as long as I can ensure them in major C's the right shape. So this sounds like a lot of work. Right. So you got to remember, okay, I got to collect all of my, my doubts of values into a matrix. I have to make sure that my inputs are appropriately shaped. Then I can do a, a multiplication. But let's check the pie torch definition again. Right. This is a single convolutional layer in the pie torch definition that we had, we can have one multiple convolutional layers. Each one has a different patch size, and each layer has a different as potentially different stridentine here they're the same, but I could very easily change them to be different. So, you can see that this is rapidly getting grow out of control. I'm trying to do this all manually using non pie operations. There is a very reason why we limited the discussion. Yes, or on Tuesday to having a single convolutional layer. So, seems like a pain. If only there was like something we could use to automatically calculate the gradients. So if you're thinking, boy, I wish I had something like that I have good news for you. Let's remember lecture eight. We got this thing called autograph in pie torch. So recall what autograph does in brief. We construct our neural network as a computation graph, where everything can calculate its own value and the partial derivative of its own value, and you connect to those things into a graphable structure, such that operations. The outputs from a single from one node can then be fed into as arguments into any sort of child. And so then I can calculate everything that say the partial derivatives of every leaf, and then that propagate them with a simple with a single one line call. So this definition of convnet using this is basically what we're going to execute that Python call and instantiate. So first you'll observe we're inheriting from torch. So this is a simple command that module this allows me to make use of all the pie torch functionality for doing things like creating neural networks with, you know, with simple commands. So now I can specify those things that I want, right, the input size number of convolutional layers and new stride lines, patch sizes, etc. I'm going to do some things to make sure that we can use the GPU. So I'm going to create this one by the activation function. Now that I've done this I can now create all of the convolutional layers. So here I'm going to create this thing called module list, and then I'll add things to that list that represent those individual layers and their properties that I want. So first, this argument is going to be the number of channels for each pixel this first argument here. So this is going to be the square root of the number of inputs. So again, if it's 784 it's height is 28 with this 28 so the square 784 28. And so now I can create this module list for for the convolutional layers. And I will just kind of zip up my number of numbers of hidden layers, patch size for that layer strident layer, and then every trip to the loop I'll add something to the convolutional layers list that contains those those properties. So what that is is, this is what you're probably going to see when you read other people's pie torch code is you, you, you instantiate these items it's basically like torch.nn.layer type. And this could be, you know, dot dense, or dot com to do your dot com one D or LSTM or whatever type of network you're trying to write. And then you specify exactly those things that we just we just talked about the input size, number of units in this layer. And then other in this case for convolutional nets we have, you know, the kernel size, I have the strike links for other types of, for other types of layers, you would be slightly different properties. So you'll see, you know, I'll create, you know, n net equals torch dot sequential, and then I'll add these things one by one. So what I'm doing here is I'm feeding in a list that automatically specifies what I want the properties is layer to be. And then because I'm assuming this is all convolutional except where I, for things like specify like number of hiddens and convolutional layers. I'll create the convolutional layers that way. And then I'll also create, you know, these linear layers for the fully connected. So, this is just your standard way of just creating your your PyTorch neural network is effectively just a list of what layers I want. And then when I do like, you know, model of input, it'll run my input through all those layers and sequence like I can specify very cleanly. I want a convolutional layer here. I want another convolutional layer. I want to do some pooling. Then I want a linear layer that I want to do like dropout and everything that I, that I could possibly want to try. Okay, so now forward all outputs is going to be basically just like one pass through my, my network with inputs x. And so this forward, the forward pass will basically call this. So you can see that it's handling the convolutional layer and the fully connected layer. So what I can do is I can just say for conv layer in self.com player. So for every element in that list, which is an object that represents the layer itself. I can actually just use that object as a function over the input. And so now I can just say that, okay, specify the convolutional layer. It's got some number of hidden units. Each of them have x weights in them. If I just call a con layer of input, it'll run that convolutional layer of the over the atom. Train is pretty much the same as we've done before. So I specify what method I want to use all this should look fairly familiar. So I just remember set requires grad. I specify the type of loss function that I want to use in this case because we're doing categorization. I'll use across entropy loss. And then the rest of this stuff is pretty much just for plotting and training so to calculate the loss. So I'm my CEO's function that I just specified over why which is my predictions and the targets for this batch. And then lost backward will actually perform back propagation and then calling optimizer.step will actually perform the update of the weights. And then lastly, remember that we have to zero out the gradient every time otherwise it's going to be accumulating gradients because that's just how PyTorch works. Now we have the softmax function in in PyTorch. This is the same as we've seen before. I'm just using PyTorch tensors. So here, this is a trick here to avoid overflow actually calculate the max. But then I exponentiate and then I take the denominator. I defy divide the exponentiated version by the denominator. That's our softmax operation. And then use function is use function. This is pretty much the same. I'm just using the Torx version instead of the NumPy version. Last thing, especially if you encounter problems with the GPU, I recommend you refer to this notebook here. So you need to detach things to the computation graph, move them back on the CPU and then optionally convert them back in the NumPy to do processing with them. So if you leave things on the GPU, you will encounter difficulties. All right, so I will set the device that I want to use. In this case, I'm running on my lab machine. So I have access to the GPU. I will hit yes. So it's now running on CUDA. I can check NVIDIA. smi. I forgot to tell my lab that I'm using the machine for class. However, it does not appear that I was really using it. It's fine. So you can actually see with this NVIDIA smi command, the usage of a given machine. And so here I can see like, I don't know, someone's. I'm sure this is probably me running Python 3.8. No one else really appears beyond this right now. All right. So now that I specified my network, I can actually try to run it. So I'm going to open up MNIST. Right. So this should look pretty familiar. So what I'm doing, I'm just splitting it into train and test. So we must reshape the input matrices for convolutional networks in PyTorch. So what I'm going to do is here, I'm just going to take the number of samples, one dimensional sample 28 by 28. This actually requires a two dimensional input to be fed into the convolutional layer. It will do whatever flattening is required. Yes. What is the parameters of the parameters of Torched up module. So did you see that somewhere. Oh, here. Like these. Yeah. Yeah. So basically this is going to say what what needs to be updated. So remember self is an instance of a neural network. So it inherits from Torched up and end up module. So this is going to be one of the actual weights of this. So this is W. So what am I updating this is my collection of like W and V and V prime and whatever it's basically you remember how we would collect all their weights into like a single array to do this to do say Adam optimization back in lecture six. This is just a version of that. So I'm going to have a neural network that is a convolutional net of a certain size with a certain number of layers and a certain number of units per layer. This is going to in turn mean that I have n weights that have to optimize this self dot parameters stores like basically the address of those actual weights and memory so when I call Torched up opt in dot whatever optimizer. I pass in. Okay. What are the things. What's the address and memory of the things I actually am going to update whenever I make this call. Okay. Yeah, so this is never explicitly set because it's this is a member of Torched up and end up module. So, because my continent class inherits from Torched and end up module, it has access to this so when I call self dot parameters, because self is that instance of content, which then inherits from module, it has access to that number variable of module. And so when I create the convolutional net using that single line call that in my case it stuck inside of a for loop. It's doing things like assigning, you know, it's initializing values into self dot parameters. Okay. Any other questions. All right. So, So we open up a missed we can see that it's pretty much the same. So you can see that I've got by default, the data, the numerical representation of amnest is flattened so it has just single 50,000 rows 784 values. So pie torch requires that we actually reshape it to that two dimensional shape defeated into a 2D con player. So you'll see that before reshaping 50,000. So, we have 15,000 by 784. And now I have 50,000 by 1 by 28 by 28, so I've got 50,000 samples. And then each of them is basically a single dimension. And then each of that is 28 by 28. So now I can have things like, I can specify a batch size that I might want for that second arguments I can say. 100 of these samples at a time, because it's slightly faster than just feeding them in one by one hundred times faster than feeding them in one by one and optimizing the weights for every individual sample. Okay. So then what I will do is I'll instantiate my instance of my convolutional net. So now I call conv nets, and it is instantiated using 10 hidden units and 20 hidden units and then a final fully connected layer of five units output size of 10, because there's some classes. And I have a five pixel stride or five five pixel patch in my first layer seven pixel patch in my second layer, and I stride to for for each one. And then we train and we can see that we achieve, you know, these are lost, you know, over after just 10 epochs is going non significantly. And then finally, one thing you can do is you actually just print the network structure so it actually will print out this nice little display of what your network looks like so you can say that I've got okay. So I'm going to call it a conv layer one and two, and then it takes in say a, you know, one sample, and then it's a 10 10 hidden units for the kernel size of five by five and it's driving to two by two the second one is going to basically map from 10 to 20. And then it's going to have a kernel size of seven, seven is rather than two by two fully connected layers so you're wondering okay. So I'm going to take the output of my convolutional layer to feed into the fully connected layer how many actually dimensions is that so we can just print this out and see exactly it's 180. So this fully connected layer will take take in 180 inputs, and then map them down to basically five dimensions. Right, so in features, number of dimensions on the input out features number of dimensions of the output. So that's five, and then this last thing this is the softmax layer. This is going to take basically a five dimensional representation of every sample, and then distribute it into probability over those 10 classes. It's going to say okay we've got you me five numbers, and I will tell you what the probability that it falls into any one of these 10 classes is. All right, so we trained looks like it went pretty well. So then I will use my use function. So I have my test set, and what it will print out is it's going to give me the classes and then I'll use that to calculate the percentage that's correct. And it's 90.58 percent correct. So, you know we can try a few things like we can we can train for for longer for example if I train for twice as long, then. So we get 95% accuracy. Right. And so you can see that just by just by increasing the training size, the training length we get a significant boost you can also try, you know, nothing around the batch size, adding more hidden layers. We got to be careful when making sure that you're your patch size and your strike length are appropriate for the layer. But you can see what kind of accuracy we get. So, we're going to be doing the effects of one of the layers weights over on an input. So first let's get the hyper parameters of that first layer. So that first convolutional layer. Let's just take a look at what those values mean. Right. So it's got 10 hidden units. This is the kernel size five by five and this is the stride length two into. Now it's view the outputs of each convolutional layer. So here's I'll just take the 20th sample from my test set. I will then turn that into a torch tensor. And now just run that directly through the forward all output so I have a single sample. What I want to do is I want to grab every, the output of every layer and accumulate them so I can see what's happening to sample in the first layer of the second layer etc etc. So now I'll have like layer one weights. This will give me the weights data for this list or for this for this layer. So if you look at like seeing, seeing net dot children, this is going to say in this graph structure. I can find the children so I these are like the first child to be this conglares set second child to be the FC layer set and then inside conglares I can get like okay the first convolution layer. And then in that I can do dot weight and actually get data which is going to give me the actual weight values. So now what I can do is I can plot this. So I'll take the layer one weights. And then I will multiply this input by those weights and we'll see what each layer is actually. So this is the output. So take a look at these. So what you'll see here that's the original image this nine on the left. You will see these are just the plotted values of the weights right so these are the filters. And then this on the right this is going to be what happens when I apply this filter to every patch of this this nine so take a look at this and what do you what do you see what are these different filters. What do they appear to be doing to this image. Yeah, I mean ultimately yes but let's take let's like this look like one at a time let's take this. Maybe compare like this one and this one. Yeah. Right so and they can remember what these numbers with these colors mean right so if it's. We're using the negative version here so if it's darker it's more positive and if it's lighter it's less positive right so this filter for example this first one is probably activating more on like the outer edges of the nine so maybe it's like responding more to like lower the pixel values or something. Whereas this one it seems like it's mostly not it's sort of it's gray on the outsides or sort of ambivalent right maybe it doesn't have very strong either positive or native correlation either way. Inside the interior of the nine it seems to be mostly negative or low values but then all those edges, it seems to be having higher activations so this one this filter, at least for this sample appears to be kind of optimized to detect this type of this type of feature. And in fact you know this is sort of one case where this actually might be reflected in the plotting of the weights itself so you notice like take a look at where you see the darker values in this filter and compare that to where we're seeing the lighter values in in the feature map. Right. So sometimes it's not so obvious this one kind of seems like there might be some sort of correlation there. So, what can we say kind of generalize generalize a generalize a little bit about this. We have different filters that are optimized to kind of pick out different features of an image. Right, so some of these are maybe optimized for things more like certain types of edges maybe these two kind of seem to result in a similar feature map. Maybe these two even though they look quite different are maybe optimized to pick out similar features and these are all normalized so there's no guarantee of these these values are actually the same. But within that normalized range they appear to be similar and so does that one. Then if you look at like this these two here. So, you know, again the filters themselves have pretty different weights when you apply them to different patches, you'll find that they're maybe selecting for like the outer portion of the image as opposed to like the inside. All right, questions. All right. So what do we observe here we just did that. So, I think the first thing we're going to do is we're going to take away from this is that these different feature. They have 10 different filters and each of them is optimized to pick out a slightly different part of the image. So you put all that together and you can imagine that for the different things that occur in MNIST like okay yeah these filters can pick out like the edges that I'm interested in the curves that I'm interested in. So, I'm going to do this to identify hand drawn digits. So, this 95, 95% performance we get on on MNIST, it seems to make sense. So now let's take a look at the second conclusion layer. Right, so again I do index at one. I'll get the second layer so this then takes in 10 inputs has 20 units. So, I'm going to take the second layer and then I'll see what the outputs are when actually input some samples. So I'll take the same X, that same nine image, and then plot the outputs of this. And this looks like that. So what's going on here. Right, this is not very interpretable at all. This is taking layer one and then. So it's saying it's saying those feature maps that come out of layer ones basically the input to this layer to is all this stuff. Right. So, okay, we can see that this sort of, you know, this is reminiscent of a nine, especially if you know that a nine is what you're looking for. But then I take these values and those are a quote image, right, this has already been down sampled. This is no longer 28 by 28. This is, you know, five, five by seven by seven or something, whatever. So this is already low resolution. And so then I put that into the second layer. It gets chopped up in the patches. And then a bunch of a bunch of these wave matrices that you know, if you plot them look something kind of like these as well, and random looking patterns. And then you multiply that by those patched chopped up patched low res versions of the nine that have already been kind of processed by some of these to suck for some of these features. And it comes out looking like pretty incomprehensible. Right. So this you can't really impute any real meaning to what these things are actually detecting. It's more useful to see what those outputs look like numerically. So, this is going to look something like this. And you can see that, like, maybe this first row here is a kind of all the same for this that one output. So it's hard to say which one that is. So, basically every seven by seven filters going to apply to a 12 by 12 input, and that results in a three by three output. So this is, these are the actual. These are the inputs for layer one. And then these are the outputs for layer two. Right. So this is three by three. So, in this case, I think maybe there's one fully connected layer so this case this is this would be flattened into a single array, and then fed into effectively just like a non linear classifier. But then then you get your final, but then you soft max, then you get your final. And once again that fully connected layers like not required it just often is used. But, you know, sometimes it just like adds extra compute for no real reason. So we're sort of doing it here to show the difference between convolutional layers and fully connected layers. Okay. So we've got this. Yeah, 12 by 12 input not 11 by 11. That gets fed into the convolutional layer it gets chopped up and patched that send my seven filters applied to each one. And then that end is resulting in a three by three output. What this will do is I will then look at the indices that 12 by 12 image. And so when I'll see like how many intervals of size seven can fit in interval 12. And we'll see that it's three. Right. We prop one pixel, we run off the edge, but this is how we get from seven by seven, applied to a 12 by 12 input to get a three by three output. So just think about like how many times can I apply a filter of size n to an input of size m. And I'll tell you. And so then you see you see by their topic or padding to make sure they're going to. All right, so now what I'll do is I'll grab the first 10 samples from some random place in the test set. And I will plot them. And so this looks like what this will show is this will show the sample and this was going to show the probability that it is a member of any given class. So for example, here's a one and it's got a pretty high probability probably close to 100 that it is a member of class one. There's also kind of a low probability that's a member of class seven. Right. Here's a seven and we see the reverse. Right. Because sometimes depending on how you write a one it can look a bit like a seven. If this if this had like a little line there you could it could be someone ambiguous so yeah. So, like, for one and choose. Almost close to one. Yeah. There's a bit higher than. Yeah. Does it have to match with the seven? Not necessarily. I mean, in this case it does just because we think about think about the number of the digits, you know, zero zero through nine. The one that looks most like a one is seven out of all the other choices that you've got. So like it's likely that like the second place choice for a very obvious one like this is obviously a one no reasonable person even, you know, we confuse this for a seven. But when we're looking at the data, we see that, okay, the data one is obviously a one. Yeah. Yeah. When we look at seven, then it also has some similarities to seven. Yeah. Not that bad. Yeah. When we look at seven, then it also has to relate but in an opposite way that some similarities to one. It's likely it's not necessarily globally true. So there's probably some samples in here where has like the European seven with a stroke across. And in that case, it might be really, really evident that's like this is seven like nothing else even comes close but it's just checks on the arrow quick so. So, you know, it's not a non zero probability of one. What's that? Nine, right. So, this network, even though this doesn't really look like a nine and any real when you know you don't really confuse sevens and nines. Our network has determined that there is maybe a slight probability that this is actually a nine. Right. So I could rank all my upper probabilities in order. This one. There's even like a very small probability that thinks this one is a five. So a lot of these really small values come out from just doing the optimization usually from randomly initialized weights. It's like, it does pretty good in predicting like the top one accuracy because the choices are pretty obvious. If you go a little bit further down it's like yeah the third place choices thing is making is like, really make a lot of sense. At that point it's just almost like residual distribution of the probabilities. So we can find examples maybe that are somewhat more ambiguous. Here's another. So here's like one of those F sevens right we actually sees a similar thing where we see almost an identical plot actually. So in this case the network has seemed to have optimized you know for this type of distribution. Maybe it makes sense with a nine because like you could, you know, if you close this gap it might look like a nine. So let's see if you can find some other examples that are maybe more ambiguous may have to run this a couple of times. Let me try a different set. So, okay, here's here's one. Right. So here's a nine. And it's pretty obviously a nine but the actual value here might be like it's 50% likely to be a nine and that just happens to be the highest of all of my choices. And also things is at least probably a 20% chance that it's a four. And also a slightly less than 20% chance that it's a seven. So our network right if you were to plot the distribution of our test samples, we'd probably see our sevens and our nines kind of close, because for whatever reason, it seems to have noticed that like one of the nearest neighbors to a nine is probably some instances of class seven. But like this case this is one where maybe it's not quite so confident that this that this is a nine, even though that is the highest probability. So the individual samples they the probabilities have to all sum to one obviously, and you usually see similar distributions like you look at these two threes right there's roughly similar probabilities of it being an eight and maybe even a zero. And so you're going to see like kind of similar distributions across classes, but there are cases like if we look at this nine and this nine, you know, the distribution is roughly the same in terms of like you, you can sort of like linear scale one to the other. But this one this particular nine seems to have a much lower confidence on the probability of being nine. So I'm going to share it to like this other one, like this nine looks a whole lot more like this last one. Other observations other questions. Okay, so, just briefly review. It's really important for assignment for so good to start thinking about this now. What's the key difference between calculating your error for a regression problem versus a classification problem. Yeah. Almost the probabilities is the key part, I mean, not necessarily calculate the probability that you're incorrect you're calculating what yeah. That's what your output that's what the softmax is happening so we talked about the error so if I have my prediction, minus my, or my target minus my prediction. But as you did say, in assignment one for regression problems. What's different about classification problems, the represent this probabilities and therefore. Sorry. Everything's between zero and one right and so the targets are represented as like the ground truth values are going to be represented as what. One of them is going to be a one so it's a what it's a what kind of variable things are that I. So you have the indicator variables right indicated variables are a bunch of zeros, except in the correct class it's a one. Right. So if that if these if you think of these as probabilities instead of numbers. What does that one represent. 100% so this is the ground truth. I'm saying, I know that this sample is a member of class nine. Okay. In other words, there's 100% probability that this, the sample is a number of class nine. So if these are probably they all sum to one. What is my network outputting in terms of probabilities for classification problem. So they also all sum to one right, their probabilities. And so, if my network is is predicting that for some sample. It is 30% likely that it is a member of class nine. And my target value saying it's 100% likely this is a member of class nine. Then one of my subtracting. Yeah, so you're subtracting the different the probabilities right so and then you're doing that at a scale that includes all the classes. Right, so if I have a three class problem, you should. Sure. See, people can see on the video, we'll try and draw this. So, basically, if I have a three class problem. And my sample is a member of class two and this work doesn't work. Okay. So if my indicated variable is basically zero, one, zero, saying that my sample is a member of class two and I seem to really read that. And then my, my network will be output like okay. Point one point seven and then okay point to. So, this is my target tea. So this is. Wow. We try the red one. Whatever this one is tea. This one is why. Okay, put a minus sign there. We're going to end up with is zero minus point one, one minus point seven zero minus point two. This gives me an error for every probability I can then use that to optimize my weights. So what I want to get it I want to approach this thing. I'm going to get this thing I want to minimize the distance between this thing and this thing. The only difference here is that these are probabilities, whereas in regression is predicting scalar values. So the quote units here are just like per sentence, whereas in a regression problem they actually represent things like whatever the units are the values you're trying to predict. Other questions comments. Thoughts. So, would you need to have more. To get more accurate. Jen. Yeah, so as a general rule of thumb yes more constitutional layers will allow you to get better accuracy or get a better performance. But also a bigger network. You may be more likely to overfit. So, and this is not a difficult problem. So, you know, throwing for constitutional layers at it. And probably not going to get a whole lot of extra. I think I streamline a giant of that for more for harder problems. You know, image net action recognition problems. You know, type of video video classification, whatever the more complicated your problem is the more likely you are to benefit from additional constitutional layers as long as you're not just kind of throwing them at the wall. You know, randomly. To have me. I mean, it's going to have an entirely on the data set. So there will there will come a point if you know your data set there will come a point in which adding more constitutional layers is just not going to give you anything besides taking longer to compute. Right. And usually that point is pretty apparent. You have to empirically try and verify it. So you try like three layers and like, okay, I get. 97% train accuracy and 80% value accuracy or whatever you add another convolutional is like I'm not getting much better than this. And it's just taking me a lot longer to train. So you're trying to find that sweet spot between where you can actually train it in reasonable time versus getting accuracy. And as you add more. You're you've visited the approach of law of diminishing returns. Right. Start. You get to a point where it's like, I'm just not getting any more from adding this extra compute tower. One moment. When we get train and test results, the accuracy and train has higher accuracy in the test. What does, I mean, you might have explained this, but what does that tell us about it? Does that mean that our train went well, but when we actually texted it, it didn't. It depends on like what the gap is. If it's like my train accuracy was 97 and my test accuracy was 94. That's fine. You're actually doing very well. If your train accuracy is 97 your test accuracy is 79. That's much more of a problem. Something like that would suggest that there are probably. The humanities of the training data that it might be overfitting to. And that might be because like you have too many convolutional layers or something. And so it's like it's able to, if you got. A train set that contains a disproportionate number of minds that sort of look like this one. Right. It might be learning something about this like weird hook at the end. Or other features that are kind of spurious correlations to something in the training data that doesn't appear as much in the testing data. It's like, well, I see this and it doesn't necessarily match anything that I train on very well. So I don't know. Is it because it can be also because of how we did separate the whole set in the game. It can be. Yeah. So that this remember this is also why if you do. It's important to like do stratified cross validation. So you can say, like, I'm not just getting a lucky split or something and my test data just happens to be very nice. And perform like my training data my test split are such that they train very well and they test very well, whereas if you try different 20% all of a sudden it, it falls apart. Right. So what you really want to do is you want to try these averages and see like, if I sort of hold out a different portion of my data and then train on the remainder and validate and test on this portion and then rotate that portion. So that's the, how well is my network. Can, can that be expected to perform on a random new sample. And so that can take more time. Obviously, it's like, it's hard to train a big network like this. So often, most of like the modern tasks will have a curated train validation and test set. And then you can evaluate it on like this, the test set of this data set and everyone knows that they can go and get the same test set. Putting the test set is assumed to be kind of curated such that it's friendly is the network but not too friendly if you train on that training data. Yeah. All right. Anything else. All right. So I'll call it a day there. I will head back to my office. And you know you guys can just stop by anytime between now and for 30. And I will. I just have a good break. And I will you in a week.