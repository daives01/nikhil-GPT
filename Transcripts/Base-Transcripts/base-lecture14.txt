 Okay. So maybe I got a second. That's my goal. You want to go down. Yeah. Yeah. I do make myself. To you. More after. They're very good. Eight feet. Yes. It is. I'm not sure. I'm not sure. I'm not sure. I'm not sure. I'm not sure. It seems like a number is. A number of reasons. And I'm. 11 and 12. Try again. I think I, I had to. I had to revoke access because they show the answer to a two. So I should have put that back. Try. Try reload the page. Also try a different browser as soon as it saves it in your cash. Yeah. Let's go ahead and get going. Guys. So let me just first read the announcement about the schedule. So it looks like I'm going to be gone. This. These two days. So 28th and 30th of March. So I'm going to be in California at a conference. What we. Probably I had to do this once before. Last time is because I went to Korea and I had no choice this time. Might be able to do a remote lecture. So I think it's only be off by one, one hour. I could be could just do like. All online those two days. The other option. And this would only really be the case if my paper presentation happens to fall at the class time and I can also ask them to move that. But there's also possibility just like we recording the lecture and you're watching it. At your convenience and then I'll put like a discussion board up for that lecture. So you can post questions that worked pretty well in my grad class last semester when I had to go to Korea. So one of those two things will be the case. So it depends like 30th, maybe traveling. So like. Possibility of me just giving lecture like from the lounge and then very airport. Interesting. And then 28th. There's a small chance that my favorite presentation will will go inside of class. Just a heads up. So basically. Don't bother coming to the room on those days. I suppose that's what you need to take away from that right now. But I will. I will. So that's what I'm going to talk about. I'm going to talk about the plan. So it's been about what the plan is. So just luckily I'll be able to introduce reinforcement learning in person and we'll be able to do the two clear learning. In person as well. So like what does these two, the two middle lectures will be. Online or asynchronous in some way. So hopefully that works for everybody. Make sure that we get. Get through the content on time that I still have to. Get to travel. All right. So that was the only real announcement right now. So I will start convolutional neural nets. If it's. Well, I don't care whether you want me to or not. I'm going to start on personal neural nets. I don't. I don't care about your opinion, whether you want to or not. This is what we're doing. Okay. So you've probably heard the term convolutional neural nets. If that brings to mind your vision tasks, you're not wrong. That's what these things are mostly used for or at least where they sort of first proved themselves. So first I want to introduce you to this data set called MNIST that you also may have heard of. So if it looks like a bunch of handwritten digits, that's because that's what it is. This is the modified National Institute of Standards and Technology database. So basically what it is, is this is actually a variant on the original quote, NIST database. So it's basically just a bunch of handwritten samples of the digits zero through nine, obviously. This is commonly used for training various kinds of processing algorithms. So the first kind of neural perceptron image processor that was developed to believe at MIT. I'm not sure if you use exactly end is digits, but it was a digit recognition task. So this has been pretty much a common factor in neural machine learning from the very beginning, like you would. Even when neural nets have to be run under giant mainframe and really weren't very powerful. And then when we reached about 2010, 2012, and we got the hardware processing power and the data to make these things actually scale, then MNIST suddenly became sort of your go to, go to set. So even now, you know, vision papers will sometimes publish, you know, a demonstration on MNIST as like, marries into a baseline. And then also it's very common for say, kind of the more like, NURPS tile, like, optimization papers will demonstrate on MNIST because it's very well known and simple problems. So you're going to demonstrate your optimization on the sort of saddle optimization problems you try to find, you know, an optimal point in a highly non linear function, and then they'll go demonstrate things on MNIST clustering algorithms that demonstrate an MNIST. So we will also be making quite a lot of use of MNIST in this class because it's a very nice introductory database. So the original MNIST database was taken from employees of the American Census Bureau, and then the testing data was taken from American high school students. So basically you have you train on, you know, government employees, and you test on high school students. So you can, you can kind of see like how the testing data like should probably resemble the training data because, but your handwriting is more or less settled in high school. And so then you have adult handwriting as the training data and you evaluate on maybe young adult or youth handwriting, but not like, you know, kids handwriting where they're like drawing with crayons and holding the crayon and like a full fist. So what MNIST did is they basically took the black and white NIST images and they just normalize them to fit in this 28 by 28 window. So 28 by 28 is 784 I think. So I'm talking about that in my head I have been memorized from before and I'm like wrong I'm looking for every. So we have seven basically seven 74 pixels. And then the anti-aliasing did so just to zoom in what I mean by anti-aliasing is you kind of see how the lines are not. There's there's a little bit of blur there so the anti-aliasing is basically just a, a fought smoothing technique. So this turns a black and white image into a grayscale image where around the edge of the image there is some not fully black and not fully white pixels. And so then that, and give us this modified MNIST dataset. Like I said, MNIST is this kind of default teaching convolutional neural net dataset various types of computer vision systems. In this particular lecture, we're not going to go so much into the training, but into the intuition behind the convolution, what that operation is, and how we can actually extract features using this type of operation. So, let me see that. Talk about the ways I just want to see if I can actually talk about sort of the fundamentals of the vision system in the notebook text looks like I don't. So I'm just going to riff on that for a little bit. So the, a convolutional neural net of course is basically this kind of common computer vision backbones. Let's talk about how humans do vision and how that intuition translates to a computational setting or how it doesn't. So basically, your, your eye does not usually fixate on a single point. That's actually a very difficult thing to do. There are two common types of eye movements in humans and other like predators. And this basically, saccades and smooth pursuits. So, so, cod is basically, if you're looking at a fixed frame effectively, then your eye tends to kind of make these involuntary movements across the frame to pick out relevant features. So, for example, I'm staring straight ahead. My eye will probably naturally drift towards things that look like faces so even though I'm staring at the wall right now. There's nothing really interesting there. My eyes naturally going to pick up, you know, your head, your head, your head, your head, because these are the things that show up in my periphery that are likely to be relevant. Now, let's imagine that I am instead a hunter on the African savannah, and I'm going after a gazelle right so what I would do then is I basically fix the point but that point is moving. And so this is the smooth pursuit. This is the other type of movement that your eye tends to do so I don't necessarily have to move my head to follow this this thing that I want to track my eye will do it for me. So effectively what this means is that for mammalian vision systems and things like birds are different with chocolate and mammals for the moment. The eye is naturally drifting across a scene, just involuntarily trying to pick out relevant information. So just think of the optical flow of your eye. There's just this whole fire hose of information coming at you at any given moment. You can't pay equal attention to everything that's coming in even if you, even if you were able to represent things as pixels. You automatically do the filtering and try to pick out things that are informative. And so the idea here in convolutional neural nets is if you want some mechanism that the computer will be able to use to pick out important information at a low resolution. So if we look at say, how many weights we would use to classify this using a fully connected net. So let's say if we have, you know, 20 hidden units in one layer and 10 units in the output layer, then each pixels going to have a weight in the hidden unit. And then each unit has 24 by 24. So that's 784 sort of 20 20 by 20 is a 784 plus one for the bias that many weights. So the hidden layer is going to have 785 times 20 individual weights. And the output layer is going to have 21 by 10. So what does that equal. Well, it equals 5910 when you add them all up. Of course, if I had a bigger network, that would be that would grow exponentially more. So this is a lot of weights to train to try and classify it, you know, 28 by 28 image. So you can imagine if I flatten this all out into a 784 plus one vector input vector. This is still that's like, that's like a whole lot of weights to classify this one sample. So let's think about how this number can be reduced. So large numbers of weights require more samples to train. So if I have 15,000 weights, I'm going to need a lot of these 785 dimensional inputs to actually achieve reasonable performance. So what if we could follow some intuition from the vision system and basically just provide every hidden unit with a part of the image. So now imagine that behind my eyes, I have some my cortex. And it's got a bunch of connected neurons in some fashion will just assume that I'm going to use the brain and the computer is basically analog to this point. So then I have all my optical information coming in. And I can have individual units that are effectively specialized to pick out different things. So for example, I could have things that are specialized to pick out like certain colors and maybe here I'll be able to recognize the contrast if you're wearing like a dark jacket against a light background. But then I may have things that optimize like pick out human faces, right, because this is a common feature that I need to attend to. So in all the things that come at me. This would be my training data set. I need to have different specialized units to pick out those individual relevant features. And then I can combine them into things. Okay. So I see a human face and then I see a human limbs and legs and feet. So this is probably a human I can have different neurons effectively activate on seeing those different things. And then you put them all together. In this case, just using some sort of linear sum. And it's like, okay, well, this satisfies this checks all the boxes of features that I need to identify something as a human. So it's probably a human. So that's basically the intuition that we're trying to go for the convolution neural net is trying to be more efficient than just processing the entire optical flow effectively one pixel at a time. So we can take part of the image, let's say a little patch, maybe 10 by 10. And then we can assign each unit a random patch. So then we would have 100 plus one by 20 plus 21 by 10. So now that reduces our number weights down to 2030. And so now we're about one seventh of the original number of weights. Now the problem is that by slides my image image up saying to 10 by 10 patches, and I stack them side by side and they have 10 by 10 pixels that are being fed into one unit, and the adjacent 10 by 10 pixels that are fed into another unit. What if that split goes right down the middle of some interesting feature. And the left half and the right half when viewed in isolation are not actually all that relevant, or you can't deter you can detect through all that relevant. Right. So it may be that just seeing the left half of something on the right half of something is not going to give me meaningful information I need to see both of them together so I need some sort of overlap I need my 10 by 10 pixel, or 10 by 10 patch my other 10 by 10 patch, and need some other patch that kind of straddles them. So instead of just stepping, you know, discreetly across my entire image. I really want to do kind of a slide. And so this is now replicating something closer to that psychotic movement that the eye does in that the eye is darting around but it's not just fixating on like an individual square in your visual field. It's in that moment that it slides is taking in that information. Right. So if I'm looking at someone's face, and I'm kind of looking at one eye and then I moved to the other eye. At the same time also to get you know information about say the shape of their nose or their forehead or things like that. So now I want to try and replicate this. So, you know, you can add more units to cover more parts but that the weight, the number of weights starts growing again. So now we have this optimization problem of, I need to cover all parts of the image. And I want to do it without growing the number of weights. So we have the sort of dual constraints that we're trying to try to cover here. So, can we figure out a way to cover more parts of the image without increasing the number of weights in each unit, and the number of units. So, obviously, should be no surprise the answer is yes. So what we can do is we can take a hidden unit that will receive these 10 by 10 image patches. And then apply this unit to all of those 10 by 10 patches for me given image. So now we have this lens that just seems like a 10 by 10 patch. So this can be shifted around the image sort of like looking through a pinhole camera. So, you know, we can just pick out individual features as it as it's going. And so then each of these outputs can then be reassembled into another image. And that quote image should have some level of interesting features about the about the original image. So if you have our 10 by 10 unit, this would result in basically the 28 by 28 digit image will have a two by two set of 10 by 10 non overlapping patches, and then we can just put it over on the right and at the bottom. So we can apply one hidden unit to these four patches and this produces four outputs that can be arranged to a two by two matrix. And then each of these outputs represents how well that pattern matches the intensities in each patch. So now I have basically this filter that has some weights in it that will optimize or that will activate at a certain level when it encounters certain types of features. So, you can see a part of the image that just really kind of aligns with whatever those weights signify, then it should have a high activation of a see some that's like contrary to that, or just doesn't activate it then it's going to have a lower activation. So now we can see that I have weights that will then be effectively. So I'm going to use the word designed for now actually not going to get into training in this notebook but I have weights that have values that are intended to activate on certain types of input features and non certain types of others. So this process of shifting the focus is this convolution. So this conclusion operator is basically this shift of focus across an input, and then using those weights to basically process of the entire in the entire each segment of the entire input. In the same way, and then determining which regions of that input are actually most relevant to this, this particular weight matrix or what we'll call a filter. So the weights in hidden unit often called a kernel or filter I'm going to use the term filter for the duration of this class just with the same consistency. But these two terms are used more or less interchangeably. So you'll often see the word kernel using place of this kind of in the analogy to like an SVM kernel or something like that. So in this case, we was filter just think of it this way. I'm trying to filter out the parts of the image that are relevant, right, and my filter is going to be some numerical weight matrix allows me to do that for certain types of features. So I'm just going to have a bunch of different filters. And so them all put together should allow me to select the different relevant types of features for a bunch of different classes. So if I have 20 units in the hidden layer, I'll have 20 much smaller images produced, and then the weights in one unit, they could be values that say result in a high value when that image patch has like a vertical edge. So the other would be optimized to activate when it sees a vertical edge. Then the second unit might have like weights that optimize for another type of feature like a horizontal edge or a curve or a diagonal or something like that. And so then when it encounters features like that it should then output a larger value, whereas that vertical edge filter if it sees like a curve like the bottom of a three. So that's not very well aligned with what it is, what it is designed to activate on so it's not going to output a very high value. So this allows me now to effectively say for different parts of the image this filter is designed to activate on certain things like a vertical edge. And so I get a high value here so this is sort of a vertical edge filter. This is a simplified example because in reality with MNIST there are sort of limited numbers of features you can use to combine into images where you have curves you have straight lines or is online, diagonal lines, etc. In actual image classification. You're not going to have individual filters that activate on like very specific say entity level features it's not like this is a dog filter. It's more like this is a filter that responds when it sees like things that are triangular right that that's going to be that's going to respond when it sees things that like the ears of certain kinds of dogs. Often you'll actually have filters that are optimized for multiple things you may may very well end up with say a filter that will have a high activation when it sees a dog or part of a dog would also when it sees. Part of a car. And that just happens to be it's optimized for multiple things and the other things other units that get activated are also very important for determining what the output of that that network is. But in the simplified case with MNIST, we now have this network that has 2,230 weights. It can process an entire image and really instead of increasing the number of weights we just have some slightly additional processing time for the convolution and something packed on storage for storing those smaller outputs. So this digital computation and storage costs is generally pretty small and it's much more desirable to have this than to try and have a fully connected network. So, we have like one weight for every input which would be every pixel. So now we come to the question of overlapping. So now I want to like shift this lens when instead of just like taking my pinhole camera, I actually want to move it instead of like pinpoint pinpoint pinpoint pinpoint. I want to move it all the way so that I get a continuous picture of the entire input. So what I can do is I can take this thing called the stride is basically just a length. And this is like how much do I want to shift the lens of this unit. When calculating my patches. So I might shift it by like one pixel or two pixels or something. But if I say a 10 by 10 image, I want that stride likes to be less than 10. So I capture some overlap in between the different patches. So, if I just do one with my 28 by 28 image and my 10 by 10 patch, then 28 minus 10 plus one that would be 19 patches left to right. Similarly, top to down, some more square images. And so then each unit will be applied 19 times 19 times to produce a new image. That is 19 by 19 or 361 output features. So now each unit will produce 361 values. And so when we weren't doing the convolution, each unit produced one value and the output layer received 20 values one for each unit. So now the output layer receives 361 times 20 or 7,220 values. So for this slightly elevated cost of computation storage. I'm basically able to get, you know, in this case, 361 times the information in that final output layer. So instead of trying to make my judgment about my category over 20 values, I now have over 7,000 values, and those are bound to be much more informative. And I should be able to find, get a lot more information out of those values. Okay, questions on the intuitions. All right. So let's move on to some actual examples. So what we're going to be doing is we're working on hand crafting the process of creating an image, making patches performing convolutions and we'll see what types of activations or outputs we actually get. So, I'll import the MMS data set. Let's look at the, look at the shapes here so I printed out x train t train x val t val x test in t test shape. So we look at these numbers and kind of decode them for me so how many samples are there, how they represented, and what are the output classes. How many samples are there. So, somebody says 70,000. So there's 70,000 so we have 50,000 in train, and then 10,000 in validation and 10,000 in test so we're going to split them using this train now test but so how are they represented. Yeah. 20 years or they're 28 by 28 arrays. These are flattened now so they're just there the way this is stored is basically just flattens the pixel values so they're pixels. So what do you think. How are they numerically represented in those 78 element arrays. Yes. Or normal or normalize version of that basically yeah so these are these are or zero to 255 or zero to one. So these are great scale values so eventually they're either already standardized or they will be so eventually they're going to be squished down to between zero and one where zero is this fully black and one is fully white and everything else is some shade of gray. And then what are the output classes. Yes. So, this is the end this data set so it's for digit classification. My goal is to take this thing. A bunch of these and see you know which of these classes is it so we might look at a sample like this one here. Where my mouse is and say, well, that might actually be a nine. Right, even though it's it's in the row four. So this would be labeled with with four, but our classifier, you know, when we train it might actually not do very. So, okay so there's our data set so these images are graced the options so how do you think if they were our GB images. How do you think the representation might be different. Yes. You know you have three different values right yes you basically and they would still be zero to 255 or zero to one. It's just one would be the red channel one would be the blue channel will be the green channel so now when you when you consider that you just have to change the representation when feeding into the net slightly to account for basically that third dimension that is the color. So we'll keep it simple with race scale and just for now. So if you look at T train the first 10 output samples. So what is what do you think this means. What's the what what what is the first image in the data set. The five yeah. So these are not ordered right we don't have all these zeros, like at the front of the data set and then all the ones and all the two. So these are in some sort of random order that they presumably shuffled and they save the data sets when you open the pickle files in this order. Okay, so if you look at this, like the seventh sample right this is going to be an instance of a three. Yes. Yeah, these are the target samples so T T is what we're always using to denote the target output labels. So this is a classification problems if they're not scalar values they're there, their, their class labels. So in this case I think the class labels correspond to the actual digits but remember what we said about the orthogonality of classes. My classifier could have all the three is under label five for some reason, right wouldn't be very intuitive, but you could do it. It doesn't have any interpretation over like the number three, it sees the number three and it's just a collection of pixels it doesn't know that actually corresponds to, you know, the number three and its internal structure the way we board here is just happens to line up because that is, of course, a very intuitive way to do it and why wouldn't we do it that way. Okay. So, let's now take this and plot it. So I'm going to use the IM show function. So this allows me to basically display data as an image. I can just put it in a pie plot. So I'm going to make this draw image function that will put in the image and label and it's just it will drop plot, draw the image and it will put the label about it. So I put three, then there's a three, and it will give me this class here. The, and this data set is natively like this with the background is all zeros that is fully black pixels and then it is basically white on black. That's not again, not a very intuitive way to to view pencil drawing so I'm going to create a function that will actually invert this and draw it like the pencil image so I'm going to kind of draw the inverse image. So that looks like this. So now, instead of 10 by 10 patches like we were talking about this you seven by seven patches to 28 by 28 image so this will divide in it very nicely. So to do this you can use two for loops, and you step across the columns left to right and then an outer loop that steps down the columns top to bottom. And then I'll collect each patch into this patches list. And then the patches in total right 28 by 28, and that divides into or the seven divides in that very nicely. So with my seven my seven patches I end up with something that looks like this. So, first thing you might notice is like some of these patches are all black, and this is well, why do you think this patch is all black. And actually the way that we've written this function is basically doing the negative of the image, negative of all zero is still zero. So this basically this assume there's going to be a range there, and in this patch, because it's empty there is no actual range. So, so I will modify this function to include the max. So basically this is going to enforce a range from negative one to zero so that way when I invert it. They're actually, it does, it does force a range on that and makes the all the all black patches all white, because the inverse of negative one is one. And so that's going to draw it all way. So draw that okay. Now it looks nice and uniform. So all intensities are zero so I specify this minute max value. I can, I can switch that to to all white. So if I want some overlap, then I'll want seven by seven patches that shift by say two columns and two rows and we use a stride length of two. So now instead of shifting by seven each time, I want to get that overlap. So now what I'll do is I will then go through my patches and then add more patches that include those intervening intervening pixels. So now I have 196 patches. So basically 14 by 14. So I plot this takes a minute. And this is what it looks like. So this is sort of your, your stride version of this three image, and it gives you a pretty nice sense of what it what your, what your patch is focusing on as you move it across the entire image. A couple of issues with this you notice here on the, on the right side and the bottom. You have these odd size patches because basically my stride has now run off the side of the image. There's nothing left there. So we need all the patches to be the same size. And that's like fully connected that still require fixed size inputs. So, so far, you basically have to specify your input size. And if you deviate from that, then it yells at you. So we're still in that territory when you talk about recurrent nets later. We'll get a, and some instances of when that is not the case, but for the moment we're kind of stuck in this world. So what we're going to be doing is we're going to just discard the ones on the right and the bottom. So just this is the most simple technique to use right here. Usually more desirable is actually pad. So that is, I want to basically take this last one and pad it out so that when as I stride. I'm not, I get to the end, and I'm never going to run off the edge. That's a little more complicated to implement it doesn't really affect anything you want to do here. If you want to see that I have code that that does padding for you. But for now, we'll just use the cropping solution. So basically, if the row where I'm at where my stride starts, plus seven, my stride length is greater than the size of the image or the same for the column I just stop here. So now instead of 196 patches I have 121 patches. So 11 by 11. And so now it looks like this. So you can see that we still keep the entirety of the image. They that we're interested in in this like there's no pixels being cut off. But I also have the advantage of having these nice square patches all the way through. Okay, so now just, we have this additional storage, but how much storage are we actually using so these patches. Are they actually using lots of stores that can I storing every single one of these as a separate place in memory. Or are they just views on the original image race or review is a shallow copy, where if I change the view I change the original. So hopefully they're just views and so I can test this by modifying the original image and then seeing redrawing the patch and seeing if it's if it shows a modification so let me look at the first four rows and columns. They're all zeros. So now I can take this upper left column of that first patch. So if I change all those to one, and then I print the first five you can see that I've changed those first four to one and then there's all zeros. So now if I draw it again. We can see now that those first four pixels in the top left have all turned to black. So I change the original image I didn't change anything about that patches list. But then having done that when I when I draw those patches the change is going to apply. Okay, so don't want this to scrub our actual processing so let's just reset those pixels. Any questions. Well, the, the, the trimming here is what makes them uniform so they're all uniform by default. So the count the first 11123456 78910 11. So all of these, these are all uniform squares. Right. And so then once I once I get beyond that my stride starts running off the edge of the image and so I'm just like well let me just stop here. Yeah. Yeah, so this is, this is why padding is generally a slightly better solution because then if there is interesting information at the edge. You don't want to lose that. And this is such that all the images are designed to be nicely centered and so like I can be pretty cavalier about sort of throwing things away at the edges because I am reasonably sure it is nothing interesting there. So, just keep in mind that like, this is a nice task to demonstrate how this works and it's pretty intuitive but like just because you demonstrate something on MNIST is no guarantee it's ever going to work on anything else. Other questions. All right. Okay, so now let's talk about weight matrices as a kernel or filter. So how would we apply how would we create a unit as a filter and apply to all patches. So if I want to multiply something by these patches which is seven by seven weight matrix I need to have something that is of a shape that can also multiply by those patches. So it's just going to be another seven by seven weight matrix. So I'm going to have a patch size, and I'm going to have a filter size as long as those two are compatible I can now do things to that patch with that filter. We'll start by just kind of hand crafting some patches and see how they react to different types of features. So let's make a patch that should detect diagonal edges from lower left to upper right. So, this is, I'm just going to kind of craft this filter we see that we have a bunch of negative ones here, and then a bunch of positive ones here. So how this detects edges is basically. So if you want to have a different size so they won't fly together. I want to have some negative weights that should kind of de emphasize things on one side of this edge and some positive weights that should emphasize that. Or these just sort of preserve the information on the other side the edge so if you imagine an edge that's like. Black above and then white below or something like that. So if you're looking for a negative side, you would want to maybe de emphasize some of that information on the positive side you might want to bump it up a little bit more. So if these two were aligned perfectly and basically say well this is a perfect match for the shape that I was looking for. And I should have where where I have like a bunch of ones. They're going to multiply by these ones, and they're all then you send them all together and you're going to pretty high value. What about the other things. And then you just all multiply by zero becomes zero. And so you basically have an output that almost looks almost exactly like the inputs in this case if it was if it was a perfect match. So what we'll do is we'll apply this patch to a bunch of different kind of handcrafted images and we'll see how how they respond. So if the patch is all zero and what we'll do is I'll store all these experiments for plotting. So then if the patches is all zero. Remember, this is a negative. So this in reality it's actually black. So in this case, what we're going to see is that the black pixels are the positive values and the white pixels are the negative values. Just because we're using our our invert inverse drawing function. So here's this patch. So it's blank. Let me multiply them by the weights. And that's, it's still all blank. Right. It's all zeros. Everything multiplies to zero. No, and then it's it's it's matrix multiplication. But you saw everything, but everything is still zero so everything comes out of zero. So, seven, seven by seven array of zeros times seven by seven array of literally anything is going to give you a seven by seven array of zeros. Okay, so now the patch is all ones. So here's our patch. So now it's fully black. What do you think, what do you expect this might look like when I multiply that by my handcrafted edge filter. Yeah. Might look something like the edge filter. Now that's a reasonable supposition. But what happened. So a couple of things happened. One is that there is actually in reality, there's some grayscale in these values, but the, the inverse drawing kind of eliminates that. But you do see this sort of vertical line. Remember how matrix multiplication works. So I take every value multiplied by every other value in a row or a column and send them all together. So basically, a single value is incorporating information from the equivalent row or column in the other matrix. So, what I'm getting there is not necessarily the, the, the exact patch. But I'm actually getting effectively a graphical representation of kind of how much information is relevant to this patch. So generally what we're going to see is effectively the more the greater the distribution of higher values. So we match the actual physical configuration of the individual values, but the overall distribution should be higher if the patch is a good match for the filter. So it's pretty intuitive with this version because anything multiplied by all zeros becomes all zeros. But because of the matrix operation and the sums across rows and columns. So I'm going to necessarily reproduce the exact patch. So what this means is that when you multiply a patch by a filter, you end up with something called a feature map that is sort of, it's kind of like a low resolution, low resolution like just of the input and that there's things in the input that kind of resonate with this patch. And the output will reflect that numerically, but it's not necessarily going to reproduce exactly like something that's reminiscent of what was in the input. So now it's had this checkerboard pattern. So alternating zeros and ones. And so now when I any guesses what this might look like. Have I successfully scared you off of making these kinds of predictions. So look, it'll, I mean, we'll have some features of this it'll look sort of like a chessboard in that it'll, you'll, you'll see, you know, some of these, some of these filters here. Or, I can just draw the draw image I think this will sort of show it in more light so here. And I take this real quick so you saw these this part here was black. It's just because there's a threshold there about point five and so that negative drawing is where it's below point five it turns it all white if it's both point five returns it all black. So that's just an oddity of the way that we're drawing things right now. So. Okay, so what if the patch actually contains an edge from bottom left to top right, and I'll just use the just imagine that the images is inverted. What do you think is going to. It's going to look like. That's a very plausible. Well, this is this is the actual image so that this is the actual patch so it looks something like this. So yeah, you do you do see places where it like responds very well, right, where in this bottom, in this bottom right here. There's a pixel value that's like your white because when I saw over the rules and columns I get a very high value there and then it's been it's normalized. But then all went on the on the left side where it's basically not a good kind of match for that for that filter you're going to get a low a low output. So what about the reverse. No more predictions. Okay, let's just see it. So we get something like that. So it sort of looks like we take this image and this image it's sort of like it's been rotated and inverted. Again, and this is just because of that matrix multiplication. All right, one more is weird pattern. So I got this thing. It kind of looks like something on like a rug or something. And then so it's feel it does have some of those features that I'm interested in right it's got this horizontal edge, but it's also got a bunch of stuff like above the edge that maybe I don't want to emphasize quite so much in my output. So that's going to work something like that. So now you can see that it's similar. This one here is similar to the one above. And that they have similar features, but this other information here is basically like this is part of this patch and this this filters only partially relevant to this patch so I don't want to have as high an output. So, again, so here's all of them side by side. So here's a. So what was intended to show but without the negative image. Oh, ignore, ignore this one is because I ran that cell twice. So. Get rid of that. All right, so in this case, effectively we start by multiplying something. I can't remember because we're not using the input imagine this one is all white. So if this is all white this one on the right should be all white. This is all black multiplied by my filter, it gives me this sort of odd gradation. This is the checker board I see kind of some features reminiscent of that checker board. So this is a combination of features of the input and kind of the visualization of the, the pad of the filter if you were to do that. And then here, we see some oddities in that the, the visual representation is like not very intuitive in terms of reflecting either the input or the filter, but it does if you combine to the entire thing. So actually sort of correspond to how much relevant information was in this in this patch relevant to that filter. Questions. Yeah. So, I mean, for the whole training process is basically to automate this. So what we've done here this, this, what I've tried to show here is this is the operation that's happening inside a convolutional network once you've arrived at those filter rates. But it wouldn't make a lot of sense for me to sit around and hand craft all of my filters, I have to have a very deep knowledge of my data set, it might be impossible, because I don't know everything every type of visual feature I might encounter. Instead, what we would want to do and we'll get to this next time is we're going to want to take a bunch of data and basically train the weights in these filters to better optimize for that data to tease out those individual features. So this is how we end up with these convolutional filters that can be optimized for both parts of a dog and parts of a car. So if you think about image net which has 1000 different categories, much more complicated than MNIST, only as 10. How much space do you need to store all the information about those about those different categories? Well, it's less than you think if you're very efficient about packing information together. So I could have a filter that part of it, you know, it's it will respond to this dog type feature and also respond to this car type feature. And then there are other things that that some of which respond, some, some neurons respond to dog features, but not car features and other respond to car features and not dog features. And that both neuron would be activated along with one set or the other depending on what the input would be. And so the precise values that are trained into those convolutional filters can be kind of opaque. You don't really know exactly what they're responding to and that's actually an open topic of research. I'm wondering if the most common images we do, you can use them in sequence tasks. Because of the stride operator. So for example, you can they're, they're not like necessarily great a natural language task with that you can use them in that. If I want to compare, if I want to learn something about context, I can have a window that's going to look at say my center word and say one or two words on either side. And then I move my window and say, okay, now I'm focused on this word and these are the other things on either side. So pretty much anything that you can use this, this kind of striding operator, but you can use a convolutional that for whether or not it's really optimal for that as a different question but it's possible. Yeah. Yeah. Okay. Yeah, that makes sense. Yeah. So similarly sequence type data, you know, where you want to look at say, you know, a DNA base or something in context to things about either side of it. It could be useful there. All right, other questions. All right, so that we've kind of denoted this is just like patch times weights. And so this is something that's called the feature map. So if you see that term, it's basically saying these are the inputs times some weights is going to give me some representation. So basically the feature map is not really meaningful in and of itself, which hopefully you've seen here. When when the output is like not obviously, you know, kind of shares some echoes of both the input and the filter but it's like not, you know, it's not the element wise multiplication of either. So it provides information about how much response do I the filter get out of this input that I just received. So basically, you can think it was like, how excited am I by this thing, how much of a high number am I going to put out. So if I'm, if I'm a filter and I'm optimized for a particular type of feature, I get really, really excited when I see that feature not put a high number. So if I'm using a particular type of feature, I think that's like irrelevant to me, then I will do much. Right, so you think of this like the activation is going to be, you know, literally in this case a response to a particular type of stimulus. So basically the more positive values in the matrix that's created in the feature map. So in this case, since I inverted the colors is actually the more white in the image in the right column. So the image will, will output when you multiply it by that filter. So for example, if the filter defines a top to left left to top bottom left to top right edge. Then only one of these features this one I'll give you the answer. Looks like it's produced a very positive value. And so that's actually this one. So, because these are normalized. And maybe it may not be entirely clear like what the weather, the top one of the bottom one would output a higher value. That's just because we're normalizing all the values into it into a specific range. If I were to print out the actual numerical values, this one should have like objective be higher values. Okay. Yeah. Image. So, oh, it's because I should be like 10. There we go. Be careful when you run your Jubilant Book cells. So this patch has this well defined edge and when you multiply it by the edge filter, it will output a very positive value. So now let's look at our three patches again. Let's take a look at this. All right, so take a look at some of these patches and you know do you see patches that look like they might respond well to that edge filter that we handcrafted. So there are features in this that probably share some information so that maybe like this patch here, perhaps. Perhaps this patch here this one there. Maybe some some relevant patches in here that this filter would be able to detect. So we can apply this filter to all the patches. And so then to do this, we just need to multiply the intensities in a patch by the corresponding weight and then sum it up. So we can do that here. This will just show me the outputs, the actual numerical outputs. This of course is hard to read. So let me make it into an image and then draw it. So the dimensionality this image is going to be 11 by 11. So if you remember from before the way we broke up our patches seven by seven and then discarding all those in the edge we went from 14 by 14 patches to 11 by 11 patches. And so then each of these when I multiply that and then sum it up should give me a single value. So I have 11 by 11 patches. So now I can do this. So now what does that look like. Kind of looks like the three sort of looks like the three if like you squint the whole lot. Like if you do that is like, yeah, you can see the three. So basically what this is then is. Now remember this is not the inverted one so just where we're black was white now white is black. So now we have those say these high values here. These sort of fall kind of in like the crook of the three the arms of the three, which has something that sort of resembles that edge right so if you look up back up at the three image. We're talking like these, these parts here. Right, so that has kind of that that edge type feature that we're interested in. So we have to apply all of your weights to filter all the patches in a single multiplication, which makes things even faster. So we have my new images like 11 by 11. So 121 patches, and then a seven by seven. Patch. So the shape of the weights then should be also seven by seven so they multiply together. So this will just reshape my patches array pass negative one, what this will then do is it will take that seven by seven and turn it into 49. Right, so now I have 121 samples, each of which has 49 individual values. So what are those. So remember let's count up. Let's count these up so there should be 11 by 11 patches there's 121. You just flatten the image row by row. So now I have all of this row, followed by all the second row and so on and so on. Each of those will basically have 49 values associated with it for each patch. Okay, so then if I reshape my weights into a 49 by one from a seven by seven. This allows me to multiply this new reshaped array by those weights. And so now the new image is going to be 121 individual pixels with one value each. And so now it is allowed me to get the exact same thing. So this can be the this sequence of events can be basically a neat way of optimize or speeding up your computation. So here for loops fewer opportunities for mistakes. And then you also get to use the speed inherent and numpy. Okay, so now the idea is to come up with a number of these weight matrices. So if I have my three. It's only so useful to have that certain type of edge right I also want to be able to detect say horizontal lines. Vertical lines curves things like that. And so I need to have all of these things optimized in in my filters so think about the m this data set we see all these different types of features in there. So it stands to reason that if I have you know some some network, right, let's just say you know I've. Very bad neural network here. And just imagine them. I'm not going to draw those lines between them. But you know for some input. You know if if this weight has a high activation and then this weight has a high activation and then you know it's square as we can now prepare. Then if I see these. Right, should correlate with another with a different class compared to if like I see this one and that one. So again, I use this metaphor before. You can think of this as like one of those pachinko machines where you put your coin in at the top and it bounces off a bunch of different rods on the way to the bottom. And some some it comes out at say one of 10 slots at the bottom. So the goal is I'm putting in my input and depending on those weights it should sort of effectively divert it. It's passed through the network in terms of which knows have the highest activation. They're all going to output something, but I'm going to be interested in those things that have you know that that really high activation value. And then that is going to kind of when I run the softmax function, squish that into a probability distribution where it's got a peak at some class. And that's going to be sort of where your sample comes out at the bottom of the, the, the pachinko machine. So I need to have these different types of filters in my, in my neural network. And so each individual node is going to have some sort of different filter that lives inside of it. What is that, right, like you were saying we do not want to have to sit there and handcraft all of these for maybe for MNIST like it wouldn't be too infeasible to do that. And so if you're not using a lot of features and you probably actually could sort of handcraft an appropriate number of filters, and then have a neural network of the right size such as this is different filter in each, in each unit. And maybe you wouldn't get bad accuracy but does anybody really want to do that when you could just learn it all from the data. It doesn't seem like a very productive exercise to me. Is we want to learn these weight matrices. So basically every, every unit in the hidden layer is going to be this convolutional unit with this n by n weight matrix, plus one weight for that constant bias. And then we just like back propagate some error from the output layer to the hidden convolution layer and I'll play the update this units wait so for example. And so if we apply this, we have a softmax layer that will output in this case, 10 values that the probability that each of the sample that samples falls into each those classes. And then I can compare that to my one hot vector where one of those elements is effectively 100% and everything else is zero, and I can see exactly how wrong I am so if my samples actually is seven, and by randomly initialized convolutional that predicts a four. And among other things, the current weights are activating in a way that will output a four I need less of those and any more of the units that that are going to activate in a way that's going to output a seven. And so then I can I can backprop this error, and then update those individual weights. So this is basically just what we've been doing all along, but it's kind of at a different scale, and it's a little bit tricky. So this unit's going to be applied multiple times to all the patches. So you think of my my three image. Every single one of these patches is going to go through every unit. Right. And so I need to kind of get an error metric. That's that represents the overall picture. So, for example, if I have like a four and a seven, they both maybe share a diagonal line type features something like that maybe it's in a different place in the digit but it got that feature. So I want those. Maybe I want to preserve like those some of those diagonal line filters, but remove or or optimize away some of the other ones. And so my back propagation operation demands an error. That's kind of this holistic view of how wrong were all of my patches, all of my filters and apply to all of my patches. So what I'm going to do then, and I'll get into the math later is we're basically going to sum up all the resulting weight changes that result from applying each unit unit to each patch. And then that sum is going to be part of that error term that I use to update the weights. Okay, so before we get into the code for doing this let's just revisit the method for dividing an image of in the patches. So we use these two nested for loops. So we all know that four loops are slow and they introduce opportunities for error so since convolution is this very common procedure. NumPy has this ad strike as a strike function that we're going to do this for us. So there's the stride tricks library that will import, and I'll define this make patches function. So here what we do is we pass in X this is my input, and then I specify what size patch that I want and what stride life that I want. So then I will flatten this. I'll take my square images flatten them into a contiguous array. And then, I'll just look at how many samples do I have the image size is going to be just the square root of the second dimension so that is the number of pixels. And so then I can compute the number of patches that I would have given this patch size, and this image size. So then I can this will just automatically use the stride tricks.az strided function to reach to give me this array that has everything reshaped into the right number of number of patches. So I pass in my x train, and I have my 50,000 samples, each of which is a 28 by 28 images 784 pixels, pass this through make patches I'll look at just the first two samples. And I'm going to have a patch size of seven so we'll assume square patches seven my seven, and a stride length of two, like we did before. So now the number of patches is going to be two, the number of images by 120 121 patches for each image, and then 49 pixels for each patch. And so now I can actually plot this so just running this make patches function will give me the following so the first two images in the data center of five and a zero, and here are the fully strided versions of that. So, pretty neat trick that you can use to automatically patch your images. Okay. All right, so then the weights in my filter that I defined a seven by seven. So I'll reshape those into 49 by one. So now let's make two filters. One of them is going to detect edges in one direction and one in the other direction. So then what I'll do is I'll take my weights I'll just copy that into two identical columns. And then if I print out the first one you can see these are still there's that patch that we that filter we define. Now I'll redefine the second column to represent the new filter so you can see here that we now have negative ones kind of on the bottom left side and positive ones on the top right. So what kind of opposing edge filters. So now here is that second filter that we define to see now compare this one to this one and we have edges going in opposite directions. So then I'll just take the entire all my entire set of patches multiply it by my array of weights. I now have weights representing basically two columns representing the flattened version of my two different patches. I can take my output, which is this case two images and multiply them by that all at once. So now this is going to give me two images by 121 patches and then basically two values for each right and then those two values should allow me to to to plot. So here are my original images and then these are the feature maps for each of them using those opposing filters so I've got my five and I've got my zero, and you can see with one type of filter we're clearly getting a higher outputs on parts of the image, where we're getting the exact opposite output on using the other filters so if you compare say the top line of the five using the first filter and the second one. They're pretty much in verses of each other. They're not going to be exactly be so depending on the specific pixels in each individual patch, but clearly, using an edge filter for like one direction versus an edge filter for another direction to basically get make these complimentary feature maps out of each stage. Okay. All right questions. So one thing that is not in this notebook that I want to talk about that is actually in like the NLP version of this is pooling. So let me pull that up real quick and we'll just talk about that. That's one thing that I didn't mention in this. So, see it ends. See if this loads okay. All right, so where is here's the pooling party. Okay. All right, so we kind of do the same experiment that I showed that I showed you guys right here. So if we start with our feature map. In this case, the, the half sizes five by five, let's pretend it's seven by seven. So the feature map ends up with 49 individual values. So I need to get some single representation out of that. And so I could just summit, right, that's sort of your, your default strategy. So this feature map is going to record information about the precise values and the input. And so basically small changes in the locations of these could really change the output. So for example, if I have my three, and I strided differently or the images offset by like one pixel. Given a fixed convolutional filter, then the output that's going to be pretty different and that may not make that much of a difference right I may not be all that interested in whether or not the part of the path, the part of the image is like shifted by one, one pixel or not. So one of the key features of convolutional nets is they're basically, they should be invariant to translation when trained properly. So that is, it doesn't really matter where in the image. The thing I'm interested in is my convolution neural net should always kind of respond to that in more or less the same way for with n mist, we sort of allied this because all the images are nicely centered, but for robust image processing you want some singing on this kind of can give you more overall picture. So that is you want to preserve as much information as you can, while reducing the size of the input to subsequent layers. Obviously we can change the stride length to do this but also more robust approach this thing called pooling. So pooling will also rely on the stride across the feature map. So basically it's going to take something like this, and then kind of break this up into its own patches and perform some operation over those patches. So common pooling operations are averaging or max you'll hear average pooling or max pooling. So if I switch this image here, you might correspond to these numerical values. It's not going to find this pool function that's going to take in the feature map and a specified method. And so then what it's going to do is it will take this end by end patch over the feature map, and then either compute either the average or the max of the values in that window. So if I were interested in this in this feature app and I have like a two by two stride, then it would look at say these four. These four values, and it would take either the average of the max of these four values and say this is kind of the the gist of this part of the feature map, and then the next two values it would look at, and it would take either the average or the max. So if we take this feature map on the left, and compare it to the average pool average pooled version on the right. Do you think that this is sort of like a reasonable approximation of the things that appear in the feature map. Yeah, right we see darker lower values in the bottom left on average, lighter values on the right and maybe some middling values in the upper left. Adding this extra specific information might not get me a whole lot more than just looking at this. So, we can look at what average pooling versus max pooling often does. So pooling layers often occur after activation so you'll have some non linear function then you do the pooling. So, as I note here, CNN's usually use Raylue but when I wrote this function I felt lazy and I use 10 H just because I didn't want to define a Raylue function. You know what, it happens. So if you have the following feature maps, and then we pull the features using average features or max features, you're going to get slightly different results. So, if you look at, say, the two that the one that we were just focusing on, right, if the right side is average pooling and the left side is max pooling, we may be getting a little more information in the average pool version. So we're in the max pool diversion, effectively these two segments come out to roughly the same value. So these are different ways of further reducing the size, the input to the future future layers. So we can also do say pulling over the sample image using that edge filter. And so that's going to, you can have like a sample image in this case this is just a square, and I have the same edge filter defined here, and then I can, then I can pull it. So basically here is the original feature map. So just the raw feature map. So this is the average pool diversion. And here is the max pool diversion. And so all of these roughly represent the feature map in approximately the same way, but instead of having say 49 outputs, you have four. And so this allows us to further reduce the input size, and maintain the speed of the convolutional operation, while still maintaining the information about the input. So that is kind of intro to convolutions in the large. Alluding to how we actually need to train these things of course, and so we're going to get into that next Tuesday. So we'll have to do convolutional network training next week. All right. So if there are no questions, then I want to go.