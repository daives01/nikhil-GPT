 you , the intimate I mean I'm looking forward to the day when it's like you know passable as real meat and kind of why you the guy? and why what? I mean I don't mind you know it's just sort of waiting for the price point to come down until it becomes yeah well it's fine yeah I mean it's like it's obviously not meat like it's never going to pass for like actual meat but you know not huge but you know my you guys are my radizio my mom's vegetarian and like for but for like weird reasons basically my mom some reason like the post office thinks my mom lives at my house if she lives in Canada so radizio steakhouse keeps sending her like coupons for some reason they're like my mother and not me I probably could it's not like it's in her name or anything I don't need beef though so it's like you know the steakhouse they probably have some stuff like good eat yeah yeah I'm looking for it I actually looking for like a good restaurant to take like visiting faculty candidates too so I like rare yeah so uh 2020 so like 20 and a half years now yeah yeah that's good too yeah we call to the board to the table or to think of it yeah okay yeah yes okay so you're naming all the places that I've been taking like once so yeah nice yeah yeah all right I should probably stop talking about restaurants let's go ahead and yeah it started as I mentioned like if you have good Fort Collins restaurant recommendations I am looking for them for professional reasons and also because I haven't gone out a whole lot since we moved here because pandemic and then baby so you know tried like the same you know five restaurants in Fort Collins so send me recommendations if you got them okay so I did post a link to that chat GPT event if you're all interested in watching that not an assignment or anything it's an assignment in my seminar but if you are interested and you have two hours to spare because I'm sure you all will do you know I recommend watching that we will probably talk about some of those same points when we do like you know ethics and machine learning at the very end so it could be interesting for many of you all right so where we at right now is do a quick share screen and let's take a peek at the schedule I'll kind of get you going on where we're headed next so we are now we should be back on track assuming I get through the end of notebook 10 today so I'm going to finish up QDA and linear description analysis and then we'll do linear logistic regression so quite a bit of content although we got through most of that on Thursday so thank you all for getting your assignment one submissions I believe everybody submitted on time which is great there are few people who didn't submit although I assume that those folks will probably end up dropping the class it's usually what happens so keep that up also so as I mentioned it is very much to your advantage to get things submitted on time so the reason for that is as follows so I'm going to extend a re-grade opportunity for assignment one for those of you who choose to take advantage of it here are the terms I will write them out in some of the announcements later so any late panel you first of all you may resubmit your assignment ones for re-grade it's going to be due a week from today at midnight no extensions no hard deadline of February 28th you're not required to do this there will be a five point deduction so that is if you do it perfectly you will still get a 95 this is you know I don't want people chasing those last few points if you got like a 99 stick with your 99 if you resubmit and get a hundred it's still going to get a 95 okay if for some reason you do worse on the re-grade we'll keep your original grade but I know that there are some of you not too many but some of you who probably would want the opportunity to rectify some mistakes so this is going to be your opportunity to do that so I will re-open the box this evening any late penalty will still apply so that is if you turn it in a day late in addition to the five points will get an additional 10 points off this is why it is very much to your advantage to get things submitted on time it's also in future assignments in case I decide to do this again right so the same policy will apply so automatic five points the maximum you can get on this is a 95 although that may be very good for for some of you you can rectify any mistakes we'll run it through the greater again and we'll grade your your discussion it is due at the end of the day one week from today so midnight on the 28th absolutely no extensions no exceptions you're not required to do this so if you're perfectly happy with the grade I recommend you keep it this is intended to be an opportunity to rectify mistakes that is both little overhead for you and for us many questions on this this is not necessarily going to apply to all assignments I will decide on a per assignment basis depending on how things turn out that assignment so for this one this is this is extended for this assignment any questions? okay I'll post an announcement with the same details today so let's get back into the material so where we are now we're now the 21st so I'll finish up notebook 9 hopefully get through notebook 10 if we don't finish that then we should be able to get through 10 and 11 on Thursday next assignment will be rolled out a week from today so just keep an eye out for that and then A2 is due by default on Thursday extensions for those of you who have blanket extensions should already be factored in okay so I have my meaning controls okay so let's continue with notebook 9 font size okay for everybody good okay alright so just to recap we were looking at introduction to classification what we ended up doing was we're basically looking for we want to find distributions in the sample where things items that are closest to an exemplar of a class would basically fall in a distribution that is represented here by a peak a peak where the further you get from that sort of ideal point in the search space the more the probability of falling into that class will take her off right so the idea being that there are distributions where there are clusters in your search space and you want to find the cluster the probability where a point falls close to that that peak in the distribution as closely as possible and you want to find that you want to have that distribution represent the data as as good as possible and so what we do is we end up drawing these normal distributions and saying I'm going to assume that there are these points where if my sample falls right at this point is basically 100% likely to be a member of this class and as we get further from that point we get a lower probability being in that class and consequently a higher slightly higher probability of being a member of other classes so we model our distribution somewhat like this what we want is that distribution is really multi-dimensional and it can taper off at different rates in different directions and so we're looking at the types of functions we can use to model that we ended up modeling this as a property of base theorem right so this is base theorem here where I have you know p of a given b equals p of b given a times p of a divided by p of b and then p of b is basically the sum of the joint probabilities of b and all possible classes in this case what we were calling a so we went through all the math and so using the assumption that each class is normal distribution we can then try to define the boundary between two classes so this is going to be basically online or hyperplane where I fall on one side of that class of the boundary of a member of some class I fall on the other side of the boundary of a member of another class so we went through you know all the mathematics is basically a lot of exponents we use natural logarithms to bring make this function slightly more tractable good reasons for using logarithms include that we can now do addition instead of multiplication so I'm now not multiplying a bunch of small numbers together so my probability of ending in underrun is far far lower what we ended up with is basically have this discriminant function that we'll call delta so basically this is the function of a class k for a sample x and we just want to find that boundary where the discriminant of for class say class a of x is equal to the discriminant of class b for x that's going to define where it's equal probable that that sample is a member of those two classes. So this is just the two class problem this equation is quadratic and x we call this quadratic discriminate analysis right because the discriminate function is now a quadratic function. So all the math probably make some of your eyes water so we'll now go through it in Python. So first let's just make what we do usually do will make some some dummy data. We'll define it in terms of D dimensions so we can arbitrarily vary the dimensionality of the data as desired. So in this case we'll define our samples x and t where now there's just a single single component to each sample, but this can be you know any size you want. So we'll now have examples for each class. So now we define x one and t one is these are samples in class one right we're just going to arbitrary classes one into. So we'll just say, there are some samples that are members of class one there's some samples that are members of class two. We define the normal distribution and then the shape that we want to sample that from the normal distribution into just one thing to note here is just for this data we've defined there to be a wider variance in the samples belonging to class two. So we put all this data together. So what you can see here the way we stack this is we basically take the x one samples and x two samples vertically stack them. And then we take the the T one samples and the T two samples these are the targets and vertically stack them so now we basically have all our x ones are inputs x one's the next to the targets T one and T two. And now we stack them together, we have a standard way of representing our classes where we have the target label says we've done before. So our data set up in pretty much the same ways we have done. So now imagine that instead of the the actual class labels we just have data right so we basically don't know T one and T two right. We just have data we don't know how it's generated. We don't know the mean the covariance of the two classes so data might look something like this. So if I just have these are my samples and I've got some class labels I don't know how I generated these these classes. So I don't know these means and covariance, covariance is. So let me just try and try and try and compute the discriminant function for this so I'll start as before. I'll just separate the inputs, separate the input column of the target column. So similarly, right so I did dimensional data I'm going to take columns zero through D. And then I'll just take the last column to meet the target. I will define my means and my standard deviations over my data X and standardize it. So that's not done before. Right, we do not standardized T of course because this is now class labels. And so now we're trying to predict basically a discrete value, rather than some continuous value that might have a specific mean and standard deviation. So now we need to find this Q day discriminate function. Here's the mathematics of that again. So, one, the one key term here, right, let's break down with all these things are so discriminate function for class K at X is going to be negative one half times the logarithm, natural log of sigma of K minus the one half times remember X is the sample, so we can use sub K is the mean of that of the class and questions that's the distance. This is a set of samples. So each of them is going to be a different X minus the view for that sample should be the same so transpose that times the inverse covariance matrix again times the distance, plus the natural log of the class, probably the class. Okay, so C equals K is just whichever class we're interested in. So we know to calculate all of these things except for that covariance matrix, Sigma. So let's consider a couple of ways of doing that so we can define sigma, you know, as some arbitrary values, and then we can say take sigma times the inverse to sigma should give us the identity matrix. So now I can use this other function pseudo inverse so p inverse. So we should get something that if I use the pseudo inverse function that is very close to the inverse and I'll define this in the moment. I should get something that is very close to the identity function in fact I do. Right, so we have one and then some very, very small numbers here. So effectively zero. So now if I don't find sigma slightly differently. So instead of this, you know, one two two one instead of one two one two. If I try taking the inverse of this is going to throw an error. Right, this is a singular matrix. So what is a singular matrix we've looked at a single matrix is basically a noninvertible matrix so we have typically about two matrices a and B, right if B is the inverse of a, then a times B will equal B times a which is going to be the identity matrix for whatever the dimensionality is a so a singular matrix is going to be one that's noninvertible, right for for whatever reason so we'll assume it's a, you know, a square matrix, but it's it's not invertible for some reason. So what can we do in that case well what we can do is you can use the pseudo inverse function so this can be what we call the more penrose pseudo inverse of the matrix. And so this is going to be the generalized inverse of the matrix using its senior value decomposition, including all the large singular values. So what this does is this now allows me to basically compute something that's close to the inverse of any of any matrix. So it doesn't necessarily if the matrix is noninvertible for some reason, I can still use the pseudo inverse to to get something that is arbitrarily close to the inverse for that. So if I now define Sigma times the pseudo inverse of Sigma remember, remember what's what Sigma look like it's one two one two. This is going to give me this is going to give me an output. Right. So this is not the identity matrix of course, but it will allow me to to to compute this. And now I can define a function that allows me to use the the QDA equation so same similar inputs that we have all of the, all the pieces that we need to put together we've got the inputs. We've got the means of the different classes, the standard deviations are sorry, the means different this is is new. We have the means of the data, the standard deviations the data mu being the, the mean points of the different classes, Sigma is the covariance matrix, and then prior is going to be that prior probability of the class. Right. So remember this is a generative model. There is a prior probability there. That's basically how how common is my class overall in my data. Right. So I need to know the base. So you can see that we're going to take you know X minus the means time to standard deviation. So these are my standardized X values. And now I'm going to take that and then subtract mu. Right. This is so that's going to give me the distance of the standardized input from the mu of the, the mean of the class. Then what I can do is I can then compute the determinant of Sigma. So now, this is commented out here because it's not going to apply in this particular implementation. If the determinant is zero, then we will raise this error for the senior covariance matrix, but instead what we're going to do is we're going to actually use the pseudo inverse to allow us to at least get an answer for for all cases, even if it happens to be singular. Okay, so now we can start putting the pieces together right so here by negative one half right so negative one half times the NP log is the natural log by default of the determinant divided by a negative one half times the sum of the product of X times the inverse matrix times X, and then we're just going to reshape that so that all my samples are organized in the right order. And then so this this here should give me a single single dimensional matrix. I want that to be two dimensional. And so then I'm going to just add the long problem of the class to every element. So to use this we need to calculate the mean the covariance and the prior probability so we got that right you mean Sigma covariance prior probability. So what about P of C equals K, this is going to be the prior probability distribution of class K. So if I have no prior belief that one class is more likely than the other, then it's just going to be the number of samples in that class divided by my total number of samples. In this case, if you look at the data that we used before it's 50% because we had five or 10 members of class one and 10 members of class two. We don't pretend we don't know that though. Right, so we don't know how this data is generated. So we'll just sort of try and figure it try and figure out if we can recover the right answer. So, let me see what all my classes are so I'm just looking at instances of T where their class label is number one so I can just do this by basically doing t equals equals one. Apply the equals one Boolean function over my entire over my entire array. And then I can just reshape it to list all my samples out tonight you can see that it was first 10 are all true because t equals one and the rest are all false. So now I can represent my class one and class two is those indicator variables. Right, where it's either zero or one depending on the which which class is in. So I'm going to have K minus one classes that are all zero, and then for that cave class it's going to be one. So now I can define the mean and the covariance for all of those for all those different classes. So I basically just pull out which samples in my, which rows of my data are members of class one which ones are in class two, and then for each of those, I can just take the standardized inputs for that class, and then compute the mean for each of each of my classes and same for the covariance. So now what we can do then is I will compete the prior probability rights, this is going to be, and one is going to be the sum of the class one rows, so that is just how many there are. Right, same for n two, and so and will just be len of t. How many samples do I do I have. So now I can compete the prior probabilities for class one and class two just by dividing those respective ends by the total number of samples. So we can see that you know in this case we already know that our data is evenly split, but the same code would work if I have an data set, this will allow me to do that. And this will allow me to then factor in that prior probability. So in case I had a data set where I had 80% samples in class one and 20% samples in class two right the overall probability is some sample falling into class two should be lower and I want to factor that it did being a generative model. So let's look at the covariance for Sigma one, right, this is going to be this value. So now we can apply our discriminant function to some new data. So I've been able to define a function that given this information will give me the discriminant function. And then I can actually just compute the output probability for that forcing new data. So now I'll create 100 new samples. And so this is going to be kind of representative of similar data. So then what I'll what I'll define is to two models right so this is a generative model you always run K models for K classes. So if I like two classes I need to run two models. If I've got 10 classes I need to run 10 models. And two instances this dis QDA function and all I need to do is put in the distribution that I'm interested in. Right, so the distribution and the prior probability so for the first instance, I'll put in those values for the first class and the second instance will put in those values for the second class. And so now what I do if after I run this, it's going to put output values into D one and D two, and you can see that it should for each of these 100 samples, it should give me a label, basically, you know, true or false or a probability that when rounded would give me zero or one. That tells me how likely it is it falls into for the first set into class one and for the second set into class two. So we'll look at it. If you were to run this notebook and increase the dimensionality of the data, you would still work, right, because it's written to to accommodate arbitrary dimensions. But if the data is more than one dimensional will just plot with respect to the first component. In this case, there is only one component. So to obtain the value of the normal distribution from the sample we've got two choices. So we can either start with the discriminant function value and then transform it into the full normal distribution value, or we can just use our normal distribution implementation directly. So in this case, we will just define this normal D function where that has the inputs and then you and Sigma. So, I've already done those values right so we have different view and Sigma for each class, and you can see already that since this is just a single dimensional sample, the mean value for class one is around centered at point one to where is the mean value for class two is centered just shy of point, point three, or, oh sorry, my mistake. That was the covariance. The first class is centered at negative point nine and the second class is second centered at positive point nine, and the covariance is argument here and as we mentioned when we generate in the class two data, it's got a wider variance. If you remember that, hence this covariance being being larger. Okay, so what's the, what's the normal D function so X contains samples one pro and by D, new is that mean vector so just D by one, and then signals the covariance matrix. So then what I can do is basically try to recover the normal distribution by using the discriminant function basically reverse engineering. So if you look at this, this code. You'll observe that it's kind of very similar to the the qda code. Just I'm kind of doing everything in reverse right and try to get back to the normal distribution from the discriminant function where previously we define the normal distribution, and then we drive the qda function from that. So let me define that. There's our new one in you to data again. So I'm going to plot what the discriminant functions look like for this data so all, all I'll do is obviously exponentiate the probabilities, and then I will apply the discriminant function to that. This will give me a nice, nice quadratic curve. And it looks something like this. Okay, so for this data. So, you can see where that peak is for the blue curve being class one for the orange curve being class two, and you can see that you know the the peaks are different right about negative point. This is standardized. So this is the standardize like negative point nine and positive point nine. So this is probably about negative point nine still but then the peak for class two is probably about 5.5 something like that. So we can see there's a wider variance in the second class. Same thing as we see here. So this is this this first chart shows the discriminant function, you know, the actual value plotted as a function of the input feature. And then qda here, this is going to be the probability from directly from the discriminant function so here you can see more clearly. So those peaks that we were after, and then that kind of attenuation in the probability and so again here that wider variance in in class two is also evident. And so then here this one is just the qda using the normal distribution, and you can see that we get a very similar output. Right. So this is what we want. We were able to successfully recover that that normal distribution from the discriminant function. So there are only 10 training samples per class, you can expect the results to change a little bit, because it's quite a bit of noise. But what if we have like more dimensions than samples. So, for example, I could set d equal to 20 right now I have 20 components for each sample, but I still have 10 results, and I can run the exact same code, and what happens. Well, what happened here, right, we're not getting results. And sometimes I can run this again, and it may plot something for like one class, but not the other. And even when a plot something will often see if it if it plot something for both classes will basically get a flat line here, before it be probability using the normal distribution. So, so something goes wrong each time sometimes a different thing goes wrong. Clearly there's something not right with this. Right with this distribution. I need to accidentally exited. Okay, here we go. So that stigma is very close to singular, which means that the columns of X are close to collinear. So the determinant of the singular matrix of words is zero, and it can't be inverted. So we'll discuss some ways of handling this in the future. But we assume a single normal distribution, as the model of the data from each class. And this does not seem to lead to a huge complex model. But let's say if how many parameters there are if the in the mean covariance matrix, if the data is D dimensional for some value D. So, that means that the mean is going to have D components right so I'm just going to take the samples that have D dimensions and I'll take the mean for each dimension. And so I'm going to have like this D dimensional vector that represents the sort of the expected mean of the distribution of that class. Now the covariance matrix would have D squared components. So, if it's one, if it's just like a single dimensional sample, we saw that our our sigma is just set a single value, right. One value by one value is one value. If we had a hundred components, then the covariance matrix would have 10,000 parameters. So, in reality, the covariance matrix is symmetric. So it just has basically D over two plus D sorry D squared over two plus D over two, or D times D plus one over two. These unique values, we can compute the remaining values just to be know those values. But that's still quite a lot. Right. And it grows, you know, not quite exponentially I guess but probably no merely. And so we have one for each class. So the number of parameters, including the mean is going to be K for the number of classes D plus D times D plus one over two. Right. So a lot of a lot of samples. So if the data distribution is then under sample that class boundaries going to have a lot of errors in it. Right, because I'm using if I have 10 samples with 100 components. I'm not going to necessarily be able to find a very good mean for each of the classes that I have that I'm interested in. So we're going to basically over fit to those few samples and if things, you know, there's basically a lot of variance the more the more components you have that could end up on either side of that class boundary because you didn't do a good job of finding that. So we need to remove some of that flexibility from that normal distribution model. So when we can, what we can do is we can restrict all the covariance matrices to be diagonal. And so then we'd have these basically ellipses that you can draw in the data. These will be parallel to the axes. We'll come back to this point when we do dimensionality reduction. So we can really work well if our features are well correlated to each other. So now we can force all the classes to have the same covariance matrix by taking the covariance matrices for all classes and averaging them element wise. But this is doesn't seem like a great idea on the surface. Why, why not. I think single covariance matrix for all classes seems like it might. So no. What's the answer. Why is why is averaging all my covariance matrices a bad idea to get a single covariance matrix for all classes. Yes. Yeah, basically, that's it. Right. We look at the variance in different classes could be radically different. And also the more components you add the more dimensions you add the variance in individual dimensions could also be different. So even if I have like a three dimensional sample. So variance in one class in dimension three could be way less than the variance dimension three from another class. Well, the variance and dimension one is a lot bigger. Right. So averaging that you're basically losing a lot of information about in how much when I move along a certain dimension. My distribution starts to fall off. Right. So this is not like a great idea. So what we can do then is we can use the average covariance reach class and then waited by the fraction of samples for each class. So why do we do this. Yeah, similar. Okay, you're off the hook again. They run around thinking as soon as because he skipped class last week. So yeah, basically, we, we can assume that if my classes are like uneven, uneven distributed, then if I'm averaging all the, the co variances, it might not be so detrimental. So it's like has a different sort of a different variance in it in some dimensions, but it's, it's under sampled in the overall data set. Right. So if I've got one class of which I have 10 samples in a data set that has 10,000 samples like if I neglect this somewhat, it might not be the end of the world. Right. Because it's so rare. I don't know if fitting a model to the means of standard deviations defined by these data that I have for that class is actually representative of what other members in the class would actually be. Right. So it doesn't make, doesn't make a whole lot of sense to overweight, you know, or lends too much credence to just a few samples that might be really noisy or whatever. So we can actually do this and see a better result than, than using unique covariance matrices. So now to remember our, our discriminant function. So this is the discriminant function. And so we're basically just trying to find values for where D of some K is greater than or don't have some K is greater than don't have some other K. And then use the same covariance matrix for every class. So now instead of sigma sub K we just have a sigma. Right. So now we can, we can use the discriminant function plus the natural log of the probability for each of those classes. This can be simplified as follows. So you're, I'm not going to go through the math. Basically, you're just multiplying out all the terms so that I am able to have a simpler function. So my discriminant function is now become something like this. Right. So D, don't just sub K of X is equal to zero to X transpose times the inverse singular or covariance matrix times mu sub K minus one half of mu transpose times the inverse covariance matrix times mu sub K plus the integral of the problem of K. And so now you can see that unlike the previous function might not be obvious because in that you don't, you probably don't remember the whole previous function to go back and look at it. This is not linear and X, as opposed to taking a bunch of square roots and then trying to bring the exponents down. So now this can be rewritten as delta sub K of X is equal to X transpose w sub K plus constant. I'm sorry. It looks a whole lot like that linear function right. So remember, we did linear regression we did neural networks we basically have some inputs x multiply them by some weights w and you add a bias be which is usually just sort of subsumed into the weights and you train, you train the way for that bias. But it's especially the same thing, right, if we started with Y equals MX plus B, where M is a slope, and B is a is a Y intercept, B is a bias right this is just is going to be some constant value. And then the slope in this case is multidimensional, just defined by weights is the coefficients that I'm going to wait each each each input by. So then using the distribution as each other to models and restricting the covariance matrix. This gives me a linear boundary. So this is now called linear discriminant analysis. So both QDA and LD are based on these normal distributions by modeling the data samples in each class. So I can say for some sample, what would this look like if I try my best to model it using a normal distribution and trying to find that boundary between my classes. And so QDA has this flexibility, but LDA is actually often better in practice, in particular cases where we have understandable data or high dimensional data, right, for reasons that we saw before, because we don't necessarily want to have that full flexibility of trying to define a covariance matrix for every class. And then when in reality I can model the data in a more flexible way or in a more practical way with fewer computations using linear boundary. All right, questions, do the example, yes. So yeah, if you have a data set and it's not very, you said comprehensive I'm going to put words in your mouth and I'll say like, yeah, sort of, you have a sparse, you have a sparse data set. So, yes, very likely you'd want to use LDA, because you can, the intuition is basically, you don't really have enough information to be very confident in the colloquial sense about what your covariance is going to be. So, trying to, trying to fit a covariance matrix to like every data set, you're going to probably overfit to any peculiarities of those samples. And so instead I can define one more general. Yes. So let's say you have like some high dimensional samples, but you can figure out that like most of that variance is actually captured and say the first two or three principal components, right. Then in that case you might be able to reasonably fit like a QDA model to that, where it's where it's where it's, it performs a little bit better, because you can basically infer that the risk of having that large variance in those higher dimensions is actually really low, right, because they're not actually capturing all that information. So it could just be, you have some sample that for some reason you've captured a hundred components of, but just the first few components are actually where what's really important for making some sort of classification distinction. And so those other say 97 components just adding more place. Right. So if you could get rid of those, yeah, you definitely could. So, just keep in mind, just all these machine learning techniques are individual tools in your toolkit. And so most of your job, you know, if you apply this in your careers or in your research is going to be trying to figure out like what's the right combination of tools that I want to use for my data. Right. So do I want to do PCA on the data itself, or do I want to do PCA for like visualization or something. You know, it may be helpful for one but not for the other. And do I can I do some dimensionality reduction technique that would allow me to use a technique like QDA because it's faster than say trying to fit in neural network to it because these extra 90 plus parameters are just like not really that relevant. You can figure that out. That you've probably solved have problem. All right. Okay. Other questions, comments. Yeah, so remember we had that Parkinson's data as we had samples to class, to class problem. They basically have features extracted from the voice. So we're going to go back to that and then class by using QDA. So first this calculate the means and the covariance matrices I'm just going to use the same splits that I had before. So if you remember, we had like 170 or 195 samples like this, or something like that. And then we split them into a train test split using a giddy 20. So it was all 50th gen of models to each class. So two classes just zero and one. And so then I will standardize my trains. And then I will compute my means and my covariance is for those two classes. And then before I'm going to run these two discriminatory models over the samples from class one and samples from class two and see given what I know the labels are how correct am I. Then we do the same thing over the test set. Right. So again, I'm going to be using X test but I'm using the same, the same computed means and standard deviations for the data and then also the muse for the two, two classes and the covariance is right. Then I'm the last term here is going to be the prior probability of the class of my class zero is healthy and my class one is Parkinson's I just going to take the total number of each of those classes divided by the number of samples. And then I can return that into some percent correct. And so you can see where we're going already but in this case, using QDA, right, the train percentage percentage is like 98% correct. And the test percentage is about 87% on this split. If I run again, it might get slightly different numbers. But you can see that there's a significant under performance of the test accuracy compared to the training accuracy. So what we can do now is we can write this function is going to do it multiple times you're going to try different splits and run it multiple times so again, as I think I mentioned, we want you want to typically you might want to try to average like over a bunch of different splits just in case you got a lucky split once, right, and you don't want to necessarily report those results, because perhaps someone trying to reproduce the work wouldn't be able to and they're going to be like, well, I ran your exact code and I got a different results. So what gives, well, what gives is that there is a different random seed or something. And you got a huge having to get a lucky split of the data that no one else can actually ever reproduce. So basically this function is going to do what we just did, just a bunch of different times. And so we can see that I will make a, you know, a split of the data run my two discriminate functions over the train and test data print out the percent correct. And so now what I can do here, I can basically do this run park. I put in the data file and my training fraction. And then, you know, that number, we're doing again, right, I get slightly different numbers, you can see like now we're getting 92% test accuracy. So the training numbers pretty much stay the same, right, because this is about as good as I'm going to get using QDA on on this data. But for different splits, you know, I may get sometimes radically different percentages. So we have range here from about 84.6 to 92.3. And we can see here, we compare these two in this case, the test accuracy is identical. Right. And this is not necessarily because you have the exact same split, which is absolutely to be that you identified the same number of samples as being incorrectly correct. So, let's just, you know, for your review, you can just consider how would you get the values for these for these different things using base theorem. If you need some practice. So, you can just look at these. Look at these points and then go up in the notebook and just see how we would get these these different values. So now what do we need to change, we're just doing this with QDA. So what do we need to change for all this to run it with LDA. So let's write this LDA function and see if the same classifier or the LDA classifier, which assumes all the classes have the same covariance matrix does better than QDA on the Parkinson's data. So we showed that we assume the same covariance matrix by waiting it and find the number of classes, then our discriminate function becomes as follows. So then what I can do is I can write disk LDA that's implementing this function instead of a quadratic function over the same data. And so then I can redefine run park to use this function disk LDA instead of the QDA function. So here we're going to run the QDA function that I run the LDA function, and then we can see how they compare. So if I run this right now we can see here's my QDA result and there's my LDA result when you run it five times. And then you all take a look at this and see, tell me, you know, what you observe. So how does what's QDA doing versus LDA. So, yeah, so a lot of this is based on that's on the split, right? So based on which 20% we're holding up for tests can have a significant effect. But we can see that the QDA percentages are like routinely north of like at least 95 often up in the 98, 99, where the LDA numbers are lower 89% 93%. The test fraction can sometimes be significantly lower actually for LDA but also sometimes a bit higher. So for example, here's here's one case where basically the LDA is beating the test. The test is actually even beating the the LDA train accuracy, and it's also significantly beating the QDA test percentage. So we can see you're the QDA probably over fits. And LDA is a little bit more generalizable often, but if you run this again, for example, you know, run it a few more times, you'll get some different data. Right. So here's here's cases where for most of these numbers are pretty, pretty identical for the test. So, I'm going to stick like say this sample. Same test accuracy, even though the LDA train accuracy was quite a bit lower. So, then what I can do is I'll just like write this out into a file. I'll call it QDA LDA. And so then if I just run this, then we can actually see the probabilities. Now, if I look at this, here's a sample that's class one is predicted as class one, and then the probability is actually, you know, 20% or something. But the second class is just point zero zero one percent. So remember these classes don't sum to one is just looking at which one is more probable. Right. So even if this is only 20% likely to be a member of class one, 20% is still a lot more than close to zero percent. So that's going to be the answer. Yes. The one both. So you can basically see there's no way to definitively tell what it's there. We're fitting, but it's very likely for seeing my train numbers being 98% and my test numbers falling significantly below that. So it could fit really closely to this data and there's enough resemblance between the test data and the train data that it's it is lifting it up and it may actually get. Be higher than a, than the LDA model. Not necessarily. Usually it's going to produce a lower result, but it shouldn't produce a result that's like so much lower than the train. Okay. So it's really, it's much more about that discrepancy between the train accuracy and the test accuracy if it's really over fit to the train data is like your train data is going to be like close to 100%. It's really good to think that the training data. If your model were good. You would expect to see a similar number on the test. Right. So if I'm seeing 99% train accuracy and 82% test accuracy for a problem that is this simple. You're probably seeing overfitting. There are more complicated problems, you know, some of the very complicated like objects or action recognition problems that the state of the art on the test set is like 40% or something. Just because the problem itself so hard. Yes. Yes. Yes. Yes. Yes. Yes. So you, you're going to look at the discrepancy between the train data and the test data. So here, for example, this one, right for LDA, the train and test accuracy is really close. So this model is trained well enough to get 89% accuracy in the train data. It's also generalizable enough to get similar accuracy in the test data. Okay. So if I look at this one, for example, my train data is like 98%. Great number. I love to see that. The test accuracy is a lot less than that. Right. So there's something in this way this is fitting to the data that it's like not so flexible to see able to generalize quite as well. So you're the ideal model is one where you just get like really good train and test accuracy. But in most cases, you're can't be expected to do that. What you don't want to end up with is a model that is somehow fit to some sort of peculiarity in the training data, such that when I give it new data, it just kind of, you know, falls apart or just doesn't do as well. Yeah. Yeah. I mean, it does, it does depend of course. It depends what the use case is right do I want to. If I'm just interested in like just fitting to the Parkinson's data, maybe like you D is great. Right. So, I mean, I think it's actually in some cases, it's doing better so like I could stick with that. But if I wanted a model that's like, okay, I just want to be able to handle an arbitrary two class problem. And I don't know where the data was generated from, and I don't know those, you know, I can just calculate the means and covariance is but I don't know kind of the distribution. The data was sampled from, I might want to air on the side of some of it's more flexible. And then I'm like, what am I trying to use it for. If I were running a test that's like, I just really want to fit to Parkinson's data of this form specifically, then you probably just want to do whatever is going to give you the best tie at the best test accuracy, but it's like, I just, I mean, we running a bunch of different samples, maybe we're working a lab where you're trying to do two class class creation, like a bunch of different diseases or something, just as like a filtering to send people to, you know, a specialist or something like that. You probably want something that's more generalizable. Or not, just keep in mind like, just because you can doesn't mean you should right machine learning may not in fact be the best tool for this if you think of like a medical environment that stinks are pretty high. So you may or may not want to use a technique like this. This is just a demonstration of like, you know, if we have data set up in this way, this is how you can model the problem. Well, I mean, in this case we're not measuring our messy because it's a classification problem so our messy is error on us on scalar values. Right. So if you're having a if you're predicting continuous values than the RMSC will indicate you the cow, how close are you to predicting the correct value. But for classification right your metrics are different. Right. So here we're talking about accuracy, you might also do like precision or recall or F one like we talked about in the second lecture or like, you know, area under the receiver operator curve. You know, there are a bunch of different metrics that you can use and part of your task is to pick the right metric for the task at hand but like, if you're doing with a classification problem like our messy would not be the one to do because you have to it's basically squared error over some units of what's the units of classification there aren't any of those labels. Okay. Yes. Yeah. Yes, yeah, yeah. Basically what I'm saying is like if I look at like this class right this is a misclassification so that's the member of sample one or class one. It's predicted to be a member of class two. Because the model when factoring in things like the prior probability of the classes in this case it's equal probable but then also like the features. Okay, there's an 8% chance that this is a member of class one is a 9% chance or 10% chance is a member of class two. Neither of these is objectively good. Right. Wouldn't put money on this result. But these are the only two things these are the only two models that have got. So I must choose one. Right, you can handle the problem in a way that's like, if I don't get a value that's like above 50% or something I'm just going to say I don't know. I'm not going to use this result. But if I handle my problem in this way if I said at the formulation this way, whichever one's higher wins it doesn't matter if that higher value is actually objectively low. Yes. So the last this class. Yeah. I mean, we would have to look at the exact inputs for that sample. It happened to be that way but there might be, you know, there may be something that's like a typical about that sample. And, you know, we're talking about the voice features for Parkinson's so like maybe that person had a particular timber to their voice already. Right, maybe they had a naturally shaky voice or something like that, or actually maybe this is, I think, class class zero. Maybe they just have like a really solid voice that even the shakiness that comes with Parkinson's doesn't really change that part of the vocal signature all that much. So it could be. Yeah. Yeah. So it could be. I mean, this is not a severe outlier but of add a case where it's like, this is 95% likely to be a member of the wrong class. You might be an indicator that's not all right. All right. Okay. So what I will do now, I will start and run this notebook on your own. I'll start the next notebook here which is classification with linear logistic regression so we're continuing with linear classification. And we'll just follow up kind of with an alternate method for a discriminative model rather than a generative model so just as a point of point of fact so a discriminative model I'm basically looking for for the individual features. What is most relevant to this class. I don't know about the prior distribution so much. I don't factor that into my model. So an unbalanced set can have repercussions for how you fit your model. Because there are fewer samples to pull your decision boundary in a particular direction. But I'm not going to be using like prior class probabilities and making this decision. So this linear model is for classification we can have this masking problem where if I have to, to a few samples of one class. So we have this kind of result in masking so we had these different membership functions that other other than linear functions. So first we use these generative models to model data from each class, and then convert that to probabilities using base theorem and then drive those quadratic and linear discriminant functions. So now we're going to instead of doing that instead of having like two model that's going to give me a probability for being in one class and being in other classes. So I'm going to get the probability. What is a consequence of this if I'm doing this, then I should end up with a probability that it is a member of one class that if it's greater than 50% I'll classify it as a member of that class because there's no other comparison to make. I can't say it's 50% likely to be member of class A, but 30% likely to be a member of class B there over class A wins. Is it a member of class A well it's 50% likely or more so therefore yes, or it's less than 50% and therefore no. So, in this picture the problem was that this line for class two the green line is just too low. And in fact we'll see that in the middle of the range all the lines are too low. Right. So, if I'm looking here. So, if I'm looking at the red line for class B, I'm going to say that this is the highest for this set of samples, but none of them are particularly high, whereas for the ones on the edges. We see values for these these two functions of the red line red line the blue line are quite high and so you could probably be reasonably confident of that. So, we can actually represent the probability function as basically there's some function some predictor function for X parameterized by weights w for some class and I'm just going to sum that or divide that by the sum of the outputs for all functions, for all the classes. Right. So if I assume that f of X parameterized by w is rated from zero. We haven't discussed exactly what F looks like yet, but we can see the w represents those parameters that you're going to be tuning to fit the training data so now we're back in trying to optimize weights. So, we know that this expression will give me a value between zero and one for any X. So now we also have probability of C given X express directly, as opposed to modeling X given C for every class and then running all my models using this theorem. So this is going to be an arbitrary function. So let's just give it another name was called G for now. Right. So G is the probability of C given X, which is given by the output of F for function or class K divided by the sum, the sum values of F for all classes in M. So now we need to choose something for F and whatever that is, we have to have some plan for optimizing its parameters. So what's our plan. So now what we're going to do is we're going to try to maximize the likelihood of the data. So that is to say I've got some data. And I know that all my samples in these classes, but in my in this data belong to some set of classes. So we need to try to maximize the distribution of classes such that the likelihood of seeing this data is maximize. Okay, that makes sense. So I've got some data. And I want to see what classes do I need to define what distributions should I infer such that the likelihood of this data is the greatest that I can get it to be. Okay. So, if you have training data consisting of samples X one through N, and then these indicated variables for classes one through K. Remember, these are all going to be ones or zeros, where it's one where it's a member of that class and zero otherwise. So each row of this matrix should contain a zero one, a single one, and then we can also express my samples as an end by D matrix. But for the following examples will be using single samples more often. So it is going to be the product of all probabilities for the class of the end sample, given that and sample for that example. And so a common way to express this using indicated variables would be this so here's my indicator variable raised to T sub N K that's the indicator variable. So the probability raised to this value, right is either going to be. Is that is either going to be raised to zero or raised to the one. So let's say I've got three classes, and the training sample N is from class two. So the product is going to look like this right so it's going to be the product the probability of one given X raised to the T sub N one times the probability of C given X raised to the T sub N two, which is probably C given X raised to the T sub N three. So of course if I raise anything to the zero becomes one. So now only one of these terms is going to remain. So let's say it's a member of class two, this is going to, this is going to reduce to the probability of C equals to given X. So now this shows that if we use any variable's exponents we can now select the correct terms to be included in the product because it's basically looking at what actually is relevant here. My class of interest is class two, I really only be looking at the probability that it's in class two. It's not back to computing a single probability. So if this is the data likelihood what we do to maximize the data likelihood. So again, I'm going to be finding some weights w. So that maximizes the likelihood of actually encountering this data. So if L of W, right, so if this is previously likelihood of the data be. So now I want to find the W of the like that maximizes that data so basically just looking and trying to solve likelihood of W is going to be for for all N and all K I'm going to take the product of those probabilities of that sample. So now I'm finding the derivative with respect to each component of W. That is because it's a derivative in high dimensions and not back to computing gradients. But there's a whole bunch of products in here and what happens to multiply a bunch of fractions together. It approaches zero, right. And so the more multiplications I have the the closer it's going to get to zero. So I'm going to make it easier by working with the log problem, the log likelihood. So I'll just call this L of W. And so now I can do things like convert all my products into sums and then bring my exponents down in front. So now I take some for all N of the sum for all K of the indicator variable times the log problem of the class. And I'm just trying to find this is going to be a negative number always. Unless like, well, it has been a number. I'm just trying to find the least negative number. So now unfortunately, still trying to solve the gradient. Of course, the gradient of the log likelihood with respect to the weights is not linear in X. So we can, as before, simply set the gradient. The result equal to zero and solve W. Right. So now, if you're paying attention, this sounds a lot like her neural network problem. Right. It's now no longer a linear function turned by my linear function into a nonlinear function. So we'll do a similar technique. So we'll call this gradient ascent. And if you're wondering why we're doing a sent and not decent. Just think about the properties of a logarithmic curve that make this appropriate to think about the shape of a logarithmic curve. So what I'll do is I'll initialize W to some some value. And then I'll make a small change in W in the direction of the gradient of log likelihood with respect to the weights. So should be back in familiar territory. This is starting to sound a lot like neural networks, or just, you know, linear regression, even with SGD. So I'll repeat this, this step until I seem to get to some sort of maximum value in the log likelihood. Right. So this is a form of convergence. I'm just going to see my, my value does not seem to be increasing very much. I've probably reached about the maximum on this gradient that I'm never going to get. So what we see here is now what's the value of W. I'm going to take the previous value of W plus some, some value alpha times the gradient of the log likelihood with respect to W. So, alpha is going to be that, that constant that affects the step size, which sounds like. Learning rate, right. So again, you know, also sometimes labeled alpha. Remember that W is going to be some matrix of parameters. Let's say we'll have some columns that correspond to the values required for each F, of which they're going to be K minus one. So we can work on this update formula one column at a time. So here we have this, this is for each column. And then I'll just combine them at the end. Right. So this, this weight is going to be weight, the weight plus alpha times of the gradient for all of those individual weights, those individual components. So now let's remember that we have some function, which is called H. So the delta, the derivative of the log of the respect to X is going to be one over H of X. And then we have this function, this probability function that we just labeled G. Right. So now I'm trying to figure out what G is. So I can now rewrite my log likelihood function, just put G G of X in place of the probability. So now my, my gradient with respect to weight J of the log likelihood is going to be some for all N for all K of T sub N K divided by G sub K of X sub N times the gradient of W sub J times G sub K of X of F. So if you're wondering why the above works, just remember what the derivative of the log of X is. Right. So, driven that for long as one over X. So we can actually do that. So now it would be really nice if the gradient includes the factor G sub K of X of N. So it's going to cancel with the other one in the down there. So we can rewrite the function to get this. So if we define F of X of N parameterized by weights of K as E raised to W sub K transpose times X. Right. So again, we see this thing that should look familiar double weights times inputs. So if we rewrite this such that G of, G of case, of X of N, so G sub K of X of N is equal to F of X of N parameterized by W sub K divided by the sum. So now we can work on simplifying this. Right. So we take this function that I defined here. I'm just going to rewrite it in terms of this new definition of F. So we get this. And so now, if I take the gradient of this, I'll end up simplifying to something that looks like this. Right. So if I have the gradient of W sub J. Times the sum for all K of E raised to this quantity. Over one over that times E raised to that same quantity. So now if I take a look at this. What's going on here so I have the gradient of W sub J raised to E. Sorry, times E raised to the W times X. So now remember what our indicator variables were doing. Right. So if it was a member of that class, I get some value. It's not a member of that class. It's always one. So I'm taking the derivative. I should get zero. If it's not a member of the class of interest. Otherwise I get an actual quantity. So, therefore, what we can end up doing after all this math is basically saying. For this value, what I can end up doing is I can just define this function, where it's going to output some, some value if it's a member of the class of interest and zero otherwise. So now if we take this substitute back into the log likelihood expression, we end up with something like this. So we basically have okay so the change in W sub J of the log likelihood is going to be the sum for all and for all K times the variable over our function G divided by or sorry times the gradient of W sub J times the times G. So now what I can end up doing is I'm just going to take this change in delta sort of delta sub J K minus the my function G times the input. So now this gives me this update rule. Right. So we see what we had before. So previous or previous value of W sub J plus alpha, my learning rate, times the sum for T sub N, N, sorry, T sub N J minus G sub J of x. So this is the sum for J of x sub N time, X sub N. Let's focus on this term here. Right. So I went through all that very, very fast. Let's focus on what's going on here. So if we look at my indicator variable, either is zero or one. Right. This is going to now be some probability value. And so if I have my samples that are set up basically saying, it's got a probability that's zero or one or zero. Now I want to predict something that's going to give me a meaningful error. So let's say about three classes where my indicator variable is zero one zero. I want to subtract from this something that is going to be of the same dimensionality that have to be three terms. Okay. I also want to be a meaningful error. So what's a meaningful error in this case basically let's say that we've got three classes, where my indicator variables are zero one zero. Think of those as probabilities instead. 0% 100% 0%. So now if I can have a predictive function is going to output probabilities that can be directly subtracted from a value between zero and 100. It's going to give me a meaningful error. So if you imagine that I have some function that says, okay, got three outputs and it is. Point three, point six and point one. So we can now subtract that from being take that and subtract it from zero one zero. So we'd have zero minus point three. One minus point six zero minus point one. So now we're back into the kind of traditional error formulation of how wrong am I. So I'm trying to predict my probability. My ground truth is saying, well, there's a zero percent probability to member of these classes and 100% probability to member of these, this one class. I want my output values to approximate those values as close as possible over all of my samples. So this function, whatever it is, and you may be thinking of names for this is really just another way of representing error. So this time it's representing as an error in probability instead of an error in some scanner value. So, let me finish this I think. So, just to summarize what we've done. So I have my probability of my class given some sample and some data likelihood we want to maximize so what I did is I took my probability function. So, I'm going to add some function that's going to model this appropriate for right now I'm just calling it J versus or G. We want G to have the following properties. It should be bounded between zero and one, and it should sum to one for all possible outputs. So this value should be raised of this value raised e raise to the W sub K times x sub n, remember this is just my input x times my weight t. And so this should raise if equals value of K is less than K. Otherwise, it'll be one if K is equal to K. So now what I can do here is then for the likelihood of W is going to be the product for all and then all case of these probabilities raised to the indicator variables remember writing the probability function in terms of as this. So now the gradient of the log like the respect to W is going to be something like this so I now take the gradient to be the log likelihood. So I'm turning all my products into sons, because I'm taking the derivative of the natural log I can now bring the indicator variable down and then also divide it by my function g. And now I'm going to multiply this by the gradient of my weight g or sorry my weight W times the function g, just a K of x sub n. So now what I end up with is the simplifies to for the sum for all n of x sub n my input times my error. Right, so I get some output here. And I subtract that from my indicator variable is the convention thought of as a probability. And then I multiply that by the input. So that's the gradient this now allows me to turn this into an update rule which tells me how much I need to move along that gradient in order to optimize those weights. So, last few minutes questions about this will pick this up again on Thursday. Yeah. So the function of them to score up a bit. Yeah. This is functional. And. I don't think you mentioned that it is less than this one is going to be capital K that is your business. So K is a number of class. So big, big K is a number of classes. So this is an individual class and basically it's going to be there's a there's a class of interest. Maybe it's like 012. And so it's going to be the, the probability of falling into the individual class. When for for all classes, right, this should probably actually. This should probably be like a summation I think. So yeah, I think I'll fix that. It's a bit of a typo. So remember here we have the output of this is going to be what's the probability of it being in this class. And there's all this probability should sum to one. Okay. Alrighty. Yeah, so I will go back to my office. You can come to office hours if you want to be there until 430. And good luck on getting a to completed.