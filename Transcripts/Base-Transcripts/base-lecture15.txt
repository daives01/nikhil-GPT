 All right, let's start. So weather is absolutely awful and like a bunch of people are feeling sick. This time of year. And today is my birthday and so my president to myself is having a short class. Thank you. I mean, I'm turning like 36, which is like, mathematically, it's actually kind of interesting, but legally it means nothing. And it just means that you're like that much closer to 40. So yeah. Okay, so does anybody need to come to office hours like after class? All right. Yeah. That was going to go out my other present to myself was canceling office hours, but I sort of felt like kind of a jerk doing that. But if you don't need to, then I'll do that and I'll hold off sours in the regular time on on Thursday. So all right. A one regrades are back for those of you who submitted them on everybody to be slightly better, even with the five point deductions. So this seems to be said paid off nom the leaf for most of you. And sire has finished the 82 grades. I just need to review them and I will post them. So I will endeavor to get to that in the next 48 hours, at least. And then under about a three is do the Tuesday after spring break. So you know, if I Thursday would be the last time to talk to me about it in person, I will be online if you need to reach me over break. But I may not be able to respond like very promptly on anyone and traveling. So just FYI, you do have some time, but it's because of the intervene some implications for accessibility. So just make sure that you stay on top of that. Any questions or anything? Yeah. I am not here the 20, what is it, the 28th and the 30th, I think. And then I'm gone. Yeah. Yeah. So that's the last time I'm going to talk about the next week. And I'll be accessible, it's just done, but probably either do a remote lecture or recorded lecture those weeks that I'm on. Okay. Any other questions? All right, cool. All right. So last time we talked about convolutional neural networks. So this is effectively just a network that includes a weight matrix called a filter. That way you multiply it by things that quote match that weight matrix should output a high value. That's like this high activation. But we use these high, these handcrafted filters. And so of course the question is, you know, why do we waste our time, you know, hand crafting the filters. Isn't the point of machine learning to basically look at a bunch of data and figure out what matters automatically. So this lecture and the next will actually be about performing the training operations on convolutional networks and how they differ from say the standard feed forward. And what CNN's are able to do that feed forward networks are not able to do so. Just as we recall, we take our image we break it up into these patches. This allows us to do among other things kind of simulate the scanning continuous motion of the eye over an image. And then, and we can take each of those patches and try to tease out what the important information is in that patch for any particular class. So this is a classification problem. You can use CNN's for regression problems. We're not going to cover that here. But we'll just treat this as an image classifier. So this is sort of this canonical computer vision problem that we're going to be talking about here. So what we're going to do is we're going to be taking our neural network classifier class that we had from before and turning it into a convolutional version. So the classification part is the same, right? I have K classes. And then I, that means I have K output nodes and I run my softmax function over the values in those output nodes that gives me whichever one is going to be the most likely class I take the argmax. And that's the class output. So again, I have indicator variables. And my error is going to be the probabilities for all of my classes. So I'm going to take that and subtract that from my indicator variables, which is a bunch of zeros with a one in the place where the class is the correct answer. And that makes it 100% probability, minus whatever probability it predicts for that class. Do that for all the other classes and my goal is now to try to get close as close to that distribution of all zeros and a one as I can for all samples. So in this example, we're going to be just using a single convolutional layer. I was going to demonstrate how the mathematics works is not getting too complicated. And then we'll assume that our samples are these two dimensional arrays. We'll just assume black and white images. So we'll have them be square and have the same number of rows and columns. So, and then maybe not this one, but the next one we'll talk about how to do this using pipe torch. So we're using multiple convolutional layers. So our neural network classifier CNN class consists of pretty much the same components as the existing neural network classifier class so we have our init functions as the constructor. We need two more arguments in addition to the usual ones that we have in the CNN, the neural network classifier. So the first thing is we start with the things that we've we've had before the number of inputs, number of hidden per layer, the number of outputs, and we have the activation function. So the first three and this fourth one, those are the things that we've got already in our existing neural network classifier. To make it a convolutional neural network, we have to add the patch size and the stride. So the patch size is given my input image. So the size I'm going to break it up into in patches and the stride is going to be how much overlap do I have between those patches that is how many in this case pixels. Am I moving from patch to patch so it's my stride if my patch size like seven by seven my stride were also seven. This would mean I'm going to take a seven by seven patch them going to move seven pixels to the right. There would be no overlap between my patches. So we don't typically want that what you want is a stride like that's going to be lower than the patch size. So I had a patch size of seven by seven and a stride like the two. Once I clip out that seven by seven patch I move over to pixels clip out seven months from that location and keep going. So these are the two. These are in addition to the standard for arguments so the patch size that's going to be the same as that kernel or filter size. So I'm going to take a look at the different ways that we can actually get our filter to match the size of the patch that they multiplied together so I can actually get a meaningful scalar value out of that. So to make the right number of weights right previously we had this kind of wake make weights and views function that would automatically. So we can just switch them into an array and then in our constructor we can create the appropriate size of the different matrices that allow us to perform this operation between layers. So this becomes a little bit more complicated in that we have to deal with the convolutional layers in a slightly different way so first I'll initialize the weights and I'll just build this matrix of all the weight shapes. So I have my n inputs. I'll build this list of shapes. And then first I'm going to build the shape of weight matrix for the convolutional layers in this case we're just going to have one. So right now we'll just assume there's going to be like one convolution layer followed by n fully connected layers. So shapes will be initialized with the following so you're going to have self dot patch size time self dot patch size plus one. So it's going to also have the number of hidden layers in there. So let's just focus on what this means first so this is going to be self dot patch size let's say it's a seven by seven patch. Right, so this would be actually to make it five by five just for simple math so if this is a five will have five times five that's 25 plus one and this one is our bias. This should be a 26 by whatever the hidden layer size that I'm projecting into. So again remember what happens when we create a neural network I have an input size to that layer, and then an output size of that layer that's going to be projecting from, you know, n dimensions into n prime dimensions, whatever those values are. So in this case because the convolutional net layer happens on the input. The input size should be the input to the entire network. So this is going to be my patch. So I'm going to take the patch size plus one for the bias. I'll take say a five by five patch string it out into 25 individual numbers, plus a single one more number for the bias. And that's the input that goes into the convolutional layer. So then the convolutional layer should for each of these inputs map it into dimensionality equivalent to the size of the next layer. Right, so this should be the number of hidden units. So I'm going to take say it's a 10 hidden units. I'm going to take my 26 dimensional input, and I'm going to somehow map that into 10 dimensions. That's going to be the size that I accomplished in the unit. So if I apply a number of inputs is 784, then my input size would be 28. This should be 28 by 28 image. And so then, and in would be the input size minus the self minus patch size divided by self dot stride plus one. So what this does is going to basically tell me for the next layer. So I should expect to have into each unit, assuming that I'm segmenting my image according to the specified patch size and stride lines, allowing for making sure that I'm taking just the floor division to make sure that I always get an integer. And then I multiply that by the number of hidden units to make sure that I'm getting this that many samples, they go into each of those units. And so then for each hidden layer in my and hidden per layer, except for the first one. I'll then append the number and number of inputs plus one. And then the number of hiddens this is going to be the shape of that next transformation. So from this. Just all this part highlighted this all deals with the convolutional layer. And this deals with all of the, the fully connected layers. Okay. So because I've just specified kind of by fiat that there's only one convolutional layer. I just deal with it all here. And it's specified at once. And then I can use this for we have to deal with all the, the fully connected layers. Yes. Is there a reason why you're using that because you're trying to change the input size. I think this is just basically a reflection of what's in the existing code. So probably this doesn't necessarily need to actually be here. You probably just get away with deleting with moving it but this is currently what's in the neural network class fire code. It's just leftover. Other questions. Yeah. So for, for the task that we're going to do, which is basic classifying basic shapes one convolutional layer is definitely enough for more. Let's just take RGB images, for example, you're going to have three channels going in. So you're going to want to, for example, some have something that's going to be able to select features, relevant features from each of those channels. It gets a good deal more complicated. I want to get, say, my pooled or just did version of those input features, then feed it to another layer that's going to tease out like higher level features. So what you typically see happening is like for actual images, you do that. You can see in the earlier layers, often things happen that are like detecting edges, you're featured as a kind of do things like those diagonal edges and basic shapes. And then you combine those into more structural features so the deeper you go to the network, the more structure you get. That's how you can actually recognize the real images of, you know, people and, and, and things and real pictures in the world. Okay. Other questions. All right, good questions. So we'll define this make patches function. So what I need to do is we're out to have some way of automatically converting that input matrix into the patches. So this is going to be very similar to something that we saw in the previous lecture. So what I specify is just my input, and then the patch size and then the strike length. So these are the things I need to actually segment the image. So the trick here is that I am going to be passing in X as a flattened array. Right, because I need to have some fixed input size and then one initial term to the bias. And so then I need to turn this back into something that can actually be segmented horizontally and vertically. So for X for the shape of X, I will take the square root of that and that's going to tell me what are the dimensions of this image on each side. So we will assume that all our images are square at this point. And even more complicated convolutional nets. Usually you squish the image into a square shape. It just makes the math easier for arbitrary image sizes. So this is basically going to tell me how many for this patch size and stride length, how many images I should expect to have. And so then I can use that stride tricks library that I showed you last time to basically compute the actual image patches, you know, for for every individual individual patch. So I'm going to be going to be going to be going to be going to be going to be going to be going to be going to be a square shape that into an appropriate size array and then I'll return that so that now I can move through all of my samples and then how I have all of my patches accessible for each sample. So now that we have the make patches function we need to modify the use function. So what I do here is similarly. I then convert those flat and samples and departures to remember that this exit this point is still a single dimensional array representing the pixel values. Run this through make patches. So now I have X patches. And so then I'll run that through the forward pass which will see next. And this will actually perform the convolution operation over over the patches and then return the predictions. And then run my soft max over that last element in the prediction. So remember, why is here is basically the output of every layer like we've done before it's accumulating this list. So I take the last element of that list, this should be the output of that final layer. Run the soft max layer over it. This should give me the actual probability is for every class. And then I just take the argmax to actually tell me which which class index is predicted. So again, if we have 10 classes, if one of them shows up as 11%. That's the highest, even if it's not very high is going to predict that. So, always good to look a bit more at kind of at your, your actual probability distribution rather than just relying on the output label. The forward pass, we actually have to mark a bit to handle the input as patches. So this also has to flatten the image from the, the output that we get from the convolutional layer defeated into the fully connected layer. So a common tactic is not necessarily globally useful, but one that is very common is to have some number fully connected layers after the convolutional layer. In many cases, this has been shown to improve accuracy, but it does. So at the expense of compute time, you don't really always need to do this and often just having convolutional layers, depending on the task can be enough. Nonetheless, it's very common to see these fully connected layers, appended to the, the end of convolutional net. So in the forward pass, what we'll do is, if I'm in my convolutional layer, so this is all kind of the same right this is the forward pass I have my regular activation or my, my 10 h activation, depending on which activation formula function I specified. And so now if I'm in the convolutional layer which just in my example is known to be the first layer, then I need to flatten each sample into a vector to output that into the following fully connected layer. So that's what this does. And then for the other layers, once this is flattened, then it just gets fed into the next, the next layer, and then the same operations that we've seen before occur so I finally get like the last weights. And so then I can append the outputs of that last layer times that last weights plus the bias this was going to be that final prediction. So train is actually not that difficult to modify. So the only thing we need to do is actually just create patches from x, because the way they make patches function instruction is going to have them already in the right shape to feed into train. And so then I'll use that as the input matrix to the optimizer call so now the way this is written, we have this function arguments for the optimizer was previously it was going to be X and T for regression problem. And then X and your TI or T indicated variables for classification problem. So now this is still classification problems we still use. Again, we still use T indicator of ours, but now the inputs is going to be like the patch to version of X. So the way that this is written, this is actually a very straightforward change. Er if we actually change nothing. So that's straightforward. So why do we not change anything in the error function. So what's the error in the classification problem. This class is this is this is this probabilities basically right so I'm just trying to compute a sort of a distance metric how long I have wrong I am. This is going to be one of these terms is going to be 100% and I take that 100% minus some probability distribution of it says it's like 93.4. Then it is you know what the 6.6 off. And all those other numbers are going to be some some number off as well. I'm trying to minimize that distance. In the convolutional net has anything changed about the classification aspect of it. It's the same right so the error term is the same with what gets what what changes is the backdrop part. Right. This is the gradient so error is the same in that that final error term is just the difference in predicted probabilities. But how we actually back problem through the network to account for this convolutional layers is quite different. So I'll show you how to do this here once you move to pie church we don't have to do this because we still can just use autograph and do you know lost on backwards. Nonetheless, I think it's very useful to understand exactly what's going on here. So the back prop loop is going to step backwards with layers and you have to add this special case when you reach the convolutional layers in this case, the first layer. So going back to the fully connected layers is the same as we've been doing. But once you get to this convolutional layers, you have to understand where exactly the error gets back propagated. So if you consider that in a fully connected net. I've got some weight values that sort of live inside those nodes and I want to optimize those in the convolutional net. There's a weight matrix, right that filter is the weight that sort of lives inside the node. One, there's multiples to get supplied to every patch of the input. So it's not just like a single component. And so you have to allow for all of those differences. So here, basically the way we end up with that that delta that's back from the fully connected layer is going to have different values for each convolutional unit. And so these different values as all result from the application of those convolutional units to each patch. Because when I apply the same kernel, same filter to different patches and you get to get a different value. And so then the delta, their corresponds to each of those applications going to be slightly different. So what I'll do is I'll sum those delta values, you know, by multiplying each one for each convolutional unit by the values in the patch. Because the patch is different. It's going to result in a different output. So I'm going to wait that error differently. So in order to do this, I'm going to reshape that delta matrix to the right form done like this. And so then I need to reshape the convolutional layer input matrix to a compatible shape so I can multiply them together. So I'll take that input. So this is that input turnings that you multiply by the error to actually get the amount by which you update the weights. The trick here is that because I have now a matrix of delta values. I need to have an equivalently shaped matrix for the convolutional inputs. So now you can calculate the derivative of the error with respect to the weights with the convolutional layer with this single matrix multiplication. And then the fully connected layers just work as as they have been. So the only real trick here is just understanding that you have multiple applications of each patch of each filter to each patch, because the patches are different. The resulting values, those feature maps are going to be different from the same filter applied to the different patches. And so then the, the error that you're going to get out of those features that come out of those different patches multiplied by that filter is going to be slightly different. I have to account for that. So I do that by collecting everything into a matrix, and then making sure that my inputs are reshaped into the shape that is compatible with that matrix so I can do the whole thing in a single operation. All right, questions about the components of the C and classifier. Okay. What we'll do is we'll in my code that's hidden from you. I'll define the network classifier as a new class that will extend the neural network classifier. And so then you can use it as in the following. So we'll do is we'll make some simple images these are either squares or diamonds. So first define my square. So you can see here I've got a bunch of zeros and then some ones that define the shape of a square, similarly for the diamond. So you can see here, you look closely, and see those ones. I'll draw those images by defining this draw net image function so now you can see I've got an example of a square and I've got an example of a diamond. And these are centered within my within my image frame. Okay, so this works fine, but right now if these were my only images will be my net would be really good at classifying squares and diamonds in this exact position, which is not very useful to us. So I'm going to do it. If I'm trying to detect zebras. I want to detect a zebra, whether it's on the edge or in the center. So one of the key benefits of CNN is that they're invariant to translation. The law of transit CNN will basically be able to pick up features corresponding to an image class whether it's in the center of the image, whether it's off the edge, even if it's like a little further from from the camera or closer to it. As long as it's not like, you know, so far away you can't see it, of course. So what we're going to do is we're going to create a function that will generate a bunch of images like these are going to randomly shift them left and right and up and down. And so this would challenge the fully connected nets, but not the convolutional net because remember the convolutional operator is sort of like having this pinhole camera that scans over the sea and it says, oh, I see a corner that looks like a corner of the space that I just came along to a diamond there. That's important. It doesn't matter the exact place that it matters that I see it. So I'll define this make images class. I'll make 20 or 40 black and white images the diamonds or squares and I'll sample a few of them to show. So this is going to create training data that has sort of randomly arranged squares and diamonds from classification task. So in this case 100 of each class. And then I will split that into train and test. So we get something like this. Right. So each time I run this I'm going to get a slightly different set. And so we can see here. There. Yeah. So now I get a slightly different set of images you can see here that we've got, you know, big squares that are nicely centered squares that are in the corner some squares and diamonds that are like tiny and shoved off to the edge. So at least this should challenge a fully connected net and allow me to demonstrate, you know, when a convolutional net is actually able to perform. Yes. All of these square diamonds are fully linked in our frame. Yes. If we were to introduce ones that were cut off so I swear that was in the corner. Yeah. It might challenge this one somewhat I suspect it probably wouldn't challenge it a whole lot because if you consider if the squares cut off in the corner, you still have very square features if my two classes are square and diamond. If you imagine a square and a diamond that are cut off in the corner at the equivalent position. I'll have a square that looks like this. And then there's the frame in the corner of the diamond might just be like a slanted line. Right. And that could be enough basically I would end up with is possibly feet filters that are optimized to detect say, not the whole diamond but just the slanted line. But that also then connects to something in the output layer that has a high activation when it gets for the diamond class, for example. So for this example, probably not for a real example, probably. Right, especially if you have things that are like rotation, you know, I'm trying to classify animals from different perspectives, or even, you know, chairs there's a, you probably heard of the image and that data set is a common, you know, 1000 class. A computer vision data set. You know, some researchers showed that the because the images are nicely cropped and framed. You're sort of getting your chairs and cups and bottles and things, but they're all like this canonical pose. And so if I take a picture of a chair from like up here, all of a sudden, that doesn't show up an image net so an image net trained network doesn't recognize that as a chair because it doesn't, it doesn't have the features of the chair. So cases like that you want more, you want some combination of more sophisticated network and maybe just like better training data, but in this case, if that were in the training data, it probably would be okay if that yeah. So, you can, there's a couple of ways to do this. So you can have an additional channel that's a depth data, for example, that's the operation of the depth data so some like the gesture recognition stuff can be trained here at the CSCU. That's the depth channel for that. One of my students actually defended his master's thesis yesterday, I did a thesis on convolutional nets over 3D geometries. So actual meshes, and there's some very interesting properties of meshes that you have to account for to make them invariant for CNN. And they're still it's still challenging. And also there are 3D CNNs, so they're actually instead of a 2D convolution, actually have a 3D convolutional operator, and that can be used for both 3D data and also say like multi channel like video data you can have the RGB channel and then a third channel that is like time. And so you actually look over like multiple frames. But these are way more compute expensive, of course, the moment the moment I added another dimension. So I'm going to go back to X squared to X cubed. And so every additional pixel basically, you know, I'm have to process that three times. Yeah. What are you doing with pixels on your spectrum? If I trained it on black on white and tested it on white on black, it probably would have a lot of issues. We run the same issue that your training data must more or less resemble your testing data. So if I trained on images look like this and then I inverted this and use that as the testing data. I suspect we'd have a lot of problems. If you train it on both, then it probably would be able to accomplish classification of both, assuming that the network size is big enough to accommodate the filters required to optimize. Well enough to accommodate both of those features. So I suspect again if we had like the square diamond task. If we did that sort of by color version might have to increase the size of the hidden layer a little bit but it probably wouldn't be too much of a big deal. Yep. Other questions good questions. All right, so now we've got, we've created our CNN we've made the records and updates to the patching the forward pass the use function and the back propagation. We've created training data that would challenge a fully connected net that is probably not going to challenge a CNN. So now we can actually try to train this. So our net has been defined to accept these two dimensional input matrices we need a flatten each image. So first what I'll do is I just look at this look at my train samples. So you can see like I've got 10 squares here. Let me take a sample inputs for train and test. I've got 200 train samples to test samples. These are 20 by 20 images so each of them can contain 400 pixels. So what I'll do is I'll try two units of the convolutional layer followed by one fully connected layer of two units. I'll use a patch size of five and a stride of two. So what are my classes and run NP dot unique over T train so I've got two classes, diamond and squares and have two output nodes. And so now what I will do is I will import my neural network. And so here, this is x train dot shape rights. This should be that 400 number. This should be my flattened input size. These are my two hidden units and these are my, my output size. And then I specified my patch size and my stride life. So I'll train for 2000 epochs of the learning rate of point, oh, one using Adam. And I'll do that for a minute or so, even that. And this is, this is our results. So, took four seconds to train. Perfect accuracy on the training data and pretty close to that on the testing data. So what I can do now is I can look at my individual samples and see what the probabilities are. So, these are the, this is what I predict. So, zero, these are squares class one, these are diamonds. This is the probability for each. And then the blue line is the actual ground tree labels. So we can see this basically one sample in fact it looks like the very first, maybe not the very first known is the very first sample appears to be the very first sample and got it got wrong. So, of the 20 testing samples, you got 19 correct. And one of them appear to classify a, a square as a diamond with a slightly above 90%. But in all other cases, it got it correct and the most. The one they got closest to being wrong was this one where I thought like it's about 19% likely to be a diamond, but it's actually a square. It's a pretty good performance. It's a simple task, but this demonstrates, you know, what a convolutional nerve map can do. All right, so now let's see what our units actually learned, and I can do this by drawing images of the rate matrices. So first I'm going to look at the shapes. So this should be a five by five patch plus the one bias weight. So I'm not going to visualize the bias weight, not least because I can't turn this into a filter. I can't turn a 26 sample array into a square. So I'm going to lob off that bias weight reshape it into two five by five rays when representing the first unit one and the second unit. And we'll see what it actually learned. So what do you notice here? The square looks funny. Do you think there seems to be something that looks like a diamond right. Do you think it's actually reflecting the fact that it learned the shape of a diamond in that configuration. Not really why not. Right. Visually, and especially in this case, because remember the diamonds are randomly distributed across the frame. So this is just sort of coincidental. It might be because of the way the way the weights were initialized or other factors of the training. But let's not be misled into thinking that because we see the sort of diamond shape here that this is a, this is necessarily like the unit that has a higher activation when it sees a diamond. It might be. And maybe there are a high portion of diamonds that are nicely centered from our maybe just got kind of unlucky with the randomization. But it's comparatively unlikely. Who's calling me. Oh, God. Like at this time of the day, like my undergraduate university that calls and ask for money. Anyway, see, we'll do to you too. Anyway, so what we can see here is that although we see a diamond isn't necessarily indicative of the fact that it that filters like learning the diamond features in this configuration. So, let's print the weights. So this is the, the first unit. This is, this is going to be this one. So you can kind of see, let's take like this first value. This is like pretty low as negative point six, whereas these two on either side of it are point five three and point five eight. So you can see that this corresponds to the values we see there so each hidden unit contains this five by five pixel filter. And then the individual pixels represent the value in the in that matrix. So, unlike the last lecture where we had this very specific kind of edge. These units have been trained at the same time to rep to optimize for both squares and diamonds of different shapes in different places in the image. And so then also it's the, it's a five by five filter over like a 20 by 20 image. And so as the, as we move across those, those patches, these individual values are going to reflect some sub segment of the image. Okay. And so they do like pretty well at this task, but they don't really resemble like square diamond features visually. So, if we train to CNN to detect dogs. Would you recommend, would you expect the filters in that network to visibly resemble dogs. Not really. Sometimes you will see when you say take the filter time on the inputs times the filters the feature map not the filter itself. Sometimes you actually see like high activations on like features of a naturalistic input image so you can see if there's a filter that appears to activate more for certain types of edges. And you can see the kind of see the same image of like a German shepherd or something in places where maybe there's like a diagonal edge for a certain type of filter that activates with that type of feature you're sort of see that it matches those those features in the image. But as you get deeper and deeper into the network, those visualizations become like more and more, you know, obscure and opaque. And it's kind of it's easier to visualize, maybe for those more intuitive features in the early part of a convolutional net but much not not so much later in the net. Similarly, these images, these filters can be optimized for multiple things so both of these filters are optimized for squares and diamonds, you know, maybe some more than the other, but it doesn't both at the same time. So this is generally true for all filters in a convolutional net they're all optimized for all classes to a certain to basically varying degrees. So you have some that are going to be more optimized for certain classes and then less optimized for certain classes they should all activate a bit. You know, at least like a non zero amount or mostly non zero amount, depending on what they're mostly they're most optimized for. So now let's repeat, we'll use for convolutional units and two fully connected layers after the convolutional layer so here we have, using my four units and then I have the two fully connected layers with 10 units each. And this time I'm going to train for 1000 epochs and so 2000. I'll use Adam again at the same learning rate. So the last one took four points something seconds. This took 5.8 seconds, even though I trained for half the number of epochs right this is because it's a there's one there's twice as many convolutional units. There's also these two fully connected layers it's a bigger network that's more back propagation do there's more operations. The test percent is still good, but it's actually not as good as the previous one it's 98.5 instead of 99.5. So, I'm going to just generalize those probabilities look something like this. So there's some samples on it still seems to get that first sample wrong. In fact, it got it more wrong in terms of the actual probability of the incorrect class. And then there's two samples of diamond that is also getting along. So the training converges faster, but the generalization of the test data is actually not as accurate. So you might say that, you know, perhaps it's like overfitting to something in the training data. And that's quite likely in this case because the task is so simple that the bigger your network gets the more likely it is to to learn kind of spurious correlations in the data. So we will visualize what the four convolutional units learned looks like this. And then once again, you know, we can see that there's no clear correlation between the actual physical shapes of the input images and anything that's being learned in these convolutional filters. So you can see, you know, where they're like, for example, in this in this filter. So there's two pixels here represent individual values that filter that tend to have high activations kind of regardless of what they're looking at, at least relative to what's in this data. And then like this one has like a high activation with, or is as a high value in the filter, just like right in the middle. But not a whole lot of like visual correlation. So let's see how fully connected non convolutional neural net would do. So, let's look at sort of the non patched version. So I got my two, 200 samples train and test. So instead of using my C and M class of trial use my normal classifier. And so notice that I don't use the patch sizes and input I don't use the stride length. And also because this is not an instance of neural network classifier and not the CNN in that in it functions not going to call like the make patches or anything. So the way you've written this code is that in the CNN, it's set up in a way that will automatically do the patching for you if you instantiate this type of networks. I don't need to do that pre processing of the data before I instantiate the network does it for me by virtue of the type of network that it is. So if I have, you know, we run the convolutional version first and then I'll run the fully connected version. So, this is for hidden units, and then to, to fully connected units 99% test accuracy. If I do a fully connected net with the same, the same network architecture. One, you'll notice it trains a lot faster. Right, because this the convolutions mean there are more operations that have to be done because I have to apply every patch every filter for every patch. So it trains a lot faster, you know, two seconds versus five ish, but test fraction is only 83% as opposed to 99. And it looks like kind of a big mess. Right, there's a bunch of squares that are being misclassified as diamonds and a decent number of diamonds that are being misclassified as squares. So what did our filters actually learn. So, we get this. So now take a look at this. So, what do you think it's tried to do here by looking at these filters. So remember, our, what our inputs look like, right, we've got squares and diamonds randomly scaled and randomly moved about the frame. And it tries to optimize for all of the data at once. Right, that's what a, a neural network does. So let's take a look at some of the things we might observe. For example, we might see. You know, there is sort of a line here of high values is also one at a similar position in this other filter. So it might be that, for example, there could be a lot of squares that are in this region or some there's lots of horizontal lines in this region in the training data. And so it's learned that, well, there's probably something here at this location in the input. So if I optimize to detect that, I will tend to get a higher accuracy. So it may have learned effectively to look for a certain type of feature at a specific location in the input. Right. Similarly, you might observe that like there's a diagonal line like here and maybe another one there. So you can see some sort of diamond. Esque features here. And so this might be another indication that a lot of diamonds happen to occur at approximately this location in the training data. And so it tried to optimize for that because by predicting that it got a better, it got less error and a better, better performance. But if these things don't occur in the test data, or don't occur in as significant a proportion, then it's going to incorrectly predict things that may be a square that has a pixel at this location is going to go, Oh, well, this is correlated with being a diamond in my training. So I'm going to predict that. I'm going to predict wrong. So the invariance to translation is one of these key features of the convolutional net that a fully connected net is not going to pick up on because it's trying to learn everything it can about the individual pixels and the data. And it generally just doesn't do a very good job because maybe if I had a bigger network, it would do a better job with a simple task like this. But generally speaking, it's not going to, there's not enough space in the network to represent the information at a per pixel level where the actual position of the pixel actually matters. Questions. Okay, so the, the fully connected net, you know, tends to over fit to the training data. And so, but it tends to fail to perform as well in the testing data so there are basically a few things that a convolutional net requires so it still requires fixed size inputs, like you're fully connected net. But the reason it can do invariance to scale is because there is in, there's an opposed ordering on the pixels in that the neighborhood of a pixel is the pixels on either side. And so we can actually learn basically relations between pixels at certain points and pixels around them regardless of the absolute position inside of the image. And so then there's also this, or this is this implicit ordering and then there's the neighborhood around the actual pixel so this allows us to use the strident, the striding operation, the convolution, and that filter to segment the data, according to relevant information relative to each other, regardless of the actual position inside the image. Okay, any questions. Yeah. Yeah, they're pretty similar and they're probably close, they're pretty close to zero I think, if I'm looking at like the edges. And in this case, this is also probably learning, you know, correlation with the data in that we, we actually don't allow images to go off the edge, for example. So it's very likely that that first pixel on each side is empty in most samples. So it learned effectively that there is zero correlation or close to zero correlation with the edges and the class, because at most you might have like one pixel or one or call the pixels if it's a square in that edge, but otherwise it's in the vast majority of samples it's probably an empty empty space. Okay. Other questions. All right, so to summarize. A convolutional nets use fewer overall weights, but they learn more generalizable matrices or filters. These filters don't necessarily match the input feature visibly, but we can train them to recognize multiple different types of features in sometimes very diverse input sets. And we can recognize them at the same time so these can be swears diamond straight lines diagonal lines curves, you know, as the network size increases the capacity for learning more types of filters also increases. What we need to do is we need to take the image segment and the patches, stride over those patches we get a significant amount of overlap between the patches. And so now these filters can learn the difference and similarities between neighboring patches that have slight differences between them. So after you standardize your input values. So these, these are just like intensity, or in RGB there just color. So you're going to standardize them into a distinct range. Convert that entire matrix into past, as we have a function that does that for you. And in both train use you do this, so that your input is of the same format in both both functions. So now the input to the forward pass is the patch diversion, not the raw input. And so then at the output of a convolution layer is input to the fully connected there you flatten that into a single vector so now just becomes basically a feature representation, because into the the fully connected layer. So the hardest part is back prop. So because we have multiple applications of each individual patch and each individual filter. And so those delta values are going to differ depending on the different features in the patch. And so there were going to have multiple values for every convolutional unit. So that has to come from the application of each unit to each filter so we sum that from each unit to each patch be some that for all the delta values. Take the whole delta matrix reshape it into K by N, where N is going to be the number of units in the convolutional layer that you want to back prop. What's the other thing we need to multiply. So we have the error. We've got the features that we have the input. And we reshape that input matrix to N by P, where P is the number of values in the patch so now K by N and by P will multiply together to give me K by P. This K by P will give me the gradient of everything but the bias weights. So this is going to be the gradient associated with K classes, or K outputs, and then P patches. So then I'll sum every column that reshape data matrix delta matrix and get the gradient to the bias weights. So we compared this to back proping to a fully connected layer where all you need to do is multiply the inputs to the layer by the delta, and then we have no doctor matrix to reshapes just all been single values. Okay, so, final questions. Okay, I'm going to take you at your word that you don't need to see me in office hours. If you do I will be there but I'll be writing. But I will give you back 30 minutes and I'll see you on Thursday.