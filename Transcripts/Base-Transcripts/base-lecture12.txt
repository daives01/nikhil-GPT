 I want to go to the end. Yeah, I want to go to the end. Yeah, when I was like. I got to go to the end. I got to go to the end. All right. All right, guys, let's start. So I appreciate you, Hardy souls who came out today. I actually was not expecting. Any people I was like. No. A dozen to 20 people in the room, which is. Twice as much as I would actually be here. So. Yeah, thanks for. It's for the rat. And wait for. To catch. Okay, there we go. All right, so. What we're going to do today is I will just go ahead and share the screen. I don't think I really have. Accept that. Assignment to is due today for. Most of you. So if you haven't requested, if you need an extension. And your default due date is today, I'm sorry, it is now too late. So. Please make sure that you get your assignments submitted. So, so I'll. Let's do today. We were. Assignment one, if you choose to do that, are going to be. Mixed Tuesday and that's also the same day. We're going to assign assignment three. So I'd say. Around now is probably when things start to get. Pretty. Pretty intense as far as the workload for this class. So. It's about it as far as class announcements. So what we're going to do today is I'll finish up the. The classification with logistic regression lecture. And then I believe we are exactly on track for where we need to be. Is the 20. 23rd. Okay. Let me get through this and all. See, I think that. No, but 11 is like pretty, pretty short. So. See even get through that today. Cause we are now a little bit behind. Okay. Any questions. No. Okay. Any questions. Nope. All right. So just a refresher on. Last time. So we're doing classification, of course. And so we talked about that in terms of probabilities. What we want to do is we basically have a bunch of classes. And then for example, we want to predict the probability that it falls into each of your end classes. And so then what we have to do is we basically have. Maximize the likelihood of seeing the data that we actually have according to the class distribution that we've got. So, whatever the data, of course, in this case is going to, is going to include. The samples and whatever numerical features to find each of those samples and then. A label for each, for each class. And remember the label is now going to be an indicator variable or a one hot vector. Where it's all zeros except for a single one. Where in the index corresponding to the class that the sample belongs to. This can also be thought of as a probability. Right. So it is a 0% chance. The ground truth is basically. You know, sample sample and is K or is not K. For whatever class K. Another words for the correct class K, it is 100% likely to be that class and 0% likely to be anything else. So think of this as probabilities, everything is zero or one. I'm now trying to minimize the distance between my prediction and the ground truth, which is now just a probability distribution. Right. And being a probability distribution has to sum to one. So we have the data likely to go on a maximize now. So this is going to be we have some function G. It's basically going to be my prediction function for my sample. Parameterized by the weights K for every class K and then divide that by the sum of the prediction values for that for every class M. Right. So we define the gradient with respect of the log length with respect to to W. And we end up with update rules. For each individual weight W. This should look kind of superficially familiar to what you're what you already know from doing progression with neural networks. That is, I'm trying to update an individual weight. And so I need to take the previous value of that weight and then add something to it to move it in the right direction. That something is going to be some constant learning rate, let's say alpha. And then this is going to be the sum of all my errors for my for my samples. Right. So W J J corresponds to class. And we're going to better optimize the weights corresponding to that class. And you did effectively. How wrong is my prediction for every sample in my data set for every sample and so that what I'm going to do is I'm going to have some target. For every sample in my data set for every sample and so that what I'm going to do is I'm going to have some target value. This is going to be that indicator variable where all zero is the single one in the right place. For sample and for that class J. So basically what this means is that now for this sample and index J, there should be a one and there should be all zeros over. And so then what I want to do is I want to subtract from that my prediction. Right. So this should be this function G. I'm going to technically have a different function G for every class J. And this will tell me the likely class. I just need to do this for every class. And then of course I have the other term there being that input. So pretty standard. Optimization function at this point in that I have some learning rate I have some error term and I have some input. Right. And these are the three things plus the weight that I need to use to optimize the value of that way to the correct value. So, of course, we're doing this in going to be doing this in Python. And so we want this to be some some level of optimize for speed. So I'm going to take my update rule for W sub J or J is some class as given above. We can see and check out the well the expected shapes of these arrays are going to be right so remember we're dealing with these weighted sums of inputs. And so we're going to be adding this constant one to the front of each sample so what's the dimensionality of x sub n. So, we're going to be going to be D plus one where D is the number of of measurements or features for that sample plus one, which is that's my bias column. And then by one, because I'm only doing with a single sample right now. W sub J should also be D plus one by one. This is just just a single, a single class. And so then then T sub n J minus G sub J of x sub n is going to be some scalar. So again, this is my, this is my error measurement how wrong am I. And so I need that to be expressed in terms of a scale of value. So this all works. But you'll notice that the sum is over N. And then each term in the product is going to have n components. So that means we can rewrite these as matrices, and then do it all as a dot product to basically do all my computations cross the entire data set at once. So just to work this out, we'll kind of do something similar to what we did when we worked out the update rules for neural networks using matrices, so as we can remove the sum and basically just replace the subscript n for all samples with just an arbitrary placeholder which is used as star. So I can remove this sum right here. And so now for whichever value of n, I'm just going to be updating the associated value of T sub J for that sample. So consider that remember we have, we want to set up all our, our data as these big matrices are where my role rose with the samples. And then my columns would get are going to be the targets. So in this case, the target is going to be, you know, a n by K array for K classes. So what are the shapes of each piece here. So this is going to be T minus G. This is going to be n minus one so it's right n by one. So for every sample I should get a scalar value saying how wrong this prediction is. And so now X, right, likewise, I removed the end and I'm just using this kind of placeholder star. So this should be this is going to be basically my big X my big collection of all my inputs. And so this should be n for n samples by D plus one, where D is the number of features for each sample plus one for the bias. And so then W sub J as above is going to be D plus one by one. So this is going to work now if we transpose X and then just pre multiply it and then we just define G is a function that accepts X. So if you look at this, and then here below. They're basically just identical, having removed the song, not only all in you, he was transpose my, my big X to make sure that my shapes line. So now what we've got is we basically have an arbitrary update rule for all of my X for a single class J. So now we've been able to effectively get rid of the summation over n. And so now I want to get rid of my other subscript which is this J. In other words, you're going to try and make this expression work for all the W's. So now we've successfully got rid of N. Let's do the same thing to get rid of J will just replace J with our placeholder. And so likewise, every time I see a J here, I end I just replaced with the star. So now I have nicely I end up with sort of T sub star star, which, if you remember how we handle this in the intro to neural network lecture, this is basically just saying, I'm going to replace this with a big matrix of all of my T's. So here the star star means I can account for any row in any column. So now this is just going to be big T matrix of all my target values. So, if W star, which is basically down just W, this is going to be D plus one times K. So now think about the what we're trying to map to is going to be probabilities over K classes. Right, so this output should be of dimensionality K. What are we trying to map to those K classes, keep your size D plus one for the bias so this is going to be D plus one by K. T T sub star is just big T you remember so this is going to be N by K. How many samples do I have N, and it could be each of them could be one of K classes so and by K. And now G of X is going to be N by K minus one. So this should, this basically going to predict my, my samples, and then for each of them is going to predict you know some, some class. So actually what is it called on. This isn't correct. My mistake. No, times. Right. So, G of X is N by K. So now, T minus G of X should also be N by K of course we have you know, and by K minus and by K equals and by K just all that wise. And so T or sorry X transpose times T minus G is now going to be D plus one by K. And so now X transpose times T minus G plus one is D D plus one by K. And so now we have the update equation for all my W's being for any W I'm going to take the previous value of that weight, plus learning rate times the input times the error term. So far so good. So, A will be some constant or alpha will be some constant. T is my end by K target indicator variables, and then G of X is the prediction function over X over the values X. So this, what does, what does G, what does G look like basically. So we define 4K from one through big K. We said that this function will just take it to be the shape of e exponentiated to the W times X. So we do the exponentiation, because we basically need to end up with a subtraction, we need to be able to do T minus G. But previously, if you remember we worked in the world of logarithms. And so if I can't necessarily easily subtract, there's nothing that I can take the logarithm of a subtraction very easily right if I take the logarithm of a division right now I can do I can subtract the logarithms, but I actually want to be subtracting the scalar values itself. So to get around that I'm going to do is just going to exponentiate both sides. Now I can actually do a subtraction. So if G sub K of X is going to be my prediction function for that class K divided by the sum of the prediction outputs for all of my classes. Then I can change these to handle all the samples X. So now instead of for a particular class K, and for a particular sample and I'm just going to take F of X parameterized by W. So now I'm back in my familiar territory of I have some inputs X, but I'm trying to map to the right outputs. My job is now to optimize for the right weights that will let me do that. So I can just say that F of X parameterized by W is E raised to the X times W. And so now G of X is going to be F of X parameterized by W divided by basically be some over all the rows for X parameterized by W. So I basically have one, one class, and then I divide by this song. So given training data X, which as you recall is N by D plus one, and then class indicated variables T their end by K. We can then perform the following expressions. We perform the operations with the following code. So first what we need to do is we need to create this function to get indicated variables from the class labels because you might find that your class labels are not set up in these one hot vector representations. So you need to translate them into an appropriate indicator variable format. So it's the same as trying to go from this, you know, one to two, one, three to one, zero, zero, zero, one, zero, zero, one, zero, zero, zero, zero, zero, one, or how many classes you got. You might discord notifications beeping at me. I'm going to do that. Okay. So everybody is doing more that please. So we're going to do, we're going to find this make indicated variables function. This will take in the, the, the T column matrix that this is just going to be like a two dimensional matrix that's N samples by one. And then what I will do is I'm just going to pretty much just reshape this into into the into a two dimensional matrix. If it's not already, and then just take those individual values and map them to the appropriate index in the vector that's otherwise all zeros. As a demonstration, let's take the above sample right here. And then I will run it through the make indicated variables, and this gives me the same thing. Right. So these are now the above indices as indicated variables. So now for how many things am I trying to predict, it's going to be however many classes I've got and what I want to predict is now the probability that my sample falls into those classes. So I'm going to be dividing by the sum of all predictions I can ensure that I'm going to get a value that's a basically a percentage out of one. So I'll define G that does all of that here so now I have G of X of W. And so I'll have my, my F, which is going to be I'm going to exponentiate. And then I'm going to take the denominator and then I'll return my G's my G values, which is going to be that F's value divided by the denominator. This G function is also sometimes called the softmax function, as you probably have heard of so softmax function is this common final layer activation function. Now, as you see what it does is it takes your scalar values and maps them all into some probability distribution and sums to one. So, like the sigmoid, right, which converts things into a value normalized between zero and one. The softmax does that to accept it does it makes sure that that that that sum is normalized so that all values some two to one. So, as you can see the sigmoid is useful for binary classification right if everything is going to be between zero and one. Then if I have a class of interest, all I need to do is sigmoid some value and it's going to tell me the probability between zero and one that it falls into that class of interest. So this is useful for two class problems because soft or because sigmoid. So, I have to do everything between zero and one which means that to get the probability of the other class I just do one minus the sigmoid. Softmax of course I can't use the sigmoid function for multi class problems, because if I have a class of interest, all that's going to do is tell me what's the probability that this is not a member of that class, which is a binary classification problem. If I actually want to know which class it falls into I have to be able to normalize all the probability so that such that they some to one turns out maybe we'll review this after spring break. So, is a generalization of the sigmoid function for multi class problems and it's a fairly simple derivation to show that that is not I believe in this notebook but we can get to that. So softmax function is basically as you may have seen, you know, exponentiate x times w said as you take your final output aside, you know, prior to the softmax exponentiate it then divide by some of all exponentiations. So now the updates to w can be formed with code such as this so make way indicator variables. I will define the weights in the appropriate shape, define some learning rate, and then for every step in my specified number of epochs. I will then take the softmax of x and w right in this function for multiplying x and w. And so then we can use my update rule to better optimize the weights. So this in a nutshell, for a very simple linear classification problem is how I can use the softmax function and so I'm doing neural networks what I'll do is I'll just have the appropriate insertion of hidden layers to handle non minority and then do backprop in a very similar way. Question so far. I'm curious why is everybody gravitated this side of the room. It's even when people are here that side of the room is much sparser for some reason. If you all come in through that door and it's just closer. Anyway, okay. We have it we have an unbalanced distribution into this class right if I just want to predict like where someone's sitting in the class is almost always going to be on this side because it's very empty over there. So here's code for applying linear logistic regression to the Parkinson's data so we're still in the world of linear linear equations here, because I don't have my hidden layers to allow me to insert non linearities. So I'm so I can still use the softmax function of course to squish everything to be appropriate problem distribution, but what it does is just taking a matrix multiplication of x and w, which is still a kind of standard linear linear equation that we have been doing since the beginning of class. So we'll do the Parkinson's data set again. So let me read in the data. So this should look familiar 195 samples of 24 features each. So then I will do the same type of data cleaning that I did before. So of course I don't want to include the status, which is my output label in my inputs. And now you're going to include the names that's not helpful feature. So I dropped those. And then I slice off the status column this becomes my targets. So now I can see that I've got 22 input features in x. These are the names of all those features. I have a single output that status. Standardization function, as you've seen before. Right. So I'm just going to compute some means and standard deviations and standardize the inputs. Now also going to import the QDA LDA implementations of using the previous lecture. This will allow me to compare the performance of logistic regression to the two previous methods that we use, namely QDA and LDA. Okay. So now to generate our training data and our validation and testing partitions. What we're going to do is we're going to partition data into folds on a class by class basis. So this is kind of similar to what we did earlier in that we're going to define like how many folds I want. And then I'm going to use one of them to be testing data, and then one of them to be validation and then the rest to be training data. The only wrinkle here is that I need to make sure that I have the approximate the same portion of samples from each class. So this function does this is called generate generate stratified partitions. So what this will do is it's going to generate my sets for training validate and test for both my inputs and my targets. Or I evolved if I don't want to validation that I can set validation to false. And it's going to give me a dictionary that's key by the class label so they want to retrieve say the training and the validation sets for class zero. So what we do here is I'm just going to basically shuffle all of my rows. And then I'm going to pull out the appropriate set for each fold. And then I'm going to partition my folds, or I'm going to, I'm having partition my date in my folds. I'm going to segment off which collection of full and I want to use for training validation and test. Basically, this just does all that. So in the this part here, this is just going to make sure that I'm roughly balanced across all of my classes. And then this part will generate the test and optional validation fold and then return the rest as as trained. So if you run this, what we'll do here is I will print these reach of my folds. And then we'll see that the first number, this is like my number of training samples and then my number of validation samples and the number of testing samples. And then this will be the. The number here is the partition or the percent of the partition that is class zero. So if you remember that we have the healthy samples and the Parkinson's samples and it's not a balanced data set. But we want to see the, we want to see roughly the same proportion of each class across all of my folds. So we can do this in this trick using NP dot mean. Right, so this is going to be computing the arithmetic mean across the specified axis. And what this does to train equals zero is going to turn everything to either true or false. And so if this is true, it's going to be a one of its false. It's going to be a zero and NP dot mean will handle that automatically. So these are the individual samples. So then is that NP dot mean of a equals equals zero is going to be the portion of samples in a whose values are equal to zero and just replace this constant here with whichever whatever thing you're interested in. So now what we can do is you can write a function. That's going to iterate all the possible ways of making train validation and test sets from and partitions that I specify. And that'll also train my QDA LDA and now my logistic regression models over this data. Okay, so what I'm going to do is I'm going to save time just using the break statement to stop execution after just one run of cross validation. And then to use all runs, what I have to do is I'm going to have to calculate the mean errors or accuracies over all cross validation runs. This is what you're going to be doing in a three. So does everybody know the term cross validation who is not familiar with this term. Okay, some somebody that's fine. So basically cross validation. I have a data set and I maybe it's small, or maybe I just want to make sure that I'm not just getting a lucky split. What I'm going to do is I'm going to basically partition it into these folds and I'm going to hold one out as a thing to be tested on. Now remember, maybe that particular split is lucky for some reason. We talked about how you can just sort of depending on your random seed or other factors. You can get a particular testing split that just happens to perform well right if you hold that testing data out. The remaining training data is just very indicative of that testing data. There's nothing really in the testing data that is maybe unusual from the perspective of the training data. So it's in that case it's quite likely that you would get a very good test performance. Right. But we don't know that you just segmented out like the 20% of the data that had that property. So if you stop there, you could report a really high test result but then someone else who runs your code partitions a different split, or they have a different random see they get different rows and that split and all of a sudden you're showing like 96% test accuracy and they get 68 or something. And they're like, well, what's going on? This is way lower. So what you want to do or one thing that you can do is to take that test split and rotated different one each time and then average all those results. So for example, if you had some test split that was like, okay, you got 96% on this one, and then 76% on the next one. And then like 86% across the remaining three just to make my mental math easier where you end up with a average test accuracy of 86%. And so that's much more realistic than either the high end the 96 or the low end the 76. And so that in that way you'll get a more accurate picture of the actual performance of your your model trained over these different splits in the testing in the training data. Clear cross violation can also be useful technique when you have a small data set. So, for example, it may take. Let's take this sample. We have 160 195 samples right this is not huge. It'll do fine for these linear methods. But let's say I have a more complicated problem that requires your network. Often it requires more samples to actually converge to an appropriate model. And so you need. Some, you know, you need like 90% of those training samples to get your model to convert you hold up 10%, which in a sample that has 195 samples to begin with. You don't need you know, like 19 test samples right and the results may not be all that indicative right again you have the sparsity problem. So you could end up with a with the lucky split problem. And so one way you can kind of ameliorate that is to cycle through these different testing splits. So a number of reasons why you might want to use cross validation. We will do that here. And so you're going to have to implement this in a three using similar methods. So we'll have this, this version of the run park function. I'm just going to call it, we call it run park log reg for the digestive regression. We'll also include the outputs of QDA and LDA. So this will output the prediction accuracy using all three of those. So I'm going to run my generate stratified partitions. I specify how many folds I want on this case. I'm not going to use validation. So I'm just going to generate train and test. And I now I compute my means and my standard deviations. I standardize my x trains and my x test using those means standard deviations. And then I will do all my pre processing up front. I'll attend my, my column of ones. Now the new stuff for linear logistic regression is now I have this make indicator of our function. Right. So this T train and T test now gets transformed into T train I and T test I. So now I should have some one hot vector representation or indicator variable representation of all of my, all my target samples. And now the rest is as we see. Right. So here to this point, ignore this likelihood list for the moment. We just initialize our weights, specify some learning rate specify some training time. This is my forward pass because it's linear. I'm just running through the softmax. So you can imagine this being just a, a neural network with no hidden layers and a softmax on the output would be the same thing. And I do my backwards pass my weight update as we shut as we saw. What I'll do is now here I will convert the log likelihood likelihood. So if you remember, we were calculating the log likelihood of the data to make the mathematics easier. We need to convert that back to the actual likelihood because that's what we're trying to maximize. And so then then that will, what I will do is I will append this likelihood per sample to this to this list so I can plot it. So what I can see is after I plot I can actually see the likelihood of the data as training as training proceeds. And then I will print the percent correct using the logistic regression method. And I'll do the same thing using the QDA and LDA code that we did before. And the rest is plotting and then I will define this percent correct to turn my results into a coherent accuracy. Okay. So this is the percent correct for each one. So the logistic regression is 89.2 train 78.9 test. QDA and LDA have really high numbers. We're actually wait a second. This is the same because. Right. Oh, my God. This is the issue. So sorry, sorry about that. Okay. So here we go. Run this again. Okay. I just more like it. So we see logistic regression and LDA have. Kind of similar performances, although we'll actually see the literature progression as a higher test accuracy this time. QDA we see the same thing that we saw last time or received this really high training training accuracy and the test accuracy is not as impressive. Suggesting that QDA again, maybe, overfitting to this data. These are the weights and you'll see the basically the weights of the second column or just the negative of the weights in the first column, because this is a binary classification problem. So if it's not one, it's the other. We're using the softmax is that's generalization. We could recast this problem and one that uses the sigmoid because again, one minus sigmoid of of of X will give me the probability that the signal of X is that X is not in the class of interest. Now, we look at a single sample. These are the individual values. These are standardized. So now the samples times the weights right this is going to be that first sample times w. This gives me some scalar value right 1.23 negative 1.23. Okay, we can already see, you know, what what the outputs going to be. We already know like which classes it's going to be in, but just to run everything to completion, what I'm going to ex I'm going to exponentiate this right now this is this is basically e raised to these values. So, I'm going to get a negative right 3.45 and 0.3. I sum the exponentials is now 3.7. So now I normalize these. 0.92 and 0.8. So now these sum to 1, I just do do a Sandy check here. Right. This does some to 1 and the argmax is going to be the first element or class zero. So this isn't this is a sample belongs to class zero. So to this point you could probably tell what the answer is going to be just looking at the scalar value, because this is a binary classification problem. It's going to be one or the other. If it were a multi class classification problem, it would not be this is so easy to see that. And also the softmax has the nice problem, the nice properties of making everything consistent with your error term relative to a probability value that you do need. You can sort of see what the answer is by the time we get here, but you actually need to softmax in order to get a meaningful error term that I can use to do things like wait updates or back propagation in neural networks. So, finally our charts here. This is the, the likelihood of the data, according to the just a progression, we can see that we end up with just like from point five, you go up to play about point eight, go to that. And now here this plot you can see the outputs for each individual. Each individual classification, what we will find is in most cases LDA and the district regression are performing the same there. Probably only two samples where LDA says one thing in the district regression says the other. QDA. Again, we have, you know, some overfitting in the in the training data so it's not performing quite as well. So we can run this again. More time. So we can run this a few more times you see similar trends, and generally what we find here is that at least for this data, we just took regression can usually achieve a higher test accuracy of the ODA, even though maybe the training accuracy is the same or slightly lower. This may be a problem where the district is more suited. So run this a few more times, and you can see similar trends as we have seen before. Okay, so now the code above. This is using SGD in the gradient of the log likelihood. So, do we have a better way of doing gradient descent. Remember gradient space, ascent, not decent yet. So we can try Adam, right, we've demonstrated that Adam often converges faster with, you know, with fewer training epochs. But first you need to find your error function to be minimized and is gradient function. So previously we were trying to ascend the gradient, but the function to be optimized. We actually want to try and minimize the gradient so you can use a general solution using any kind of gradient to send to algorithm. So, in this case, what we're going to do is we're going to create this function as the negative of the log likelihood. So this allows us function, and then the gradient function also needs to include the negative. So, here's the definition of log likelihood above and it's gradient below. Also written as matrices. So, I'll just start by stopping. Okay, I'm going to stop the girl. Okay, didn't break. Sorry about that. I think I stopped for accidentally. So I'll define this second version of the function that will be using Adam, and import the optimizers class that I have before us, not can specify which optimizer I want. So, here, additionally is below the softmax function is I now define the negative log likelihood function, and then it's gradient. So now I can use these to get the output of my gradient descent operation at each step. And so now this is the thing that I'm trying to minimize. And then what you're, what you may want to do is like convert this back into the likelihood of the data. So here what I'll do is I will take this to likelihood. Right, so I have the negative log likelihood. What I'm going to do is it's going to take the negative of NLL or the negative log likelihood exponentiate it. This gives me back into regular likelihood. So now I can define this likelihood trace that is going to be instantiated instance of Adam that takes the negative log likelihood as an input and it's gradient, and the training number of training epochs learning rate. So I'm just to find this error convert F function to likelihood and so this error convert F is an argument is a member function of the atom optimizer. And so it's just going to call this here that I've defined. So it's automatically even convert the negative log likelihood back to the just plain likelihood. Then the rest is the same right we just do just a progression QDA and LDA. Print this. So here this is written error is a little bit misleading just because of the way that the neural network class is written. This class is written to originally based off of the regression problems and trying to minimize error. Here, all I've done is I'm now printing the likelihood of the data instead. So just to know when you see error here, you'll notice that it actually increases sometimes. Because we're trying to maximize the likelihood of the data, which is what's being printed out here. Okay. So maybe a more. We could rewrite this, you know, actually print the negative log likelihood and see it to climb. But then it would sort of go opposite what the chart actually shows. So here we see the error or likelihood climb to some threshold is about 0.89. And then we also see the, the training and test accuracy. And you can see here that using Adam at least in this case, train accuracy is only 88.9 or test accuracy is actually 96, which is significantly better than the test accuracy for either QDA or LDA. So run this again. And then we see, you know, similar different split here but we're still we're still getting basically better test accuracy than the train accuracy using the regression with Adam. Just throw that out there to some of you. What would you do to change this to run SGD instead of Adam for the linear logistic regression. Just think about how the code is set up or we'll go back to where we instantiate that highlighted a little bit. So what would you do to use SGD instead. And the answer was probably a lot simpler than you think. Well, I'm so I always use soft max for all classification problems. Right now I'm just I'm asking this is a lead into the next lecture just just to sort of telegraph where I'm going a little bit. I need to change a word, a single word here. What do you think of the back. You agree. Yeah. Okay. So basically, the, we have this optimizer class that that contains implementations of Adam and SGD. So to do that, all we need to do is change SGD to Adam and then change the change the type signature of the function and whichever way it demands. So, which brings me to the next lecture code reuse through inheritance. Before I do that. Any questions on. We just took a question. All right. So, okay. 11 code reuse by class inheritance. This will be pretty useful to you. And I think this is relatively short. So, like, we'll let you go early. So what you have done saying a too so far. What we had you do, for example, is you complete a neural network implementation. And then we say here's a cell copy your whole neural network implementation into that cell and make the following changes right and some of you doing that or probably thinking, this seems like a very inefficient way of doing things. Why am I doing this? Well, it is not a great way of doing things. Obviously, instead, what we can do is you want to make relatively small modifications to the existing classes and functions. So, what you can do is you can add arguments just like for the new behaviors like what type of activation function you want to use and what type of optimizer you want to use. Or you can use class inheritance to extend the original class so I assume that all of you are familiar with doing class inheritance in some way, right, maybe in Python, maybe not, but presumably you've least taken Java here. And so you've done class inheritance. So what we do just an example here that uses the neural network class. So, to allow say the 10 h or the Rayleigh activation functions. You know, you had used this keyword to specify 10 h or Rayleigh. And then you in that copy of your neural network class, you have to add the additional Rayleigh behaviors and the gradients and everything like that. So instead, let's let's use class inheritance to do the same thing. So, I'm just going to write out the optimizers file. So this is right here. And then I will put it in its own Python script. So now to use this class need to import it like we've done. And here is a. Something about to give away the answer to. Maybe I should just end class here. You want to get like 30 minutes back. I'm going to stop here. No. Feel like I shouldn't let this like. Yeah. What's that. I, we're right where we need to be. And I think the due date slipped every, every year, a little bit. I have to calibrate them appropriately. So. I'm going to call it a day here actually. So I don't give away the answer. Okay. So you all get, get, get 30 minutes back. So I will, I will see you next week. Bye.