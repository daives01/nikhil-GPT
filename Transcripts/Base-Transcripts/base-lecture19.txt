 Okay. Can someone confirm you can see and hear me okay? I'm like, you okay, you can't. Okay, great. Yeah, sorry. The, um, ironically I've been. San Francisco and Wi-Fi here is like terrible. Um, so hopefully this will hold out. It seems to be okay up here in my room. Um, ready. Give me a few minutes to let everybody in. We'll get started. Okay. Yeah, good enough. I'll get started. So yeah, thank you everybody for for logging in. This should be pretty short. I was just doing the next lecture and it is pretty brief and one after that is relatively long. So what I'll do is I will just do. Let your 18 today and then just take Thursday off and then tomorrow or next week we'll resume and we'll be back on the expected schedule. Should the Wi-Fi crash for some reason and you lose me. What I will do is I will just record this and I'll post it of course as normal. So I guess only just a couple of announcements today. So we are working on assignment three grading and we will get that back to you by the end of the week or so. And I know some of you had some questions on assignment for I will do my best to respond to those in a timely fashion. I'm traveling tomorrow. I'm going from San Francisco to the east. I'll have like I mentioned before, like they lay over in Denver. So maybe I can spend some time responding to your messages. So one announcement that I want to just send out to to all the CS undergraduate through here as you might be. We are in the process of recruiting a new chair of the department and our first candidate. Bruce Draper is going to be doing his campus interview tomorrow and Thursday. And as part of the chair search we are. We want to basically get feedback from every constituent group on campus. So our in the departments that includes of course faculty and staff and then people at the college university level and graduate students but also undergraduates. So if you are a computer science undergraduate, I know there are a number of people from other departments and forgive me for just sending this out to the CS folks. If you are computer science undergraduate and you have some time around lunchtime on Thursday, you will have an opportunity to meet with Bruce. And you want to get free food. I think you also get a chance to talk to him about his vision for the department how he sees it evolving in the next five years during an ostensible term as chair. And maybe you're any of your concerns or known or get any questions answered. So, at the very least this is an opportunity for you to get some free food. This would be in a room 305 of the computer science building. So if you're interested in that, you have an opportunity to talk to our incoming chair candidate, and we'll have one more after that probably early next week, where the same opportunity would be extended to you. So if you're interested in that, I would encourage you to take part in that we would like to see I would say, maybe a dozen or so folks at least attend this lunch. If you have any more questions about that feel free to reach out to me I can give you some more information if you're if you want. So that's all I had to to announce right now. Any anybody got any questions about anything. Okay. Let me. Screen share up. Okay. All right. Let me grab my water. Okay. All right, so this will just take as long as it takes and after that I'm going to let you go. Okay. So basically what we're going to talk about today is going to be reinforcement learning in continuous spaces and this is going to be just a slight evolution of the quote 1D maze example that we use last time. So, remember, 1D maze, I suppose it's just a fancy way of saying a number line. So we're just we have some determined goal position on the number line and our goal is to try and move our agent or our, our objects, whatever it is. Into that position. So that is if my goal position is 5 on the number line from 1 to 10. Then what I want is if I'm to the left of index 5, and then I'm going to move to the right, and to the right of it, then I'm going to move to the left, and if I am exactly at 5 I want to stay still. So the way we model this is basically we have a discrete action space, which is negative 1 0 1. And then we will take these steps in goes given directions. And I have basically a cost associated with taking a move. So that is, I get a reward of negative 1 for taking a move unless that move lands be in my goal state, in which case I get a reward of 0. In this case we have a cost function that we are basically trying to to maximize the reward by virtue of taking the fewest steps possible or the steps that are most likely to land me in my goal because I get a non negative reward for that. So that's kind of our our deterministic version, or discrete version. So I think about how we can model this in a continuous space. So that is, let's, I'm going to say we have a marble. So now I have basically a flat, you know, flat plane. And I can move horizontally along this. And I'm going to do that not by say picking up and moving my marble but not going to do it by exerting forces on that marble. So we can think about how this is actually going to work in say a real physical space and you can imagine that if I have a rolling object which is saying to the strain the motion from into a single dimension. Then my the state actually the modeled using the position of the object and its velocity. And then my actions are going to be forces that they exert upon that marble so I can actually push it to the left or I can push it to the right or I can do nothing. And then imagine this is going to be a slightly different scenario like, which I brought a marble with me of course I didn't think about that. But if you imagine if the marble is already moving in a direction. Right. If I, if I may take no action is going to continue moving in that direction will soon basically I'm not going to have things like friction. So I'm going to say it's a friction in the space. And I'm going to model this as a as an environment that has friction if you had a more realistic simulator. In this case we're just going to be doing it kind of text wise and Python. But let's imagine that we have a marble and I can push it in one direction or another or I can do nothing. So of course if I'm already moving in a direction. And I do nothing it's going to continue moving in that direction. Because if it's got a high velocity say moving toward the right. If I push it toward the left if I exert a force into the left it might not slow it down. It may not move it start start moving toward the left. It will slow it down. And it may in fact slow it down enough that it will, it will basically stop the motion. So my goal now is that if I have a goal location. I'm going to have a slightly different policy. Right. So you imagine now that instead of taking these discrete steps left to right. I'm exerting forces. And those forces are going to have effects on the continuous motion of this object. And so the policy is going to be quite different. So unfortunately I can't be seeing any of your reactions. I hope that was a clear explanation. If it's not what I recommend you do in this remote scenario is basically just like raise your hand or put a question in the chat or even just like speak up and interrupt. So honestly speaking of an interrupting is probably the best way to get my attention in this in this scenario. So our goal is now to get the marble in a particular position on the track. And so we're going to be doing this using a similar method. Basically I'm just going to be using my neural network as a regressor. And we'll see how this is different from the discrete case of the exam last time. So import my neural network with regression so I do my standard imports with NumPy and pandas and pie plot and then also I import my optimizers and my neural network class. So now we're going to define our reinforcement function, digital state function and next state function. But they're going to be defined to basically model this dynamic marble problem. So if you compare the functions as you specify them here to the ones in the notebook from last time you'll be able to see the differences. So what are our variables. So we'll have first x sub t must be position on what's called the unit meters for now. It's going to be whatever it's a big marble right it's going to be moving in terms of meters, but this might as well be centimeters. And then x prime is going to be the velocity in meters per second. Okay. So if you look at what is my position at time step t plus K that is x sub t plus K. This should be my current position right at time step team, plus the integral over time current t to t plus K of the velocity. Right, so that makes sense if I'm moving in a constant velocity over N seconds I need to calculate how far I moved it over N seconds, and then add that to my current position that's just going to be the interval of those values. So, you know, we all should be familiar with with how this is done. You know we've probably all done. I hope you've all done. Intervals and your calculus classes. And so we know that velocity is the derivative of position acceleration is the derivative of velocity, and what's the derivative acceleration it's actually we call it jerk. And then beyond that I don't think they have turned for that. So we're trying to now calculate the integral the anti derivative of velocity with respect to time. So, of course, this is what this year. So I have a good meeting calculating with respect to t. I'm going to be taking the integral of x private sub t. So, that is the case position from now is going to be the current position plus the total of all velocities over K time steps. Of course, this is a continuous, continuous calculation. So for any time step t calculating change of positions is basically going to be the change in t delta t is going to be the current position, plus the change in t however many time steps have elapsed, plus times the velocity for those time steps. So, I'm going to be calculating velocity is going to be the same. I'm just going to be adding the acceleration right so external times getting the acceleration at time t and this, of course, would be in meters per second square or whatever unit of distances per second square. Now, this is a continuous problem is mentioned, but of course we have this issue that I'm sure you have encountered before whenever you try to calculate. So, in terms of class we have computers being discrete machines. So, we know that the exact integral curve is unknown, but we know that it's starting point is a particular value. So, if we have my starting point of integral curve as a sub zero here, we can use the order integration method to approximate it so that is I take a small time step, some discrete time step, and then I approximate the value of this curve over those time steps right, we can see that because the size of this time step is large enough that my approximation was to get to that time step four is somewhat higher than the actual value of the integral. And so this is going to be if you remember, when if you study calculus like I did. One of the first things you probably did was break the curve in the area of the curve into basically boxes right discrete time step so you might draw something like, like a terrible drawing would say I have a time step, you know, something like that and then the next time step I might approximate it like this much, and then so on and so on. And then basically the integral is taken to be the sum of all these and then of course, if I have some time step, you know, T, if I decrease his value of T I'm going to get a better and better approximation I subdivide this suddenly I have these these small boxes that are basically giving me a finer grained approximation of the area under the curve right. So that's the other integration method, and then of course we can show that as the limit of this that that's a division approaches you know infinity, then this is going to be an accurate approximation of the actual area of the curve. So whatever delta T is where we define that as it's going to be the order time step so here, you might be fairly large value so maybe over a second this isn't in imprecise approximation but over say 10s of a second it's going to be better. So how do we model this in actual code. So first let's define our functions. So our valid actions right remember these are forces now so I'm going to be exerted a force of negative one or one or zero on my moving object. So now my, my functions have to be defined some differently so for the reinforcement one right I take an S and S and of course this is my current state on my next state. So I think that is, is state five. But of course I know that the likelihood of landing at exactly state five is pretty small. So I want to basically optimize my policy to approach state five as close as possible and I want that the reward for that to be zero when I kind of get within the approximate integral of that value. So what I'll do here is I'll define this as basically if my, if my next state is within index of one within one unit of my goal, then I will get that zero reward otherwise it's going to be negative negative one for every action that I take. Okay. Initial state is very similar, except I have, I'm going to be sampling continuous value instead of just some value between like zero and 10. So I will take, you know, don't add some random noise to my to my starting position. So the next state is kind of the continuous analog of the previous version that we had so element zero is the position. Element one is the velocity so now this is broken up my state now into two distinct values. So not not just the position but now this is. And then my action is going to be one either negative one zero or one. So the different here now is I'm going to define my order integration time steps so here I'm going to say it's a 10th of a second. And so now given my velocity that is s one, I'll be able to add that value. So this is per second so we're going to multiply that by how many seconds you're in my time steps in this case point one. I'm going to add that to my position. So force is the action that I take. So I'm going to add the mass of the object so here I'll just define the mass to be a particular number. And then we can here we have say a little bit of friction here that we can start. I was sorry, I guess I was incorrect and saying that we're going to model this completely friction this environment we're going to have a small amount of friction. And so then given this I'm going to take that delta and then multiply it by the force divided by the mass minus point zero five for to account for some friction. So this will be my new velocity. So now I'm going to have some bounds on it of course so I don't want to roll off the edge. So if I hit the if I hit the boundary that I'm going to set the velocity to zero I'm going to stop moving. Okay. So my initial state function above that's going to sample initial state. So it'll be a two element array, and the first element is a position to the marble. So the first element is the starting velocity so starting velocity is always going to start at zero. So if I end up I started like at seven point something. Then it's going to be sitting there, and I want to figure out what action I have to take, and that might be exerting a force to the left or to the right. That's going to move me to a different place on my on my line. So I sample my initial state, it gives me six point one three, so we can say okay my goal is five I started six point one three and kind of to the to the right of this. So given this I'm going to randomly choose from our three valid actions. So here it says okay I'm going to take an action of one that is a zero force to the right. So I'm going to do this, you know, a number of times. Sample these actions and then calculate the reinforcements accordingly so here I'm going to do this for like 1000 time steps. What's going on here, well I'm going to make a random choice from my valid actions. I'm going to use that action to calculate what the next step. State is I'm going to use the value of those that state to calculate my reinforcements. And then I'm going to append that to a list and I'll plot it. So. So I'm going to run this and we can see what happens so after 1000 time steps I started at in this case it's starting me at 1.1 and it is getting to 5.56 and try it again. So here, we start at 5.8 and we end up at 6.6. So try again. I ended up just hitting the wall at 10. So briefly, let's try and gloss this graph a little bit hard to read, but we can see this is actually like a bad. Let me try one more. So here, okay, so we start at 8.8 and we can see that the position kind of to move toward the left and then it goes back toward the right, then it takes kind of it's just toward the left pretty consistently. And then it starts to go back toward the right and eventually it ends up pretty much not far from where it started. The other thing we can see here is that where we look at the reward here is the red graph. You can see that it's usually negative one, except for when my position is kind of within one unit of being at five. In that case, the reward is zero. So we can see the modeling of the of the position and the velocity is sort of hard to read if the orange line there, but you can see how, you know, when it's exceeding to, for example, moving very, very swiftly toward the right and so on. So you might have noticed something kind of weird about this and we'll get to that in a moment. Let me plot my last few values. So if I look at the last 10 positions and the last 10 velocities. Sorry, I read that again. So in this sample. Now this one starts at 3.5 and ends at 4.5. So look at the last 10 positions, right, 4.46 kind of moving toward 4.5 last 10 velocities is, you know, 0.0. 0.02. 1.5, 1.02 again. And then the last actions you can see here to go from time step. Let's say. T minus 8 to T minus 7. You can go in and say maybe T minus 7 to T minus 6, for example, we can see that it's got a. No, sorry, I'm reading this backwards. My mistake. Sorry, I'm pretty tired. So this compares 0 and 1. So here we have an action that is 0. And then the, it was moving in a given direction. And so then the velocity is like 0.02. We have take this action of 1 and the speed increases. And take an action of negative 1 speed decreases, take an action of 0. The speed decreases just a little bit due to friction. Take an action of 1 again, it starts to increase. So did the marble ever get to the goal position? Well, not really. We see some places where it approaches the goal position and that those are where we see this reward is being 0, but it never stays there and it doesn't ever seem to learn anything about how it's supposed to approach that position and stay there. So what went wrong here. Well, it's a couple of things. One is basically we're just replacing the state with a state plus action. And then just calculating the reinforcement actually just based on like two instances of the same state. Right. And so what we need is we actually need to have some way of approximating what the best action is according to the state action pair inputs. So that is we need to be using our network as the cute function. So we need to define our epsilon re function. Remember what the epsilon re function is. This is going to be there is some probability that I'm going to take a random action. Why do I take that random action? Because I might find some sort of strategy that does actually get me to the goal that I want, but it does it in a very suboptimal way. And so once I discover that strategy, I do not want to keep repeating that strategy in case it's very circuitous or very cumbersome or something. I have to allow for the possibility that there is in fact a possibly better option there. And I might be able to find it by instead of exploiting my, my best current strategy, I'm going to just take a random action. So the epsilon greedy function is going to specify some value of epsilon. And so if I sample a random number that is less than that value, I'm going to take a random action. Other than that, I'm going to take the greedy move that is going to exploit my current best, my best strategy, according to my, my q function. So remember the, the neural network is approximating as basically a function approximator that's allowing us to learn what the current best move is according to the state action pairs that I have sampled from the environment previously. So this is the epsilon greedy function that's basically the same as it was before. So I have the difference here is that the state instead of being a single value, it actually be two values. Right. So now it's going to be positioned in velocity instead of just position. But the function is written the same way and all I need to do is just pass in that to element tuple or list or whatever into my state. So now I have my, my q net that will allow me to just like use that, that function. So now I need to be able to train that function of course so here we can see just the epsilon really part is not actually training the function. This is just using it was starting to be optimized. So what I'm going to do is I'm going to generate some samples that represent my experience from conducting various trials in this environment. So I've defined my environment is now having a model of a position and a velocity representing my state and then a set of actions that I can take that are going to be discrete actions, but they're going to have kind of continuous response within this space. So in terms of, I'm not going to be picking up on moving my object and be exerting a force on it which means it's velocity will speed up or slow down in whichever direction. So make samples is written generally to accommodate for whatever whatever my representation of my state is so here I define these function kind of placeholders. So I'm going to do my initial state next state reinforcement functions are. So, here, you know, I just call these functions so next state F is going to take in my state and my action, give me my my next state, our end is my next reinforcement. So that this give me the resulting reinforcement from the previous state and the next state. And then epsilon greedy will just use the q net to choose the next action. Let's try this. Right. So if I create an s. This is going to be the leftover s from my my sample kind of randomized run here. So you can see that I've got 4.57. That's my position 0.39. This is my my velocity. So basically, at the end of this, that last kind of randomized 1000 trials. So this is a variable that's at position 4.57 and it's moving relatively slowly toward the right. So this, obviously, would eventually get to the goal, but it would probably would keep going and overshooting it. So this s state consists of position or velocity. So now this is a two element array as shown. And so this tuple s a is actually going to be three elements. So therefore, we're going to modify this make samples function, just to allow our state variables to contain multiple values. So here's that. So what I will do is I'm going to update the state, sn from sn a. And so then here, the rest of it is pretty much the same. So this is a very small, a small adjustment. So compare this version here. There is no next state. There's no call to the next state f function here. So what this does is this is going to take my next state. I'm going to assume that my starting action is always zero. And so my next day when I take no action is going to be the same as the current state. Okay, so now I'm ready to train. So now before we do this, we're going to make this plot status function to show how we're doing. And we can look at the various different things that we're going to plot. So now I'm going to show a bunch of different graphs, and we can go through all of them. So what I want to plot is, you know, one, the probability of taking a random action. So that is my, my exploration over the number of trials. So if I'm doing a long decay, then I would expect that as my model gets better and trains more than I'm going to have a lower probability of taking a random action because I'm fairly confident that my, my best, my best strategy so far is actually going to be close to optimal given the amount of data and training that I have. Second thing, starting position of every sample right remember we're creating these samples. And each time I create new batch of samples it's going to re-initialize my environment and start my marble in a new position sorry we want to see how my training is given that one time I might start my, my episode at, you know, 2.6 and then the next time I might start it at 7.3 and then one time I might start at 10 next time I'm going to start it like a 4.9. And so as my, as I train, I should have a better idea of how to get to the goal from my starting position. So here I'm going to mark in this graph, I'm going to mark the goal position at five so you can see how things kind of evolve. Next thing I'm going to plot is the latest policy shown is the action for each state so we should have some graphs showing that given my state, this would be the best action right and this should be some sort of continuous manifold at this point. So I'm going to make a difference between the mean reinforcement versus the trial so we actually smooth this and we'll show the mean reinforcement every 20 trials. And then five is going to be velocity versus position right so this should be, you know, well trained policy well to lean intuitive so if I'm close to five, I should be moving kind of slowly in the right direction so they get as close to five as possible and don't overshoot it. So I'm going to check this fill between function is kind of cool you can we'll see what that what that does in a in a moment. And then plots. I guess this should be six through 10 on the Y number seven through 11. This would be the max Q value versus the state, and then versus the and actions actions versus the position of velocity. So we can see kind of how this looks and what what to inferences we can make from the two D versus the three plot. And then the rest is drawn. So let's write a function to test it will pop the marble down at a number of positions and we'll try to control it and we'll plot the results. So for N trials, it's running positions, I'm going to run a bunch of simulations for a certain number of trials and then plot that. So now we set up the standardization. This is pretty much the same as before right we have our, our means for my inputs and my targets. I'm going to use that to standardize my, my values when I'm using them to train the Q net. And so then I'll plot, you know, my, my standardization. So then here, but this, what this is basically my, my training and my plotting. So I'm going to have this is gammas my discount factor that is how much do I discount projected reinforcements like far in the future. Not very confident of those. So maybe don't want them to have a huge bearing on the actual training of my model, because, you know, they're, they're, they're so far in the future they probably don't matter all that much. And then, and then a number of repetitions of the queue update loop. And then the number of steps between new random initial states. That's a learning rate specify my final epsilon so I can calculate my decay rate. And then I create the Q net I'm going to use just use a fairly simple one that has 10 hidden units as a single layer. So here's my Q net structure. So three inputs. So for the position of the velocity of the action. And one output. So this should be the best predicted action. Where they started the best predicted the best predicted reward. So then I run set up standardization, and I'm going to specify some initial epsilon value. This is one, because when I start I don't have any best strategy. Right, so I'm saying this to one, I'm just going to say start by taking a random action and then I'll burn some that and then they can decay my absolute value. And I collect all my samples, run train and the rest is for plotting. So let's run this real quick. And there we go. So, we can see here, for example, this is the random action probability starts at one. And you can see it's sort of rapidly decaying towards zero because this should be learning something. So exactly is it learning. Let's take a look at the mean reinforcement. So starts out. Like kind of point eight five. I'm not sure what the initial, like the very first initial state that selected was might have been somewhat close to two five, but it pretty quickly drops towards negative one. And then so sort of just a bums along here for about 300 time steps. And then it starts to actually learn something right so we can see here that it's at this point, right about 300 time steps. It's sort of start to figure out what the best policy was. And so now here, if you look at the maximum of the, the Q values here and into the end 3D. We can see that, you know, for example, what's the, the key value when the position is very close to 10 and then the velocity is negative, right is negative point three. And then kind of the converse is true when it's when it's like close to five and the velocity of zero. The actions, this is pretty interesting so you can see that when my position is five. There's sort of this sharp boundary. Given the, the actions that I have available to me. So, at the state trajectories for epsilon equals zero. So, this is my desired position. This is five. And so we can see that if I have say a position of five the state trajectories for the loss of 10 towards zero. Whereas if I have a lot of positions that are say closer to 10, then my state trajectories, you know, 10 toward the negative numbers where there's my positions are zero my state trajectories 10 toward more positive numbers. So, this is the policy for zero velocity. So we can see the kind of how well this maybe has trained. So this is maybe not quite as optimal as I would like so I would expect that, you know, my best policy would basically be our are are are zero L L L L L. And so, for if I met zero velocities that is if I'm not moving I mean one of these, one of these states, where should I be moving. Well, what this has learned so far is that if I'm in states eight nine or 10 I should move left, which is good. But it's also learned that if I'm in state zero through seven I should move right, which is not quite right. So we can try to train this again to see what happens. I'm just taking up to like 15 seconds or so. I'm kind of see similar patterns starting to evolve right this this curve starts to more familiar. So does this one. And again, I think this is yeah. So, actually you got a little bit worse that time. Right. So now it's basically learning that if I met 10 I should move left. But if I met anything else I should move right. So try it one more time. And then we'll move on. So, I'm going to see things changing here over time right now it's basically like learning a strong policy just moving to the right. Okay, now things are starting to happen. We're getting some, some indications we just start moving left there. This is done again. So, maybe this is not like a terribly successful policy I'm going to try one more time to see if I can get it to kind of demonstrate something nicer. So, I think that the way that we're doing this is that reinforcement learning has a tendency to be unstable. And so often you start to learn something and maybe your ex your epsilon is decaying too fast and you kind of find some sub optimal strategy and stick with it. Or, you know, like what this is doing. But you can see that you know there's it sort of finds some sort of degenerate strategy and it's kind of sticks there for a little bit. So, you know, we're going to have a lot of strategies maybe we need a bigger neural network that would take longer to train. Maybe we need more more samples per per episode. That would allow us to to kind of get get more experience. So, let's just sort of accept that this is sort of learn to kind of some kind of sub optimal policy actually seems to be doing worse now than the, the previous like five times that I tried it. So, let's print out the last last 10 rewards. So what this has learned here is that if I over time, the last 10 rewards of rewards that it got were negative one. So this was basically somewhat unsuccessful in training. By, you know, a couple of different things like we can try maybe adding a adding more hidden units. Try this again. So you can see this taking longer to train this time. But now where we at. Yeah. Starting to learn a little bit more a little bit earlier. Now the policy for zero velocity in this case kind of seems to be just sort of bottoming out with always moving to the right. So what we're seeing here in this example is a strong tendency for the instability of reinforcement learning. So, in this case for this for these continuous problems. Basically what we what we find is that say cable learning is not necessarily always the best choice. And so often we encountered these problems with stability when trying to use a sort of DQ network so we have other policies that I alluded to earlier in class, such as the actor critic methods. So we have say things like a DPG or a soft actor critic or an A to C. And what that does, we have these two neural networks where one is the actor that is to choose the action. One is the critic that tries to predict how good or bad the action is going to be. Right. And so then the actor tries to get better at predicting good actions and the critic tries to get better at predicting the quality of the action. And then these two networks are updated in tandem. So that's one way of kind of solving some of these or at least addressing some of these instability problems that we encounter with continuous spaces. Okay. Anybody have any questions. And so this again, just for fun. Now we're seeing something desirable here. So now we have a policy that's kind of approximating what we'd expect. So this was now now that we're done here. Let me show you a case that we've been moderately successful. So, if you look at my, my policy for zero velocity. This is where my mouse is that state five, this is very one to end up. And so what this policy has learned is that when I'm less than five want to move to the right. When I'm a greater than six, I want to move to the left. And when I'm at either five or six, I stay still. So this is actually, this is a moderately successful policy and glad we actually managed to get one in that it's approximating when I get close to five at least knows that I should actually slow my marble down. Whereas if I'm far off to the right, I need to be accelerating toward the left and far off to the left and he'd be celebrating toward the right. And this is reflected in some of my, my action policies, so you can see here that where I have this last sample actually appear to be quite successful, where my part my marble position is within this red bar that has been one unit of five, and my velocity is very close to zero. So similarly, if we look at the actions here, it's learned that if my, by position is say 10, and my velocity is like negative four, then the action, this is, we really probably don't want to do a whole lot here, for example, because I'm moving in the right position, maybe I don't want to accelerate too much. Whereas the opposite is true, if I am say close to zero and my action and my velocity is four. Right, so I might want to accelerate a little bit, probably not too much, whereas if I am at 10, and my velocity or a close to him, I've lost is four, I'm moving fast in the wrong direction. And then I need to kind of accelerate or stop my motion, start moving back in the direction. So now we look at the state trajectories and now this is the pattern that we would expect. So, as I am, if I'm at zero, and my action is my velocity is zero, then we can see that what I'm what I'm doing here is like as I'm moving in. In toward five, I kind of want to increase my velocity, but then I also want to start by cross over five, I need to be kind of moving back towards decrease my velocity moving back towards the left. Okay. Questions. All right. So, in conclusion, I suppose, continuous spaces are challenging for reinforcement learning. And so you need to have sometimes more sophisticated techniques to try and solve these. And we find these cases in, in scenarios like robotic manipulation, where you're trying to control some say some some movement of a joint and continuous space made to manipulate an object. And so this is a problem that can be solved with a lot of continuous sampling and learning from experience. But it often takes some fairly specific hardware. And the ability to sample, you know, lots of lots of experience from my environment. Okay. So, next time so basically Thursday, just going to take off because I'm going to be traveling. And then we will reconvene next Tuesday. What we will do on Tuesday is going to be the reinforcement learning for two player games that has learned to play tic tic tobe. And that's going to be the subject of assignment six. And we see if I have the schedule up since it was here. And then I will also assign assignment five then. And so that's this is at present. That's going to be the day that assignment for is due so assignment for what we do. I will sign assignment five to have two weeks for assignment five. And then I'm going to assign assignment six before assignment five is due, but you should have plenty of time for this. And I hope you're all working in your proposals. That's going to be due next Thursday. Question in the chat. Very interesting. Okay. Thank you. Okay. So I guess there are no other questions or comments. Then I have no problem with calling this a short day. And I will see when I get back home. Do I have a I will not have office hours to go back to the conference. But if you have questions, again, feel free to email me and I will respond as quickly as I can. Thank you. Bye bye.