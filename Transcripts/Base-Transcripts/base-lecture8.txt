 Okay, let's get going. Very, very good chance this class is going to be short today. Because if you look at the schedule today is the ninth and we actually finished this notebook on Tuesday. So we're actually like a day ahead. And Ray Lou is a pretty short notebook. So I'm going to do that. And then I will assign the second assignment. And I didn't bring my GPU machine so I can't run half of the next notebook. So I'll just call it a day after assignment to you and then I will set up my GPU laptop over the weekend. All right, so assignment one do tonight. If you don't have an extension of course, some of you requested one, some of you have a range one. So, but for the rest of you. If you hear me say this and you're like, Oh shit, I did I forgot to request in the extension and I really need one. I think the best thing for you to do is to get something turned in today. Okay, therefore, the various reasons about how I run this class is much, much more to your advantage to turn things in on time, even if incomplete, then to try to, you know, spend a couple of days getting things up and then submitting it late, because you will get a 10% late penalty. Well, 10 points not not, not percent of the remaining grade basically like minus 10 for every day that it's late so it is definitely in your best interest to get something turned in today. Even if you feel it's not complete. Just trust me on that one. Okay. Are there any questions about. Yes. Sorry. I will have all those hours. Yeah. So, yeah, I will have office hours at the usual time at starting at three three. I will, yeah, well, I'll try to give you feedback. So what I have a rubric about different points that I will send you. And about like, and use what we try to do is basically say, you know, count it off x points for not doing why if you do perfectly. We'll say like great job or something. So, if you get 100 don't expect a lot of feedback. If you do something like super cool in your notebook that I can't resist commenting on don't do it but if you just checked all the boxes you don't just take your 100% and you do do get cut it off. We'll try to indicate what that is for. Other questions. All right, cool. So, what we are going to talk about today is Ray Lou, who knows what Ray Lou is. Okay. So, we talked about activation functions right activation functions are these non linear functions that we applied to the output of a hidden layer, and meaning that when we back propagate the error through the network, the weights in that the network function layer are effectively optimized with the assumption that a certain non linear function will be applied to that output. So, that is, if I'm applying the 10 h function to something. And I want a specific target result. And then there's some transformation that needs to be applied before applying the 10 h the particularities of that transformation are going to be different. And then the other question is, what does activation function mean for versus when it's not. Right, so that's the activation function. The activation function is constant. It's usually not parameterized very much, except to maybe some very specific ways. But basically it's what you typically do is you apply an activation function to an entire layer. Right. So, you may have heard of this thing called Ray Lou, which is this weird term that people all the time. My goal here is to basically demystify machine learning in this class and so the one of the goals is to tell you what all these weird incantations actually mean. So, we, if we're going to address non linear problems and assignment to is going to be non linear regression using neural networks. And in some way to include non linearity, we do that with activation functions. And any sort of non linear function can in principle be used to introduce non linearity now. If I'm assuming that my activation function after my transformation of my inputs by my weights is the 10 h function, then my optimization is going to be performed in a certain way, so that my weights end up at certain values. And so, if I'm going to do that function. Of course, when I perform the same optimization operation, the weights are going to end up different. Right. The reason for that is because when we are performing back propagation into hidden layers. One of the things we have to incorporate is the derivative of whatever non linearity that we apply on on the, at the output of that layer. What you, what you can, something that you can do and it's not really clear why you would do this but you can do it is if you have a neural network where you have no activation function on any layer. And it's just a linear pass through. Right. You still end up with a just a kind of convoluted way of solving a linear. And so what that means is that when you're applying these non linearities when you're applying. If you had no non linearity, if you remember how we have, how we perform back propagation at the output layer. It's just the error term times the input to that layer. Right. Then if I had no non linearity on my hidden layer when I'm trying to apply back propagation to that hidden layer and the derivative of a linear function is just one right x equals x. Then I'm not applying any extra term to my back propagation operation. And so in if that were the case, then the back crop for the hidden layer would be the same as the back crop for the output layer. So what that means is that when I change the activation function, I just change that one term the derivative whatever that activation function is given that we of course need our activation functions to have some desirable properties so that when you take the derivative values don't vanish. They don't go out of control. They allow you to keep optimizing and taking the appropriate size step down the gradient toward that global minimum. So, as you probably read, so it was kind of rambling right we have desirable properties include computationally simple right because we want these to be faster and we applying them at a high number at a large scale. You don't want to be computing some arbitrarily complex function for multiple units and multiple layers. We also want, you know, for initial small weight values that function should be close to linear because as the weight magnitudes grow want the function to be increasingly non linear such that when you get when you reach magnitudes of extremes you're not having those values greatly affect the input much more than input values that are maybe closer to zero. Again, the derivative of the function should be computationally simple for the same reasons as item number one, I'm going to be taking the derivative during backprop again at exactly the same number of of activation operations that were during the forward pass. That must also be computationally simple. And then the magnitude of the derivative should decrease as the weight magnitudes grow, and this should perhaps approach some sort of asymptote so that if I have an extremely high value. It's not you're disproportionately affecting the adjustment in weights during that propagation. And then finally the maximum value of the magnitude of the derivative is limited. And as you will see, some of these properties are more desirable than others and some of them we can actually let slide a bit when certain other conditions are met. So, given these properties that we identified previously, we also identified two functions and their derivatives that have these properties one is the sigmoid function, given like this. And this derivative is sigmoid of x times one minus sigmoid of x, and the 10 h function, given by this where it's derivatives one minus 10 h squared, both of these obey these properties. Sigmaoid of course is this s shape curve bounded between which value and which value. Zero and one, right, and the 10 h function is bounded between negative one and one. So, these two functions effectively have the same shape, just with different bounds. And so then the derivatives also have kind of they have they have basically a hump in the center. It's just that the maximum value of that of that hump is different for the two functions. All right, what do these functions actually do right let's take a look at them so we'll plot them with code that you've seen before so I will define sigmoid function the 10 h function and their respective derivatives, and plot them using different types of lines. So here is the plot right this is what we see. So the blue lines that's the sigmoid as derivative red line that that's the 10 h and its derivative. And so we can see how they obey these nice properties where one I'm close to zero. This is more or less linear function both of these. The maximum values as the as the inputs go to extremes are bounded same as true for the derivative has a limited maximum value for sigmoid it's 0.25 for 10 h it's one. And then also that the derivative decreases towards zero as the values go to extremes. Okay, so we see the derivative of the sigmoid in the 10 h function decreases as x gets further away from zero. Now remember the three things that we need to actually compute a weight update over some predicted output. So, one, the learning rate, right, generally speaking, what's the size of the step that I'm taking along the gradient. So, I'm trying to put an instant target. Right, how wrong am I if I'm very wrong. The total step that I want to take would be bigger, right scale by learning rate. And then the gradient at z right how steep is the is the slope. If I'm trying to approach the bottom, I'm trying to approach a place where the slope is effectively zero. And I'm trying to find if I'm on a steeper portion of that gradient, then that's going to affect how, how far I moved down. So absent fancy things like Adam, which also further further, further adjust the total size of that step based on things like previous weighted averages and variances. These are the things that you need. Right, so you take some combination of all of these, and this determines how far and in which direction I need to be moving along my grade. So what this means is that if the gradient is small, then the weight update is likely to be small as well. Generally speaking, this is what we want. Right, we don't want weight updates to be too huge and shoot us off into into the abyss or, actually, we're trying to approach the abyss the bottom of the, the gradient. And then it's going to shoot us off into into space up the up the curve of the gradient. So we like these derivative functions that don't explode as the magnitude of x gets larger. And we don't want that weight up to be too large such that we miss. So local here I should change that to global. We don't miss the global optimum entirely. Okay. If the down if the activation functions are too small, there's some downsides as well. So if you have some function H that's our activation function. And if the output of that function is too close to zero, then the outputs in that layer with that function will be close to zero. And sigmoid actually has this problem, which is one of the reasons why prefer a 10 H right sigmoid. We have really negative values, which may often be the case, you have values close to zero. And so, as opposed to 10 H where the set of values close to zero is pretty small 10 H of zero is zero. And as I move away from zero, it linearly also moves away from zero until I start to reach more extreme values. Then also if the derivative of that function is too close to zero, then you can't communicate useful information to back propagate weights. Right, because I'm going to be multiplying the change in weights by among other things the derivative of the activation function. And if that's really close to zero or is equal to zero, there will be no change. Okay. This last point in multiple applications is generally known as the vanishing gradient problem. And it shows up in a number of different circumstances most particularly in our ends will talk about that briefly later in the course. And among other things what this can do is it can cause the training of the neural network to just slow to crawl or stop. So if you end up with a place where all of your, your gradient derivatives are zero or a bunch of them are suddenly individual neurons that might be useful for the problem. Those weights are no longer updating. So I could end up with my weights at a place where they're sort of arbitrarily far away from the best solution, given that neuron in that in that layer. But it has no useful information about which way to go. So it just stops. And so both the sigmoid function and the 10 inch function can suffer from this problem. Everybody clear on the motivation so far. Okay, so we use this thing called Raylou. Raylou stands for rectified re linear unit. And here refers to the neuron. So we have a neuron that this really function is applied to it and we'll call it a really unit. And so you can also have a really layers. Right, where the entire layer uses Raylou activation. And we also talk about Raylou networks, where all the layers, all the neurons use the Raylou activation. And it turns out that these Raylou networks actually has some very interesting properties that we'll discuss briefly at the end. So, in the example so far we've been using the 10 h function for everything it's like this is our default non linearity. This is the only one of exploits so far we're just going to apply the 10 h function to everything. We don't really talk about 10 h network so much but you could, right, that would just be a neural network where you use nothing but the 10 h function. So, you know, the function of the network because they have some interesting properties tend to be considered a single group because those properties apply or appear to apply to all Raylou networks. Okay, the function itself looks like this. So, already you can see that unlike 10 h. This is a what kind of function. So, it's zero if the value of the input is greater than or equal to zero. Otherwise, it's x, if x is greater than zero. In other words, this is basically taking the max of zero and x. So, there are multiple ways to write this function. And we will use the for implementation will just use Np dot maximum. If I have some input a, and I take Np dot max of a I get this. So all it's doing is just taking the positive part of x. So if I am zero or less. Yeah, I'll put a zero. If I am greater than zero. Yeah, put his x there are some small debates about whether you need to bounded at zero. So it's like less than zero it's going to be zero or it's greater than equal to zero it's going to be x. We end up mathematically end up in the same place. Although, I think people who study like real deeply the mathematics of machine learning may actually have some strong opinions on this, because based on where you make that boundary. Sometimes the properties the network may actually change. For our purposes and I think really need to worry about that. So here's our real function. And it simply takes the positive portion of x. So what's really derivative. Right, so we'll describe it here and you can tell me what you what it thinks, what you think it looks like. If x is greater than or less or if x is less than or equal to zero, then Rayleigh of x is a constant zero. So the derivative of any constant is also zero. So for this portion of the function, where it's less than or equal to zero, the derivative of that should also be zero. So that should make sense. Now, if Rayleigh of x equals x, the derivative of y equals x is one. So we should all know that. And so therefore, if x is greater than zero, then do you do x is equal to one. So the derivative of Rayleigh also known as the rectifier function is called the heavy side step function. So what do you think the derivative of this function looks like when I when I show it next. So I see a line. Yes. A line and then another line. Right. What do we think? Okay. So, we define it de Rayleigh as the headed and he provides a heavy side step function. Right, we get this. And this is this is what we said. So we, of course, are assuming it's, it's, we're drawing it like the continuous function. So it does draw does it's best to draw a vertical line here. And really it's drawing it between like point negative point zero, or between zero and like point zero zero five or something because we're sampling 100 points. But yeah, we're seeing is it's zero from negative infinity to zero and then suddenly it's one. So let's see what actually happens when we apply this to some weights. So first of all, I'll define this term s being a weight sum. And then I will just take, you know, create some random numbers. So let's assume that these are inputs to a Rayleigh function. So first of all, let's see what happens if I take a take the 10 inch function to this. Right. So I get this. So if these are my my original inputs. This is the 10 inch of that so 10 inch is zero is zero. The 10 inch of the negative numbers are increasingly negative as the negative number of the magnitude of the negative number increases. And then also increasingly positive bounded one as the magnitude increases so basically what we have is I have a bunch of, a bunch of ways and sort of squishing them into a distribution between zero and one. Rayleigh, the, I can also basically take a function like this for some input s for those elements of s that are less than or zero, or less than zero. I will set them equal to zero. And then that will give me the output. So Rayleigh, that's looks like this. So this is 10 inch of s of everything is squished into that s shape curve. And this is Rayleigh of s where all the negative values are simply set to zero. So, if I take there's s again. So now what about the gradient of s. So I guess point here, just this is one of those peculiarities of memory so here I put in s and then I modified s in the function. Right, so I return s and I can take Rayleigh of s and it's going to give me this and then if I print s again, because I modified s there, it's going to give me s. Okay, so just to be careful in this implementation will do is make a copy of s. I actually have a deep copy of s I'll store that is why and then why will be the Rayleigh version of s. So I can return why and then if I print on why. Oh, so I'm going to go ahead and then why. There we go. All right, now what about the gradient of s. So I will just define again, derailleur. I'm sorry. The gradient of s. As in, well, so s this s is modified in line. Right, so this is modified in sort of this version, this non memory safe version of the function that I wrote. So really s what we want to be doing is returning s to the original value here. Right, so if I were to, you can do this again. So, so. All right, so let's start from the top here. All right, so I'm going to define s. Okay, now s is this. I'll take its 10 age. Okay, whoa, let me define Rayleigh using s inside the function. Now I take the Rayleigh of s. Okay, now I get. S with all the values negative values at zero. Oops, I changed s in line in the function. So I actually changed the value. Of course, if I apply a really to this, I will get the same thing because all it's doing is taking values that are zero or less than setting them to zero. And so the only thing that gets, quote, changed here, things that are already zero. So nothing actually changes. So a better version of Rayleigh will make a deep copy of s. Of course, this takes more memory. So let me define that function, not me redefined s to what it was previously. Right. So now I have the original s, y equals Rayleigh of s, y, and then. S again. Right. So there's the original s. So now I. All right, so now what about the rating of s. So let me define my d Rayleigh function. So s will be just n samples by n units. First, I'll just create a deep copy. And so now I will just define the heavy side step function in in line. So here we have dy where those elements of a bit are less than zero, those become zero, where they're greater than zero they become one. Now I still to deal with zero. So here, because every value in the function has to be given as it has to be given an output value. I don't want to be, I don't want to define I don't want to end the function here. Right. Because then if I put in a zero, it'll give me some, some error probably. So I will just decide that, eventually, the same thing is true, where if I just set this equal to zero, I'm doing the same thing as if I did that. So now d Rayleigh of s gives me this. So all the negative values are become zeros. All the positive values become one, and there is one zero in there that remains. Okay. So, there's that again. And there's just multiple ways of doing this right so I can take the, I can just copy Rayleigh. Right. So there's another alternate way of doing this would be because Rayleigh is already setting those negative values to zero. And then I'll be the Rayleigh output, which where the negative values were already set to zero and then derivative of those would also be zero, and then all I have to mess with is now zero or the positive values. So that's one way gives me the same thing. And then another way would just be to make a copy of s. And then I'm just going to return basically a Boolean, where if dy is greater than zero, this would be true, which if I do as type in that turns it into a one. So this is basically just saying, all of those, all those elements of this input that are greater than zero will become true or one, and everything else will become false or zero. So these are, you know, more or less efficient ways of performing the same operation. Okay. So now the weird part right why does this actually work. Because it doesn't really seem very intuitive that this piece wise linear function would be able to introduce non linearity into a neural network. Right. Why, why would this, why would this work. It's not totally linear in that it's got these two pieces. But it lacks the, the clean curves of the thing more to the 10 H, and just sort of looks like I locked off the bottom half of my linear function for no reason. So, why does this work. Remember that the sort of modeling curves like the following adapted from notebook five just time using the 10 H function. So again what I'm going to do here is I'm going to create some, some inputs and then apply a no non linear function to them. So in this case is just, you know, this is this is like basically linear function with some random noise. And then I'm going to train a model to fit that. And then the same, this is just doing linear regression except I include a 10 H function here so I'm sort of trying to optimize my weights with the 10 H in the mix to model on a linear operation. So, this gives me this, these are my actual weight values weights and bias. And here's the model. Here's what the model predicts the orange line. Right. So you can see that it gives me a very slightly curved line. This is effectively trying to do its best to straighten the 10 H function. So it's, it's applying weights that are really small such that when I apply the 10 H function to it I get something that's like really close to the original input. Okay. So there's a bit of a curve here if I were to train for longer, probably this curve would might might flatten out a little bit. So, so what these functions do is they're going to bend the otherwise linear regression to better fit the data so you may observe that by adding this noise. There is, I'm actually, because of the random sampling. There actually is a bit of a curve to the data. Right. So if I were to model this purely with linear function. Maybe it wouldn't be quite as good a fit as I did it with the 10 H function. So Raylue does effectively the same thing, except instead of having a smooth differentiable curve all the way through. It's picking one point in the line and making one bend. Okay. So this is a lot more competition the efficient obviously, because it's just, it's really just a linear function with a bit of a twist. But it's also less precise. So let's say that I have this data below. And they have these blue points and these orange points. And let's say that I wanted to somehow draw a line to separate them as cleanly as possible. Right. So what I'm trying to do is I'm trying to draw a line that will be about here. And then here, you know, going to try and fit as closely as possible may not quite get it because these points really closely overlapped and then go down again. So we could probably do this with a 10 H layer or two. So if you remember from notebook five, we had two distributions. So these random noise samples where one kind of curved up, and then one kind of curved down. And we can just apply a 10 H function to a linear function. And kind of fit that if there's a single curve. Then we had one that sort of look more like a parabola. And so now I want a function that's going to go down and then go back up again. And you can do that with a single 10 H, but I could do that with say two 10 H functions. One that would put the curve for the negative values and then become flat. And then one that starts out flat and then it's zero starts to climb. And then it waits if I have two hidden units, each of which has a 10 H function on it. And the weights in one are effectively taking the negative values and making them close to zero. That's going to give me that curve that starts flat and rises. And then I have another one that takes a very positive values and makes them close to zero. That's going to give me the one that starts to fall and then ends up flattening. And then I add those two together and I'll get that nice curve. So that's how a 10 H layer would do it. So this would basically draw this like smooth envelope around the orange points. Of course. Right. So it might, you might be able to infer that like, okay, I'm, I get one line and one bend. So I'm going to put my bend here where my cursor is. I'm going to have straight lines on their side. Okay. Probably not too bad of a fit, although you do risk having certain points like maybe this one that fall on the wrong side of the line. So this is going to be less precise. But what if you have a bunch of different really units, and each of them is going to be optimized to put that bend in the right place. This could approximate the same curve. So it's sort of like, instead of drawing a circle. You draw like a 36 sided polygon. It's going to look a lot like circle. Or 100 sided polygon or you pick your number the more you the more you get. Better it's going to look like a smooth curve. So by these really units working together this would allow us to approximate the same curve. So outside of this non linear function. All the neural network operation is doing for the unit is why equals WX. So that's just moving the line around, and then, and then stretching it, you know, rotating it basically choosing the slope. And so then you moved around by some bias be to figure out where to where to place it. And so the right bias vector. It will allow you to choose smart places to place each of those bends and then the remaining ways will sort of tell you how I'm supposed to. And then you can just rotate the entire the entire curve. And so then each of these let's say what I could do is I could optimize for like a small piece of the curve is kind of a shallow bend like that. That's going to handle some part of maybe. From all the values like up to negative four or something like that, or up to negative two. And then I find another one that is optimized to place the band and it's like a different region. And then I add all those together, and it's going to do a pretty good job of creating something that looks like that curve that I want to separate these regions. So what that means that Ray was real strength is in numbers. And so we can have a really large number of these rectifier units that actually approximate the non linear behavior of functions like 10 inch. And they do it, even though using them in greater numbers because it's so much faster to calculate the Rayloo function. They actually do it with greater speed and greater efficiency. So if you want more on like how does this work, there's this explainer here that you can check out. It gives us pretty nice examples of trying to separate you know sort of curved regions and data. Alright, questions. Okay. So now back prop is relatively straightforward, because as I've kind of foreshadowed already, the only thing that you really need to change is the derivative of that activation function. So you can think of it as if there were no derivative back prop in the hidden layers the same as back prop in the outer layer. So all the only thing I'm adding is the, the derivative of the, of the activation function. So we have remember for our neural network operation. If V is some hidden layer, I'm going to multiply x by that apply some activation function h gives me some scalar value z. I add a bias to those and then multiply those by output layer weights w and that gives me my prediction. So here is backdrop. Let's just look at the w line first this is backdrop in the output layer. So this is going to be t minus y. This is my error term how wrong I am. I'm going to multiply that by z, which is the input to that layer. I'm going to do this for all of my samples for the entire dimensionality of the output. Right. And then there's some learning right in there. So the same thing all I'm doing is all I'm changing is basically this part of the equation when I go from the output layer to the hidden layer. So now instead of just the error term taking the error term times the output weights times the derivative of z in this case the input to that layer. So what is what is Z is actually going to be a function applied a function h applied over x V. So that has to be incorporated into this operation. And so I need to know what the derivative of an activation function is. So if H is 10 H, then the backdrop operation will look like this. Right. So I'm going to have one minus z squared, because the derivative of 10 H is one minus the 10 H square. If H is Rayleigh, then the backdrop through these is going to look like this. So I'm going to have everything up until this point the activation function is the same. So the first part is multiplied by the derivative of the activation function, which because if the activation function is really is this step function. So now it's going to be dependent upon what the precise values of each element in Z are. So I'm going to take in some weight matrix Z, and those elements of it that are negative or zero, become zeros, and those elements of it that are positive, get left Okay. So, let us view how those weights actually change. So what I'll do is I will create some data represent sample inputs and targets like you've been doing. And then I'll create some randomly initialized weights. And then I'm just going to simulate one iteration of training with the two different activation functions, tenation, right. So basically versions of Z and the output as assuming that the activation function is 10 H and then one assuming that it's really, and I'll store the error using each one perform backprop, and then calculate the change in each way. So these will be the delta between the previous value of those weights and the current values. All right, so we do that. Now let's view how the weights change depending on the layer and which activation function. And I'll just view this in a pandas data frame, and just reshape the data so we can, we can view it. And here we go. So if these are the initial weight values, just for say the first nine here. This is what happens after I apply the ten H function. Okay, so we can see that for example, for those values that are very close to zero, they remain very close to zero for those values and all them pretty close to zero so you don't. So that's not really very pronounced, but those things that are a little further away from zero, you know, get a little closer to negative one or one. And then we can look at the amount of change, right, how much each each one has has changed. Now, Ray Lou, we can see that after the Ray Lou, the Ray Lou function. There are certain values that have some change applied to them, and certain others that are completely unchanged. And this is all dependent on the whether they were, you know, positive or negative after you take that and then multiply them by the inputs. Okay. So now we can look to the same thing for the output layer, right, and we see similar phenomena. The only real difference that here we don't have that many. We don't see any values where the value has not changed at all. And this is mostly because we have this additional computation in the way that hidden layer. That's providing other information. So finally, Ray Lou's not the only linear unit type activation function you can use. There are plenty of others really lose the simplest. But you can use J Lou, this Gaussian error linear unit, this is the one that has been used in most state of the art language models. So I don't exactly know what GPT uses for certain, but I'm pretty sure that it's some sort of J Lou function. And then there's the sigmoid linear unit, which is basically just x time sigmoid of x. This ends up being another smooth version of Ray Lou. So it's sort of it's, it looks, you know, typically like the Ray Lou function. Except it's doesn't have it doesn't have like a sharp bend and zero it's got more of a smooth curve. So all of these, well, not, not necessarily all there's many of them. You they try to smooth out that bend and Ray Lou, because it makes for a nicely different functional way through, whereas, yeah. So let me, let me do let me do leaky really first. So leaky really was basically instead of being sort of clamping everything at zero before. For all negative values, and then just letting x pass all the way through. What we do is we let a little bit of that negative value through so there's there's some constant value usually like point one or point one. And what we'll do is it will take for negative values, it will do, you know, K whatever that is times x. So it's going to a little bit of the negative value through but not that much. So maybe take one, one percent of that negative value, and then the, and then the positive values just get passed through. So parametric Ray Lou is the same thing in that we're going to try and learn, or we're going to try and let through some of those negative values. But we don't know how much right so I could just leaky really everything, but maybe that constant point zero one is like not the best value. Maybe, but I don't know what it is I actually want to learn that from the data. So we basically have a tunable parameter that allows us to figure out what the best value of this quote negative leakage is. And so this can actually be learned during training as well. And exponential unit, what this will do is in order to keep mean activations close to zero and actually exponentiate the negative values. So all of these are effectively methods of leaving the positive values alone. And trying to do things with the negative values that are meaningful usually in combination, and also don't cause your training to collapse. And all of them are usually more computationally efficient than 10 H, some of them like like parametric Ray Lou, you have to, you actually have to tune a parameter in there. But they all have different uses, and they can be very useful for different types of problems you will find like convolutional neural nets, usually use combinations of Ray Lou layers. Okay. Questions about Ray Lou. Oh, the place where you. No, I don't. Okay, so I guess there's two answers to this. Yes, you can, as in it would it's definitely possible to write a function that does that. Does it satisfy the nice properties that we want with neural networks that's less clear. Because pretty much everything gets standardized and so you're assuming that everything is going to get clustered around some mean, which thing is standardized to zero. And so zero tends to be the best place to start making differentiations about your data. So yeah, you probably could write a function that's like, I'm going to maybe learn where the best place to start. And then you're going to be letting positive values through is. But there's a good chance it's probably going to be like really overfit to a particular data set, because you're, there might be some data set like where you you standardized everything. And for some reason, there's like a weird secondary cluster sort of somewhere else that's not zero. And it might learn, okay, this is like a really good place to start my linear part of my Ray Lou unit for this data set. And once you train on that data set and you try something else, even like different testing data. It's no guarantee you're going to have that same property and testing data so it's just. You know, back of the envelope guess really sort of seems like this would not be something that would be very productive although maybe people have tried. Other questions. All right. So, let me go to assignment two and go through that real quick. All right, so, general shape of this assignment is very similar to assignment one in that we give you some starter code, you to fill out parts of that starter code. Apply apply that to some dummy data to make sure that your functions are apparently written correctly. And then you actually have to perform some experiments on it and discuss your experiments. So, you will give you in this case a neural network implementation. And then this assignment correctly you will at least have a complete neural network implementation that you can use yourself. Do you want to do that well that's questionable because it's all written in NumPy doesn't have GP acceleration so you can use it. And if you're a talented hacker you can probably, you know, translate it into pie torch or something. But we'll also use a pie towards neural network later so you don't need to do that. Again, if you succeeded to assignment you at least have in principle a neural network implementation that you can use an experiment with and does give you a lower level of control than apply to our to attend flu implementation so we do have that, that advantage. So you're going to need to do that to complete the neural network, define a function that partitions that data into training validation testing sets, like we showed in the previous lecture, apply to a data set, and then define a function that's going to run experiments and type of parameter values, and then you're going to describe your observations. So first thing you're going to do is you need to complete the optimizer class. So we give you a mostly complete optimizer class that has complete implementation of SGD. What you need to do is refer to notebook six, and turn that SGD implementation into an atom implementation. So this cell will implement SGD, and then there will be some implementation of atom that you need to complete. Right. So we give you, for example, we already give you like, you know, MT V T beta one beta two values. What you need to do is you need to finish the atom implementation by updating these values and the values of self dot all weights. Okay. So then you can test that. If you do, if you correctly, you should get the same results as shown below. What do we see here so I alluded to this property last time. So your, the test here is basically finding the minimum of a parabola SGD finds it exactly. Right. We know the minimum of the parabola is at five SGD finds it at five. It gets very close, but doesn't quite find exactly five. But this is supposed to, this is the expected value. Right. You're supposed to get 5.03900403. Okay. If you can also see here that, for example, both of these are training for 100 epochs and SGD starts with an error that's very close. And in this case, because it's a very simple problem arrives at zero pretty quickly. So it starts with an error that's like way off 16, but it also gets it closes that gap within that hundred epochs. So if you look at the change in the level of convergence from the start to 100 epochs, atoms basically eliminating this error about 16, where SGD starts very close and only has a little way to go. So you can also see kind of some of the properties of the two functions there. Now you are going to implement the neural network class. This is going to call the optimizer's functions. So we already give you that the call to optimizers is done. So first you need to complete the use function. So you can make use of the forward pass function in your use function. I'm not going to give you much more information than that. You have to figure out exactly how you use the forward pass function in the use function. This should not be too difficult. I think if you've been paying attention to what the forward pass is and what back prop is and what it means when you see the error. Okay. So do that. The rest of the class is pretty much given to you. So complete the use function. You can assume that X is standardized, but you have to return the unstandardized prediction from the use function. So then you test out this test neural network function. Your results should be the same as these below. So you can test that there. And so we give you these graphs and basically if your your output matches these graphs done it correctly. Again, as always, I recommend. Download this notebook, create a copy just in case something gets messed up. You can always revert to this one and you also compare your results visually to this one and numerically. Okay, now what you're going to do is you are going to cut and paste basically create a copy of your own network class cell here and then modify it to allow the use of the regular activation function. Why do I want you to cut and paste? Well, mostly because I don't want you to risk screwing something up that would use results that you're trying to test. Okay. So yes, it does. It does seem kind of extraneous to copy and paste this large chunk of code. But I think it will help organize your, your stream of thought and also prevent you from maybe creating on, on desired results. Okay, so what you're going to need to do is you're going to modify the neural network to allow the use of the activation function. So right now it assumes that the activation function is 10 H. So if we go to in the train function, for example, you will see somewhere. Forward pass radian F. So you're going to need to do is you're going to need to modify this to take an argument activation function that will specify either the string tan H or the string Raylue and depending on that value apply the right function. There is no Raylue function in NumPy so you have to define your own. So this should accept a matrix of weighted sums and return the Raylue value, the implementation of how to do that is a function of the function. So the, of how to do that is basically given to you in the notebook which you need to do is modify that for use with the neural network class and make sure that your inputs are all correct. You also need to define at a real loop. Then you need to add some if statements to the forward pass and the gradient function that depending on the value of this activation function argument will apply the tan H function or the real function correctly. So this should be pretty straightforward if you just have a new class variable that has one of the self dot variables in the neural network constructor that will then accept the value of the activation function. So finally, the experiments so you're going to use the auto to mpg data that we've used in some lectures in lecture two. So let's apply these neural networks to predict models per gallon using different neural architectures. So kind of doing the same thing that we did in lecture three. Only this time we're doing it non linearly. So the data processing is going to be more or less similar to that you should end up with 392 samples after you remove all the samples that contain missing values. Then you're going to need to partition the data into five folds as we should as we showed in lecture. So this should return train validation and test partitions for the inputs and the targets. And then you're going to need to write this function run experiments that will have three nested four groups that basically do a grid search for you. You're going to try different parameters of the number of epochs number of hidden units per layer and the activation function so these two can be whichever you want. Activation function is just 10 H array. Okay. Now you that allows you to compare the training time network architecture and activation function and see which combinations work best. You should try zero for one of the and unit and hidden units per layers, because that will be a linear model. So, right, if I have no hidden units in my neural network is just a linear model. The way we constructed this allows you to pass a zero in and get that. So then for each set of parameter values, create and train and network using Adam. And then evaluate that neural network. So collect all the parameter values RMSC. And then when you're done constructed data frame that looks something like this. So, then, when you're before you start this nested for loop you need to call your partition function to form the train validation and test sets inside run experiments. Use the same partitions for all of your experiments. So, for example, call the functions would look something like this what you need to do is then change which values of these should all be constant right XT and folks in state five, and then you need to specify which ones you want to use there. So, these, this is just a list of numbers. This is just a list of lists. And this is just 10 H and Raywood. Then find the lowest validation error and the lowest test error and report in the parameters that produce this and then discuss how good your prediction is. So, you can try at least three different observations that you make plot the RMSC train balance test sets, versus the combined parameter values we did a version of this at the end of the previous lecture. So you can model them off of that. You can work with a partner on this. If you choose you're not required to. This is the only assignments that you're allowed to work in pairs until the final project. So, you're like before should be uploaded on canvas. Same procedure as before. And so I recommend you know keeping the notebook in a folder with just the greater. So then you will get 70% for the code and 30% for the discussion and the experiments. You can also get some extra credits by look up the the swish activation function or start this article. And then you can implement that as a third choice of activation function. And if you do it successfully, you'll get five points extra credit. And you can just do run experiments just say comparing just comparing the three activation functions, say, take your best performing set of other hyper parameters and then try the three activation functions and discuss the results. Okay, any questions. Thank you. Bye.