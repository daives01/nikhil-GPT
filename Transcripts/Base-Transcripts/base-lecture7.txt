 Okay, let's get started guys. Your screen share. All right, so I'm going to tell you a stupid joke that I just heard. Does anybody here speak French? How do you say the letters GPT in French? What does it mean in French? Means I farted. Okay, so. Basically, if you do cat I farted in French. You get shot. Thank you, Professor. Thanks to Professor. For that one. If you go to my office hours, you see the meme on the the court board. So it's like, it's like the Drake meme and it's like, you know, chat GPT. No, cat GPT. Yeah. So I didn't realize this. Another did my wife actually, even though she does speak French and has seen that meme, but then, uh, who caught me on that. So that was my highlight of the. Good morning. All right. Anyway, so now I'm going to tell you about finished talking about the dude named Adam. So let's go do that. So just a reminder that. I was on it one do on Thursday. So if you do need an extension, I will need your reason for an extension by the end of the day tomorrow. And then I can get back to you. I'm going to get back to you. I'm going to get back to you. I'm going to get back to you. But then I can get back to you with. With either the, the ascent or a counter offer or denial with your reason is just bad. But so make sure that if you, if you have that, you make sure you get that to me soon. Again, extenuating circumstances. If you do get hit by a bus before class on on Thursday. You have. Right. So that obviously I can make it adjusted accordingly. Again, once the reminder not to throw yourself in front of a bus to get an extension on a one. I believe the subsequent assignments are a good deal harder. So don't throw yourself in front of a bus for those either though. All righty. Any questions about. Hope you all have made some progress in the assignment. Yes. Yeah. In the part of the 15 points. Yeah. Because I have like a certain amount of sentences. I have my mind really chunked out like two sentences. Okay. It's fine. So I, what, the reason I have the things like that there is just because. What I'm most concerned with is basically, are you able to do meaningful analysis? I don't really care whether it's long or short. Some people are really good at doing like very terse but dense analyses and like they have, you know. Five sentences, but they really capture the gist of what they've done in those sentences. But some people, if I just say that some people tend to flail. There are certain students who really do need a kind of guideline. And so that's why that's there is like, you know, if you, if you need a benchmark of where. You've done enough, you know, here's a reasonable benchmark. If you've got this many sentences, it suggests that you would at least be able to put reasonable thought into it. I will of course read it to make sure you didn't just feed it to charge a pete to give me your answer. And because it is also good as generating, you know, fluffy sentences on end. So make sure that you're of course writing your own sentences, but that's what I'm really looking for is just strength of the analysis and whatever it takes to get there. That's important. Okay. Other questions. All right, let's continue talking about Adam. So you remember Adam from last week, Adam, he is a. A gradient descent optimization algorithm. So remember that Adam is not not actually an acronym. It's short for adaptive moment estimation. Yes. Oh, yeah, sure. How's that? Okay. Yeah. So Adam is short for adaptive moment estimation. And it has the benefits of being straightforward efficient with little memory requirements. And it's very friendly to this method that we've done. Basically stacking all of your, your weights together so that you can perform optimization on all of them at once. Right. This helps in our, in the efficiency of the. It's also appropriate for things like noisy or sparse gradients. And also the hyper parameters have an intuitive interpretation. What is that interpretation? So moments in this case is moment as in momentum. So if you think about moving down the slope of your gradient, if you're very far from the optimum and you're trying to reach the bottom, it may be wiser to move fast. Right. So if I can figure out that I probably have a long way to go. I'm just going to move quickly because I don't expect to reach the, the optimum any point soon. And so I'm not likely to start skipping back and forth over it. So what I'm trying to get to with SGD, I'm just taking these tiny, tiny steps with the goal of reaching the optimum and not, not going, you know, not going back up past the optimum. Adam allows you to be a little faster. So you can basically take larger steps when you're far away from the optimum and then slow down when you're closer to the optimum and do it in the smart way and not just having say a decay and learning rate where you start fast and are deterministically moving slowly. So, remember that the two key variables in Adam are the first moment and the second moment. So this is basically the average of the recent magnitude. So that is, if I've taken big steps recently, then it may be more reasonable for me to take a relatively large. Now, that second moment is also like, what's my rate of change over these recent samples so if I've been, if I've not been changing, probably not good means I'm probably not approaching the optimum. But if I've been changing relatively little, it means that I can still take big steps and I don't need to worry about slowing down anytime soon. If your first moment values are still large. Yeah. Right. How do these algorithms determine differences that they might be local or so it's global. They don't know they don't. They can get stuck. They can get stuck. So this is why I'm so the question for those of you and how do how do these algorithms determine distinguish local from global minimum. And the answer is they do not because this is where the data set balances really important. So one thing that happens commonly. I just didn't ask me about this this morning is that if your data sample is way overbalanced. And you have like if you're a classification problem you have three classes but you have 10 times as many members of class a compared to class B or C. And then your criterion is just like, if you're in training is just a accuracy or something. It's really easy to basically say, well, I can't get a better answer than just classifying everything as class a is going to give me 96%. And I can't really do much better than 96% for classes for three way classification tasks. So why not just classify everything as class a if I have 1000 samples of class a and 20 symbols samples each class B and C. Okay. So this can happen. And gradient descent or Adam or any optimization, however, then we'll happily let you do that. Adam may have the benefit as we'll see when you look at the loss charts of sometimes allowing you to basically skip out of a local minimum and then. Climb that hill and then find basically a steeper path of descent down to a local minimum. And so there is some benefit to basically allowing it to sort of overshoot what might be a local optimum. An SGD tends to, especially if you're learning to be very small, it'll find the local minimum and stay there because it can't go anywhere fast enough that what's you get out of that local minimum. So this leads me to sort of how do we, how do we determine the quote speed, which I can move through the gradient at every optimization step so if I've been moving very, very fast. I can probably continue moving very, very fast that is I can take bigger steps. If I've been taking big steps over say the past four training iterations, but the steps have been slowing down. So my rate of change is now going, you know, my, like the derivative of my, of my steps length basically or across the gradient is declining. It may serve me to still take a relatively large step this time but maybe make it a little bit smaller. What the suggests is that the gradient I'm moving along the gradient away the suggest that I'm still moving along the steep slope of the slope is getting slightly less steep. So maybe I am getting a little bit closer to my minimum. And so then I should probably slow down a little bit so that I'm less likely to skip out of that, that role. Okay. So naturally this is closely related to the concept of moment, a while momentum and physics so basically we're looking at the distribution of values around around a mean or variance right so the probability distribution of these values that you're sampling at each step. Okay, so what we have with these tunable parameters, these beta values. This is going to be a decay rate for the first moment estimates, and then beta two is going to be an exponential decay for the second moment estimates. And then we also have a constant here to make sure that we are not dividing the remaining division by zero. So we still have a learning rate right to this alpha row. This is the learning rate that you specified that's a constant. So I'll just say, I'm going to do Adam with a learning rate of point zero zero one or something like that. So the value is not going to change, unlike I can say some reinforcement learning algorithms. Talk about that in a few months. But here I have these other values that will say, I can perform some calculations over my first moment and second moment estimates that is sampled from the previous and training iterations to determine how I need to adjust my step size. My step size here is not a rigid constant, rather something that I can chew a little bit based on these don't use. Okay, so any any questions on the atom recap on go into implementation. Now, let's go with that. So to show this. Let's do what we have done in all this kind of work on a dummy example. So I will make some training data. This is going to use very much the same, the same formula that we use in the book five. This is basically the same function. So I create some evenly spaced values on the x axis and apply some function in this case the same function from the book five. So I'm just a non linear function with some noise added. Right, so I have, I'm interested in non linearities here by trying to have it model some variant of a sign function and making it difficult by adding some noise. So now I'm just trying to get my optimization to approximate this non linear function. I'll make some testing data, the important things that they resemble the training inputs. So I create my add ones column, or my add ones function that this will do is Jim just be a generic way of adding that constant column of ones. And I'll specify my high, some of my hyper parameters for my neural network in this case, the thing that I'm looking at is just what's the number of hidden units in my layer. So I'm just going to assume I have a single hidden layer. And I'm going to create a number of units in that that will be, that will allow me to introduce non linearities. Now, if I create 10 hidden. So I'm going to look at a number of two sets of weights, the in W right, we are the hidden layer weights so I take my inputs multiplied by those things, and this gives me some scalar values that are then apply non linear function to those now non linearized scalar values are then multiplied by the weights in W, the output layer weights to actually give me the output values. This is still a regression problems I'm just trying to fit you know, scalar values to scale other values. So now if I look at, you know, I do take the, the shape, basically just do prod of the shape this will tell me you to buy this case 10. So 20. What is NP prod. So this is just returns the product of an array of elements over a given axis. So in this case the array of elements was two and 10. It's just the shape. And of course there's only one axis. So now what I can do is just confirm this is correct by just checking these two are equal right one plus one times and hidden. Why is it one plus one. Well, we have one input. This is my input value and X, and then I have the constant column of the ones. Right. So there's going to be two inputs, one of which is going to be a bias. That's going to have a bias weight trained against that. And then there's the actual feature value that's going to have some weight training against that. Alright, so now I'm going to set the other parameters of my neural network so and hidden I'll set my, my learning rate in this case to point one. And I also just scale the learning rate in this case by the number of samples times the number of outputs. So you can do this to kind of optimize the learning rate before you start training to what you think might be at least a value in the appropriate range for the amount of data that you got. So now then I'll initialize the weights to uniform distributed values between, you know, is a normally distributed between negative one and the point one or negative point one and point one. So now let me print the current state of my neural network. Right. So I've got a learning rate in this case point zero zero five seems like a normal learning rates kind of in that range like, you know, one one hundredths down to 10 to the negative five or so. And typically, typical learning rate value. And then here my values currently of the and w right these were randomly initialized. So they don't really be anything right now. And they're also certainly not optimized to this to this function. So if you remember the pack function from earlier in the notebook on Thursday, what that does is it will take my different weights from the different layers and pack them all into a single vector so that I can apply optimization operations over all of them at the same time. And then we have the inverse of that which is the unpack function, I just have to supply the appropriate shapes that it won't unpack those arrays into, and it will give me the actual arrays back. So now I'm going to set my number of epochs I'll train in this case for 100,000. So I'll take these that this many steepest descent steps in the means where an error function. And then I'll do some sort of collection for for plotting. So, finally, this is the, the meat of the operation here so this should look familiar to you already so we have our inputs x one, multiply that by the apply a 10 h function to that that gives me z a pen and put a call and one's the front of that again, multiply that by w that's going to give it my final output. And then I calculate the error, right, just my ground truth target minus my actual predicted value, and then use these in the backwards pass so just a reminder of the pieces of this operation. So z is 10 h over the input, right, so this is going to be the derivative of the 10 h over the input. And so then the error, this is actually going to be the error term for this for the sample, I just have to transpose it to make sure that my weights are in the right shape. And then the gradient in w as much simpler, so this is just going to be the input z input to that layer times the error. I can pack my gradient values into a single vector using this this function, and then w which are my packed weights. I can just optimize all of them at the same time by subtracting row times the gradients. So what's happened here is I've computed the appropriate gradients for the different layers separately. Right, so I've done a gradient of V and gradient of w. So then what I do is I pack them into a shape that's going to be the same shape as all the ways, right, I basically have a gradient for each weight. So the gradient arrays are going to be the same shape as the actual weight arrays when I pack them, they end up in the same shape as well. And so now now I have a single array, I can do a single operation over them. So this allows me to be much more computationally efficient. It'll create error traces for plotting and the rest is just by plot. So, take a look at this. And now we can see it start to converge. Right, so you can see the train and test RMS E. So the train has converged down to a pretty low value and test is slightly above that. And after 10,000 samples. Basically, you can see that we optimize entirely, you know, pretty early on. Right, and then there's not a whole lot of improvement going on there. You'll notice a couple of things that's pretty subtle in this, but you can sort of seems a little hook here at the bottom of this graph right, this is one of those cases where, due to the atom. We use that here. This might be. We didn't actually use Adam here. Sorry, my bad. We're going to get to that later. And now we look at ignore everything I said for the past 30 seconds. Come back to that in a moment. This is actually just a weird, this is not nothing to do with the gradient is like, just a weirdness in the plot looks like. So what's the next thing here. So this is just going to be my actual predicted values and then according to the model. This is what it's predicted so it's maybe not very good. So, you know, we are the, the hidden outputs for each unit, right. So each one of these represents the output of this unit, depending on what the input is. Yes. This is just one layer. Yeah, this is this currently we're just using a single layer with 10 hidden units. All right. So now we're going to repeat this training loop using Adam this time. Previously, this is just sort of vanilla SGD as we've learned it already. And then the, the version of Adam so differences here. So same neural network right still 10 hidden layers or sort of 10 hidden units one layer. I'm not going to specify these other values so small epsilon just to prevent dividing by zero right tiny number. And then I have these two beta values. So let's just go back up refer to what beta one beta two are again. So beta two is the exponential decay rate for the first moment estimates and beta two is the exponential to carry for the second moment estimates. So in this case, these values are set to be. 0.9 and 0.999. So we can see that effectively I'm going to have a kind of a 10% decay for the first moment estimates and then a really, really small decay for the second moment estimates. Okay. And then row here I'll set to 0.001. So same thing as before, I'm just going to initialize my weights randomly. Now I'm going to specify these two other things empty and BT and then beta one T and beta two T. What are those we'll see those in a moment. So these are going to be bias corrected moment estimates that I'll use to basically update the those beta one and beta two values. So, up until this point, with the highlighted code, this is the same. So standard operations input times weight supply of 10 H, at a call of ones, multiply that by output weights that gives me the value, then take that value subtract it from the target, because we error use those error values to make the backward pass. All right, so now I need to approximate the first and second moments right these are going to be estimates about how fast I've been going down the gradients so far. So think of it as like, you know, momentum and acceleration or something like that. So what I'll do is I'm going to look at the gradient. This is going to look at the gradient, the error gradient with respect to W, because I want to look at how fast I'm kind of getting down. Sorry, this is all them together. This is the way you so I'm going to want to look at how fast I'm moving down my gradient defined by all of my weights. And so what I'm going to do is I'm going to take beta one times empty so the kind of existing movement estimate moment estimates. And then incorporate the decay value. And so remember when the first moment, the is kind of the, the mass quote unquote store the center of mass the center of product. So, the center of product distribution. And then V is going to be the second moment of the variance. So I'm just going to take the square of my gradient. And then I will use the beta two value to optimize that VT. So now I'm going to use these beta one T and beta two T's to basically correct for bias. So this is going to be so beta one T is just going to multiply that value by the currently the pre-c calculated or the set value of beta one. Do the same for beta two, and then and then we had or going to be these bias corrected estimates. So in this case, I'm going to take empty divided by one minus beta two T and then now obviously for for VT. So now how do I actually perform the weight update so the formula slightly different here. So you still have the same components, except instead of the error, I'm going to be updating based on these moment estimates. So kind of not averages but derived from the previous end set of updates. So again, if I've been moving really, really fast my errors really, really large. I can just find taking a bigger step. If my error is pretty small or seems to be moving in that direction, I should take a smaller step or I should take a least or less big step. So what I'll do here is I'll take m hat divided by the square root of the hat and the epsilon just in case the hat is zero for some reason. And then I'll multiply that by row, and that's going to be the amount by which I update all my weights. So, store in the error trace, and then I plot, I create some testing test data and evaluate it using the foreign pass just in a single line so here. Remember, so excess one. That's our inputs testing inputs multiply by V that gives me Z, or just say a apply 10 H that you get Z add ones to Z so this is going to be the same as Z one above. So that gives me why. And then finally just plotting us before. So let's go. So what do you notice comparing this with Adam compared to SGD, but the loss curve yeah. Yeah, I didn't really they sort of I mean they individually converged right this and they sort of be kind of plateaued evaluate, but it's maybe not quite as neatly aligned or you see here. The test RBC is actually lower it first in the train, and then around 10,000 epochs they tend to switch. What else do you notice. Yeah. Yeah, the train here so I mean we may we might be working to the train data a little bit. But what, what does it just notice about the shape of the curves. The atom one is much more. Give me word abstract or just I was going to say just like bouncy or something. It jumps around a whole lot. In particular we see that it's not usually with SGD you see a nice curve from a high value to a low value and it just sort of reaches some minimum value intends to stay there. With Adam. Huge drop. Slite drop. Bigger drop. Oh, now we're going back up again. Go down again. Okay, now we start climbing. It sort of looks like, you know, when I'm running a marathon I like to look at like the elevation map beforehand and this sort of right here looks like that looks like that part that was put it like my old 29 or my old 29. That's just like hell because I'm two hours into a run and something that may be climate hill. So basically what's going on here is that this is skipping out of the gradient. Right. So there's some minimum that it's encountered here and it might be a, it might be the global minimum might be a local minimum. And it's moment estimations are such that you're multiplying that by the step that you're taking across the gradient. And we say move down the gradient because the goal is to reach this minimum. But the gradient is just some, some surface in multiple dimensions. And I just sort of keep moving in the steepest direction. If I'm moving the steepest direction from where I am now, if there is sort of, if I'm moving in this direction, there's suddenly a big hill, or maybe even just a little hill. So let's say my slope here is very steep. So I'm moving down it very fast. And then there's a slight upward hill in this direction. So my step size is sort of from the tips of my fingers. And I'm starting here where my left hand is, then I'm going to be taking a big enough step that basically lands me over here, even though the minimum is somewhere here around where my belly button is. So effectively by taking a step that big I have kind of skipped over that minimum. What Adam is hoping to do effectively is that it's hoping that maybe this is a local minimum. And by moving past it, I can get on a trajectory where somewhere further along, I will find another minimum that maybe you will lead to the global minimum, if one exists. So here what seems to be happening with this data and if I were to run this again, even with different numbers. This might have well changed is that there's a minimum in the gradient of the training data that we are able to find. So we get practically down to zero here. There's some fuzz here at the end that suggests like maybe we're taking a really small steps back and forth across some global minimum. For the testing data, remember that every the gradient defined by every data set is going to be different, even if they resemble each other. And so the minimum for the training data that we've calculated is maybe not the best one for the testing data. So perhaps the minimum, the best minimum for the testing data is back here somewhere and we actually kind of found it, but it, in the training data, there was still somewhere else to go. So perhaps it was not actually able to find that. So, Adam has some advantages and disadvantages. But you know, in this case, we look at where our error curve is relative to the training data is doing really, really well. So questions. Yes. So if you wanted to get assuming that that did. Yeah. Yeah. It kind of changes to better. It's not found out of it. Could we change the beta? Yeah, so you can, you can change the betas. You can change that exponential decay rate. You can change a number of things right you can change. The betas here you could change the learning rate itself, right, how much are my scaling the whole thing by maybe I just. This data starts and I should be taking smaller steps overall. I can even potentially just train the number of change the number of training epochs. So like, I only, I can only see this after the fact but it sort of seems like maybe I hit that minimum at 10,000. Maybe I should have stopped training there. You can use some techniques like early stopping your patients, where I can see. So if I use say my validation accuracy as my criterion, I can sort of say, if I don't see an improvement in my or validation law or error in this case, if I don't see an improvement my validation error for like 10 epochs or something. I'm just going to say, I'll stop now because I probably have not been getting this. So the number of techniques you can use the problem with this is that this dip here is in the, the gradient for the test. And I'm just plotting this for comparison when I'm training, I do not have any notion of this. Right. So I'm kind of trying blindly I'm hoping that training against this data. I'm going to be encountering test data that resembles this closely enough that this will be a good model and here, even though we kind of generate from similar formulas. It seems to be a decent feeling it's not like the RMC values are huge or anything, but maybe not as good as it could be. Other questions. Yeah. Take care. So what's the small, because you can see the represents there without having to have that. Yeah. And then we have to worry about this. Yeah. So basically here, what we're looking at this. This is in case the hat ends up being zero. So that is your movement. This is the variance of your, your second moment estimates. And so when would this be zero. This would be zero if the past and moment estimates that I'm sampling are all the same. Right. This would, this would indicate a couple of problems. One, you're not converging. You're sort of your, you've gotten the exact same error value for the past and epochs. But that might be the case you may sort of plateau over a bit and then, and then sort of find your way out. So in the case that like, you're going to be. Plus epsilon and you're going to be dividing by this. So let's assume this is zero. Right. So this was zero. It would mean that I am kind of getting, I've been getting the same error for the past and epochs. So what I'll do is I'll add some epsilon and then divide by that to kind of hope that the first moment estimates can just be enough to sort of shake it out of this, this run. This can allow this can cause us to kind of bounce out of that gradient. So, for example, if this value is too small, you might end up sort of taking a big leap. It's like I'm not moving. I'm just going to like take a leap of faith. And I'm just going to like jump way out there and hope I land on a favorable part of the gradient. So you don't want your epsilon here to be too too small in this case, because you might have situations like that. You don't want to be too too large, of course, because then, you know, you, you, you, one, don't want to just, you cut your, you're moving estimate down to nothing because that also would stop training. But then you also don't want your step to be too small to be effective. So again, this is sort of one of those hyper parameters in the Adam paper. They found kind of what they found to be best values for the betas. And I don't recall if they found the best value for the epsilon. If I, if they did, I should have written it down and get to the part of the notebook. Other questions. Okay. So, finally. Let's take a look at this. If you remember. The model for. SGD was sort of just a straight line. Right. It's the green line is just kind of straight. So it sort of optimized. It didn't really make use of the nonlinearities available to it. So it's just sort of optimized straight line through the data that approximated the correct slope. Here we can see that the blue line is the, is the training data. The green line is the model that falls it very closely, which is also witnessed by this very low RMC value at the end of training. So it's doing a really good job of fitting to the training data. And it's, it is getting the general shape of the curve of the testing data, but not as well. So you mentioned overfitting in here, this might be a case where we are fitting very, very closely to the training data and not fitting as well testing. It's not like it's a bad model. This is a pretty simple case. It's still getting the overall shape. Pretty well, but if I'm looking at training versus testing, then it's definitely doing a much better job training to the training data than this to the testing data. And then finally, here are the actual hidden outputs for this. So, okay, questions on on Adam. Yeah. Yeah. Yeah, so you. Well, I guess the obvious one is like it's simpler to implement. So if you're just getting started, I would suggest mastering SGD before moving to Adam. Also, it does have. SGD is a little bit more deterministic. So when you're in assignment two, for example, I believe you were asked to compare and contrast SGD and Adam and you'll see like the Adam loss curves you get these kind of more stochastic things where they bounce around a bit more. And then you're saying that you may have these kind of spikes where you are popping out of some minimum, and there is a small chance that you sort of get off on a wrong track. And you're training actually, you know, we'll sort of collapse after that point. So SGD is like a somewhat more reliable, assuming all other conditions with regard to the quality of your data or true. Either to implement has fewer moving parts. So, you know, one of these things, these are things that you can all try, like when you have, when you have a different different types of data, you can try different types of optimizers. Adam and its variants are extraordinarily popular. And so basically, the everything is built on an SGD backbone, but SGD is kind of it's a bit pedestrian right everything goes really slow. And so you're just taking very slow deliberate movements along this gradient. And so it's not good for really big tasks. Adam will allow you to converge faster at the cost of maybe a little less predictability, and some risk of kind of going off into the woods and your training failing once in a while. Yeah, other questions. So, you know, Adam variants are used everywhere. So most of the large language models are trained using something called Adam W says Adam with weight decay so you actually specify a value where weight has been updated in a long time. It's kind of assumed not to be important. And so then it's value will kind of attenuate. So I guess the corollary of that is basically the larger model you have for a given task, the higher probability there is that some subset of those weights are just irrelevant. So if I have a trillion parameters in my modern model to do, you know, diffusion or language generation or something like that. I probably don't need all those billion parameters some 10 million of them might be correctly optimized or could just as easily be zero and really wouldn't change the performance. So this allows us to that property sort of allows us to do things like fine tuning where I can assume that for a different task. Maybe I can use those weights that aren't really being used and better optimize them for task performance. So these different optimization techniques allow you to kind of leverage different properties to neural networks. All right, so it's now 237 so I will start the next notebook, which is going to be on finding good parameters. So, controls go to number seven. All right. Let's start this. So, optimizers data partitioning and finding good parameters. This is notebook on a couple of different topics, all of which are going to be important for doing assignment to, which I'm currently planning on assigning Thursday. But basically optimizers are these fun these operations like SGD and Adam, right. How am I actually managing my movement along with radiant in order to try and better fit to my data. So when I talk about the optimization function, this is going to be, you know, I've used SGD or I use Adam or I used our best product or use Adam W, etc, etc. So, I guess when discussing Adam, we just discussed how to create these single vectors of weight values. And so we can also view parts of that vector and find the weight matrices for every way of neural network so I can say, I got my entire weight vector. So, I have my, my update vector, and then just by performing single operation, I'll be able to update it on my way. So, so much more computationally efficient. So, here is a function that will do the, create the views on a weight matrix automatically. So let me just create a random sample. So, I'm just going to create this, this just this object. So, and actually view the values I have to slice it. So, now I can see, instead of a three by three sample I just have nine values. So, I'll define this make weights and use function. So what this does. The shapes of the different weight matrices. So say I've got, you know, weight to the end weights w and maybe other hidden layer weights of pre specified shapes. And so then I will take, I'll take all of those and then stack them into a single vector. So this gives me something like this array. So, it was great this all weights vector. Then I will, in this case, I'm just initializing it with some uniformly distributed values. And then I'm going to build this list of views. Remember, review and Python is just a shallow copy. Right. So if I change the value in the view, I actually change the value in the original, as opposed to a deep copy which creates an actual separate place in memory. So here what I'm going to do is by creating the views I can then change the value in the sort of view, shallow copy. And that's actually going to change the the original weights. So what I'll do is I'll reshape the corresponding elements from the vector of the all rates into the correct shape each layer. So for every shape in this list that I've passed in, I'll create basically a view onto the all weights vector in that appropriate shape. So now this allows me to treat V and W as separate objects. But if I modify my foot V and W objects, I'm actually modifying my all weight vector. What that what that does that is that when I actually get to my training step. And have my my update vector, and then just apply everything every operation to every element of the weight vector at at one time. So now this allows me to keep things organized in that I can just see like what the values of say V or W are. If that's all I'm interested in, while still maintaining the computational efficiency of a single operation at training time. So, you know, let's take a look what the shapes of the weight matrix is to be if I had a neural network with two inputs to hidden layers with 10 and with 20 and 10 units respectively and a single output. So I'll build that. So what I'll do is I'll specify the number of inputs and in number of hidden units per layer, the number of outputs, and then I will initialize an empty list that will store this so for every, for every hidden layer. What I'll do is all append one plus the number of inputs right the one because of that bias column. And so it's going to be one plus one plus number of inputs by the number of units in this layer. Right, so by two inputs. This is going to be one plus two so three by the number of units in the layer first bonus was 20 so we get three by 20. So then the next thing I'm going to do is all. So then for each one, right, if I three by 20, and then one plus 20 so 20 is what comes out of that first layer. One plus that is 21. And then we just have a single output. And so then what I will do is I will append you know one n and plus one by an H, and then I'll set n into an H because then H is the number of inputs to the next layer. Right. So I can just do this relatively. All right, so then I'll make weights and views. And so now these are like all of the weights right so you can see that this is these are. What's this 20 by three by 20 and then. 21 by 10 maybe, and then 11 by one. All right, so let's make some data with two inputs per sample and single target output. So what we'll do now is we'll have our target values be X and Y coordinates, and then we'll make basically a kind of a terrain map. So we'll have, I just have like a square area. And I have an X and Y coordinate and then I want to create some hills by specifying the Z coordinate is the output. So what I'm going to do is I'm just going to create like the surface, where my inputs will be say latitude and longitude, and my output would be like altitude or something like that. So what I'll do here is I will specify some centers. So this is like in my 2D plane. This is where I want the centers of my hills to be, and then the heights for each hill. So 2D coordinates so we can look at this as like my input would be 2 by 2 and my output would be 5. Right. So this is these are coordinates. And then this is the height, whereas for an input of 5 and 4, my output will be 4. So you can see here just by looking at these, you can see that this is like a highly nonlinear in that I have the same output for or 5 for 2 different entirely different values like 2 and 2 and have the same output and then 5 and 4 and 3 and 7 also have the same output. Right. So very nonlinear function. So this should be something that I wouldn't require a neural net to actually predict. So I'm going to be able to find this calc heights. So why am I doing this because I don't want to just have like these really sharp and everyone have a completely flat plane and then like one point in the air, they want to actually have a surface. So what I'll do then is like for every point, I want to be able to take in a value that's not one of these inputs and calculate the appropriate height based on what's soon you know, a circular hill with the and even drop off from the from the peak. So I'm going to be doing this mesh mesh grid function. So what this will do is it's going to take, you know, take coordinate vectors and return coordinate matrices is going to create and the coordinate arrays for vector evaluations of and the scalar vector fields over So that is basically taking these numbers here and turning them into a nice even surface. Okay. So what I'll do is now I'll create my surface. So I'm going to have to even these space lines. So I'll just have an x axis and my other x axis in this case just by two horizontal surfaces. And then I'll make a grid out of these two arrays. And then I will allow now printing x will allow me using the mesh grid function, allow me to show the coordinates of every point in the to be greater. So if we take my 20 points, I have a point at zero zero and also have a point to like point five zero and so on and so on until I get 10 right so I've got, you know, zero, you only distributed numbers until 10 and then like the next row zero through 10 next row zero through 10 and then a corresponding column at each of those evenly spaced values. So now for each of these and I have like 400 points for each of these I now want to apply the calc heights function. That's going to give me according to the hills and centers that I specified previously what the what the values for each of these points would be. So this allows me to take now not just like five points, but 400 points and create a relatively smooth surface. Any questions about what I what I'm going to be doing here. So now running the calc heights function given the centers. Right, this is going to be the height at each point in the grid. So if I have this should be 400 by one. So remember this is H right H is 400 by one. X was the 400 by two array here. So if I ran H stack by stacked X and H together, what would that give me. First of all, how many columns do that. If I stack to X and H so X has. So, so this is this is H right here. Right, this is X. So if I was on to if I stacked H and X side by side, how many columns do I have three. If I now have three columns, what do you think say a row of this stack array represents. That's why he right yeah so this should basically be the 2D coordinates on the ground, and then the height of the hill at that point. Okay. So now you kind of get a sense of what probably this is going to look like that's actually visualize this. So I'll use, you know, just some some visualization tool kits, just using axes 3D on the plot these three dimensions, and that gives me something that looks like this. Right, so I should have 400 points. Basically, these are my two horizontal axes, and then these these peaks. These are those five hills that I specified. And it's kind of smoothly interpolated the surface between those. So now you can see the highly non linear nature of this function. So, um, all right, no, I'm just going to play with the lighting for a little bit so you can make this look a little bit cooler. Using light source. So this is a library that basically creates a light source from a specified point and it'll render it. Render the surface on in sort of a really nice way. So now if I put a, if I kind of take a point light like over here, and shine it on my on my surface. It looks like that. So it's just a little more like a landscape. All right, so now we can make some data. Right, so the whole point of this. You can use these cool visualization tools and it'll make your projects and your assignments nice. But the whole point of this was to actually try to fit to this to this surface. Right. So let's make some data. So the access is going to be these points on our base plane. These are going to be the inputs. And then the target values for T is going to be the height of those points. Right. So for every point in this on the surface, I should be able to calculate a height for that, because I've already created a smooth service. I've got this Cali kite's function that should be able to take in a number of a tree number and give me the height of that point on the surface. So, well, I'll then do is I've already calculated Z, right, these are those are the heights for each of these. So just make those into my target values. Now, this is the thing I'm trying to predict. So, if you look at the shape of this data, I now have 1600 points in my inputs. So these are going to be 1600 by two, right. So this is 1600 XY coordinates, you can call them. And then 1600 associated Z coordinates. So now if I just have these stacked together their XYZ coordinates if I slice off that last column and make this the thing I want to predict, I can set the stuff really nicely as input to a neural network that has two inputs, some a miracle occurs in the middle and then there's an output. Right. That's what we're after. So we observed in the previous notebook that there may be a tendency towards some overfitting. So, who can define overfitting for me you've kind of alluded to this term and I'm sure many of you know what is overfitting. Yeah. So, I'm sure you've got to create a model that might perfectly fit your training data, but I want to fight to any other data. Right. Yeah, so we have overfitting is where the model is really good at optimizing into whatever patterns it finds in the training data. And it becomes so good at that that it's not good at anything else. And then you're kind of like, you, you train for years for use of running metaphor again right you, you, you say in bold and you like train your whole life to run the 100 meter dash, but you fall apart when it comes to marathon, because although there's like a superficial resemblance. You know, you fall like the thing that you've been training for so you say in bold overfit to the marathon. And you keep chugging the guy who will like one that has the world record marathon over if it's to marathoning right and neither of them is going to be good at the other sport. They might be better than the average person that either, but they're not going to be particularly good at that other sport. So it's also like, you pick your metaphor it's like taking a football player and hoping and assuming he's going to be like an Olympic league swimmer or something like that. Yes. One thing better. Sorry to say it again. You have a neural network that was better at those design. To like, they go, I think one thing, but better detecting everything. So that would be a very, just to repeat the question if you have a neural network that's like, was that you designed to do one thing that ends up being better at something else is a question. That would be a very weird case. Yeah, so you could argue that that is maybe a kind of overfitting, but it's like, it's sort of something must have gone wrong during training and that point. Overfitting is not necessarily. You try to use a model of the design for one thing to do something completely different. Like, yeah, sure, it overfit to the data compared to this other thing that doesn't resemble the training data at all. But you can't assume. It's like that that would be like trying to use a hammer in place of a saw just because they're both tools. Right. It's like, you have different tools in your toolkit. I do not expect to be able to chop down a tree with a hammer. It's going to be very, very difficult. So those are design choices that you can make. And like, if you assume if you design a network, assuming that you're going to do one thing. It's just, it's not fair to apply it to a different thing. So you see like, you know, that kind of bad faith critiques of some, some, some papers like, well, your network doesn't do this. Well, it was never designed to do that. So this is not the problem I was trying to solve. You're really not making a fair criticism of this right there. Probably other things you criticize it for, but that's not it. But that being said, generally the goal for neural networks, if neural networks are universal function approximators, that is, if I'm assuming that there's a function that maps for my input to my output, and my job is just to find that function. The neural network is a universal function approximator, meaning that with the right combinations of non linearities and layers, you can approximate any function and principle, it just might be a really gnarly function that takes forever to approach. Nonetheless, you can do it. So the goal with neural networks is that always some level of generalizability. So what we do, we don't want to have it. We don't spend all this blood sweat and tears in training this neural network that does just one thing on one data set once. I want to be able to reuse this, at least somewhat. So one thing that I can do is make sure that the data that I'm going to evaluate on it's never been exposed to before. So this was something that we kind of have slipped under the radar with assignment one in that you don't have to do the train valid test split, because the data is friendly enough, it's actually cyclical if you look at its temperature over a year so like the last date is very similar to the first date. So it works nicely there. But generally speaking, you cannot assume that's going to be true. So what do I do there. I want to create these train validation and test sets. So what's the role of each one of these obviously we know what the training data is for this is what you actually fit your model to. And the test data is some unseen data that I've never, that the model is not being exposed to. And this is what I want to actually perform well and this is where I'm going to basically prove that my model is good enough for approximator to this thing that it's not seen before that's what makes the argument that I've actually approximated that function that I'm searching for. Well, these things involve a significant amount of computational power, the examples that we're going to be using are not really at all that big, but we're all familiar, you know, with, you know, the large language models large vision models. Those take forever and neural network training has taken forever for a long time. One of the reasons that it didn't take off initially was that it took a mainframe the size of this room to do simple digit classification. And people are like, okay, this is a great party trick. I don't see what it's actually useful for. Well, now we have the technology and the tools to speed it up, such that we can actually use them for real things, but still often takes a while so again, I don't want to sink all this effort into training the neural network and then run it on my test data and find out that it completely collapses. I want to have some reasonable some reasonable assurance that I'm going to perform well on my test data. So what do I do for that. This is where the validation set comes in. The validation set is an extra set carved off your training data that during training you continually test against. Right, you're not training on this data. So you should not over fit to this data, but you can check your model against that to see on this other unseen data set that is not the test set. So I can use this to find things like good hyper parameters like and kind of see like, is my learning rate too high am I skipping over my my optimum the gradient. Is my neural network the right size do I have the right number and appropriate number of non-minarities in it so these are the sorts of things I can I want to be able to do. So I want to put that nice property of the training validation and test set to resemble each other. Right, so if I'm training on this hill date I need to get like a roughly uniform sample of points on this mesh. Right, I don't want to validate only on this corner. This is not going to give me an accurate picture of whether or not my model is fitting to the rest of the data. So, effectively what I'm going to do is I'm going to train on some points on this mesh. I'm going to validate and other points on this mesh. I'm going to test on further points on this. So that's what I'm that's what I'm looking for. So what I'm going to do then is we're going to shovel the samples into a random order. Right, we did this before. I'm going to partition the data into and fold. So I'm going to say, I want to create and sub partitions of this data. And then I will assign the first fold to the validation set. The second folder the testing set and then the remaining folds into the training set. So, generally we want to have both to your data and the training data, which will have a substantial enough sample in the validation and test that you can be reasonably assured that your model is going to perform. Okay, on that data. So what I'll do here, this is just going to do the, the proceeding in code. I will shuffle the row indices, we did this already in one of the previous notebooks. I'll specify the number of folds here I'm going to do five. And then I will divide my number of samples by my number of folds around down to make sure that I always have an integer. And then I will accumulate those different folds into the different samples. So like, I'm going to, in this case, just by convention, I'll take that first folder to make the valve set second folder to make the test set, and then the remainder make it the training set. So now let me print out the number of folds, five, the number of the number of the number of the first four, and then how many samples in each one, right, so 320 by two, two inputs, and 320 by one. And then I'll put. Okay. So now what I can do is I can specify which one of these I want to use for which fold so the. So the x x validating T validate is going to be fold zero, right, there are two elements here. The first one of which is going to be the, the training and the second of which is going to be the targets. So then I'll do the same thing for the first fold this is going to be the test or the second folder to the test set, and then I'll just stack the rest together into the training data. So now if I look at the shapes of each of these, the training data is 960 samples, each of which has two inputs and then targets. And then I have 320 samples each in the validation of the test. This is a pretty generous the size validation and test set in that the validation and test set or like one third the size of the training data. In this case that's okay, because there's like not that much noise in my in my data that I'm trying to predict. But the size of the validation and test set is another one of these things that you want to be judicious about choosing when you're when you're performing training. Okay, so. Now I'm going to basically run a solution to assignment to that you won't see because I've saved it off previously. But we'll see how we can actually use a neural network to fit to that that hill data. So what you can do, you're not required to do for assignment to but once you complete the neural network class definition you can save it off into a file. So you can do a neural network.py and you can reuse it. Right, so assuming that you perform well and assignment to you now have a implementation of a neural network that you can then just reuse indefinitely. Okay. And then later when we are through with assignment to I will actually just give you a neural network implementation to use another things, just in case you know you're not so you're not relying on a potentially buggy implementation of a to. So I've got this neural network to p y file import that. So now let me look at just the size of my dimensionality of my inputs right two samples, and then a single output for each sample. So now what I'll define is the actual, the hyper parameters of minor network the actual architecture so the inputs, the input layers always going to be the dimensionality of the thing that you're measuring. So two samples or two inputs for each sample, there should be two things. Right, two nodes, two inputs to the network. This list here this all just specify the hidden layers. So 10 units and five units. This could be as long as you want so basically the way that this, this neural network class is written is I can just add numbers to this list and it will automatically create a new layer of that size. And then finally I need to specify the outputs and of course the output is going to be how many things am I trying to measure. Right, so I'm in this case I've just got one thing that I'm trying to measure the height of the land at that point. So it's going to be single output so and then the default here, the is the 10 inch activation function. Because we have not talked about others just yet. All right, so now if I just print the neural network. This is sort of in my my non pie neural network implementation. This is my architecture. So I've got two inputs, one layer of 10 units one layer of five units one output of the 10 inch activation function. When you're using libraries like TensorFlow or PyTorch, they do come with a handy print function where you can actually print out the network architecture and see, you know, the sizes of the different layers. And then you can see all the fancy things that you can do there like different activation functions and residuals and whatnot. So, I have to find the train function that is much like what we have done previously. So again, a generic one that takes in the inputs, the targets, number of the approximate train for learning rate and then method is just the optimizer so you're not using sgd. So let me run it. You can see it's working. It's pretty fast. This is not a very simple, not. This is a pretty simple problem. So my error after 10,000, 20 epochs ends at 1.19. So I can experiment with a couple of other things, right, so I can try and see what happens if I use Adam instead. Right. So in my implementation, I, I've got a way to basically just pass in which type of optimizer want to use it will use that method. So let's explore using Adam instead. And we can see, you know, first of all, here we can see firsthand one of the benefits of Adam so in both cases I've trained for 10,000 epochs. But I've even in the first 1000 epochs of training with Adam, I got lower error than the whole training for 10,000 epochs of sgd. Right. So this is one of kind of a tangible demonstration of the benefits of Adam. Training takes about as long as this case, it takes about 2.61 seconds. And these little spikes here, right, that's the sort of skipping across the bottom of the, the optimum. So it's getting there and maybe trying to find its way out, finding that that's not a good way out, going back down and eventually just sort of settles there. And maybe is a little bit of movement. One thing you will see when you do assignment to is that Adam optimization is a little more approximate. And basically there's a, it's kind of a toy test function you can use just approximate the minimum of a parabola and sg will get you there exactly with the value that we know that that that is the minimum of that parabola. And Adam will get you close, but not quite close enough to be useful, but not exactly. But I guess close enough to be useful but not exact is like a pretty good motto for most of machine learning. Last thing we'll define the RMSC function, you're probably familiar with this already. And then I will, that I've defined my use function and I'll just apply that over my training data, and then just print the shape so basically here this is giving me the actual output so this is being stored and then put the shape, and that's 960 by one. So over the x terrain right 960 samples each of two inputs through the neural network, and then it gives me an output for each of those. So now let me print some, some values. So what I'll do here is I will print the RMSC for the. The training data, the validation data and the testing data. So basically what this is doing is I'm just computing the RMSC for my target my training targets, and then my predictions so and end up use of X training that's going to give me all my predictions. So now we can see that I've done a pretty good job at splitting my data such that I'm training in a way that is allowing me to get reasonably close testing or validation and testing error. Yeah. So you can do that. This is this is something I guess I, I guess I admitted that here so this is something that you can set as a as a criterion in kind of tensor flow or pytorch. Here we're not really doing this is sort of homebrew implementation. You can set it up so that you're. You can perform continuous checks against the validation remember the validation data never hits the actual neural network itself, except for you just sort of passing the use function passing through the use function to add certain every given step to see what the data is all my doing. So you can set it up to do that and print out you know every and epochs what's my validation accuracy or error or whatever metric I'm using based on the state of my neural network right now. So you can think of what we're doing here at the end is I'm just doing a check on the validation data. So what I can do here is if I am not sure, like, so you're not supposed to touch the testing data until you're ready to apply it. So what, what often happens is, you know, evaluators will keep like a hidden testing set, and it's just like you don't get to see this. You have to write your neural network the best you can. And then you send it to me and then I will apply it until you did. So we're not doing that but you can imagine that we did. So testing data is off limits. I want to know how I'm doing. So I will check against the validation data and if this value is lousy. So if I just print these two, and it's like, okay, instead of point one on three, it's like 10. Okay, this neural network was wildly bad. Something's wrong. I need to like add more hidden layers I need to add use a different activation function, whatever it is. And so then I can try and change the hyper parameters of my neural network so I get a lower value. Yeah. Like, we have to go to our implementation, the like optimize the hyper parameters, or is that something that you'll provide that like great search. So, it varies in the so if I remember correctly in assignment two you're given it's like assignment one you're given template code. And you have to fill out like trade and use function and something and you have to change the optimizer to change from SGD to Adam. So, you know, in assignment three you have to do that in some sort of grid search. Okay. All right, let me, let me get through this in the remainder of the time I think we're almost there. Okay, so we trained right it seems like we're doing a pretty good job. Let's actually try to visualize this so now what I want is I want to be able to take my my surface. Show all my training points, show my validation points and show all my testing points. So let's take a look at that. So the blue points are trained the yellow points of our vowel and then the green points are test. And so we can see that we seem to be doing a decent job. Right, so the training points. It's kind of it's a little bit difficult. I don't think I have this. No, it's not set up to rotate. But you can see the training points are like very closely fit to the surface. And the validation points and testing points are mostly to there's a few like here's a testing point that's kind of maybe not so close. So it seems to be doing a pretty good job of being able to predict the height of the surface from the the XY coordinates. All right, and then finally, let's see if we can visualize what the hidden units have actually learned. So previously, we had these lines saying, okay, for this x value, my hidden unit is outputting this value. So I'm going to see if we can do something for for this data. One other thing to notice like as we get close to the edge here you'll see how we kind of see these these dips here. So this suggests that basically it's not very good at optimizing for this local neighborhood probably because of a lack of data off the off the surface. Right, we have no points here. And so it's kind of continuing transit may have observed from this direction, leading it to be a little bit lower than the actual value. And so it's not a reasonable prediction of like what might happen if we extended the terrain in this direction, but we don't know. All right, so visualization. So this is, I'm not going to go through this code in any real real depth. This is just for visualizing the outputs of the hidden layer three dimensions. So this is what we get. So you can see now, how this is not necessarily very interpretable per se, but we will tell you for each input what the unit and every hidden layer is putting out. And this might be useful for, you know, if you want to trace the path of like a single point, right, we want to figure out like why is this testing point down here. And then you can actually calculate what each hidden layer is outputting with a network of this size. This is like reasonably tractable like you could probably do this math if you were motivated enough. The black box nature of neural networks just comes from having it happen at scale. Right, if I have 10 or more, you know, dozens of hidden layers each with thousands of units. And that becomes really hard to trace you know what's going on with a single input. All right, last thing. We need to examine the effects of various hyper parameters so we can very different things right we can try different lengths of training different hidden layer structures. Different different optimizers etc. So you can try all these things. And each of these are called hyper parameters right the parameters are the weights. Right, so my neural network is a function or combination of functions parameterized by weights. So what I'm trying to solve for are those parameters. So those are the coefficients, the hyper parameters are the things I actually have direct control over. So I don't go in my neural network and tune all 1 billion of my weights. What I want is a function is going to let me do that automatically. And in order to achieve best performance. I'm going to be looking at those things that actually have control over such as the learning rate training time model architecture optimizer etc. So the hyper parameters are just the property of the architecture that you actually have direct control over. So now is grid search. But I can say that for different combinations of learning rates training duration layer sizes and optimizing and whatever else you share your care to examine. I'm going to instantiate a version of the neural network with those hyper parameters train it see how it's doing against my validation or my test data. And then from that I can decide which one I want to use so the role of the validation data is really is really key and trying to find the right hyper parameter combination so if you assume the test data is not to be seen until testing time. So I'm going to be looking at the level on my test data that's cheating. So instead I'm going to tune on the validation data that I'm just assuming is reasonably resembling of the test data. So in this case, this is similar to what you're going to be doing I think in assignment three, except that's classification. I think maybe. What you're going to do is you're going to try a bunch of different parameter combinations and then try to observe trends like what happens as the learning rate decreases or what happens as I train for longer, or what is SGD doing, you know, worse than Adam and by how much. So due to time I've already run this I'm not going to go through this again. But what I want to look at are which parameter values are best. So I can plot them. Right, I can just plot the RMSC, but that's not really helpful but I want to look at is every plot with respect to hyper parameter values. So, oh, crap in there. Well, a bug happened. But if we go to one on the calendar should give us a version that we can look at. Two minutes. All right. So, all right, so now we can check out the different hyper parameter combinations so for example, I'm going to look at what happens that to the training validation and testing when I use the SGD optimizer. We can clearly see that this is a larger architecture training for longer, and that seems to work best. Right. Whereas for Adam, I can see that I still get that with the architecture size seems to matter more. Right. So here I have 1000 epochs and 5000 epochs with the same architecture. And I don't get that much improvement when training for 5000 epochs. So this suggests that if like I'm trying to optimize for compute time, maybe Adam is going to give me the best bang for my buck, which we observed already in that previous notebook. And then another way to look at is just put it on a pandas data frame. And then we can actually compare each one so I can see, you know, where do I see the low RMSC number as well. And I see them with Adam more than SGD. And I see them with larger architectures. So it seems like the things that are most important are going to be the architecture size and the choice of optimizer. Right. And so once I've come once I've got that, I don't get much benefit from training for much longer. Right. So I get a minimal benefit from training for 5000 epochs versus 1000 epochs. Overall, yes, the best thing to do is just to train the hell out of it 5000 epochs with Adam on this big architecture. Yes. This is true. Right. Yeah. And so that is true. What's the reason for that. So if you look what's on the Y axis RMSC. Right. And so we also see that these two, like the best SGD performance is not at all comparable to the best Adam performance. And then the best SGD performance. So if you were to plot both of these on top of each other, we basically see like SGD kind of up here. And then Adam is really kind of showing how strong it is. So this is sort of a method of deciding what are the best type of parameters for me to use for this data. Right. That is all for today. So assignment one, do on Thursday, if you need an extension reminder to get that in by your request and by tomorrow or my consideration. All right. We'll see you Thursday.