 Yeah. Yeah. Yeah. It's like the number three, bed hospital in the nation, or maybe it's even the number one now. Yeah, and I get a 20% discount as an employee, which is like, it's a lot, right? But you consider how much, how much treatment costs. Okay. So we connect to. Yeah. Yeah. Yeah. Yeah. Okay. Okay. Let's, let's get guys. So when we begin with a note, mostly to the people who are not here. So I'm going to like tell you guys and those who are on zoom. Around this time of semester is when I start to notice that people are not coming to class. And this class is roughly 75 in person and maybe another dozen or so online. So like. I count the number of faces and it's like a lot less than 90. So those of you who are watching this recording. I do appreciate it when you let me know, right? And there are some people I can, I can see who have already requested clearance to attend remotely. And I appreciate that. If you're not one of those people and you're just like skipping class. I do start to notice, especially when it's like not negative 15 degrees outside. There are certain days where it's like a lot more forgiveable to skip class. It is now currently 31 degrees. So today is not one of those days. So just consider this a warning that I'm addressing to the folks in person who don't need the warning. Not coming to class without a reason is going to affect your participation. Great again, semester and I assume that most of you don't want that. Of course, just keep in touch with me. You know, if you are sick for a long period of time, I understand, you know, just all the. The usual things, but if you don't let me know, I'm going to assume you're just skipping class. So. And then what am I supposed to do? Okay, so. You guy who was talking to me about the ethics cloak. What's your name, by the way, Hunter. Sorry, I'm trying to learn folks names and it's hard to do in a class. So Hunter was asking me about the ethics cloak, William today. So if you're interested in hearing me and some other folks talk about chat GPT that's going to be happening in the LSC theater. So basically, you go in the LSC and like kind of as you're going in turn away from the food court down that like green line hallway and just sort of keep going. And you'll get there eventually. It will also be recorded, I'm told. And so I'm going to get the link from them and I'll post it. So, you know, if you aren't able to attend. If you're interested in watching some of that discussion, so it should be pretty interesting. As a result, I have to be there for. So I will have to have to close office hours at 345 at the latest in case any of you were going to come by. So, so eventually the office hours will begin like the moment classes over if you need to talk to me I recommend just like catching me on the way back to my office and when we get there we can sit down and chat for a bit. But I will need to be my office at 345 or so to head over to LSC for the event. Okay, we are, but I think we are almost done with grading a one I say we, when it's sire doing all the grading. But I think probably the weekend, right. Okay, so yes, so, so show give me the grades by about the weekend all just spend about a day or so reviewing them so hopefully. I'll be able to get back to you by the end of the weekend maybe Monday at the latest, which should be in time for you to take any feedback from one and and incorporate it into your assignment to submissions. So, there are no other questions I will start the lecture on classification. You guys got anything you want to discuss. It's me I forget who all I think it's like five people it's from all around the university so it's basically like I'm the only person in the university who does NLP as a primary research area as far as I understand so like now I'm getting like lots of requests to talk about how CS he was handling this like I'm handling I'm not sure the university policies. And there is no distinct policy yet. My policy is don't. But it's going to be me. And there's like some folks from I'm going to say like economics or the business school maybe English department. I don't recall exactly but there's like four, four to six people total. So it's basically a panel discussion for about two hours. So, you know, me and other people just riffing on the topic I literally know prepared remarks aside from what I rehearsed in my head so I would assume so I will say we'll see what happens right I mean but I think it should be an interesting discussion I went to another one I did panelists another one last semester. And then they decided to chat to PT so they asked me back. But it's a good discussion usually it's pretty lively and there's a lot of interesting questions and things we discuss so if you can't make it in person I definitely recommend watching the recording and it just should be pretty important. And I'll probably touch back on some of the material we talk about when I do the last lecture on on ethics and and abuses of machine learning. Okay. So, if there are, did you have a question. Oh yeah. I am told it starts at four. So I believe the commences it for. Yeah, I'll probably be there at least 10 minutes ahead. Yeah. Great. So, let me share my screen. And let's start talking about classification. Okay. So, so far we've talked about regression problems, which is basically taking scale mapping them to scale or output so my input is some set of numbers and my output is also a continuous number or set of continuous numbers. But many of the AI tasks that you might be interested in. Are not necessarily just predicting numerical values. Right. Something's what I want to sign a meaning to things. So that is to say, how can I represent meaning numerically. Right. One is an abstract quantity that kind of resides in our brain and we have interpretations of. And the other is a quantifiable metric that we can actually, you know, perform numerical measurements on. Right. How do we get these two things to align doesn't seem very intuitive. So we've done regression using linear models, and then also non linear models, a K neural net. So now that we're starting this unit on classification in similar form, I'll be basically showing you linear examples of classification, and then non linear classifications using neural networks and we can see what is most useful for in which circumstances so let's just take a look and see how linear models are not suitable for classification. So let's, we have some data like this and say we got 13 students are taking a test. And yes, you could plot the, the grade as an actual numerical value and then maybe it could be solved for using a linear function. But let's say that instead we have this data plotted as x being the number of hours at every student studies for the test. And then the y axis is at the letter grade. Let's just say ABC. So instead of a numerical value between like say 70 and 100 is just one of three categories. Right. So it doesn't matter so much if this person gets like an 83 and this person gets an 87. This is a big. This is what we care about right now. So we could still try to do linear least squares on this by using the integers one, two, and three for the different classes right so now I could have x being the hours of study, and then just plot one, two, and three almost as if this was a graph or something. And you could do that. And then you could put your function to the data. Right. And so you could get a line that maybe looks like this given this data. This is not a great fit. Right. If I were to say, try to calculate the R squared value of this or something. It would not necessarily be very high because if you look at whatever value this is, you know, four hours. So we expect the value to fall here somewhere around 1.5 right when fact the value is to. So, even though this kind of fits the data sort of relatively well. It's not a great idea because you have to convert these continuous y axis values into these discrete integers is one, two, and three, 1.5 is not allowed. Right. 1.5 is not one of my classes. So without adding more parameters, I need to figure out like where I have to split these, and I could use this general solution of like splitting at 1.5 and 2.5 and if it's like above 2.5 like I'm just going to round up and say it's a three, right, which would be an A. But if I do this, then rats. Chuck Anderson wrote this part of the notebook obviously because when things are work for me I use more colorful language. Nonetheless, rats, the boundaries are not where we want them to be because we can see here that if I'm just if I have my 13 samples and trying to arrive at an even distribution. I'm basically grouping one sample that I know is a B with the C's and another sample that I also know is a B with the A's right so already there's noise in my output set. So I can already tell that by doing this, I'm going to have a very sub optimal model. So for example, if I have this five say x equals four. It's going to say, okay, well the printed output is 1.4 which is below the 1.5 boundary and therefore it's a one or a C. So already, there's going to be at least two samples out of my 13. They're misclassified. I can do a lot better than this. The solution would be to represent the, the class labels, not as numerical values, but to rather decouple the modeling of those values from the actual boundaries. So using one value to represent all the classes doesn't allow me to doesn't allow me the kind of flexibility. So instead I won't use three different values that are all orthogon. Now, there's no clear linear dependency between say class B and class C right so even though the data may be broadly monotonically increasing. If we plot you know the grade versus the hours of study. It's not a good fit because the basically the representation of the different classes is kind of orthogonal to each other right so this could just as easily be three completely different classes of, you know, subjects, that maybe that maybe if you're in a study that or if you're a field that requires like on average fewer hours of study per night, for whatever reason, just because the nature of the field, you could use that to predict, you know, this predict what what field someone is in based on their hours of study per night. So class modeling basically what you want is all your classes kind of your orthogonal to each other. So one way to represent these orthogonal values are just to think of them as say vectors. Right, so if I have three classes, this could be a three dimensional vector where every, every class is just perpendicular to all the other classes. Okay, so one way to do this would be just to represent these what we call indicator variables. So if I have three classes of indices 01 and two, and I'll just put a one where it is a member of that class and zeros elsewhere. So, indicator variables or one hot vectors, right, are basically the term that we use here. So, we can model class one as one zero zero class two is zero one zero class three zero zero one. If you think of how you might plot these coordinates in 3d space you basically end up with something that looks like this right so if my, my thumb is the x axis my index fingers the y axis and middle fingers the z axis. You can see how if we assume that all my fingers were unit vectors, all these vectors would be orthogonal to each other. Okay. So now I can just let the output be this this triplet this that has you know for however many value however many classes I have it has that many values. And I want to maximize the predictions that I get close closest to the indicator value that represents the class that is the true the actual true value. So then all you can do is take these values and convert them to a class by picking the max. So, for example, if the class is 001, then this should why one should be one or close to it why two should be zero why three should be zero. And if I take the arg max is going to say okay index zero a k a class one is the most probable class. Okay. Yes. Do you say why one should be close to one or two people. Both it should it should be one. So, I'm going to get it as close to one as possible. I mean a little ahead of myself with the with how we calculate error and classification. And we will get to it so much until the next lecture. So for the moment you can just assume that we want class one to be one. Okay. So now, if I represent my outputs this way, then I can just have two values on the y axis zero and one. So, that the class in question is one if it's one else right zero. So, if I plot the data like this now we can see that for example all my a's are ones in class three and everything else is zero, and these would be all my bees, and these would be all my seeds. I can still fit a linear function to this data. Right again it's not going to be a great fit, but it would look something like the following. So, what I can do is I can overlay them to see which one is the maximum value so for example, the blue one is basically the line fit to the a data the red line is the line fit to the C data, and the green line is the line fit to the B data. So now if these are the boundaries of my actual classes I just look and see for each of these lines in which class or which which which model has the highest value at that point. So, the line is my C's we can see that for these samples, the value on the red line is higher than the other two. So therefore these would be in members of class one, the green line is the highest for for this set so then these would be members of class one, and the blue line is the highest for members of this set these members of class three. What if the green line were slightly too low. Right, so now if we look at this sample here right this one, which line is the highest at x equals this value where the mouse is the red one right and the red one is the sort of the class zero line right the C line if you will. But for this sample that we know is a B it's actually predicting a C because the sea line is highest. So why might this be the case. One reason that we could that could be the cause of this would be a massive problem right we have too few samples of class two. So if my class if only have three samples of class two my data looks like this, and the line that is fit to this data is going to be lower than this green line. So if you're going to be much more like this. And so if I have too few samples of a particular class then novel data that belongs to that class is kind of not going to be modeled as being a member of that class. So there might be no values of X which the second output Y two is larger than the other two given a linear model. So that is class two has been masked by the other classes. So there's five samples of class zero and five samples of class three, only three samples of class two, it becomes harder to pick out new samples of class two from new data. This is the same issue as we would encounter with an unbalanced data set. Right, it's more probable that something is going to be a member of class zero or say class one or class three. So I'm going to get a lower error if I predict those more often. Right, so this is going to, this is going to make me under predict instances of class two. So let's think of what other shape of function would work better for for data like this. So you can chew on that if you have any answers, you know you can, you can spin them out later. So I told this thought and was trying to example real quick. Well, mostly try an example for the rest of the lectures and not really all that quickly but we'll go off to an example and come back to this later. So we'll be using this other data set from the UC I ML archive and particular data set of Parkinson's disease. So what this data set contains is the. This data set contains is 147 samples from subjects with Parkinson's disease, and then 48 samples from subjects who do not have it. For these, each of these samples, they extracted 22 numerical features from voice recording so basically they recorded the voice, and then they run, you know, a bunch of like, you know, spectrograms and things over and for a transforms to extract different features. So the feature engineering is kind of already done. These, these features are assumed to be relevant features from vocal analysis to predict Parkinson's, and then they're labeled. So we just have zero for healthy subjects and one for subjects with Parkinson's disease. It's actually from a collaboration with the University of Oxford and the National Center for Sports and Speech and Denver. So let's download the data read it in. And then just do some statistics over the data. So, if you look at this, we can see that we have 195 samples that is 147 plus 48, and then 24 indices. So we have the 22 numerical features, plus one for the label, and then the other one is just like an index. So 24 columns. So if you look at all the columns we see here. Okay, so we have name right this is the index, and then we have all these different features. So, so there is, this is status. There's status. There's status. This, for some reasons, not the end, but we know that this is the actual value. So we're going to extract that status value. And this is going to be our targets. Right. So now we now we've taken the 24 features sliced out one of them. This is actually the feature we're trying to predict. And there are 195 by one values in that array. Okay. Remember that other. There's that name column. That's just an index. This is not going to be a predictive feature. Right. You can't predict whether someone has Parkinson's. If you know their name, or if you just have like some numerical index representing where they are to study that's not correlated. So we're going to drop that and then we're going to drop the status of course because you don't want to have the thing we're trying to predict in the training data. So, because then what would it learn. Yeah. It would just ignore it. It could just learn and ignore everything but the status right the status column and the input is obviously a perfect predictor for this is for the output. So it's like, well, here's the answer. Everything else and I don't learn anything. Right. So, one thing that we do in in analysis is just sort of you look, look, is, is a model. Does the training data effectively answer it and obviously, and if it does then that's contaminated training data. But also are the features indicative of the answer. Right. You, you have you want to give the model enough information to figure out the answer without just giving it the answer. Okay. So now these are the features that we actually actually use so I don't really know what all these things necessarily mean, maybe some of them mean something to some of you but they're, they're just, you know, features like frequency and volume and local quality and things extraction So let's print the values for each of these so you can see that the ranges are very different. Right. We have some that have means that in the hundred and some that have means close to zero and some that have negative means. And then there's a wide distribution is given by the standard deviations as well. So let's look at the occurrences of the individual values. So we can see here that as as was mentioned in the description of the data. So we have 48 samples of zero, that is people without Parkinson's disease, and 147 samples of one that is people with. So this is basically just a binary classification negative positive for for this trait. And you can see that the sample is is unbalanced. Right. We have way more samples who have Parkinson's than without. So this is going to be one challenge with this. So for we have the small sample size overall under 200 samples so not huge even though we do have some pretty good data for each one. And then it's very unbalanced that we have like a hundred more samples of one class than the other. So what we want to do is we'll just force equal sampling from the proportions of the two classes and use them and building our training test partitions so we'll use 80% for training and 20% for testing. This is a very typical split on that you will see. And then specify the training fraction so you can run this notebook and mess with this value to your liking. And then we can extract the class for each of the subjects, and then permute all the, all the data, take the training fraction. And this should keep roughly the same proportion in the training and test sets as occurs in the actual data. So what we see now. So for the training set we've got 156 samples, and for the testing set we have 39, and this should be roughly the same proportion that can verify this by taking the number of class zero divided by the number of class one, and we can see that it's roughly one third. This is not going to be exact, but it's close enough. So 32% of healthy subjects versus 34% of healthy sub or three to one ratio I should say between healthy subjects and Parkinson's is it's decent. Right. And then this compares to the original data set these numbers are very comparable. This is what we want to see. Okay. So the least square solution would be first will standardize the inputs. We don't need to standardize the outputs because now they indicate the class. Also, the outputs are zero and one. So if you standardize them. And I'll put something very similar. So what you, what you don't want to do is when you're doing classification as compared to regression, you don't standardize the outputs because they're basically numerically indicators and actual class value. And then there's just some mapping that you use or just convert the numbers to say a string label or something like that. So, then you just calculate the linear least square solution. So the training function for this would look something that looks pretty familiar. So we have, we calculate our means standard deviations we standardized that we then insert our bias column. And then instead of doing iterative training using SGD right now. We can just do linear least squares. So I can use this the linear algebra library from non pine just do that. And then I'll have a use function that basically will do the same thing. We just, we have the model we standardize it insert the column of ones and then multiply by the weights that were in this case we're calculated using squares. So let's take a look at the data real quick right these are those 22 columns that are supposed to be interesting features. And so then I will just take this feed that into my train function. And then I'll insert the bias. And then because this is a linear model what I can do is after training I can actually see what weights are assigned to each of those, each of those values. So here we go. So of this, right, just take a look at this list real quick and see, you know, which ones do you think appear to be the most important. Remember how we interpret those weight values. So, what do you think is the most important feature shimmer. Yeah shimmer right so this is basically a highly. Well, so this one is highly positive correlated and then there's other one that is highly negatively correlated. What else. So the meter is pretty high. So those three. Probably and then this MDVP or AP I don't know what that means but it also has a fairly high negative value. So like those four features are probably getting us most of the value for a model. Right. However, well this model does it's doing mostly on the strength of those four features. The rest of those values are like pretty close to zero. So now it's Tesla linear model right so to compare the target values of zero and one, you need to convert the continuous output to zero or one whichever one is closest. Okay. And this would be something that you can do with a number of different functions but we'll have this one function convert to zero one is basically just computing the distance from the target. That is either zero or one and then just figuring out using the argument whichever one has less distance. So, I convert some of these samples where you can see this function at work. So whichever one of these guys is closest to it will then assign to be that value and you know that is not bounded at zero and one. Right. We can go above. Right. We can have an output that's 1.1. That is much closer to one than it is zero so it must be an instance of class one. And then there's things like point five six right which is very, very slightly closer to one, but I must make a choice. And so to choose one. So classification algorithms just to keep in mind is basically I'm giving you a fixed set of possible classes and you must choose one of these things. So, unless you explicitly have I don't know as an option. It's never going to say I don't know. It will say there is an infinite infinite infinite testimony. Small, a greater chance that it is a member of class one. So it must be a member of class one. Right. So this is the difference between like point five zero zero zero one and point four nine nine nine nine. Yes. Yes. So we'll talk about that when we do classification with neural networks. And it's pretty easy to do that. So you can actually quantify, you know, sort of your confidence interval and say this is, you know, I believe it's one and I'm 90% confident that dip will. It's going to fall in the range that's going to be classified as one. You can get it to do that. Yeah, so you can usually most models will not do this by default, but you can write some code that will do it for you. Okay, so let's use our, let's use our model. So I'm just going to, I'm using the model over the training data right now. Just to sort of test how it's doing over the data that's been exposed to right we see if it's at least a good fit to this model. And then I will do the same thing for the test data. Right. So what do you expect might happen, you expect to see a difference between, say, the percent correct train and the percent correct test and how much. Right. You can guess, I mean, 10 or you know, I was just looking for like a lot or not much. Okay, but here we go. So let's run this and we'll see we just convert all of these to zero and one and then we brought that to percentage. So in this case, actually, they're the same. And this might be, you know, you run this a couple of times with different training splits and you'll see some difference. There's a whole lot of difference. In this case, because the features are quite predictive. And the test data is just, you know, there's so many features in there. The test data is very, very resemblance of the training data. So it's, unless you get like a really unlucky split and there's like some peculiarities in the training in the test data. So random splits are kind of this double edged sword because you can, you could get a lucky split. Right, you could, I could do this. And I could say, Oh, wow. I got the same testing and training accuracy and therefore this is as good a model is never going to get. Right. But if I run it again, and it has a different random split and may not see that. And so when you're reading a paper or evaluating model, you need to look out for whether they mention, you know, is this like an average of 10 runs or how are my splits done is like the best of and possible evaluations. And when you're writing a paper, you know, it's good practice to report that as well. And also think about how you're how you're presenting your results, you know, are you averaging over multiple runs. So you're using random splits or are you doing like participant wise splits. Yes. For tests that are like, I guess, what they want to use the software boss buyer to kind of alleviate the random splitting. Yeah, yeah. So you basically, for something like this exactly when it's a small test set right there's a higher chance of maybe getting some slightly odd samples in either training your test set. And so, if you are unlucky enough to get that. Then you either have an overvalued evaluation or like a suppressed evaluation. So yeah, you can use like different different models and ensemble them you can run multiple times and average them. You can do, you know, voting. There are multiple ways of handling this. But for for small for small test size or for small test sizes and things like this, it's generally good practice to maybe like get a second opinion. It's like, this one model is doing really well. How do I know it's not just something weird that this particular model picked up about my training data. So it's good practice to evaluate multiple times multiple models, etc. So, let's do some visualization. Right. So what kind of visualization could I use to check the results so what I can do is I can plot the true class of the output and then each training sample so you basically have to two series, what the actual class is and then what my model predicted that. So that may look something like this. So you can see that the blue line underneath is the actual class. We can see that these are like my 30 odd is zero samples and these are the rest of the one samples. And then those things where the orange dot doesn't hit the blue line. Those are the misclassifications. Right. And so we can see, for example, what do you notice, like which class has more misclassifications. Which of the which of the classes zero or one has more misclassifications. So this is zero. Right. And that does make sense. Right. If you think about the imbalance in the data set, because you're going to minimize your error. More, you're going to get a greater minimization of the error by predicting more things as as ones on balance than predicting more things is zero. So it's basically losing more or losing less by misclassifying zeros and it is my misclassifying ones. If this set were more balanced may not be the case. Look at the testing data, we can see kind of the same phenomenon. There's fewer samples, of course, but we can see there's only one misclassified one class and three misclassified zero classes. Okay. So, remember what we did for convert to one is basically saying, I'm just going to have a decision boundary at point five, and doesn't matter how far it is from that decision boundary. If it's above it, it's one if it's a blow it is zero. So we may want to know for these samples, do we have any that were just like real borderline in cases. Right. There could be one where it's like, well, I classified as a zero, but just because my output value is like point four nine, you know, whatever. It's like, okay. You could easily could have gone the other way if there's like a slight slightly different split or something like that. So it's like really borderline maybe you don't want to take that as like absolute truth. So here's the actual continuously predicted output. Right. And so remember, it's not bounded at zero and ones we have values that are below zero and values that are above one, but we can see, for example, if you look at some of these misclassifications right, these samples here. These are the ones that are like just barely below point five. Right. So they were classified as zeros. They're actually ones, but it's like these actual values are probably, you know, point four five to point four nine somewhere in there. So you can you can think of this, excluding the, the ones that are above one or below zero just think of it as like a percent almost. It's 49% likely to be class one, which means it's 51% likely to be class zero. So it's like if you're when it's when it's election season. And so like the other predictions like saying well X is like 51% likely to win this race. Yeah. Or no. And then they lose. What happened? Well, it was a coin flip right. It was not, you did not have a good prediction for that. So like you're saying it's 51% likely does not mean it's a sure thing. Right. So this is a cognitive bias that people have. Similar thing we see in the testing data. Right. So here is a couple of samples. And one thing we can observe actually is that if you look at the samples that are misclassified as one. That are actually zero. Those output values are much higher. Right. These are falling and like this is like point eight point six point seven something like that. This is also an artifact of that sample imbalance. These output values because there are fewer zero samples. Kind of drift arbitrarily toward that that one boundary. Okay. So what is the shape of the boundary. So imagine that you've got just two variable attributes. X one and X two within our least squares model will predict make a prediction for some sample. And it's just going to be our linear model right w zero plus w one X one plus w two X two. And so then for the Parkinson's problem will just have this decision boundary at point five. And if it's less than point five, this output values less than point five, then that is zero. Otherwise it's greater. It's, it's one. So the boundary is given by the equation. You know, some of all weights times the times the inputs equals point five. So what shape is this actually. So the above methods what we call discriminative. That is they're trying to they're picking up the features and trying to say, how do I wait these features so that given a set of input features I can discriminate one class from another. Right. It's not taking really into account. Things like how many classes there are. There are many instances of each class that are just like in in the data set or in the wild. So another alternative approach is to basically create this probabilistic model from each class. So that is, this is a generative model. So that is, there is a underlying base rate for each class. And that's going to influence your prediction. Right. So if, if I'm looking at, you have to know something about your data, I'm looking at classifying Parkinson's. One of the things I want to know is like what's the overall base rate of people who have Parkinson's in general. I might also be more interested in like what is what's the base rate of people who have Parkinson's who come into this clinic, because this is where my data came from. Right. So, if I'm looking at data from a particular source, I may be more interested in the class relative to that source as opposed to just like the class in the wild. So this is one of those. One of those things we just have to be aware of like your environment so like a lot of studies that are done at universities of course. And so this is a bias towards the sample population right so there's this acronym weird Western educated industrial rich democratic. And so, you know, most people who come to a university are going to fulfill like at least three of those five categories. And so there's a bias toward people who come from those societies and against people who do not come from those societies because they're left out of the data. And so, like, what is the prevalence of this class in my capital weird population rather than what is the prevalence of this class in the entire world because your sample is not of the entire world your samples of a particular group or set of roots. Okay, now before jumping into simple general models. Let's just do some review of the probability theory that we talked about in in lecture two. And if you want to review this many of these concepts were raised in the entropy section of that lecture. Let's take some boxes of fruit. So here are boxes. We jars is red one is blue one, and they all contain each contain some number of apples oranges and strawberries. So, if we count the number of fruit in each jar, you can see in the blue jar there's two apples, six oranges and four strawberries and the red jar. There are three apples green ones. One orange and then two strawberries. Right. So there's 12 fruit in the blue jar and six fruits in the red jar. So, there's two probabilities of a fruit from a given jar that is in the blue jar. There's two out of the fruits, two of the 12 fruits or apples six out of 12 oranges for four and 12 or strawberry so there's like a given the blue jar. There's a 50% chance that if I pick a fruit out of that it's going to be an orange. Right. Whereas if I have the red jar, there's a one in six chance and I'm going to draw an orange out there because there's only six fruit and one of them is an orange. So, let's say that first I choose a jar, and then I choose a fruit from that jar. Right. So now instead of a single event that is here's a jar pick a fruit out of it. There's two events. First you pick a jar, then you pick a fruit out of that jar. So now we need to know the probabilities of picking the jar in the first place. So let's just say for argument's sake, that the probability of choosing the blue jar is 60% to choosing the red jar is 40%. So, you know, the number is in the real reason behind this you could say these are equal probable because there's only two jars and you could choose them randomly, but that that wouldn't change the probabilities, which is what I'm interested in showing here so let's just say that these are. So then the probability of choosing the blue jar and drawing an apple out of the blue jar is that joint probability or the product of these two choices so point six time point one six seven or point one. So, in all the multiplications, we can see that the joint probabilities of these events are these different values here. And you'll notice that these don't sum to one. Right. Why would they sum to one. They wouldn't because they you're basically picking the jar first and then you're picking the fruit from within that jar so whatever whatever value you finally get is predicated on the probability of those two events, you notice that the sons here are not exact do the rounding, but they're really really close to point six and point four. So basically what I'm saying here is that the probability of picking the blue jar and then any other event relevant to that happening is the same as the probability of picking the blue jar. Right, so if there are a limited number of things that can happen after I pick the blue jar, the probability of any of those things happening after picking blue jars the same as probably just picking the blue jar. So, what's the probability of event X and the probability of literally anything else happening while the probability of literally anything else happening is always one. So, just multiply by one. So let's combine these into a two dimensional table and show the joint probability of the different events so it looks something like this so I'll have the jars on the rows, and then the fruit on the column, and they look something like this. So you can see that here are my numbers that are just the sum of the probabilities of the of the jars, and then these should be the probabilities of just like picking a fruit, right, and these together, right, at all these together they should sum to one. So there's where I get my, my someone. So, now just symbolically, let's just represent these as variables so let J be a random variable for a jar, and F be a random variable for a fruit. And so you can represent them like this so here the joint probabilities, just given as you know, P of J comma F. So, basically written, you can usually just eliminate like the, the equal sign here so you just maybe write this as like probability of blue comma apple, and the assumption is like you know which one of these refers to a jar machine these refer to the fruit. So, these can be used in base rule. Well, we just saw as an example of the product rule. So that is this so if I take the joint probability is going to be equal to the probability of some event given another event times the probability of that event so if I have some event F and some event J, then the probability of F comma J is going to be the same as the probability of F given J right vertical bar means given times the probability of J. So now, since these two, these are joint probabilities so I can just swap them right the probability of F and J the same as the probability of J and F and just looking at co occurrence is no conditional dependency. So we also now know that if I swap these then I can rewrite this equation as probability of blue given orange times the probability of orange. And so now we know that this equation here, and this equation here are the same. P of blue given orange times P of orange equals P of orange given blue times P of blue. So now I can divide both sides by P of orange. So what I do is I end up with P of blue given orange equals this side of the equation P of orange given blue times P of blue divided by P of orange. And so now the general form is this this is base rule P of A given B equals P of B given A times P of A divided by P of B. So if you want to remember the pattern, it's ABBA AB, and you just need to remember where you put your lines. So ABBA AB. So in other words, the posterior probability, don't laugh, is this thing here so this term. This will equal the prior probability. The latter term in the numerator. That is the initial degree of belief in a or the base rate of a, and then times the evidence that is P of B given a divided by P of B. So that is the support that B provides for a. Okay. So on the right hand side of base rule, all the terms are given in the fruit example, except for the probability of orange. So I know I can get all of these terms, these initial probabilities and the probability of the jar from the tables. I'm just pulling directly out of those values. I have to still calculate the P of orange, but I can do that using the sum rule. And so this would just be the probability of the fruit, the joint probability of the fruit being orange, and the jar being some jar J, and just some that overall jar is J. So in this case, if the jars blue, the probability is point three, and if the jar is read the probabilities point zero six seven. So we end up as point three six seven. So in other words, base rule can be rewritten by having in the denominator, the sum for all events. Be or sorry, events a, the probability of the joint probability of B and a. Okay. So, given that we can then put back the, the equivalence, the joint probability that we calculated earlier in terms of conditional probability. And so for all J sum, the conditional probability of F given J times the probability of J. So these are all multiple different ways of representing base rule, depending on what values they actually have accessible to you. And this is known as marginalizing out, and can be done doing using the joint probability or the initial probability, depending on what you actually know. So now we can do this in Python. We can actually represent a conditional probability table as a two dimensional array. And I kind of doubt going to get to this notebook today, but I believe I plan on that I'm a day ahead anyway. So let's include the row and column names as list and then write function to print the table. So I have your names and fruit names, and I write my print table function. And it prints out a nice little display, like I had earlier, including the songs, right. So we got like 18 total fruits. That's at the bottom right here means 12 fruits in the blue jar six fruits in the red jar, five total oranges six total, five total apples six total seven total oranges and six total strawberries and here's the distribution in every jar. So now I can just like some across the axes to get those those some values. I can calculate the sums of the fruits in each jar by doing this. And now I can calculate the probability of drawing each type of fruit given that we've already chosen a jar. Right, so I get take the jar sums, and then it's divide the counts by that. And then create this conditional probability table that gives me those values that I saw above. And you'll note that when we do this, we actually see sums to one at the ends of the rows. Because what's the probability of drawing a fruit, a given fruit, or a known fruit, given a known jar. And then you sum all of them, right, that should equal to one what's the probability of drawing a fruit, whatever it is out of a jar. It's one, right, if you're not concerned with the actual characteristic of the fruit is. We can do it more if we code the probability selecting a jar, as an array of point six and point four. And so now I can use this to calculate the joint probabilities just by multiplying those conditional probabilities by the jar probabilities. And so now here's my joint probability table. And now we can see that some to one only occurs in the bottom right as it did before. So this is this table is all possible results and so this better sum to one. Now we're back and like, what's the probability of an event. And if that happens anything I don't care about its specifics therefore it's one. So how do we get the probability of a fruit from this table. So we just marginalized out to remove the jars by something over the jars. And this can be accomplished just using NP sum over the access that represents the jars. And here we see those product those fruit probabilities that we saw before. So the probability of a jar, given that you know which fruit was drawn. And so now we can calculate that as p of jargon fruit equals p of fruit and jar divided by p of fruit. And so now we get this. So just we can represent all these as NumPy arrays and with some pretty simple operations, just keeping track of your axes, you can get all these joint or conditional probability values out of it. So if you don't know which of these values you need you can use these operations to get it. So now let's use base rule for actual classification. So instead of our fruits. Let's look at hand drawn images. Right. So now I'm trying to say I pick a image out of a quote jar. And I want to look at it. I want to see okay, what digit does this represent 039. So let's let I be a particular image. And then to classify I some digit like four, we need to know the probability of a digit for given an image I right so it's assumed there's some. A bunch of samples of hand drawn for is another digits. Right. And so those are individual instances, and they all belong under a class. Right, that is the digit. I want to be able to classify the image of the digit so I need to know the thing that I have is the image. What's the probability of each of these 10 classes given this image. Now we probably only know P of I given digit. Right. But if we assume that I is one over the number of images. Right. So what's the probability of drawing this specific image. It's one out of however many images I've got. Right. And then how many of us the probability of the digit being for if we assume it's evenly distributed, then this is going to be one out of 10. Right. There are 10 digits and like I'm just choosing them randomly should be 10% chance of getting any single one. So then we can use baseball by plugging all those values. I'm sorry. In this data set now. So we'll assume that these these are all unique samples. There may be some here that look like really similar, but they're all unique. So, may's rule will give us the following. So, P of four given I is going to eventually simplify to P of I given four times point one. Right. That's the P of digit being four divided by one over the number of images. Okay. So, for a simple generative model above we had used your function as a discriminate functions. There's a boundary. And if it's on one side of the boundary, it's one class of us on go side of the boundaries of the class. Now, if we had three classes, you would end up with three discriminate functions. And then you compare the values to find the maximum value to make the class prediction. So a discriminate function is this describes a curve that separates points in the data that describe different classes. So if you remember from the neural network lecture, I had the one example of like a curve of blue points and a curve of orange points. Right. So if those represented different classes I'm just trying to draw a line between them. Right. So instead of trying to draw a line to fit data, I'm now trying to draw a line to actually separate individual points. So for n number for n classes, the number of discriminate functions is going to be either n minus one or P, where P is the number of predictors that is important features that are strongly correlated with each each class, which are one of these is smaller. So a different way to develop a similar comparison is to define a probability distribution over each of the possible values, which are generative models. So basically I have for every class I have a different model. And this is going to incorporate things like the base rate of that class in general. And so then I just run all of my models. And I'll see whichever which one produces the highest results. So these all eventually should generate some sort of probability. Like, if I have three classes, one, two and three, it's like, okay, it's 50% likely that it's a one 40% likely that it's a two and 60% likely that's a three. Right, it's going to say three, and those classes don't have the probabilities don't have to sum to one. Right, because each of them is a separate model. It doesn't know anything about the other models. So how would you like to model this probability distribution or typical cluster. If you believe that data samples from a class have actually got to be close to a particular value. So that is that the samples cluster around a point. So if I have some n dimensional representation, all my samples from one class. Kind of cluster close to each other, right, or at least they're closer to each other, then they are to other things. The cluster could be very loose or could be very tight. But the point of a cluster is just that things in that cluster are closer to each other than they are two other things. And we can do clustering algorithms and we will later. And you can also do like a canary's neighbor to assign new values to one of n clusters. But we want to kind of cluster over some central point in the sample space, you want to pick a probabilistic model. It's going to have a peak over that point. So whatever point this is in n dimensions. If I describe the distribution that distribution should be close to that point. So that is things that are closer to that will have a higher value according to this model. Therefore it also falls to zero as you move away from that point. So to construct such a model we want a couple of different characteristics. So that is the value of that model will decrease as we move away from that central point. So I have a sample that is far from the central point for a given class. It should have a low probability of being a member of that class. And then the value will always be greater than zero. So the least probability that you can have being a member of any given class is going to be zero. And so if X is the sample and new is some central points, you can achieve this by taking one over the distance between X and new. So it's magnitude of the vector. So let's take a new to be 5.5 and make a plot. And we can get something like this. Right, if I use the function that I defined previously, then this will give me something on the order of this. And it meets our criteria of clustering around a point, but it goes to infinity at the center. And you can't control the width of the decay, which central samples may appear. So, first of all, we can take get rid of that infinity issue by taking the distance to be an exponent, so that when it's zero the result is going to be one. So that is, if I take two to the one that is sorry, two to the zero, right, if these two are exactly the same, right, this is just going to be one over one. So this is base two for this to make things simple. So now we can want to see how we do a calculation with a scalar base and a vector exponent. So, I'm going to try to do this. This is not going to work. Right, so I cannot exponentiate an int to a list. Data types are not compatible. But if you use an umpire, you actually can do that. Right, so this is a lie to do an element wise. So neat trick there you don't need to stick into a form. All right, so if we do that. Okay. So this is better, right, it doesn't go to infinity as we approach this point. It is capped at one. We're close off way too fast, right, so I want things that are like really close to this but not quite at this. I probably want that that probability to be like .9 something not. I don't want that probability to fall all the way down to like .8. Right, so we don't want to be so exacting that you must hit this, this, this central value exactly in order to actually get a reasonable probability of being in that class. So we want to change the distance to this function that's going to change more slowly at first, but when you're close to the center but then continue to fall off. So what if we square this right so now I have to to the distance squared. This looks better, right, so it starts to fall off less slowly. So this is a much nicer shape. And so now we can scale the squared distance to vary the width. So if we scale it by .1, this means that the probability of a sample mu being in cluster X is going to fall off 10 times this fast. So this looks better, right, this is a pretty nice nicely shaped function. So we could just pick a center and scale factor that best matches its sample distributions. But let's make a single one more change that won't affect the shape of the model is going to make the calculations simpler. And so that is we're going to change the base of the logarithm, right. So, you know, I assume you're all familiar with some of the properties of logarithms and we'll see how they come into play to fit a model to a bunch of samples. But the logarithm of say two to the point one times the square distance, or that is what is rewrite this is Z. So what's the logarithm of this? We know we can convert between logs. So if we're talking base 10 logs, then log of two to the Z would be Z of log two, or Z 10 to the bottom two. So this means we can pick the base of whatever we want. So there is a number as you're all the way, that has some really nice properties when it comes to logarithms, which is E. So the logarithm of the so we use the natural log, so the natural log of E is one. And so now if I take natural log of E to the Z, this will be Z times the natural log of E, which is equal to Z. So this makes things a lot simpler. And so now our model is going to be the probability of X is one over E to the race to the scale factor times squared distance, or each the negative scale factor times squared. So this is just going to be some constant. If I plot this again, this is really not going to change the distribution. So the scale factor is a bit counterintuitive, so that is the smaller the value, the more spread out the model is. So let's divide by the scale factor instead of multiplying it and we'll just call it some variable sigma so we can tune that value. Let's also put it inside the square function. So that's going to directly scale the distance rather than the square distance, which also makes the calculation simpler. So now we end up with something like this. So we're going to end up taking derivatives should not be surprising. So we'll take derivatives of this function with respect to parameters like mu. So then that's multiply by one half so that we bring the exponent down. It looks it's going to cancel with that one half. Again, this is all just to make them that simpler when we actually go to do the the computations. All right. So now the one remaining problem is that this is not a true probability distribution. So this problem distribution must have values between zero and one, and then it must have values that sum to one over the range of possible values. So we satisfied the first requirement, but not the second, and we can fix this by calculating the value of the integral and dividing by that value, called the normalizing constant. This is called the Gaussian integral, which ends up being the square root of two pi sigma squared. So if you want to go into more about why that is check the article. Now we finally have this definition. Right. So we scale by one over the Gaussian constant, and then we take a time raised to the negative one half times the square distance over a sigma squared. So if you've arrived at the normal or Gaussian probability distribution or 10 technically the density function thereof, assuming mean new standard deviation sigma and thus varying sigma squared. So now you know a bit about why we use the normal distribution. And it's so prevalent because it has these really nice properties, and desirable characteristics that are useful for for constructing probabilistic models. So now you just hear some more about you know probably theory from various authors. So you can read more about that at your at your leisure if you desire. So now before getting into Python, we need to define the multi variant normal distribution. So we should go to multiple dimensions because we don't know how many classes are going to be dealing with. And we need to have a normal distribution that's going to allow me to do that. So in order to handle multi dimensional data and not just scalars. We'll go up to multiple dimensions. And so now if we say have two dimensions that hill you've been drawing is going to be a mound. Right, look at looking like those hills that we drew in the neural network lectures like lecture seven or so. So basically we're just going to have a two dimensional base plane and define some coordinate. And then we want to have this distribution fall off appropriately in all directions. So we'll define now x and you to be these two dimensional column vectors. And we're doing two dimensions here so we can visualize it well. But once you've established the convention in two dimensions, it can be easily scaled up to over many dimensions that you need. So what should sigma be. So you need scale factors for both dimensions. This will allow us to stretch or shrink shrink the mound in both directions right because you may not want to fall off equally in both directions. You might be more clustered around the inner particular dimension or along a particular axis. So in two dimensions, the difference vector is going to be basically x minus mu. So you end up with these two values d one and d two. And so then the squared distance is going to be d one squared plus two times d one d two plus d two squared. And so now you can see where the three scale factors go. So we have s one here and then s two here and then s three here. And so this can be written in matrix form. If we collect the scale factors like so, we have two instances of s two, because we multiply it by two. Right, so now I can put s one here in the top left D s three here in the bottom right and s two here along the opposite bagel. And then think about, yes. So think about. If you have multiple dimensions. So think about how we expand a polynomial. So if you have the, if you have just a binomial, you end up with a basically a square plus two a plus b squared right so we have a, you know, a cubic function right it's going to be a. So I'm going to be bad at rallying off function on my head like a que plus two or three a squared b plus yes. And then the coefficients get arranged in the matrix. You're going to have like three instances of s or two instances of s two like three instances of say whatever s three would be and then two instances of s four, and then that's five. So now what we can do is we can now have D transpose times sigma times D. Okay, and so now if I do this if it's the inverse the identity matrix is just cancel out to the sigma. Okay. So if I have D transpose times sigma times D, then I have D one D two times sigma times the original D. And so this will eventually expand out to this where I have every instance of the scale factor, the right number of times. Okay. So it's more intuitive to use scale factors that divide these distance components rather than multiply them so in the multidimensional world we can achieve this just by taking the inverse of sigma. Right, so now we're coming back to that inverse matrix. So now the normalizing constant is a little bit more complicated. This is this will involve the determinant of sigma. That's going to be the sum of the eigenvalues and basically a generalized scale vector. So what's an eigenvalue, you can go to that here, right so I mean values and I mean vectors characteristic vectors of linear transformation. So this is going to be this non zero vector that changes at most by scalar factor if you apply some linear transformation tax. So you could you take a vector and you stretch it you rotate it you scale it, but that you're just going to be a value in there that will let most change by the scale. And that's going to be the eigenvalue associated vector would be the eigenvector. So you can skim through the Wikipedia entry on determinants or whatever source you wish. But basically the multivariate d dimensional normal distribution is going to be given like this. So now you can see that this bear certain resemblances to the function that we had previously. So we're basically trying to figure out what the value of sigma is. And so we can see all these terms in terms of d and sigma, big sigma that are basically defining what little sigma is. So I have a multi variant distribution. There's going to be a standard deviation and basically all of these dimensions. Right. So I need to figure out what that value is going to what those values are going to be because my probabilities might not fall off evenly in all dimensions. So I could have different values in the different dimensions, and I can collect them all into a matrix, and then represent this function in terms of that matrix and those. Okay, so definitely not going to get through this all today. So all this means is that the Gaussian distribution is a nice choice. It's integral sums to one is values always non negative. It has a derivative of a natural logarithm, which is very nice and very convenient. So we can divide P by this kind of nasty function, but it contains all the terms that we need right x minus mu is the distance from the sample to the cluster center. And so now I can. These are distance constants rights now I can multiply them by the covariance matrix. And then I reuse that covariance matrix in the calculation of the normalizing constant. So if mean mu is some D dimensional column vector, and Sigma is a D by D symmetric matrix. So in addition to the above reasons for this, this has a number of interesting properties so that one is the central limit theorem. So that is the sum of many choices of and random variables of 10 to a normal distribution random variables. Yeah, trends towards infinity. Let's play. Let's go to like, what can we reasonably get through. Okay. Maybe I'll get as far as QDA in the next 15 minutes. Let's go for it. So let's play with this theorem a little bit in Python. And we can use this interact feature. So what I can do is I can plot a uniform distribution and then the sum of all the samples and I'll then be able to mess with this value a little bit. And so you can see that as if I start with n equals one they're basically just almost evenly distributed. Not entirely but close. And then as I increase the number of samples, it very rapidly starts to approach this curve. Right. So as I get closer and closer to to infinity. Right. I get a nicer and nicer Gaussian curve. So this is what I'm trying to approach and I could increase this value. It just take the notebook would might crash. I'm not going to do that. But you can play around with it on your own. You can see that as we approach greater and greater values this curve starts to get smoother and smoother. So. There's that. So now how would we check the definition of probability. According to what we just calculated. So first you need to function to calculate P of x given mu and sigma. So that is P of x. So that's the first thing that I'm going to do is I'm going to put this in the vertical bar mu and sigma. So now if I put in my normal distribution function with inputs x mu and sigma where x contains those samples as an n by d matrix. Mu is the mean vector and sigma is the covariance matrix. So what I can do here. Let me just run this code. If I look at. The shapes the matrices and last calculation. So, if V is going to be x minus mu. Right. So this is that distance. So all my distances divided by that or minus that cluster center. So these should be all those values. And then the normalizing constant is just one by one. And so this is going to multiply by something that is n by d. So, and then take the dog part of n by d times D by d. So this should come out again to an n by d. So one by one times an n by d. Then I take an n by d. This also much flight comes out to an n by d. Some across all of the axes or some across all the access one. This is just n. And then I can reshape it into an n by one matrix. So now if I take this and then I got an n answer is one for each sample. Right. This is what I was after. So I have a bunch of inputs and transform them into a single column vector or column matrix that has an answer for each sample. Okay. So let's look at it. Do just an example with some some dummy numbers. So if I create an x a mu and a sigma and print them all out here those values. So now I want to see if it's, if I run normal distribution, this will say, if you know, for given this, this mean and this covariance matrix. If my input samples are one, two, three, five, and then 2.1 1.9 right just two dimensional coordinates. These should be the probabilities of falling into each of those classes. Right. So we can see here that given this, this mean and this covariance. So this is not a great fit for this sample, whatever it is, because it has a little probability falling into all those classes. It will predict that it's an instance of class three, because that has the highest probability. Right. Although it's began if we're talking about like say confidence, it might not be very confident in that just might be slightly more confident than anything else. So to really see if it's working with plot these. So we make a surface plot in three dimensions to show the value of normal D. Do the 3D plotting that we did before. So I'll use actually access 3D. I'll plot, create my, my mean and my covariance matrix. I'll just create the Z mesh. And this looks something like that. So given those, that mean and that covariance value, we basically have a probability distribution for this class that looks like this. So just imagine for however many classes I have, I just get a bunch of hills like this in different locations, right. So it's going to be wider or narrower, right, depending on how rapid the falloff is. Okay, so finally back to that masking problem. I'm going to zip through this and I will, I will review it next time. So if you were thinking of the radial basis function to fix the masking problem, you're right. I don't know if any of you were actually thinking of that. But if you were congratulations. But remember what a radial basis function resembles or if you don't know what a radial function basis function resembles, you can know that it resembles a normal distribution. So let's say we come up with some gender distribution like the normal distribution for some class K. So I want to call this the distribution of P of X given class K. So how do we use it to classify. So for each of those classes, we run that model and take the highest value. So we do actually do better than this. Think of Bayes rule. So we want to know the probability of the class given the sample, not necessarily probability of the sample given the class. So how do we get this from the probability of the sample given the class we can apply Bayes rule. So given this is just jump to the second line here. So P of K given X equals P of X given K times P of K divided by P of X. We can get the joint probability, we can get the value P of X by marginalizing out over all the joint probabilities. So P of X and C. And in other words, because this is equivalent to the conditional probability times the probability of the class. We just now need to sum over probability of X given K times the probability of K. For two classes one and two, we can then classify a sample as class two if the probability of C equals two is greater than the probability of C equals one. So now I just run this I can rewrite both of these in terms of Bayes rule. So now just the probability of X given C times the probability of C is greater than the probability of the other class. So now all you need to know is the probability of the class and then the probability of the sample given the class which I should have. I can factor out these, because this is just some constant. So this is a constant in both sides is the same sample. And I can just say that I don't actually need to know what this value is, because as long as it's constant these two will be the proportion will be relative. Okay. So using the assumption that our general distribution for each class is a normal distribution. Now all I need to do is take this and all the function times the probability of the class in question and this evaluated for both classes. So, running out of time here. So if I do okay if I do this, then this will this will simplify to the version below. There are a bunch of multiplications and exponentials here. So I can make that a bit simpler by using logarithms. Right. If I do the logarithm, it's going to bring the exponential down in front case and point. So if I just take the logarithm of the covariance matrix for the second class, this allows me to bring the one half down in front as a as a coefficient. I can get rid of the E bring the one half down. Now I'm back in terms of the square distance and the inverse covariance matrix. So now all I need to do is calculate this, add the log of the log problem of that class and do the same thing for the other class, and then compare which one is is greater. So now we can define the last, the each side of this last inequality as some discriminate function. I'm just called that delta for class K. And then the new sample, the class of a new sample x is going to be the argmax for all those discriminate functions. And so then the boundary class between the boundary between class one and class two is going to be the set of points for which the discriminate function is equal for the two classes. So if you want that boundary, it should be 5050 which one I'm in. And this equation has to be quadratic in x, meaning that the boundary between class one and class two is quadratic. And so just to find something you may have heard of called quadratic discriminate analysis, which is a quote linear way of doing discriminative or doing classification using generative models. So a lot of terms in the mix here and kind of counterintuitive. It's, I'm looking for a discriminate a function for probability distribution, which I'm using to define generative models of different classes. And it is a quote linear way of doing classification, even though the function is quadratic. So I apologize for that I of course did not invent these terms and their usages. So it's just a lot to kind of keep track up. Okay, so we will do we'll go through qda code and do linear discriminant analysis on Tuesday. All right, thanks and I hope to see you at the event this evening or watch it afterwards.