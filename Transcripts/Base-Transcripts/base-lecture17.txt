 Okay, welcome back everyone. If you enjoy a good break. So, let me start with the schedule where we're going. Okay, so what's going to happen like I mentioned before break I'm going to be gone next week. What I'm going to try to do is actually get us a day ahead. Mostly today, because we have a review of softmax and see it and then we kind of always build it after spring break but usually. We don't really need to spend the whole class on that if any. And so I'm going to try and basically get through notebook 16. So I can move 17 to Thursday and then do 18 remotely next Tuesday. And then ideally we can all effectively take the 30th off, because I will be traveling and you all can have a free day. I guess if we, if we managed to do that. So hopefully we can that's my goal. So, okay, so the next Tuesday, I will be in California, but I will just do lecture remotely so you can come to the room and attend on your laptop is really wanted to. But you can just, you know, watch from your house or wherever, or if you can all come in here and someone can have a projector you never watch party for lecture. How am I supposed to know where you. Okay, yeah, you do that and then you like write a reflection on how this was a different experience for you two pages minimum and he gets an extra credit. Okay. Yeah, and you can't use chat GPT to write it for you. Okay, so. A three is due tonight. I. So here's one, one thing you should know about a three that I discovered. I people have mentioned there's some, you're having some issues with the automated greater and some things like you're seeing to be doing it right and maybe it's getting some different answers. So in some cases. I've discovered how many of you were using an Intel or an M1 Mac to do your homework. In some cases, and this might be the case with other types of newer processors as well. There's some slight differences in the architecture of the chipset that actually caused some slight discrepancies in the way that it ends up performing grades and operations. So I said, I'll send you all the discussion that we first kind of found this last year. And we, we seem, we think we can't put the way to address it. And so this means that it might take a little bit longer to get your three grades back because we have to sort through some of these issues. But I just want to warn you, so if you are worried that the autograder is maybe not is giving you things that you can't track it down. I recommend you come talk to me or send me the, the issue first so I can maybe review but also know if you're using the new Mac. Sometimes you're using like Python 310. This may be a slight issue. So I wouldn't necessarily get like too upset about that if you're like losing points on one of the issues and it seems to be like there's a slight discrepancy. Yes. I think we are grading using 3.9. So if you want to just be assured that you're writing the same version that we're grading on use 39. That's not necessarily going to change anything with like the architecture of the chipset or something like that. So that's also an issue. Unfortunately technology continues to evolve. And so our notebooks that were written back in like 2019 or whatever are not necessarily like fully automatically compatible with everything that's out right now. Yes. Probably not. No, if you're getting full points the autograder, especially even if you're using like a newer computer, most likely you'll be the same for us. If you want one thing that may help is like, you know, if you were concerned, you can submit along with your assignment, take like a screenshot of the auditorator and make sure that the output is like present in the notebook. A lot of people don't don't do this. Never clear your output before you submit because then we have to run the whole thing and we're not going to see what you actually run. So like, you know, run the auditorator show us that your output is, you know, you're getting full points. We'll run it again, of course. But if you notice some of these issues then that at least tells us that like we can check with you ask like what your, what your system settings are. Check it against and you know, a machine that has those same settings, where it's close as we can get. So yeah, apologize for that. That is an issue, mostly with like the GPU part of the code. But it's one of these things with the, with just newer machines and the way that they handle certain types of instructions. All right, so today. Lecture in whatever form it takes, which is hopefully mostly just. And also assign assignment for. Assign for tends to be the trickiest one for a lot of people spare warnings a lot of different moving parts to plug together I'll get to that when we, when we assign it. And then on Thursday project proposal so I know some of you come talk to me about your ideas. So you if you have an idea and you want some preliminary feedback on and I am available. The proposal. I'll assign that and then you will have, you know, little over a week I guess to write your proposal, send it back to me and I'll have time to give you feedback and request any changes or clarifications. Okay, so that being said, does anybody have any major issues that you want to review regarding classification, convolutional neural nets, or the softmax. Or the softmax function. And if you do. Let's take a few minutes to talk about that before I go to lecture, if you don't. Then I will start reinforcement learning. Everybody feel pretty good about softmax. Okay, remember it's just like you're just computing error in the same way. You're just now you're subtracting probabilities from probabilities. You're just doing a lot of the functional nets. You basically have a patch diversion of your image that allows you to simulate kind of the scanning function of the quote I over the image. And then you allow you train your filters to have higher outputs when they both match certain things in the input. And then with a set of properly optimized filters you're able to do classification for things that have different types of image level features. The homework of course we can come back and review this material and office hours. But hopefully I think everybody's at least somewhat confident on this. Right. And it's okay to say no, I'm going to stress that once more. If you're not confident about it, there's probably other people in the class who are equally not confident, but less, less brave than you. So if you speak up, you're probably doing your classmates a favor. But I'm assuming from the silence you all are completely expert at the softmax function and convolutional nets and you're going to have no problem with assignment for. So, I have no choice but to take you at your words. I'm going to start to lecture 16. That's a sign for there we go. Okay. So this is the beginning of our unit on reinforcement learning. So we'll have about four lectures on this. So the introduction we're going to do is just sort of the simple, we'll call tabular reinforcement learning so no neural networks involved at this stage. On Thursday, we'll show how you can use neural networks. So reinforcement learning can, for some people, be very intuitive and for some people can be very not intuitive and it might be kind of how you've been conditioned to think about machine learning. There are a couple of different ways to think about reinforcement learning. One is that you are training and using your model at the same time. Another kind of similar way to think about this is this is sort of like supervised learning, except you're not getting all your samples at once. This is basically you're in a quote world, and you have to explore the world and that process of exploration is what gives you the actual samples. And so this is in some ways it's like learning by trial and error, which makes sense, because a lot of human learning, especially, you know, when we're, say, infants learning and manipulate things is done by trial and error. Right. So the example that I gave last year was when my daughter was like kind of learning to eat with the spoon. Right. And so she likes the taste of apple sauce. And so if she loads, he loads up the spoon with apple sauce or reloaded the spoon with her with apples for her, and she sticks like the handle in her mouth. She doesn't get the apple sauce, but she sticks the spoon part in her mouth. She gets the apples. She likes the taste of apple sauce. So basically, the agent has things that make it quote happy or sad. And it wants to make it wants to be the most happy it can be, or at least the least sad it can be. And so that's the way you specify your problems is analogous to that. So this is sort of, you know, trial and error learnings like this agent world model where the agent explores the world and learns about what types of inputs, give it positive reinforcement or positive rewards, and what types do not, and it tries to maximize the positive rewards or minimize the negative rewards. For example, I assume we all know how to play a tic tic tote. So if you look at a board set up that looks like this so this is the current state of the board where X and O has made two moves each, and your X, and you're trying to choose the best action right you have a number of different possibilities. I guess this is this is not totally exhaustive for me actually maybe. Yeah, this is awesome. So this is we have five five open squares on our grid, and you can place your your X in any of them. Of course, depending on where you place the X is going to be, you know, a better or not as good move. Right. So if you look at this, you know, this is sort of a qualitative judgment. But if you look at the ones that are outlined in green, these are good moves. So for example, if you if I put X here as X I now have two paths to victory. Right. So if O puts their blocks me by going here I can still win by going there. Similarly here, right, there's there's one path to victory so maybe it's not as good as this one maybe this is like obviously the best one but this one is still pretty good, because like, oh, could block me there, but then I still have like a couple of other places that I could go. So one maybe is not as good right so this is not really, it's not really obvious like how I set myself up for for victory here because I put X there, and I have to fill out like at least two more squares before I can win. And if I don't really open another path to victory without an additional move. And like maybe these are marginally good moves but like not this one, this one is actually very good I'm not sure why this circle is dark green I would say like this one and this one is probably the best moves. And like these two are okay, but maybe not the obvious best and this one is probably like one of the worst moves you can make. So, basically, these may not these moves may not like immediately lead you to victory, but you can see how they lead you to victory in like within the next move, or you can see how like this one for example, it's not immediately clear how in the next say and moves you're going to, you're going to be able to win the game. So, so we can be represented as quote state. So, we have a set of possible states will denote this s, and all these can be discreet. So, for example, in tic tach toe, that is the cardinality of s is going to be less than infinity. So we have tic tach toe game positions right so at most, even the board is empty there's like nine possible moves that every player tonight right. So, I can, I can move like one step in every direction, but depending on the size of the maze and how many squares are blocked off is going to be a finite places, a set of places that I could go sequence of steps in a plan so there's a defined goal. And I'm trying to figure out how do I get from where I am to the goal, there's going to be and this is this set of discrete steps I can only choose a finite you know one one of these defined things I can't really like do multiple things that at once or something like that. And this would be, you know, a finite set. These can also be continuous values right so these could be joint angles of robot arm, or this could be like the angle of an inverted pendulum page. For example, or you can have like the position of loss of race car right if I'm trying to control the vehicle. I have continuous values that I can pull at like any given time and they're not necessarily like integers, or they could be, you know, they could be just like fractions to a specified decimal. You can also have a parameter value for network routing strategy or something so any any type of basically planning problem planning control problem can be realized as a reinforcement learning problem where you have to define the states that you can be in. Now when you're in a state you must decide what action you take in that stage that action is going to move you to other states, depending on the correlation between states and actions, you know, an action could keep you in the same state or can move you to a different state. And the actions can also be discrete values. Right, so the next moves are the where I see the X is in red. And so this is going to if this is my state here, each of these actions going to put the board into a different state. Right, and that's going to then dictate which actions are more or less advantageous in that state. So, in the in the maze, where I am is is part of what I need to know but also I need to know where could I go. Right. And so that's going to be, you know, from where, where, from where I am now what possible moves are available to me. And then I can move on one side so I can't move there. Right, that might restrict the set of actions that I have available to me. And then like rearrangements of a sequence of steps in a plan so for example, there might be multiple ways to get to the goal from where I am. Right, it doesn't necessarily matter which one of these things I do first at this state as long as I do, you know, some subset of them that gets me to move towards the goal. But these can also be continuous values. Right. So if I'm my inverted pendulum or my robot arm, there's a torque that you're going to apply to like the pendulum or the joint. And that's going to be a continuous value. So, you know, I think in terms of this project that we're working on to control and very depending on using a joystick to the joystick, right, that would apply a torque. So, you know, I'm trying to get to the core of the core of the core. And then, in the race car example, right. So if I'm trying to actually control the car, if I know where I am, and I'm trying to like, we'll learn how to like drive on a racetrack. I'm going to have to steer the car and apply a certain amount of acceleration in order to move the correct way along the track and not say, you know, drive off the racetrack or drive off the road or something like that. So, you know, there's certain parameter values that you're going to need to set in order to wrap traffic through your network. And so these are going to also be continuous. So, what we want to do then is given a state and a set of possible actions I want to choose the action that's going to result in the best possible future from the state. So, in order to do that. So, I have to have some way of representing that future outcome. So, for example, what should this, what should this value represent so in tic tach to I might want to determine if I make this move. What's the likelihood that I'm going to win the game from this position so for example, we can see if you know the rules of tic tach to if I am if I make this move. And I'm X. I should be able to tell that by doing this there's 100% chance that I'm going to win the game. Right, no matter what O does at this point, I'm going to have at least one path to victory. Right, whereas for like one of these, maybe it's more likely to not that I win the game but it's not certain. Right, I might have say 67% chance of winning and a 33% chance of it being a draw or something. I'm going to have to be able to quantify that somehow. In the maze is going to be like how far am I to the goal. Right, so like I know where the goal is. And I want to be able to figure out like what the number of steps is is going to take me to reach the goal if I am in place P and I make move M. And then basically this might be like efficiency and time and cost or something right there may be multiple ways of getting to the solution. I probably want to choose the one that's going to be like the most efficient, right, or has some some added some benefits. So maybe if I'm like playing, you know, Pacman or something right I might want to choose the path is going to like to pick up the most PIPs or something like that. And then I'm going to have some energy that you might be wanting to balance robot. I might need to might have some energy constraint. Right, so I want to choose the move that's going to allow me to conserve energy the best race car right time to reach the finish line, but also take a minute to try and make sure that I'm not that if I make certain moves, I could say flip the car over. Right. And that would, of course, delay me getting to the finish line if ever. Never crowding maybe your metric is through. So all these things. What you what you decide to measure is going to be again critical to determining what type of strategy you actually arrive at. So I'll show if time permitting. I'll show an example at the end where basically like by measuring certain things, you can learn a different type of task. And of course, that's contingent upon the ability to measure that thing. If you can't measure that thing, you have no way of quantifying your, your progress toward that, that particular goal. So with these correct values, these multi step decision problems can then be reduced to the single step decision problems that is I'm just going to look at the cost set of possible actions. I'm going to pick the one that has the best value. So this is going to be guaranteed to find the optimal multi step solution. So the dynamic programming problem is basically, I've got some multi step problem. And I'm going to be looking for the optimal sub solution by solving for the optimal, sorry, the optimal global solution for solving for the optimal sub solution in every step. So that is, if I'm, if I know where I'm at right now. And I can find what the optimal solution is for my current situation, then I can be guaranteed that whatever I choose now is going to be a part of the ultimate best solution that I write that. And so the, the cost of a single action is going to be what we call the or the reward of that action from that state. And so then the value of that state action pair is basically the expected value of the full return. So I have a full sequence. I have it each, at each step. I'm at some state. I take some action for each of the state action pairs when to get my reward for my reinforcement. And then at the end, I'm going to get the full return as the sum of all of those rewards. So for example, if I look at this example, I started one of these states, and then I get small r for every reinforcement. And then the return is going to be the sum of all those. But if you look at this just if you take if you take a look at this you'll see that these numbers don't add up left, right. So if I'm here, it looks like my reinforcement is point to my return is point nine. So that doesn't really make sense but he added backwards that is right to left it does. Right. Why is that? Well, that's because I'm trying to get to the end here. And I use that by trying to predict the sum of all reinforcements that I'm going to get when I get to the end. And then the next step is that is, if I'm here, then maybe the optimal action that allowed me to get here would be going from this previous state. Right, let's assume this is just like one path to a lattice that we'll see in a moment. But basically, if I add this up right to left, all of a sudden, I can see that if I do if I followed this path that I'll get the return of point nine according to the sum of all these little reinforcements. So I mean like some state s sub t at time t. Upon taking a sub t from the state I'm going to get the one step reinforcement, and then the next state so if I find here, and I take this action I'm going to get into this next state I'll have the reinforcement in this case of point three. And so now I can continue this until I reach some goal state, case steps later. And so then the return r sub t starting from state s sub t should then be the sum of all of my reinforcements. So that's why this works. So effectively, what I'm trying to do is if I'm in any of these steps that proceed the goal, I want to be able to predict what the best action is I'm going to do that by estimating the sum of all possible actions that I can take from this state, and try to figure out the best So then I'll use the best returns to choose the best action so this will become a little bit clearer if you look at this example. So, five, this lattice here. My goal is to get to one of these end states. Right, and this, the, let's assume these states are actually all the same. So there actually to be a closure here but we're not showing that. So basically, if I'm looking at this, if I go along this path, then my return is going to be point nine. If I go along this path, my return is going to be point eight. If I go along this path, my return is going to be point six. So which of these is the best path to take. The first one, I think the first one. You all agree. What if I'm trying to minimize costs rather than maximize reward. The last one, right. So really depends, you know, are you are you maximizing or minimizing that is what does your reward or cost function actually represent. So, for example, if it's the race car and you're measuring time, you probably want to minimize time. So, if you're playing a Pacman and you're measuring like the number of pips you collect, you might want to maximize that. Or if you have like, are you trying to conserve some resource or you trying to gain some like extrinsic reward or something like that. So if it's like energy that you want to minimize, then you might want to take like this bottom path. Right. So let's say that this is actually a cost. What is the energy required to move between each of these states. And so this is interesting because in this case, the first reinforcement you're going to get is actually the highest value. Right. So you want to minimize the total return. It doesn't necessarily make intuitive sense to take the path that has the highest reinforcement. If the reinforces actually a cost, but you can see that from this state. You then actually can get to a sequence that allows you to take a bunch of really kind of small cost steps. And in one case, that has no cost in exchange for taking a big cost step at the first time step. And so this actually ends up being the best the best path to take if you're trying to minimize say expenditure of some resource. Right. So, conversely, you're trying to maximize. In this state, it might make sense to take this because you get like the largest reinforcement. And if you think that's a reward that might seem like the best action, but ultimately you're going to get the lowest return. Right. So you may want to take one of these sort of lower reward first steps, because you can figure out that you're actually going to get a higher reward later on in the sequence. To do that, we need to know these values. Right. So I need to have some sense of what's going to happen if I'm in some state and I take some action and try to estimate this for basically all combinations states and action. So where do these values come from? So we have a couple of options run. One is I can write the code to calculate them. So this usually isn't possible or if it is possible, it's not really a problem. You need to solve reinforcement learning. Right. I can kind of brute force my way through it or use some other type of solution. And you can probably do this for takto because it's a fairly constrained problem. And there's a relatively limited set of things that you can do that will, and you can typically account for all those. You could use dynamic programming. This is also usually not possible because it requires knowledge of the probabilities of the transitions between all the states for all the actions. And if you don't have that, then it becomes a lot difficult to offer. What are those what those rewards would be. So you can compare this to things like the hidden Markov model. This is a type of model that we use in natural language processing to try and figure out like what's the probability is say moving from state x one to state x two or from state x two back to state x one. And then we also have these things like one of my quote observations that I've ever stepped. So you can use, you know, certain types of dynamic programming algorithms to solve certain types of sequence problems. But again, if you can do this, your problem is probably constrained well enough that you don't need to be doing reinforcement learning for this. So, a true reinforcement learning problem is one that you are looking for examples. You're basically looking for lots of examples of both solving the problem successfully and not solving the problem successfully. So that is, I want to see for my inverted pendulum if I'm trying to balance. I want to look and see like, given a an angle and a velocity of the pendulum. What moves the same trying to balance it, you're trying to balance like a pen on your hand or something like this. Right. So I'm trying to keep this balanced. I do like a really bad job. And then over time, maybe I become better, better and better at it. And I learned that by figuring out if I can feel see the pendulum. And then I know that I know that I shouldn't move and I should maybe make a certain movement that's like small or large, etc. And I do this a whole lot. Right. So in some sense, you know, learning to walk, you know, is some sort of reinforcement learning or at least some kind of reinforcement learning feedback happening between the brain and the body. We can teach AI agents to learn to quote walk by running them through an obstacle course. And they sometimes come up with these really weird walks like, you know, they're not necessarily by people and you can have like a three limit agent that sort of has this weird triangular walk. And so it learns actually will come out through through the environment by this by this example sampling. So basically these examples going to be represented as these five tuples so that as you have the state where I am the action, what I do, the reinforcement, like the return the the reward that I got from making that action in that state. And then where I end up, so that is the next state and then the next action. Yes. Assuming you're trying to maximize. Right. Well, again, you have to perform a major problem. So if you're assuming that you're trying to maximize the return, then yes, the one that has the highest return is typically going to be, you know, at least an instance of a good plan, possibly the best plan. It gets more complicated as you have like more continuous scenarios. But generally, yes, now if you're trying to minimize, of course, then you're basically just inverting that it's going to be like the lowest return. Okay. So there are a couple of different techniques broad families or techniques that we can use. And so I'll will focus on technical difference and also talk about what we'll do a demonstration at the end time permitting. So basically, in Monte Carlo sampling, what I'm going to do is I'm going to take an average of the observed returns and then assign that to every state action pair. So that is, I have a basically some value function that takes in my state in my action. And this is going to approximate the mean of the return from that state and that action. Right. So, in order to do this, I need to actually wait until the episode concludes that is I either reach a goal or I fail like I time out or something before I update my state action pair variable. So for example, I'm trying to solve a multi sequence problem. I still my randomly explore the world. And then I reach my goal, then I can see that I got my goal and I can see, Oh, hey, this is actually really good sequence of events. I want to do more like this in the future. Or if I say I got a timeline of like you can take 10 steps and then you're done. Right, you get maybe 10 attempts to solve the problem and I try to move through the world and intent attempts I do not reach the goal. And then you can see that and say, well, this was not a good sequence of events. I should do less of this in future now. The trick is that in maybe a suboptimal sequence of events, maybe I made the first three steps of my 10 were actually optimal. Right. From where I started, those took me directly toward the goal and then I moved off track or something. Right. So, doesn't necessarily mean that all of the everything in that sequence was bad. Similarly, if I do reach the goal, I might have started off that. And then I stumbled upon, you know, a path toward the goal. So again, just just like the opposite. I do not necessarily want to a deer to everything in that sequence, because there might be a better way. Right. Maybe I reached the goal in like seven steps. But I actually the best path could have gotten me there and four. Right. So maybe I don't want to continually just exploit the strategy that I happen to stumble upon, because there might be a better one. Okay. Now, temple difference is the other strategy. So basically what I'm going to do is I have the value function. I'm going to evaluate that over the next state next action pair. So that is given where I end up if I take this action that I look at all the possible actions that I can take in that. In that state, and I'll sample each of those actions in turn and then put that action and that then my current my next state in the value function. So I'm going to use that as an estimate of the return from the next state. And then I'm going to update the current state action values that is value of sub T a sub T. So that is the value function of sub T a sub T is going to be approximately equal to the return from the next, the sorry, the reinforcement at the next step. Plus the value function at the next step, which is an approximation of the entire return. So that is this is small are this is an approximate approximation of the big R. And now what this does is then I can update my state action pair immediately after taking the action. So I can basically see I took this action from the state. And I have a pretty good estimate of how good or bad it was. So I can see if this is going to get me closer to the goal or not. So if you take a look at this, this little graph here, I'm trying to estimate the return are from state B. So this is where I am right here. And so basically I've got a situation where I can move from one of these states through state C, and then from state C can either move to state W or L. So I have a hundred examples of failure. And then 100 the hundred first time I go from a to C to W. And I go to L and I do that 100 times. So basically 100 times I go from a to C and then I lose. So I have a hundred examples of failure. And then 100 the hundred first time I go from a to C to W. Now, then second time I start in B. I go to C and I go to W. So this is a contrived example. Right. So I have one example that shows me what the estimate of the return from state B is. I'm trying to figure this out. What this would look like. Of course, these two methods is basically from Monte Carlo, every example starting in state B leads to return of one. If this is trivial, because I have one example starting in state B, but at least a return of one. Right. So every example and starting state B of each return of one. The mean of this is one, which is a prediction of a win. Now, the temporal difference method which showed me that the reward from C to B is zero. And then from state C, I have 100 of words that are negative one and two that are one. So in this case, what this would end up being is a value of negative point nine six. So this is a very likely loss. So, temporal difference takes advantage of the cash experience given in the state, the value learned for state C, whereas Monte Carlo is going to take just those samples that have the entire sequence, including my my start. So basically what do I do in this in this situation, if I go from B, if I start in B, you know, if I'm trying to win, what type of method might I use. Right, Monte Carlo, right. In this case, it's pretty trivial because I've got my Monte Carlo method gives me the only the only only prediction of a win starting in state B. So this is not necessarily like how you would actually choose you would have a better distribution of your actual samples. So, any questions so far. Let's see where we are. All right, so let's take a very simple maze example like a stupidly simple maze example to the point that you look at this image and you think like an image fail to load. But here's our quote maze, where G in the corner of these walls represents the goal. So, I'm, I can have, I can be in any position here will still will soon there's a grid, and I can, I need to decide whether to move up, down, left or right. So, to do this, we need an estimate of the number of steps needs to reach the goal. This would be big are the return. We need to formulate this in terms of the reinforcements, that is the law. So first of all, what reinforcement do we get for making a move. So for every move, it's going to be one. Because we don't really know. If any of these moves are going to get us closer to the goal. Then big are is going to be the sum of those to boost the number of steps to the goal from each state. So now you can see that if I have a bunch of cells in this that represent where my agent is, it's going to look sort of like one of those little mind sweeper grids there's like a number associated with how close. How many steps it's going to take me to get from here to the goal. And I want to basically traverse the shortest path. So the Monte Carlo sampling will assign this value as an average of the number of steps from the goal to the goal from every starting state. And the temporal difference will update the value based on one plus the estimated next value. So now for question is whether we do Monte Carlo update or temporal difference. Let's look at this comparison on this maze problem. So we've talked about this value functions that is the V of S and a. So how do we actually represent this function. So the simplest way to do this is actually just as a table. So you can imagine that let's take this maze example. I have a state that would be represented as some cell, say numerical cell, and then action that's going to be one of those four things up down the left, right. So the value that should be some representation of how many steps it should take me to get to the goal from the state given that I take this action. So we'll take this tabular function. And we will actually write this function called the Q function as this table. So this state action value function is basically you take in both the state and the action and the value is the, the prediction of the expected function of future reinforcement so the maze problem. This future reinforcement are basically a sum of a bunch of ones each one representing a step to the maze. So terminology queue comes from this thesis by sky called Watkins University of Cambridge. And so what we're going to do is we're going to select our current belief based on the optimal action is in the current state by taking the argmax of the cube function, or the argument of the q function depending on if you're maximizing your minimize. So again, what we will do is, I think this for the maze problem, I believe there's some type of here like it says argmax we actually mean arkman, because we're trying to minimize distance. But typically, you can think of another way to do this is you can just realize your cost is negative rewards. Right. So now if I think like, it costs me to take a step and therefore the return for taking a step is negative one. All of a sudden it's turn argument problem and argmax problem so typically I'm going to be talking about in terms of minimizing. But usually most people are going to are trying to maximize and you if you have this problem of your you've got a cost function rather than reward. It's pretty easy to just like invert your reward values and suddenly you turn your rewards into a cost. So we're going to represent the queue table for this maze world which is going to be so I have the q function of s sub t and some action. I'm trying to find the argument for a that gives me the best value. So, if you have argument of these possibilities let's say this is the goal and s represents the state. So I have effectively a set of q values for this state. And each of the possible actions up right down or left. Right. And so I'm basically going to evaluate this q function for each of these combinations and then choose the value of the action that is the right hand side of the, the, the second argument. That gives me that in this case minimum value or maximum value if you if you are treating them as as commas. So that is we can let the current state be a position in x y coordinates and the actions would be integers one to one, two, three or four. And so therefore I'm looking for the element of one, two, three or four. That's going to minimize the value of this function where my state is represented as these two values x and y. So if I assume a grid, then I can have say zero, zero, zero, one, zero, one, one, or something like that you know you can start from the bottom left if you want. So, so we can use some Python. So first, we need to know how we can actually implement this q function. So we know we know the arguments are already it's going to be, you know, a tuple consisting of an x y coordinate then a single integer representing the value. So we can enumerate all the states, which is going to be a finite set of out of 100 possible possible positions. And then we can you rate all the actions. So that is also finite, which is four. And so then we can calculate the new state from the old state in action, and then represent all of these state action combinations in finite memory. So a fairly nice compact table that we can use. So we can store the q function in table form. So in this case, we're going to use three dimensions on x and y you could in principle, use two dimensions right so I could just say enumerate these zero one two three four five six seven eight nine ten next row 11 for a next row 10 11 12 so on so on. Maybe just a little bit cleaner and easier to visualize if I keep them separate sets we're going to do. But it wouldn't be too difficult to switch between these two representations. So we'll have a three dimensional q table, where I'm going to have one dimension is x second dimension is y, these two dimensions represent the state and the third dimension is the action. So you can think of this as kind of this cube where I've got each of these states on the x, y, a plane and that's like go every level deeper. That's representing like what the q table is going to look like for each of those different actions. Now the actual values are going to be the steps toward the goal. So we're going to be minimizing. And so for the images above, in order to get x closer to g I'm going to be moving either right or down. So we have some intuitive representation of like what correct q values should look like which allows us to calculate them. So, how can we represent this three dimensional table of q values in Python. So if we have x or x and y have 10 possible values and there are four actions. So we can create this table, basically three dimensional non py array that is a 10 by 10 by four array. Okay, so now we can represent this three dimensional table as an empire. So how should we initialize this table. This above line is going to initialize all the values as zero. So think about what effects this is going to have for the q values for actions as their update to estimate steps toward the goal. So what's going to happen is basically all of my actions are going to have the lowest possible q value that is zero. So this is going to force the agent to try all the actions from all the states, because it has no notion of what might be better than the other. So it's basically going to start like here. Every possible action that it could take is going to be zero. It won't know what the possible return is unless it takes that action so going left is just as good as going down. Right. So this is lots of what we call exploration. Okay, questions. So now updating the queue value is using the temporal difference updates will look something like this. So I'm in a state. I take an action. I get some sort of return. And then I'm in a new state and I can see which possible actions are available to me. So then I can calculate the TD error. So that is the return at this, this next time step. Plus the value function that is the queue function. And then I'll use this. I'm going to take the queue value for the next state next action pair. So track the queue value for the current state current action pair that is where I wasn't what I just did. And then I use it to update the queue value that's stored for SMT. So it's going to be some cell in this three dimensional array. So I'm going to take the queue value and it's by default initialized to zero and I'll update that with some new value. So if I, for example, start here. And I moved to the left. And this should give me I'll have the return, which is one, right. In this case, my return is always one. This is going to be different steps toward the goal, which in this case, let's assume like, seven, my seven group just count them. So we'll have one, two, three, four, five, six, seven. So let's say I get a queue value for the next day, next action that seven. Compare this to where I was one, two, three, four, five, six. Okay. So now I have one plus seven minus six. So then that's going to be that'll give me the value that allows me to update the queue table there. And so then how much do I update it? Well, I don't necessarily want to update it just with the raw value. I'm going to scale that by something. So I have a constant here, row is going to be some learning rate. So I could say, well, maybe this is not like an atrocious thing to do, but it's suboptimal. So I want to maybe discount this one a little bit. Point one point zero one, whatever. This row is just a scale factor between zero and one. And then I take that and then I add it to the current value of the function at that at that point. And so that's going to give me the new value. Okay. So in Python, it looks something like this. So let's say the position is going to be represented as a two dimensional array, or I'm sorry to value array. And so then return this or reinforcing this case is always one. So I'll take my cue old, which is going to be basically the queue value using my old state. Right. This is a two, two elements array. And then my old action. And then q new is going to be the queue value of my current state and my action. So remember, what we're doing here is this is after I've already taken the action. So I'm updating q new as my current state. So that's where I am. Yes. What I just did, what I just did you get here. Yeah. Yeah, so keep in mind, like. The queue new here is the values at time step T plus one, whereas q old is at the values of time step T. So you have, you think ahead and kind of one step in time. But I want to update the values of the table where I just was, which I think of as being the quote current time step. So I'm like projecting forward seeing where will I be if I make this action. How good or bad is this I'm going to use that value to update my values where I am now. Yes. What do you want to be zero. Yeah, actually, you know what this should probably be non inclusive. You wouldn't, if it was zero you would never learn. Right. So basically you would have no update. Yeah. So this is probably more accurate. Okay. Other questions. Right. Okay. So here we are. And so now how much, how much do I update so my TD my temporal difference error is basically this part here. Right. So again, I'm trying to figure out what my expected return is compared to the queue value of where currently am. And then I'm going to take my current value and then update it by my learning rate times this error function. So now this should start looking pretty familiar. Right. So I have an error term. I have a scale factor of how much I update it. And then I have some value that is being updated. Okay. So this is going to be performed for every pair of steps that is T and T plus one until you get to the final step. Of course that has to be handled differently because there's no next state like once we get to the goal and done. And so then the update. Given as previously is going to become basically I'm just going to update my current queue value with instead of taking some error from the subtracting the queue value for the next state from the current state. I'm just going to take the next return minus or the next reinforcement minus my current queue value. So in Python this becomes adding a test for the goal. Yeah. In this case, yeah, in this particular example. Yeah. So it's not necessarily always going to be one. And because we're taking steps through through a maze and these are discreet. So basically, imagine if I had a two dimensional maze where sometimes I have to jump on top of the box. Right. That might have a cost or something of two. Right. Or if I have a continuous, I can decide how much I move, right. Maybe the maybe like playing mini golf or something right. And you can either hit it hard or softly. Right. You go to like, what is it like on mountain Avenue. They have like this mini golf courses. Right. That I suck at. And so like you try to figure out like how much do I need to put. Right. So they could be. A force of like one, you know, Newton or whatever. And then I have a force of like point two. So these, you always have to think about what your, what the problem we're trying to solve and how that is represented in this case. It's discreet steps through a maze. All I can do is step one square and any direction. So it's got a cost of. So I'm not necessarily always known, but you basically get that from the environment by making the action. Okay. So I'll, I will show you a block stacking example. And I'll at the end, and I'll explain how that's going. So I'll try and get through this. Okay. So now I need to check and see if I'm at the goal. So in this case, the maze of its represent as a character rate and have a letter G at a goal position. All I'm going to do is just check and see if where I'm going has the letter G at it. And if it does, then I use my, my kind of goal update rather than my normal update. Okay. And so now to choose the best action for some state x, y, I just need to look at the argument for the Q function at the row that contains where I'm at, given my, given my current state. And then if we store the available actions as this array, then I can just update the state based on the action by looking at a where a is the argument of the Q function. So this is this agent environment interaction loop that is the agents in the environment, it does something, it gets some sort of feedback from that and it uses that to decide what is best to do the next time. So we have to implement the following steps that as I initialize Q, I choose some sort of non goal state, we've done randomly. And then I repeat the following. If I'm at the goal, then I update my Q function with the temporal difference era that is one minus the Q function or an R minus Q function. And I take a new random state says I'm at the goal I'm done. I reinitialize and I try to solve the problem again. Otherwise, I'm not at the goal. I select the next action. If I'm not at the first step that I'm going to update the Q old with the temporal difference error within this case is one plus Q new minus Q old. I'll shift the current state and the action to where I where that action landed me. And I'll move those to the old ones, and then I'll apply the action to get to the new state. So in the 10 by 10 maze is going to look something like this and Python so I'll initialize my Q table, choose a random starting position. And then for however many times I want to train. I'm going to execute the following so if I'm at the goal. Then I'm going to update my Q function using one minus Q old, and then I'm going to reinitialize. So this is now reinitializing my state. Otherwise, I'm going to select the next action. And then I'm going to update my Q old using this formulation and temporal difference error. I'm going to shift my current state and my current action to my old ones, and then apply my action to get to my new state. So in order to solve a perform an RL problem, you need to know the following things. You have to have the state space that is in this case, it's the size of the maze. What is the extent of things that I could that I could possibly where I could possibly be. Then the action space. What are the things I could possibly do. So in the in the maze example, this is going to move you can make in bouncing an inverted pendulum the state space is going to be, you know, have the. Angle and angular velocity would say of the pendulum and the action space is going to be like, how can I move my my hand or what torque can I apply to my pendulum to keep it balanced reinforcement for every state action. Or at least a method to calculate it's this can be shoved off to another function. But you have to have some method of extracting this so you can get very sophisticated with this like in the example that I'll show you actually draw that directly from the simulator. So you don't have the you don't have any knowledge of it ahead of time but you basically use like a representation of world physics to figure it out for you. And so this can be either a cost or reward so usually we just call everything a reward and you inverted if it's a cost. And you also need to know whether you're minimizing or maximizing returns. So depending on this, you know, you can just apply like a negative reward there are some subtle differences in behavior with more complicated problems but generally this holds. Okay, so the Python solution in the maze problem looks like the following. So we'll start with text file that specifies this 10 by 10 maze looks like this. So I'll print the maze so here's my representation and I have some walls here and I have my goal so basically when it start somewhere in the white space and have to find my way to g. So I convert this into a machine readable format of course. So I basically flattened this into a list. So this is a small by 12 array it's 12 by 12 because we have a 10 by 10 maze with walls and all sides. Right. So we're just representing this is basically places that the agent cannot go. This is mostly just for for readability right I could represent this without these walls here. It would just be pretty confusing to look at. Alright, print it out again to make sure we didn't screw it up great that looks good. So I'm going to put in some functions. I'm going to create one's going to draw this queue service over this two dimensional state space. And then one to select an action given the queue surface. So that's what these are. This is really just drawing functions. I'm not going to go through that for time. I'm going to construct these arrays that holds the queue table and updates it using temporal differences. I'm going to use one that will update the queue table using what Carlo. So remember the difference between temporal difference and Monte Carlo temporal difference I get to update my values every time I make an action and see what happened. Monte Carlo I have to wait until I get to the end. And then I can see based on an average of how good every action was I'll update that. So I need to have four possible actions for each position. These are represented changes in rows and columns so we have plus one and minus one. And then I'll set the queue value for the invalid actions to infinity. And so an invalid action is going to be basically where I run into a wall. So if I end up here, right, going right should be an invalid action because there's a wall there. So by doing that I can basically say that this is by saying this has a cost of an infinite cost. I'm never going to take this because I'm not allowed to. So now for some parameters so we'll run 100,000 interactions, that is 100,000 updates. I'll let my learning rate be 0.1 and then I also have this epsilon. This is a random action probability. What's the random action? Well, let's imagine that I explore. And I end up at the goal. So I could take one step forward one step back one step forward one step back one step forward one step back and one step down. Let's step down lands we have the goal, right. Doesn't mean I need to take like three forward back steps and not a dog trying to find a place to nap. So in before I step down. Instead, it's a possibility that maybe I took seven steps when I could have taken one. So there's a random action allows me to basically pop out of some sufficient strategy that's still some optimal. And so by can do that by taking a random action, it might get might allow me might maybe take like some random action that that's never get me to the goal. But there's a chance I'm going to get a better, better solution. So it's not need to keep history trace of positions and reinforcements and then I need to use that to update the Monte Carlo version of Q. So I'll store this trace of x, y and a. And so now this is just for initialization display. So now let me initialize. So here's my start position. And then this is my, my first action that's just the index of where I'm going to look for my for my actions. I'll keep track of the trials and the number of times that I've hit the goal. So now we can see, okay, if I found the goal, I'll perform the goal update. If not, I'll perform the regular update. So I'm going to look at the trace. And then I'm going to update this based on an average of all of those, all of those costs. Then if I'm done, I randomly start new position. If I'm not at the goal, then I pick the next action. This can be done randomly. If I choose a random value that's less than that, epsilon, otherwise I'll choose the best action from the Q function. So, let's watch this run. So here's the T. D. Q policy. Here is the Monte Carlo Q policy. This is what it's done. Most recent trial. So that read down, obviously, is the goal. And then this is going to be the number of steps per goal at each trial. So just take a look at this and see, you know, what do you think is better at solving this temporal difference or Monte Carlo. So if I'm here, I'm trying to get to the goal. The error represents like what the best action is for this cell. So let's say I start here. Follow the arrows. Great. Let's start in the same position in Monte Carlo. That's this one. Right. Okay. Oh, oh crap. I got stuck. So pretty clearly for this, this type of problem, the temporal difference is a better solution. You can see, obviously in the Monte Carlo, I find plenty of places where if I start at some trajectory, I end up, you know, in a place where I just sort of go back and forth if I had dear to the, the Q policy. I might be able to pop out of this by doing a random action, but there's no real guarantee. Okay. So now let me show you a quick example of Monte Carlo learning for a different task. So let me, all right. Let me run this environment. This is basically a unity simulator of block stacking. So when the sometimes it crashes, hopefully it won't crash. If it does crash, it goes, I'll show a video or something. So I'm going to train for 300 steps. Okay. So we have two blocks. My goal is to stack the. There he goes. Okay. My goal is to stack the C block on top of the B block and what it's doing is it's selecting an action relative to the surface of the B block. You can see that there are times where it's basically jumping around and moving, you know, it's not even touching the surface of the B block. And sometimes it does. So we can see here in these updates every so often, one of these, one of these rollouts will pop out showing, showing me like what the reward at the main reward that it's been getting. So this is using Monte Carlo learning. What this is doing is for the first 100 steps. It's really just randomly exploring. So trying to see if I randomly explored the space, what looks like it's a viable solution versus what's not. And then the actual learning starts. So you can see here it's kind of out there exploring the space. It's really sort of verifying that there are no good moves out here. And then eventually it might sometimes it takes a while to converge. So I'm not sure it's going to get there in 300 steps. But eventually it will, it's kind of going off spinning off in space. You'll see it doing its thing there. Eventually it might find. Oh, hey, here's a good action. This actually gives me a good reward. I should try more of this. Let's see if we're going to get there. I did not. Okay, let me try again. Usually everyone's like 500 but sometimes that it as it starts to succeed. It takes longer and then sometimes takes like a bit more time. So you can see it exploring. Okay. Not learning a whole lot. Or at least it's learning what bad moves are it found something. So it should try to do more like that in the future. But right now it's still in the random exploration phase. So let's see if it. We do 100 steps of random exploration still there. Right. So now it's found it may be found a couple of actions that might be pretty good or at least closer to optimal. So you can see now it's starting to stack. So now it does some more exploration. It tries some some more stuff. We can see that it's getting mean reward of roughly 37. And in this case the reward is like 1000 for stacking the right the first time and then as a discount of 100 each time. I'm again not sure it's actually going to learn anything here. There we go. Starting to get something. So now you can see it's starting to kind of learn a bit. And then it's a we'll see what the reward is. So here now. Mean reward is like 32. It should start to get slightly higher rewards here. So 31.2. Now it's like 68.4. So now it seems to kind of start to converge on some sort of viable solution. So this is an example of Monte Carlo learning because it has to terminate the episode in this case it gets 10 attempts or it stacks successfully. And so once it reaches the goal or a time's out it will basically look at all the actions that it took and see how good or bad they are. This is using a particular type of method called a deterministic policy gradient. So it's actually using a neural network to optimize the weights. In particular, there are two neural networks, one called an actor and one called a critic. So as you can imagine the actor chooses an action and it also predicts how good the things the action will be. And then the critic basically says, yeah, you're great or boo you suck based on the action. And so the actor tries to make better actions. The critic tried to get better at predicting how good the actors actions are. And so then both of those losses flow backwards into those respective neural networks to optimize those weights. So this should terminate relatively soon and we can see what the final reward was, and we can try to evaluate this briefly. So it probably could have done with training for like a little bit longer. So you can see had a mean reward of 169. So it kind of. Yeah. In this case it's 1000. So basically if we were, if we're perfectly stacking the first time we would have a reward of 1000. So if it's, if it, the way the reward is set up here it's 1000 for stacking perfectly the first time. So this 100 for every additional attempt so if it stacks right the next time it gets a reward of 900 and then like 800 the third time and so on. Then it gets a reward of negative one for missing the block entirely. And then a reward of nine for touching the block, but not stacking successfully. It depends on what so again we're using the world physics here. So basically if it, if it stacks off center but it stays stacked. That would be a reward in the hundreds, depending on which attempt it was on. So that stack and things. Yeah. Perfectly. Yeah. Yeah. But of course you're more likely to stack perfectly the first time if you put it exactly centered. Right. So if it trains long enough, you know, it should be, it should approximate that. Okay. Questions. Yeah. So in this case we think about if we know the maximum reward is 1000. Right. So we can think that on average. This would probably give him what you know about the reward shape. All right. 1000 for the first attempt 900 for the second attempt. So basically between this number of falls between what would be a good stack between the eighth and ninth attempt. So probably on average, the model as it stands right now would probably stack successfully somewhere between that eighth and ninth attempt. Maybe a little bit better, because there's a lot of noise in this. And so when you actually evaluate the model sometimes it performs better than it appears to you at the end of training. But again, it's all, it's all about you have to understand like what the values in your reward actually signify. And so this in this case the reward value were chosen quite deliberately to kind of encourage it toward a solution. With the appropriate amount of exploration like once it finds solution, it gets a very high signal saying I should be trying more of these things. Would it be right to assume that the reward is no, it's not very difficult learning. It's that may be the case. So you can you can use like these reward shaping strategies to specify like what types of things you want to encourage the agent to do. But you could just as well train this with say a reward of negative one for failing the stack and it's a reward of one for stacking. Yeah, it might take like a little bit longer to converge possibly, but you might actually converge like a better solution. So for example, with like the off center stack it might get a really high reward and that's as good as it ever gets so it's like, oh well, this is a really good solution. It's like perfect, but I'll keep exploiting this, whereas with a more with the kind of the rewards of lesser magnitude, you might encourage it toward like a more, like a more perfect solution, though it might take longer. Yeah. Yeah. Yeah. Yeah, so you're, you know, because you store the action to every step so if you see the action is represented in zero to 1000s basically in this case values that are closer to 500 mean it's closer to getting it centered. So in this one we have five 78 and 530. So the reward is nine. So it got it on the block, but maybe it didn't quite stack it fell off. So you keep track of all of those, of each of the rewards and each of the states, and you can say for this action that I took a God reward of nine, just like, not terrible but also didn't get me what I want for this reward. This action got a reward of like 600. Right. So this is better than this other one that got me only a reward of nine. So like you're getting information from the following. Yeah until the end. Yeah. Other questions good questions. Yeah. Yeah. Yeah. In this case, yes. Yeah. So in this case, if it just sort of plops the block out there in space and it never touches the destination blocks the reward of negative one. When you saw like the block kind of moving around in space. It's really kind of exploring the space and learning like there are no good actions here in this region. So eventually I want to try and move out of this region. Okay. Other questions. Yeah. Okay. Yeah. Yeah. Yeah. Yeah. Keep in mind that like, imagine a large space where the block is in the center. Right. That space is defined is bounded at zero and a thousand. So values very close to zero or there's like way off in space. Very close to a thousand or way off in space. Values close to 500 are going to be close to the block. But the way that I constructed the action space here is that the block is like really small. I'm not sure about like the stand of the block in the in the action space is but it's very, very small. So basically you want. If I picked 500 500 exactly. It should stack. We also add. We also make this more difficult actually because we add a little bit of physics noise to it. So if you imagine this is a virtual environment, I can be hyper precise. But whenever you actually stacking things, when you release it, there's a slight motion. Right. So basically we add a little bit of a jitter or a push. So sometimes even if it stacks well, there's a bit of noise there that actually sort of simulates this release and it actually ended falling off. So there's a number of things we've done to make this problem kind of harder and more realistic that makes it, you know, at least. It's still pretty easy for you of course learning to solve, but like, you know, at least somewhat challenging. Okay. All right, I better talk about assignment for. Before we adjourn. So what you're going to be doing here is classification of hand drawn digits. So you're going to be given a solution to assignment to similar to what you've seen before. And then you need to extend it into a neural network classifier class. This is using MNIST, but it's not a convolutional net yet this is still fully connected net. We'll do convolutional nets next. And then you're also going to need to find the confusion matrix function. So here's the neural network class should look a lot like what we've seen before. So, a lot similar to some of your two solutions. Then we have, we got our optimizers we got an instance of neural network. So what you need to do then is you can test using these functions. You need to extend the neural network class. So it inherits from the network classifier inherits from neural network. So again, you'll have access to all the same functions, but you need to override the train function, the error F function, the grading F function and the use function. In addition, you also need to define make indicator bars and softmax functions. All of these are given in some form in previous notebooks. It's just this you're going to have to pull from. I want to say nine, 10 and 12, probably, I will double check that. But there's a couple of different places you're going to look for some of these different things. So you want to look in the implementations of classifiers that we've done. Look at how we override the train function and the error function gradient function and the use function and then create versions of the make indicator bars and the softmax function for what we've done before. You can do just create a sample test of this new class. So basically what this is what this is doing is it's just classifying random numbers into instances of class 01 or two, depending on the function. Or two, depending on whether they're less than 25 between 25 and 75 or between or greater than 75. So basically, you can test that function using using this. So this would look something like this. And these values are just offsets. You can see them both. So we just show T plus five just so they don't overlap perfectly. Then for the hand drawn digits. So you can download MNIST. This deep learning site goes down a whole lot, but you can also get it from here. So we've, there's a page at the Washington CS department. If you have trouble with this, I have MNIST pickle saved. So like, if some for some reason, either these sites work, just let me know I'll put it up on canvas. So the final file is already partitioned. So open it. You've seen this before, a thousand training samples, 10,000 valentest samples. So these are these, these train classes, these are the digits. If we look at those 784 columns, we now know that those are the pixels rights of these. If we split all of them up, you'll see that we just plot the values and we have a bunch of things between zero and one. So we have pixel intensities for the image right so you have pixel intensity of zero, and then suddenly we start to see actual pixels that are non zero. And remember these are laid out in rows. So that's why I have this periodicity in this plot. So you can rearrange them into a square. So let's reshape them look at the numbers. We print this image, we can see it's a five. Give you a function to turn it into a grayscale color map. And then you can plot the negative image. So let's look at the first 100 images and plot them using the labels as titles. So these are the first 100 images. So you want to check the proportions of each digit to see the roughly even we found they are roughly 10% belongs to each each class. So they're all very, very close to point one. So now let's try the first experiment so training neural net five hidden units in one layer and a small number of epochs, as you can train your neural net classifier using this these these settings and see what the final training error is. So now you have to run some longer experiments. So first you have to write code to do the following. So you have to try five different hidden layer structures, doesn't really matter what they are. You can choose you to train that for each one training network for 500 epochs collect the percent of sample script classified for the given train validate and test partitions. So create collect these into a pandas data frame. You want to log the times this is going to be how long the network took to train in seconds. So we've done that before you see you start a timer and then subtract the end time from the start time or the start time from the end time sorry. And then what you're going to do is you're going to retrain a network using the best hidden layer structure as judge by the percent correct on the validation set. So basically you've done a network search using the percent correct on the validation set use that to figure out which of your network architectures is the quote best one. So then you retrain this network and then use this network to find the find several images where that it gets wrong. So basically look in the test set and see where the networks probability of the correct class is closest to zero. So these are the images for which your best network actually does the worst on. Right. So then you draw these images and you discuss why your network might not be doing well for those images. For example, are there a bunch of fourth of a book like nine just something like that you know depending on how your network is performing. Then you need to write a confusion matrix function. So this is going to return the confusion matrix or any classification problem as a pandas data frame. We've shown this in lecture 12. So this needs to take in two arguments the predicted class and the true class. So it should look something like this, where you take in why classes and test and outputs something like that. If you want to do some of the coloring. You can use what we showed in lecture. I think to do that. Okay. So 50% for the correct code 50% for the experimentation and discussion. We'll have the greater I have not put this up yet I will do that this evening. Same, you know, same procedure. So put this in your, in your folder with your, with your notebook and run it. Finally, you can do extra credit for combining the train test and validate partitions into two matrices. And then you use Adam Ray Lou and then a single value of learning rate number of epochs compare several layer architectures by applying the cross wall validation as defined in lecture 10. And then show the results and discuss which architectures you find works best and how you determine this. Okay, we're out of time. So let you go. I'll go back to my office and have office hours in a few minutes.