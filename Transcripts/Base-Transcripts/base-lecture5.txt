 Okay, let's go ahead and get started. So, hope you all made a start on the assignment. So it's going to be due a week from Thursday by default. So hopefully that's going to be okay. I will have office hours this afternoon. And as I mentioned, I think I'm on the faculty and we may have to cancel some office hours, depending on like what we're doing with that. But currently this this week, it looks like I'm going to head off. If you're interested in our faculty hiring process, I encourage you to come visit the faculty candidates talks. There's usually going to be on using your Monday or a Thursday at 11am. So, I'm going to add a little bit more of a announcement to the CS list windows are scheduled. Other than that, anybody got any questions concerns about the assignment? Yeah. Good question. Don't you don't need to do that here. I, that is probably good skill to practice. I had this conversation the other day. So, we will have one assignment that is explicitly about that and from that point on you will be expected to do train test splits. But. Any other questions. All right. So, who's ready to talk about neural networks. So, I understand neural networks can be somewhat scary. So, going to introduce the general networks in the easiest way possible everybody please open your textbooks to page one. This is neural networks for babies. I'm going to read your neural networks for babies real quick. This is a ball. This is a neuron. It sends messages throughout your body. Give the neuron input and output and it can help us learn. Give the ball input and output and acts like a neuron. The neuron can have one input and output or many starts to sound familiar. Is there a red animal in this picture. The neuron can tell us. Based on its input. When the neuron has an answer it sends its own message. This animal have eight arms. The neuron could tell us based on its input. When the neuron has an answer it sends its own message to plus two plus two plus two equals eight. Where did the messages go. Neurons talk to each other. They connect in a network. Input neurons look at parts of the picture. Output neurons have answers to the picture. Neurons in between don't see the pictures or give answers. They're hidden. How do the hidden neurons learn to decide. Training data can have correct labels on them. After training the network has learned to label new pictures. A really big network can have can solve even harder problems with the help of computers. Now you know neural networks. Thank you. Okay. Class class dismissed now you know neural networks please go home and do assignment to. Now as you can see there's a whole lot probably missing from that. You know anything about neural networks the treatment of convolutions and back prop is not up to the standards of a major AI conference. So I think the part of the end was is now you know neural networks is loaded weak. So for the remainder of this we will go from linear regression to neural networks, not the baby version will fill in some of the details. All right. So people don't believe me when I say this book is real so like I had to. Oh, I read my kid recently repeatedly. Now my daughter's like at the point where like she vastly prefers my wife to read things. She's like 19. I'm not offended or anything. She was read the same books over and over again like I don't have to do that. But now like every she had a while where like this was her favorite book like a week ago. So like she was sent taking it to my wife to have it ever read it my wife's a historian. She's very smart but not in computer science and when she goes to get to the party like now you know neural networks like no I don't. So, all right, let me share my screens. And we'll go through neural networks for adults. All right, so, if you remember. Hide this. That. Okay. So if you remember from the linear regression lecture, I briefly mentioned that linear models can't solve the X or a problem so I assume everybody knows what the X or a problem is or what the X or operation is right. Basically if I have two inputs and then I'm only looking for those cases where one of those things is true. Right. So if I have X is zero, Y is zero. And either of those things is true so X X or Y is zero. If Y is true but X is false then X regular or Y is true. And so is X X or Y is exclusive or the same is true if the ones and zeros are flipped but if both of them are true right X and Y would be true, hence the one X or Y, right, one of these is true, but X X or Y is zero. And so this is the exclusive or only one of the inputs can be true for the X or to be true. So we can also graph this so in the version below, we have the blue X is where the X or is true. And then the red the black circles where the X or is false right so if I have zero and zero, it's false. Right, if I have one and one it's also false, but in the cases where only one of them is one and the other one is zero then it's true. So you can tell by looking at this already like if I'm graphing like this. This is clearly not a problem that a linear model can solve. Right, there's no way to fit a line to these points when the points are in a square. So that should be pretty evident. So what we do now is when to basically present the solution to the X or problem done with matrices. To motivate how we use the introduction non linearies to solve more complex problems so basically working backwards. This is the solution to the extra problem if I have the following weights, one, one, one, and one. Then, see equals zero and one w equals one and negative two and then this bias is zero. So let's write these just in NumPy form. So we set those two variables. Now I'll set X to be some matrix that contains the inputs right so the first column can be taken to be X the second column can be taken to be Y so zero zero zero one one zero one one. So write this as a NumPy array, we get the following. So now if I take those weights w and then multiply the input matrix by that first layer, I'm going to get the following right so if I do X at w. I get this zero zero one one one one and two two. So now I take this bias vector C. Add that. Now this is going to be this non linear step. Right so the so far I've just done a linear operation I to the inputs time some weights and then add some bias. Right, that gets me to this point. This is not the solution to the extra problem as we can see. Right, we have zero negative one one zero one zero and two one it's not clear how that maps to the solution the extra problem. Now what I need is some function that's going to allow me to take the output or take the take the input to an output that begins at zero rises to one and then drops zero. So I'm going to apply this non linear transformation, where it is less than zero it's zero, otherwise it's the end of the one take the max of zero and z. So I can write this function f of z that does that. And so then f of z will give me the following output. Right, so now I have zero zero one zero one zero and two one. So all that's done is basically taken this part here and the only thing that's changed is it's changed this negative one into a zero. And now I have these things mapped into a learnable space. Right, so what's going on here. We have the input being zero and zero. The output is zero. If the input is to the output is one. And if the input is one, the output is zero, and we have two cases here that's why we see one point there. The output of these linear steps effectively, among other things, turned this, this point here into basically to one. This one here used to be before the non linear operation used to be kind of down here. Right, this is zero and negative one. So if you see where my cursor is, and then the blue X and then the point in the top right, you can see you fit a line to that. Right, so that was the linearization of the input. But now applying this non linear step, this gets me this. So now it's in a continuous space that I can actually learn. So now I multiply that final weight vector w. So we had inputs X times big W, which is one set of weights plus the bias. Now I'm taking that applying some function to it to turn it into this space here, then I'm applying another set of weights to it. And that gives me the output here. 0110, which if you remember, corresponds to the X or. So now all in one function, I can effectively take this neural network where I have the weights and biases pre specified. So that gives me the output that is the X or for those inputs. So if my inputs are 000110 and 11, I apply this function X or an N. And it gives me 1001. So I've done this all in one line. Right, so it's kind of hard to tease apart exactly what the different components are. So I'll write it more legibly out to a version of X or an N. So we take the hidden weights. Right. We first of all, we take this input X. We have specified weights W and bias vector C. So now the hidden weights is going to be, I should probably call this hidden output, I guess. X at W plus C. Then active stands for activation functions to go into a minute. This is some function F that we defined earlier over that value. And then output is going to be the output of this. So the activation times another set of weights. And all that will give me the X or. Okay, so by introducing this nonlinear function that allows me to take some inputs that are mapped to a line and then deform that and then multiply some other weights by that will allow me to solve this function that solve this problem that a linear function was not able to do. Any questions. So, this example, of course, involved no training of basically gave you the solution and what the right weights were and then showed you how once those weights are in place, we can use that to solve this problem. If you want to know how we actually train to solve this particular problem. There is an article here that you can prove. I recommend not really doing that until we get to the end of the notebook because it's going to assume that you know, neural network operations like back propagation, but you presumably don't yet. So, I'm not going to be able to do that either. But once you do if you're interested in how to actually train a neural network to solve a problem like X or is very logical. This can show you how to do that. Okay. So, basically the takeaway that I hope you have seen is that all neural networks have the same basic form where we have some function applied over some other operation where that operation is just a linear operation. W X plus B. This should look exactly like what we're doing in the previous lectures. Right. We just have some, some affine transformation weights W shift it by some bias vector B. And that gives me the output. So, I have a line. And I'm just trying to take inputs and map it to some place on that line. So, what's F, F is some nonlinear function. Okay. These functions usually called activation functions. And so, last time we saw how we can do these fixed nonlinear inputs to introduce nonlinearity when it seems like there's not a linear solution to the problem. So this is somewhat labor intensive in that we first look at all of the data and see that there are some places where there actually appears to be some correlation between some parameter in the output, but that correlation is not linear. So if you remember in lecture four, we had like resistance of the vessel on the Y axis, and this thing called the food number on the x axis and there was some sort of quadratic looking curve. We're basically trying to figure out what actually is this curve. It looks quadratic. Is it actually quadratic. It turned out that the best answer is probably like X to the power of 13 or something. That took a lot of effort trying trying different feature functions over those inputs to see like what exponent allowed me to convert that input into a linear function or to convert the input into a form where I can apply linear function get the output. Right. And that's not going to scale very easily. What if we had some arbitrary way of introducing nonlinearities. So we don't know which nonlinear functions to use. But what we can do is we can pick a form of nonlinear function that has its own parameters or weights that will determine kind of the grade of the nonlinearity to introduce. Right. I'm going to, I know I'm going to deform the data in some nonlinear way. How much what kind of nonlinearity and how nonlinear am I going to be going to be making that transformation. So we want the parameters of this to control the actual shape of the function. And there are a bunch of possibilities for such functions and sure you can think of any number of nonlinear functions that just satisfy the property of being nonlinear. But we need some desirable properties that I'll illustrate here and then go into why we want those properties later. So first of all, I want to be computationally simple. One major reason for this is that we're going to be doing this a lot. We don't need to have some wild polynomial being calculated for every data point over 1000 trading the box or more. We also want initial small weight values. We want the value that function to be close to linear. And then as the weights, the magnitude of the weights increases the function becomes increasingly nonlinear. So now you can think of, you know, let's say we on the x axis, we want it to be closer to linear and then as the values grow to extremes, you want to be more nonlinear. We also want the derivative of the function to be computationally simple for similar reasons as well as the function itself to be computationally simple. We also want the magnitude of the derivative to decrease as the weight magnitudes grow. And perhaps we want this to be asymptotically. This is not always true, but it's generally a desirable property. We also want the maximum value of the magnitude of derivative to be limited. So we want the derivative itself to basically have, you know, some known maximum value. So, let's go about driving you know some properties from these properties, what a desirable activation function might look like. So, let's start with just a linear weighted sum using the, the familiar formula. X T W. So for some input sample x. We want s the output of this function to be small. If the magnitudes of the weights in W are near zero, as those values increases, we want the magnitude of s to also increase. So, for example, if we have some, some data here, just weights that are kind of arbitrarily chosen numbers. And then if we change these values here, let's just say, make them, you know, make a bunch of them much bigger. Right, we can also see value of s also increases, right, that should be pretty intuitive, because we're still dealing in the world of linear functions right now. So now, if we want, we want this function s, let's try to construct a function where s is the shape of the derivative. That we want to be the s is going to be the derivative of some function, and I'm going to derive the function f, where s is derivative by first constructing derivative and then seeing what the interval of the derivative of that function is. Okay. So, first of all, we know a couple of things if I were to take the negative s and use that as an exponent. So, that means that I can take whatever function that is, and make it asymptotically decreased to zero. So we want that, according to point number four. So we can use base E base E has a lot of nice properties, one of which is that ease derivative is. The derivative of the X is, I should say, e to the x right, and the anti derivative of the X is. So the E is allows us to deal in natural logarithms has some very nice properties with regard to differentiation and integration. So first of all, I'll do here is I will just plot some inputs that are evenly spaced, and then I will plot the E to the negative s. So of course, as we, as we all know, we can see that as. So, I'm going to do this, and the value of s increases, e to the negative s is going to decrease and approach zero. Okay, so now remember we're constructing a derivative want the maximum value of that to be the, to be, to be, to limit the maximum value of that derivative. So, here what I can do is I can then take, say, one divided by one plus either the negative s. Now, unlike this one where the maximum value is basically going to be infinity, right, as I get more and more negative. And the minimum value has a limit by doing this. I now have basically a limit on the minimum and maximum value. Right. And so the maximum value is the important one they want to control here. And by virtue of that also, we would also want like the negative of this to also be limited. Right now we're in a window of a interval of zero to one. So we don't have that problem, but we'll see what happens with that in future. Okay, so this doesn't be once as s grows more and more negative, but we also as s becomes more positive we want to bring this function down to zero. So one way I can do that is just by taking one minus one over right so now this is going to start at close to one and eventually decrease to zero. So I want some combination of these. Right. So I basically want something that starts like this and then rises and then falls again. So I kind of want the left half of this function at the top combined at the right half this function at the bottom. So what if I just multiply them. Okay. So that looks pretty good. Right. This is doing what I want. It has as opposed to zero in at both extremes. And then the minimum value is limited to point two five. So the last desirability that we want is once it's to be computationally simple. So we take a look at this function. And we can see that there are some common terms. So for example, what I can do here is I'll just create a bunch more points. And then I will recompute this function. So we see we have, you know, basically, e to the negative s, in terms of, you know, one plus one over. So all numbers we know how to deal with. One and effectively with an exponent. Okay. Great. So now what I can do is I can, I've got this and I've got this function now let me see if it has property if it's anti derivative has properties that are also desirable. One thing, one library you can use this is something called sim pi symbolic Python. This is the only time we're going to use it in class, but it is kind of fun to play with. Basically what this allows you to do is is define operations, and then actually perform differentiation or integration on them and it will give you the resulting formula so effectively, you can use this symbolic Python to sort of solve those, you know, you know, pre calculus problems that you've probably done before. So first need to in it printing, allow me to use Unicode. So I'll define a symbol, S and so just S symbol will just be. To the symbol, symbol s. So now I can run this dot diff function. So what this does. This is just going to differentiate F with respect to the symbols. So in this case, I some function f of s, but I want to differentiate with respect to s. So what I do here now is I've got, I defined my function. So s to the fourth. And I want to differentiate with respect to which symbol s. So. And now we can see if you remember your, your, your calculus. We should expect the derivative of s to the forest to be forest of the third. And so this will actually put that out for us. Okay, so we see the simple integrate function should be pretty, pretty evident what that does. This is going to integrate the function with respect to the symbol. So what I'll do now is I will define the function that I defined above, right. One divided by one plus. So I'm just writing this in terms in the simple formula in terms of the symbol, I just defined s. So now I want to integrate the function. Y times one minus Y with respect to s. So this is going to give me this one over one plus to the negative s. So if f of s equals this, then the derivative of s is f of s times one minus s. So we are now just arrived at the common sigmoid function using neural networks. So remembering that s above was defined as just a linear function, x t w. Then we have some function x some function of x parameterized by w equals one over one plus each of the negative x t w. Right. And so remember what this is this is just going to be the output of that linear operation, where we have our weights, and then make some sort of prediction by multiplying the input by those. Okay, questions about this so far. Okay. So let me define some helper functions. So I'll just define f of s. And then I will define its derivative. So df, and then I'll plot the function and then versus the value of s and then derivative function versus value s. And so we get this. So this function s in blue. We can see that it's it rises asymptotically to one, whereas it's derivative is that function we saw before I go to caps out at 2.5, and has asymptotes at zero at both extremes. So this is called the sigmoid function because it looks a bit like an s. Right. And it is bounded between zero and one. And so now we can use this function because it has these nice properties that we have identified in neural network operations so first thing we're going to do before we get to like the neural network of hidden layers is to just apply s g d to fit the sigmoid function to some data. So we're still working more or less in the world of linear regression is just we have this final step of applying the sigmoid function. So that we have some sort of nonlinear output. Okay. All right, so we're going to do is want to find weight values that minimize the sum of squared errors in the output dysfunction. So what I will do here is I'll just define again some points. I'll take this function here and add some noise. So basically this is all simple. I'll take T times or equals x times point point one, plus some randomly sampled noise from the uniform distribution. And it's going to give me some data that looks like this right this looks like I could reasonably fit a line to this data might not be the best fit in the world. But I could do it, but it might be nice if that line had a bit of a wiggle in it. Right. Maybe that would fit to the data a little bit better than just a straight line. So, if you think about what this data represents the values and x are at this point those are just inputs. They don't actually represent any real data points. So, the values and T though, are targeted values. And these are derived in this case, by some known function applied to x, where we've applied some random noise to it so those targets are not going to be neatly identified with a terminus to be identifiable from x. But you should be able to still fit a curve to it. So here's a training function. So, here I will put in my my inputs my targets my learning rate and some epochs and train for, you know, a certain number of steps. So what I'll do here is I'll just train this for 100 epochs and then plot the results. So here we go after training ends up with these weights, in this case, negative 1.7.4. If we apply F here, which is a sigmoid function. This is going to give me this output for the sample, according to the input and then apply times the weights and then apply some nonlinear function. So we get this and we can see that it's probably a decent fit. Right. This is a, you can see that it's nonlinear. And it does fit to this data decently well right I'm not sure that it's necessarily a better fit than the linear operation. It's hard to say, but you can see that also the nonlinearity being applied is quite slight. Right. It's this is not hugely different from a line. So let me create some other data. So different distribution, different function. So 1 plus negative x times 0.1. So now we get this again, probably mostly linear, but again could be fit to with a nonlinear function. Let me try this. Okay, and that fits pretty well as well. So, seems like I got a decent way of taking my sigmoid function, computing an output according to linear operation applying the sigmoid function to that. And I can use that to fit to what might be somewhat nonlinear data. So the question now is what if I have this data. How could I fit this function to that data. Right. This is nonlinear. I would want my sigmoid function to be able to capture the nonlinear area that is obviously president this data. What I've got is the ability to train weights. And once those weights are trained, take those inputs times the weights apply my sigmoid function and then fit to some fit to some nonlinear data. So, if I try it with with this data with this kind of in this curve here, what I end up with is here are the weights that it's trained. And the result is that it gives me this. That didn't seem to work very well. Let me try it again. Try me a computer more data. Right. So, if it's decently well on like one part of the curve, but not the rest. Right. So we can see here in the when x is positive. It seems to be kind of fitting to that. Okay. But when x is negative, it's just more or less a line. So, there seems to be some sort of inflection point here around x equals zero. And we want to find out what kinds of weights will make the sigmoid function. And then we'll look down from negative infinity to zero and then rise again. Right. This is what this is what will be needed to actually fit to this data here, whereas here we're only kind of getting one side of the equation. If at all. So we don't know what those weights are. And they're not really easy to find, because the only weights that I've got here are two weights that effectively. So, I'm not going to use a bias as before. So how much am I shifting up and down the y axis. And then the second weight here is what do I apply to the linear operation or what way do I apply in the linear operation before I apply the non linearity. So I'm not really doing anything that affects the slope of this non linearity in any real way. On doing is like, I've defined the sigmoid function, and I apply that over the output of the waste time to input. So I may be using two of these functions and adding them together. So what if they're like one set of weights that caused F of s to decrease until around zero, and then remain roughly flat here. And then another set of weights we'll call those V, such that if I take x times V and apply F over that. It would be roughly flat and then start to increase. Right. So we respect there now to be two sets of weights we'll call them w and V. So now we're now we're talking about the world of multi layered neural networks. All right. One layer is going to have, in this case, two units that output F of x times w, each with their own w, and the second layer is going to have a single linear unit with its own w. So does everybody get the motivation for having these two different sets of weights. Okay. All right, so now let's talk about linear models, as we're familiar with, as neural networks. What I'll do here. And I cannot guarantee you we're going to get through the entirety of this notebook today. It is fairly long. And there's quite a bit of math. But what I'll do, like usual is I'll present the mathematics first. Go through. How we are deriving the different operations that we're going to be using in the construction of the neural network. And then at the end, I'll have the Python version that translates the mathematics into code and demonstrates, you know, how you would actually write these operations. So, remember how, how we do just linear modeling. We have inputs x targets T. And then we have for every sample K, we want to find the weights K that minimizes the squared error and the kth output. So we'll say, for x sub K, I want to find the weight vector w sub K that minimizes the output between T sub K and the actual, or between the prediction y sub K and the actual output T sub K. Okay. Okay. And then we'll use that to make predictions. So what we'll do to make this, make this go faster is we'll take all these weight vectors w sub K. What will collect them as columns in a big weight matrix w. Some, the X with the total above it I'll use to denote x with the constant one column. So remember we always add this bias column so we have values against which we can train the bias weights. So, I get value for the kth output for the nth sample is going to be T sub n K. So, remember how we set this up in a linear model if I have n samples, number of things that I've got each with D dimensions, number of things I've measured about that sample. And then I have K things that I want to predict about that. So this could be just a single value could be multiple values. So if I have the first sample, and I'm trying to, and I'm trying to put it like the second thing about that in the output. So, this would be assuming that we skip the first one actually being the second index zero, this would be, you know, T sub one comma two or something like that. Okay. So we're using this to meet the calculate the error. So we have E of w. And so what I'm going to do is going to take for all samples for all outputs. So some of all n over the sum of all K. So I'm going to take that, that output for that sample minus the prediction, and then I'm going to square it I need to sum this for all combinations. Okay. So now I'm looking for effectively the value of w that will allow me to minimize this function. So w can also be calculated as remember if we rewrite these as matrix operations as x sub T increase the font size a little bit. x told a T x to, and then I transpose this. And then, or to the, the inverse of this sorry, and then x told to transpose times T. So we can compare this to solving for the value of w in notebook three. So what's the contents of all these matrices. Remember w is going to be w for every, associated with every output. And every input, right. So for their K things I'm trying to predict, and then D dimensions to every input. That's going to have a value weight value that's correlated to each of them. In a linear model what I can do is I can actually look at my weights and decide what was most important for predicting the output in a neural network that becomes less easy, right, because of these hidden layers. So, if w looks like this, then my prediction y is going to be biased x times w. What are the shapes of these things. So I have n samples D dimensions, add one for the bias so x to is n by D plus one, w has to be D plus one by K in order to multiply with this course. And so if I multiply these two things together then why must be an n by K matrix. So this should make sense, because the things that that are represented and why those are going to be the K predictions for each of the n samples. So if I have a hundred samples, kind of predicting two things about it should have 200 individual numbers represented as 100 rows and two columns. So, for every element of that output matrix y. It's going to be equal to the n sample with the bias times the kth weight vector, and this can be drawn kind of like this. So what's going on here. So I have my different inputs x zero through x D. And I'm going to multiply each of those by the associated weight, right. And then some those that's going to give me the associated output. So now, if I want to add nonlinear combinations of inputs. So I'm going to try to transform x into some function will call it phi of x. So for example, if phi of x is phi or this big weight matrix. And, you know, if I'm trying to add nonlinear combinations, I might do is I might raise this to a power or something. So this is the same thing as introducing nonlinear features in the inputs. So now what you can think of that is that instead of raising it to a power and stacking a bunch of those things together. So I have some arbitrary function that is applied to this. And I'm just, then I'm just going to replace the output x by five. And so I now use phi to represent phi of x. And so therefore, phi of phi sub n is going to be dysfunction phi whatever it is. Over x sub n. So I'm searching for a function. And now that functions can also just be represented as a linear operation with a set of weights. Right. So if I have f of x parameterized by double you my goal is try to try to solve for double you using some algorithm like s gd that allows you to minimize the error between my predictions and my outputs. So what I'm doing here, this is the part that's going to fit into the interior of the neural network and what I'm going to do is I'm going to try and predict the weights that give me the output, and then minimize the prediction error, when there are multiple transformations being performed at every step. So when we talk about, I should have pasted this, pasted this again down there. So when I say all neural networks have the same basic shape which I did like way up here I think. So if they all neural networks have the same basic form, what we can now do is we can expect this function to kind of be stacked on top of each other so I'll take this output. I'll perform another operation over it, and maybe another operation over that. And every layer in this neural network represents one more function being applied. Right. So all of these things can now just be nested. So, you know, different functions. Yeah, so remember a function you're just talking about F of some x parameterized by W, which means that the W could be different each time. So, when we talk about the when we talk about neural networks. Right. So, you can do this is going to come and use what some point. Right. So, you know, you see in this diagram that you see neural networks written like this right so every layer here there's every column red dots is a function. So, you know, each one of these is parameterized very different sets set of weights, meaning each one of these is a different function. And our goal is now we're just trying to solve for those ways to parameterize each of these individual functions at the same time. It's a function of a function of a function of however many layers you have. Yeah. So are we implementing these functions to the basic function that you have a new thing in your pocket. So, yeah, actively. Yes. Yeah. So I'll come to the I'll come to the mathematics in a moment, but you can kind of think of it. Like this. So, if I have just a linear method linear model it's got one input and or, you know, a known input and just like a known set of outputs it's a linear transformation between them. I can compute the error and then use SGT to optimize the weights that are going to minimize the right. So, if we insert a hidden layer in the middle of that. If we just look at those first two layers. And let's just disregard the activation function for a moment. It would be just like a linear operation in that if I knew what the output of that second layer was, I could use that to to minimize the error between the predictions of the outputs but the problem is the targets that I have I have a two layer that work actually correspond to what come out of that second layer, which haven't even touched yet. So, the input is going to be transformed by some function into the hidden layer. But what that number is, is not directly is not truly correlated with the actual output is because there's another there's another function that must be executed over this intermediate number to get the output. And I don't know what that function is just yet. Okay. So, like the out the output you get is it can just be considered like some sort of scalar that is not interpretable. That's all I talked about these as hidden layers. So, like if I get the output I know what what the units of the output would be what's what it's supposed to represent is it a class is it a, you know, a miles per hour or something like that. But that hidden layer in there is going to give me some scalar outputs like it doesn't really have units or anything like that. And it's not because it's kind of combined multiple channels that input data on its way to getting an output but I haven't got the output yet. So I can't assign any meaning to it. Sure. So, it can be. So the idea is and we are getting a little ahead of ourselves. This is interesting. When you have when you've correctly optimized a neural network for a task. You can actually get very useful representations out of the interior. So, for example, I think I mentioned in the first class a lot of my research involves these things called embeddings, which are basically continuous representations of classification labels. And so you can actually take these embeddings that are hidden layer representations and use them. They're just numerical values are in medical vectors by themselves and not interpretable but you can use them for other tasks and actually they preserve a lot of information. So basically these hidden layer representations preserve the information that is necessary when the network is well trained, but a human would have a hell of a time trying to figure out what the actual meaning is you can do things like cosine similarity if you're like, where are the clusters in this thing. What things is it similar to, but you know it's not something you can say like, okay, this number represents a bird this number represents a feather this number represents a microphone. Okay, so we have some function. Sorry, any any other questions. So we can easily spend two days on this notebook so that would be fine. I think that's how much time actually built into this. Okay. So now we let's assume we just we've got some function, phi that is going to be a an operation over over X. So now what I want to do is if this is if phi times w is going to give me my output. Now I want to find I want to do the derivation is going to minimize this error. So whatever when I multiply phi times these weights. And then the square the sum for all my samples that's what I want to minimize. So I want to find so you can see now that I end up with w is going to be equal to five times phi raised to negative one times five times t. Right. So now I can use it in the same in the same formula that I had before, where I have y equals phi to times w. So now I'm focusing on like if w is this output layer. This thing is actually going to produce the prediction. I want things that are going to be useful going into w. Things that go into w to be useful to predict to that output. But the trick is there's kind of the separation between the inputs and the output. So the X is the input. So it goes into some weights we'll call them the that produces phi. I want phi to be useful when multiplied by w for predicting the output. It's not had these two functions that I'm trying to optimize at the same time. But I can't there's no generic way of arbitrarily separating those that allows me to do this over a large number of samples at scale. So if I know that this is a non linear function I want to introduce some arbitrary way of having non linearities and non linear operations performed over my data. So I have activation functions. So get to that details in that in a moment. So if we take a look at this, right, this is what I just discussed, I have the inputs, something happens to them. Right, we'll call that thing phi, and then whatever happens to them, I can multiply by weights w to get my outputs. And so this yellow box is the black box of neural network. Because I don't know what I don't know the nature of this transformation. So this is, you know, pass through some non linear function, multiply by w to the output y. And now I'm just trying to figure out what the hell goes in this yellow box. So. Service, right. Can we use training data to find out training data can have quick labels on it. Right. So if I know what the correct output is, maybe I can use the training data to actually optimize the weights of the text book. So to speak. Talks about vision. We'll talk about convolutional networks in like lecture 13 or something. Right now, I was doing regression. So it's also numbers, but the same principle applies. I can use the training data to figure out what needs to go in five. So we've now just entered the world of neural networks, where five X is going to be the output of some layer of adaptive units. So five X will call it H, H is typically what we use for the activation function so I've used a bunch of different terminology here right F of F of S. Five X H of X, all referring to the same thing. Right. So because the neural network is a universal function approximator it's not really useful to talk about like the function and abstract. You want to know what function I'm talking about so we'll use H to represent the activation function in specific the activation function is this non linear function is applied over the output of a hidden layer. Right. So this sort of looks like this so now, before we had kind of something that looked like this side of the equation with the X is going straight into these blue nodes here and producing outputs. So the only thing that's different here is that instead of X going directly into the blue nodes going into these yellow nodes, where they have weights V, these X is multiplied by the V is and then before the output is produced instead of a sum. You have this activation function H. So, let's say X goes into the first hidden layer. It's multiplied by weights V which at start is just going to be kind of arbitrarily initialized. It gives you some number. You apply the function H over that number. It gives you a different number. We'll call that Z. And then you have a bunch of different Z's and Z's go into the next layer, which have weights W in them, then also maybe randomly initialize different numbers. These Z's are multiplied by these W's and then each Z you take the linear sum of Z times all the W's that gives you your output. So now I'm still in the world of trying to optimize weights I just now have two separate weights, two sets of weights V and W to try and optimize. So, the dimensionality each step will be as follows. So, and will be the number of samples is the number of things you measure about n. So X tilde will be X time X by D plus one. V is going to be D plus one because it's multiplied by this times some other dimensionality M. Right, and I'll just specify how many M's I want to get out of that first layer. So now Z tilde. So right if I have X times V is equal to Z. This means that it should be of dimensionality n by M. Right, because we have the D plus one's that should, that should cancel out. But this also has to go into another matrix operation so I need to append a bias vector onto it. So now this will be Z toda is going to be Z with the bias, which is going to be a dimensionality and by M plus one. So W naturally would have to be how many things are going to come in M plus one. How many outputs do I want K. So this should be M M plus one by K. And so Z toda times W, the M plus one should cancel out. And so we end up with an output that is of size and by K. So the final operation looks something like this. So if I have, if Z toda is H applied over X to the V. So here's H. This is X toda with the bias times V. Then I append my bias again to Z is going to be Z toda. And so then Z toda times W is equal to Y, which means I can write this all as a single function. So here we have Y is equal to H of X toda V. I apply the bias again to that multiply that by W. So the two layers in this case are called the hidden layer and the output layer in larger neural networks. The last layer, of course, is always the output layer. And then anything besides the input layer going to be hidden layers. So we talk about the last hidden layer, the first in layer and we can talk about the things that represented at different points in the neural network. H is the activation functions for units in the hidden layer. If you have multiple hidden layers, you may have different activation functions. And we'll talk about some of the different activation functions. And so we'll be doing gradient descent in the squared error. So we want an H that has some of those nice properties, the outline before, right, we want it's derivative to not grow out of control as we grows. And what that derivative to be easily calculate. So we're going to try a couple of functions. What about a polynomial. We can plot a polynomial and it's derivative to see if it satisfies the properties that we want. We have H given by this, and then it's derivative given by this. We can plot the derivative with the dash line and H with the solid line. So, just this look like, take a look at this, do you think this is a well behaved derivative? No, it's kind of the opposite of well behaved derivative, right. Well, what we want is we want things that as the magnitudes grow, the value doesn't grow out of control. This is doing exactly that. It's a polynomial function. So we'd expect that. So we don't want this because remember, the gradient descent procedures are going to take steps that are in a size proportional to the derivative. So this derivative gets huge. And so it's a high positive as a increases, and it's a high negative as a decreases. And so the gradient descent it could be very unstable. So what we want, we don't want to be skipping back and forth across that global minimum again. And so if the gradient grows out of control, we risk that we also have things like the exploding gradient problem. This can be solved. The exploding gradient problem can be solved through relatively simple techniques. So we have bigger issues, this thing called the vanishing gradient problem that we'll get to. But for the moment, we don't want these derivatives to have properties like this. So two common choices for functions with well behaved derivatives are the sigmoid function as we saw before, right, one over one plus either negative a. And then this thing called the 10 age function. So the 10 age function is given by this differences are the, well, do you know, anybody know the difference just off the top of your head between the sigmoid and the 10 h function. What are the bounds on the sigmoid function. It's bounded at zero and one right. What are the bounds of the 10 h function. Negative one and one. Yeah, so the sigmoid is an asymmetric function that is bounded to zero and one and the 10 h is a sigma is a symmetric function for its bounds. So let's work out their derivatives. So, well, we'll work out the derivatives and then I'll give I'll give them to you and applaud them. So these are the two functions so each one is sigmoid h two is 10 h and then we have the derivatives of both of them. Now let's take a look at these. Okay. So the blue lines that was the sigmoid function that we plotted earlier. We see the value top out it's 2.5. We also see those nice bounds at zero and then you have one. Then there's the 10 h function in red. So similar. It looks like a very similar function it is. It has the same overall shape. Except it's bounded negative one instead of zero. We have a steeper derivative here around x equals zero. And so we have to have a maximum value of this at one, but both of these functions will still satisfy those nice properties of derivatives that we want to earlier. So it's got a maximum bound. It doesn't go out of control as the magnitude increases. So both of these are friendly functions. Okay, so these derivatives are computationally simple. So are there anti derivatives. They decrease in magnitude as the weight magnitude grows in this case both of them do so asymptotically, and they have limited maximum values. You satisfy a lot of of nice on nice properties so anyone know kind of what what the sigmoid function can be used for. Say I have some arbitrary scalar number and the sigmoid function will squish this into a range between. What are the bounds of the sigmoid function again. Zero and one right so if I take a 10 number 10 take the sigmoid of 10 is going to give me a value that's like pretty close to one. So, sigmoids are nice for like turning things into binary probabilities, for example. And so you can use sigmoids for say binary classification tasks. And then the teenage function is useful, particularly in hidden layers because what it does is it will actually preserve some negative values. These may be useful, because there may be things that are inversely correlated with some input to that layer, be it a hidden layer or an output layer. And you may actually want to preserve that. So, generally you will see 10 h functions being used in the interior of neural networks. So, you can function of course there are plenty more that we'll go into later. Sigmoid functions are useful for things like binary classification tasks and more often will be seen in output layers. All right, questions. All right, so now this is like the nargis part of this I think. So, remember the intuition behind gradient descent. If I assume that there is a high dimensional derivative, I'm trying to find, I'm trying to send that and trying to find where is closest to zero. So, the gradient is going to be defined by the error and try to minimize the error. So, I want to, I want to get to a point where I am so close to the true solution that when I move along the gradient, I'm just the error update is being so small that I can have, I can be said to have arrived at something arbitrary close to the solution. So, remember that the use the mean squared error in this case so between each target value T sub n K and the output predicted value. Why sub n K I'm just going to take the difference. I'm just going to square that. And because every target and every output is going to be defined for a given sample and a given measurable output that I'm interested in. I'm going to sum this overall and all K, and then average it for the sizes of n and K. Okay, so now he is no longer a linear function in the weights. So this means that we can't set the derivative equal to zero and solve for the parameters like we did before. So, instead we can we can still do is we can do gradient descent in E by making these small changes to those individual weights in the in W in the negative gradient direction. So it's not I can't use like the I can't use linear algebra to like solve for the inverse function anymore, because he's no longer linear function. But the intuition behind reading descent trying to find some global minimum was as close to it in this high dimensional derivative as I can still holds. So I'm sort of doing this a little bit blindly in that I don't know where I'm going, right, but I know where I've been. So I'm going to look where I'd been and walk backwards. Go down the slope. So, the update is more or less the same is that if I have, I want to update this value for V sub J m I'm going to take whatever previous value it was minus some learning rate row times the derivative. And this is the same for the or W. For the for this I'm going to have these two learning rates, row H and row OB this be different, but often they're presented as the same. There are cases where you can actually have different learning rates and different layers and this can help convergence. Most of the major packages don't allow you to do that by default you have to do some kind of pie torture tends to flow hacking to get that to work. So, for the first cases, a constant learning rate or at least a single learning rate across all layers is what the package will give you by default. But there are, there are cases where maybe desirable to have different learning rates. So, we want to use this to find the global optimum that is the values of V and W that minimize the mean squared error. So, we're more simplified view, right, I'm not sure that this looks very simple to you with all the errors but this is the simplified view. So, we have this full picture we want to focus on modifying a single way let's say you know V one one this one here. This is going to be based on a single error between the target T one and the output Y one. So, for the moment to make things a little bit cleaner let's drop the subscripts let's just focus on the single hidden unit and the output unit that are relevant to this computation. So, this input X whatever it is goes into V. And then this gets multiplied by all those elements in the matrix V. So, we want those get some to be then have some value that get then gets turned into an act putting the activation function, which then deforms that by some non linearity this gives us Z, Z is then multiplied by W and I take this on this gives me the target, the output, and I just want to measure the difference between the prediction Y and the target T. So, the four calculation. So, this is simplified so I'm going to ignore the bias and all of the terms, right now we're just looking at single terms be multiplied so no need to worry about the major multiplication. So, if Y equals W times H of V times X, in other words, why equals W times Z, Z equals H of A and a equals V times X. Since E is equal to T minus Y squared. D E D V should be D of T minus Y squared D V. So, the chain rule to the rescue here so basically I'm trying to represent T minus Y in terms of things that I've already calculated here. So the, the error is going to be D of T minus Y squared, with respect to dy times D Y D Z, E times D Z D A times D A because each of these terms like Y's represented in terms of Z Z is represented in terms of A and A is represented in terms of V. So, again, if this looks intimidating, no fear. This is presented for your interest if you are interested in how the mathematics works when it comes to the code, all this will be done as matrices basically going to be doing this over the individual elements show how it's done as matrices and then show the code which most likely won't happen until Thursday. But we'll get them. All right, so now if I take the derivatives of all of these, I end up with something like this. So two times C minus Y times negative W times D H A D A times X. So, what this term here of course is this will depend on what H is, right, which function I'm using. So for the moment we can assume that if that H is 10 H, the derivative of 10 H happens to be one minus 10 H squared. So this page here is stack exchange will explain why if you care to go into that. So now we have a formula that I can actually plug in for this. So remember that Z equals H of A. So I can rewrite this as Z. So one minus Z squared can be written as one minus the square 10 H of the input, because the 10 H is is H. So now the entire thing D can be reduced to negative to T minus Y times W times one minus Z squared times X. So let's break each of these terms down. This is the derivative of the error this first thing here. So T minus Y, that is the error between individual sample. That's the weight that's being updated. One minus Z squared as derivative of the activation function and X as the input. So, with the exception of the activation function here. This is the same as we were doing linear regression. The three components I need are the error, the weight and the input. So what I'm adding that is new is this activation function because there is this non linear function that I have that I've performed over the input. All right, questions about that. So, um, you go back to this. So we broke down DEDV in terms of all of these things above. Right. So these are the individual elements the of the weight multiplication. So, we can use the chain rule to break it down into each of those basically the multiple of these derivatives. We can easily take the derivatives of all terms except for D Z. Right, because we don't really know. We know that Z equals H of a, but if we don't know what H is, we can't actually turn this into a formula. But we, there are limited number of things we can use for H right it's got to be an activation function that has one and has a set of nice properties. And then we've talked about so far in any real depth is 10 H. So we will assume that H is the 10 H function. The derivative of the 10 H function is one minus the 10 H squared. Just it is, it's a fact. And because if this is if H of a equals Z, then Z is effectively 10 H of, let's say, X the input. So this one minus the square can be also be thought of as one minus 10 H squared effects. Okay. Everybody cool with the rest of. All right. So, so far so good if this is more or less just a linear operation with an added on linear function and it's derivative. But seems intuitive. Okay, let's add another output. Okay, now we look like this. So same thing. Right, same things are happening. I've got a single value being multiplied by some weights V apply my function H gives me see. Okay, now Z is actually going to places. Right, so there's a weight w one. And there's another weight w two. So, just like in linear functions, I want to predict two things about the output. I have two columns in my final weight matrix. So that's pretty straightforward. So this is going to be that first value. This is going to be the second value and they're going to give me different output values depending on what those weights are these have some meaning that I can use to compare to the prediction. And so now this will now get a different error for each one. Okay. So now things get a little bit hairier. So chain rule again. So what's new here now is that instead of just having t minus y squared. I've got two things. And I've got a sum. So I need to take over the sum of derivatives equal to the derivative of a sum. And so what I'm going to do is I'm now going to sum these errors, the squared errors. And now this has to be taken with respect to V. So this works out like before. The only thing that's different here is now the numerator this equation. But now we can distribute the, or sorry, I'm a try I need not I now need to compute this with respect to the different. Right. D dy one and dy two. Okay. So, if I compute this with respect to dy one I can then put this other term out here. dy one with respect to dz. This is the equivalent to what's going on here in the single output version. But now I have two wise, because I have two outputs. And so now I need to compute with respect to both of them. Okay, so now I can have dy one with respect to dz and dy two with respect to dz inside the inside the parentheses. I can compute these derivatives. Similarly, so this works out okay. So dv is just x d z d a can be again be written as d H a d a. And so now I'll put in the derivative of the 10 h function here. So now we can think of the errors calculated in those output units is being sent backwards through to the units in the previous layer. And so if we'll call these delta values then the derivative expressions will be referred to as delta rules. And those delta values are back propagated so sent backwards into the previous layer so this is that back propagation you might have heard of in, you know, when you discuss neural networks, and is notably not addressed in neural network for babies. So that's basically the small error value that is being used to update the weights. So just like in linear regression with s g d, we use that error value to update the weights. The thing here is that in those weights, w, the value that is being used to optimize w is dependent on the value of V. So V is very, very wrong. And it might also be very, very wrong. And so the output could be very, very wrong. So if I get an output if my, you know, if my target is here pretend arbitrary space, and I, my output is here, and they're way, way different, right. I don't really know is this because the output weights were wrong or it's because the hidden weights were wrong. I have to allow for both of them. I have to assume that there might be something that's wrong about V that's making the prediction when a Z is multiplied by w also wrong. So this error back back propagation should be used, not just to optimize the weights, the output layer, but also to optimize the weights in the hidden layers, so that when an input flows through the hidden layer into the output layer is going to get me a better, a better result. So intuition behind back propagation. At least everyone clear on that you can sort of think of like, if you're a player like one of those pachinko machines, put like a coin into the top and it bounces down to the bottom each and you can think of like those. The bottom slots, those are your targets right you know where things want you want things to go and let's say you want a coin of a certain size to end up in a certain position. The weights then would be something like the sizes of the pegs in the pachinko machine. Right so you want to you want to increase decrease the size so that your input of a certain value is going to go the right way through the pachinko machine. The non linearities could be something like you replace the peg with a spinner or something like that. So it's effectively just sort of this big machine where you want that you know the input, you know where you want it to go, you want to just mess around the interior of this machine until all your inputs get where you want them to go as closely as possible. It's just instead of the wooden rods it's numbers. All right, let me see how much we got left okay okay that's going to be like bad. Okay, maybe I'll get as far as the full version of back problem. I will go behind. Okay, so, remember these derivatives if you don't remember these derivatives you can forget these derivatives and just come back to them before class on Thursday. So, we're basically differentiating with respect to two things. So, we're going to be going to be going to be a double one, which is going to be an element of this output layer and V, which is going to be an element of the hidden layer currently the only element of hidden layer in the simplified example. So, the d dv is the function that we see up here. It's the summed derivative of the squared error times the derivative of the activation function times the input, and then DEDW is much simpler. There's only one output. So there's one error, and then the input to this layer is Z. Right, so, so that's much more straightforward so the hidden layer. That's much more complicated, because you have multiple outputs flowing backwards into this single unit and the of errors for both of them. Yes. It's the same thing here. So the form was the same. It's just what the only thing is that the value in this specific numerical value in this would be different. So this can just be cloned for W to. All right, so now, if you go back and look at those update rules that we had earlier up here. Now let's just plug those back in down here and see what falls out. So the update rules for the delta, right, so new value of W is W minus DEDW. Same for W to. And then so that's going to be W plus the learning rate times the error times the input. So this, this delta is going to be rewritten like this. So now V is going to be previous value of the minus DEDV. What is the E.D.V. We gave it up here. So now this is going to be a learning rate times the different errors times the individual weights times the derivative of the activation function times the input. So one question you may be asking, where did this negative to go. At this point, this is just a constant. Right, so the if I multiply this by some constant learning rate, we just assume that this is going to be factored into that. So that if you're, if you're worried about this, don't. Okay, so now the delta H is going to be delta O. Delta one O times W one plus delta two O times W two times one minus Z squared. So this here. How do we do this is well, the reason we have this here is because of the update rule for W. We're just taking this error to be one of these delta values. And so now if there are two outputs, there are two delta values. So whenever this function here for the update accounts for an arbitrary number of delta values, all I need to know is which one is which and the slide it in the right place and you add all of them. All right. So we have two minutes left, so I'm going to stop here. If there are any questions, let me know. I will have office hours starting when I get back to my office. All right, thank you. I'll see you on Thursday.