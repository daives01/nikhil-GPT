 I'll have to make a statement. So, what do I do? I think we're going to combine a little bit of a paper. Try to make a little bit of a paper. Try to make a little bit of a paper. I'm like, I'm like, I'm a class. And then I, I took Friday's. I'm going to make a class. Yeah. I'll probably mind. I'll just speak to the good. Okay. Go ahead and start. So here's where we're at. We are still the day ahead. So this is the 23rd. The finished lecture 16 already. So I'm going to use 17. And then 18 next week remotely. And then hopefully we'll be able to take. The 30th off. Great for all of us. So today I will do. Reinforce learning with the neural network and then also discuss the project proposal. So this is going to be due of two weeks to do this. And, but I recommend that you get it in sooner. Because then I'll be checking for those to come in and the moment I sign off on it. You can start even if it's not the 6th of April yet. So I'll talk more about this at the end. Before I start. I just want to like reinforce a couple of points about office hours ethics. I wish more people were here. Just a. Just a note that. We need to be able to assist every. Fairly. And so I would request that. If you come to office hours, my policy, these is like pretty much open door. And as you observe, you come in. If you, if I, you know, have you do something. You are welcome to stay and try it. But I recommend, you know, I would ask you that you like move to a different chair or if it's going to take a while. Go out in the hall and work on it because there may be other people coming in. So just, you know, I don't. We can't really can't have people there like monopolizing our time. I'll do, you know, office hours for me. Start effectively right after class most days because they can, you know, start talking to you guys on if you need, but they ended for 30. Come after for 30. I've moved on to the things. I'm not there for office hours. And I think the same is true for lab ops. Right. There's a specified time. I come there for class questions. After that time, we are not obligated to discuss class, class content. We both have plenty of other things to do. Um, Storcher assignments early. Again, if there's a flood of people coming to either of us, you know, insisting on help at the last minute, very likely the answer at some point is going to be no. You should have planned your time better. Yeah. So just not like a global problem, but it's been prevalent enough that I need to make this clear again. So. If you. Start coming at the last minute asking for help on your assignments. Very likely, you know, I might help you a little bit, but at some point, you know, it's on, it was on your own. You need to be responsible. You all are adults as I said at the beginning of class. Part of, you know, part of being an adult is learning to manage your time. And this is the best time of your life to learn how to do that. Frankly, the habits you develop now are going to stick with you the rest of your life. So. Learn and manage your time now. And I promise you, it will be, it will serve you very well. Okay. So hopefully that is, that is clear. So please be respectful of us. Please be respectful of your fellow students when it comes to. Making use of office hours resources. All right. So any questions are very, hopefully you have. If this hasn't scared you into starting your assignments, you have, you will start your assignment after this class. Because that is going to be due in two weeks from Tuesday. So please, please bear that in mind. Right. So I will go into the lecture if there are no further questions. Hide the controls. Team. Okay. Oh, I good. Sorry, this one, this is big. And sometimes when I reloaded it, it takes like forever to render all of the, all of the formulas. Of course, he has already loaded. Okay. So if you recall, last time we introduced reinforcement learning with this thing called a Q function with Q function. Is basically the function that you were trying to predict, given a state and an action. How good is this going to be for, for my objective of achieving my goal. And also where am I going to end up. If I'm in this state, I take this action. What state am I going to end up and then what actions can I take from that state. So previously we had filled this out as a Q table. Right. So the Q table is going to basically have dimensions for the elements of the state and then for the action. So in our case of solving like a maze, we have a two dimensional state, basically XY coordinate in in a maze. And then an action, which is one of four things we've got down left to right. So we were looking for the sequence of action that either maximizes or minimizes the summer reinforcements along the sequence. So depending on whether you're, you're trying to minimize or cost or maximize the reward. What we're going to do is we're going to be looking at lots of examples and looking at those reinforcements and returns that is if I'm in an action, if I'm in this state and I take an action. How good is that according to some, some measure of goodness. Right. So again, I gave you the example of if I'm trying to solve a maze and I'm in state S and the goal is right in front of me. And I take one step to the left or the right, one step to the left, one step to the right, one step to the left. And then I move forward. I don't want to just repeat that sequence, because there's like six unnecessary steps in there. When I may have discovered that from this point, if I take one step forward, I reach the goal. Right. So I don't want to just find one sequence that works and continually exploited. So we thought the Q table as this function that takes the state and a possible action that returns some value for that pair. But we, this, this table is actually a function. So, if I have a function, we have certain types of mechanisms that I can use to find the value of that function. So now let me give you something we've talked about before if I have some inputs and some outputs. And I assume that there is a function that maps from those inputs to the output. My job is now to find the values that parameterize that function. So I need something that can say approximate that function. What type of mechanism might I be able to use? In the title. Yes, a neural network, a neural network universal function approximators. Right. So if it is a function, I can use a neural network to approximate it. And I can, for example, if I function is y equals x I can approximate that function with the neural network. I don't know why you would do that but you can do it. So I can do with a function of arbitrary simplicity and of course I can do it for a function of arbitrary complexity. So if you was just a function, let me just optimize for the value of that function. So the objective of a reinforcement learning problem is going to be reduced the objective of acquiring this Q function is going to predict the expected sum of feature reinforcements. So I'm trying to find this big R value. Right. This is going to be the thing that I'm going to try to predict with the neural network. So we're now back in a kind of a pretty standard neural network paradigm of I'm going to take in some inputs. I'm going to predict some outputs. My output now is going to be the sum of future reinforcements with a total return and my input would be some representation of the state in the action. And so this correct the correct Q function once it's optimized will be best determined the optimal next action. So now my objective is to take this approximation and make it as accurate as possible. So here are the components of this. So for for every step. I want to compute the return for all the summer, the summer of the reinforcements from here to the end. So little are this is basically I'm trying to predict what big R is from this time step to the end. I'm going to do that for all possible steps. And that's going to be the value of the Q function. So to break this down a little bit more, this is usually formulated as a least squares objective so that is going to minimize the square the expected value. So because the, the sum of reinforcements is some sort of scalar value. I can still use only a mean squared error metric to approximate this. So this is no longer a classification problem so much as I'm trying to optimize the values in my in my network to predict the sum of future reinforcements. So now I'm trying to I'm back in the territory trying to minimize error. So I have the, I have an established Q value and I'm trying to minimize the error between that and the predicted sum and I'm going to square that. So the expectation is going to be taken over a couple of sources of randomness so that as there's the action selection. Right, we talked about how I can, I have a set of actions that I can take. So, sometimes, let's say I don't want to, I don't want to exploit a correct but some optimal solution such as moving back and forth left and right before moving toward the goal. So I want to have some level of randomness of a given, I'm going to take a random action. So that random action that that's that epsilon from the previous lecture. So that's the thing that allows me to determine with some probability I'm going to take a random action instead of exploiting what my best strategy is right now. Right, so if I find a strategy that works but it's not the best one. I'll keep exploiting that. Where in fact there could be a much easier solution but I have no way of finding that because I've kind of found this run in the search space. And I just keep doing that over and over again. There could also be some randomness in the state transitions right it may not, it may not be deterministic. You know what, which state I enter, given that I take an action. Right, so if I'm in a state and I have two possible actions. Right, I take the left fork or the right fork. There's some probability distribution maybe they're not evenly distributed. And then I'm going to take either of these actions. And then finally the reinforcement received. Right, so if this is a continuous, a continuous problem. I can approximate what I think my expected reinforcement would be but it might not be exactly that. Right, due to other factors depending on the complexity of the environment. And then of course, the key problem is that we don't know, we don't know this. Because this is asking me to perform a sum over infinite elements and I can't do that. So what I'm going to use is going to use the mark off property basically does anyone know what the mark off property is. They've never heard of this term before. Kind of any sense like in what context of your this term. That's a good place to start what's the mark off chain. Like it's the last out, right. Yes. And does it use it to repeat for those in zoom a mark off chain you take the last output and that is the input that determines the next output. Does a mark off chain use like the previous output before the last typically. No, I mean, if a mark off chain is the first order mark off chain, which is by default a mark off chain. It does not. So mark off chain is kind of this something that might call a drunk walk. Right. So if I have a bunch of grids, right. And I am in one grid and I take a step forward. Well, then the grid, the square that I'm in now has influence on where I go. I could go to any of the adjacent squares, but it doesn't know that it is staying from the square behind me. So I could just as easily just take a step back. So now I do this a bunch of times and stagger around like a drunk person. Hence a drunk walk. So you can maybe you might stumble upon a path that you actually do start moving forward and then suddenly you'll verse to the left or you step backwards or something like that. So the mark off property is basically this memoryless property that is where I go next depends only on where I am now. It does not really take into account anything previous that is where I came from. This doesn't sound like it would be very useful for solving a sequential problem. But in fact, it actually is because we can now approximate the sum for all K of our sub T plus K plus one as the sum of are the next return. So are sub T plus one plus the sum of all returns from that point. Okay, well I've just kicked the can down the road by by one by one step but if I can use this highlighted up here to approximate. Let me draw this out so if I can use this part here to approximate this part. Now I have to approximate this part. What can I do to approximate the part that's just in the box. I just shifted again right so now the sum from K equals one to infinity of R sub T plus K plus one can be approximated as R sub T plus two plus the sum from K equals two to infinity of R sub T plus K plus one and so on and so on. And so at some point, right, this goes on to infinity because I don't know how many steps is going to take me to reach the end. And at some point I will reach the end and this is no longer infinity. I'm just looking at the return that I just received. And so if I approximated this way, then I should end up with a relatively relatively close approximation of the entire the entire return. Okay. So basically, because we have realized the q function as this sum, then I can approximate the entirety of the q function as the next return plus the q value of the next state and the next action. Okay, they're very clear on kind of how this is working so far. And so now I'm back and trying to figure out what the actual value of this q function is approximating that with a neural network. So let's assume that the q function is some function of an arbitrary level of nonlinearity. So I just have to find kind of a good network size is going to allow me to capture that level of nonlinearity and approximate the values in that in those ways. So the gradient of the objective so now our minimization problem is like this. So, q is going to be some function parameterized by weights w. And so now I need to minimize the gradient of this function. So if I have the error gradient, J sub w or J of w is going to be equal to the error of the expected return, minus the value of the q function for this input parameterized by these ways. And now I'm just trying to solve for w. So in other words, because we now can break, we can break this down into the steps above. This can be written as the return plus the q value at the next state, minus the q value at the current state squared. So if this function represents some kind of gradient. What kind of operation could we find to get the lowest point on the slope. What might we do to that gradient we could descend the gradient eventually was we descend the gradient successfully will find the lowest point in the gradient. Okay. So this, this is now hopefully back in some levels from your territory. So did the gradient of J with respect to w and do gradient descent. And then we'll see here q appears twice in the expression. So a pretty common approach is to take the respect. To q at the current state and we treat the next, the next value of q as a constant. This isn't necessarily correct, but it works well and makes the algorithm a lot simpler. So it requires less compute. So now I only have to compute the q function once, given, given its current values, rather than twice. So in more recent methods, a second q function, call this the target q function has been used to calculate the the q value at the next state, given a set of different parameters. And then certain intervals, we're going to copy my source weights to my target weights, and then update to target q function to basically keep these two roughly in line. Okay, so just some principles, let's recall that the expectation operator is going to be a sum of possible states weighted by probabilities. So. If D is the value on top of the fair die, the expected value of D is actually three and a half, right, which doesn't make a lot of sense because it's not a possible output. But the gradient of the expected value is going to be the expected value of the gradients. So the grading of J, we can just take the gradient of the above formula. And so this is going to be two times the return plus the q value at the next step, minus the q value of the current step, minus the current gradient of the q value. So the expectation operator requires knowledge of the probabilities of all those random effects. We don't have that. So what we do is a sample from the world. So this is now, because I don't have this I need to, I still need to see for my states and my actions, what kind of returns do I get if I take those actions in those states. So that is more probable events will occur more often. And then if we just add updates to W for each sample, you will get the expectation. So if I'm in a world and there's there's a distribution of events that can take place. Regardless of the state and the action, the statement and the action that I take things that are more probable will occur more often. So, you know, you can think of like continuous problems when balancing my inverted pendulum. If I'm. Already over the marker here I don't use the eraser. So if my pendulum is like already tilted over here it doesn't really matter what I do I'm very likely to continue to drift to that one side. So given the state in this action, the state of the world to be completely keep the markers okay. Given the state in this action, then it's very likely I'm going to end up in the same place regardless whereas if I'm more balanced then maybe there's a wider distribution of possibilities that could occur. By sampling the world with the current state I can basically get a decent doing that repeatedly, I can get a decent survey of what types of things are likely to happen given the current state of the world. Okay, any questions. Okay, so the first let's look at the grading of the q function as a table, and then we will look how you can see how we can update that using in your network. So when the gradient when q is a table. We first have to decide what the w parameters are for a table. So these are actually just the entries in the table. So if you think of what I'm, if I have like a state that is two dimensions and an action that is one dimension, and I have my three dimensional table. So I just go okay XYZ at this cell and the table. This is the return. This is what I what I expect. And so, what I just I just, I just hear what I can do is I can take that input and multiply it by those weights instead to get the return. Right. So you're not going to get like the identical values basically just get an alternate way of saying hey, instead of just retrieving this value actually want to take what's in here and multiply this by your input and this is going to give you the expected value. So, since W also weights, you can formulate the selection of this correct parameter as a dot product so if we have x of t as a column vector, this will be equal to the number of table cells and values. And these are all zeros with a single one. So that one designates the cell corresponding to S and A. And so then the queue of that is going to be that that input times those weights. So this looks a lot like the prediction step in a neural network. Therefore, the gradient of the q function will basically just be the gradient of x sub t transpose times W. And that's since this is taking with respect to W. This is just equal to x sub t. So now if I want to actually update the the weights, I need to define the temporal difference error. So in this case, this is the formula we've seen before. So delta sub t is going to be R sub t plus one plus the q value at the next time step minus the current q value. And so if we substitute the gradient of J in from above like this. So now this we can just replace with delta sub t. And so now the gradient with respect to W of J is going to be two times the error delta sub t minus the gradient of the q table at the current step. And so since we established this is going to be x sub t, then this ends up just being negative to E of delta sub t times x sub t. So in other words, for because our input except T is represented as a one hot vector where one that one indicates the cell corresponding to sub t sub t, then this value. Right. So this is delta sub t times one. And so now this value ends up just being negative to times the error of delta sub t, and then else is just zero. Now we can replace the expectation with samples, and then we'll get the temporal difference update for the table. So this is just the standard weight update. I take my current weight weight value and then I subtract the gradient and store that in the new weight value. And so if the gradient is delta sub t times x sub t. I just multiply that by some learning rate row. And this is the update. So w times row times delta sub t times x sub t. And so this is really the same as what we've seen before. So I'm updating the the q table q of s of t a sub t is going to be q of s of t a sub t plus row times the temporal difference error. And this is the same as what we did in the previous lecture. So previously the update to just the cell was implicit. So this is also the same as the weight update in neural network. So this is the same as the weight w based on an error, in this case delta learning rate row and an input x. So same components as we've seen before, just written differently and kind of arrived at using a different type of form of information. Okay, questions about that. So now the neural network version. So we've shown how it corresponds to a table in that the tabular update is effectively interchangeable with doing a weight update. If we assume that those weights are effectively just entries in the table, but using a table for q has limited use reasons why maybe the size of the table might be too large to store in memory. So imagine, if I'm trying to solve some sort of complex environment. I might have a continuous function or even if I don't, it might have too many possible state action pairs. So I might not be able to store the entire table in memory. Learning could be slow, right. So I could learn something from if I have two similar situations in different locations in my environment. Right, I could learn something from doing something in one location and eventually find my way over to the other location. And I'm not able to solve it. So it's like, if you learn by reinforcement learning that red means stop and green means go, but you can't get that traffic light, like on the north of the parking lot. And then you go to another traffic light, and you have to learn that all over again. That would be very inefficient and also very dangerous. So instead, what if I have learned that red means stop and green means go, and now I can see, well, I've gone, gone somewhere else. And then when I'm reading red and read happening here, I might have learned something previously that's relevant to this. So instead of having to represent every single state action as a set of cells like every intersection for Collins being being a distinct cell. I actually can learn from features of the environment, and then be more likely to reproduce good actions when I encounter those features elsewhere, even if I learned about it in a particular location first. Well, we can use this universal functional approximator or a neural network. So to do this, we have to make a couple of changes in the derivation of the gradient, but they're going to be things that you've seen before. So we need to approximate this queue value parameterized by w with a neural network. So we already know the values of w are the weights and all the layers. So if you have two layers, then w is going to be composed of hidden layer weights v and big w. Looks like the neural network should have a single output and inputs for s sub T and a sub T. So to find the best action. I would, I could input s sub T, try all possible actions a and then calculate the, the output of the network for each one. Then I pick the action that produces the best neural network output. This is not how you would actually want to do it. But if you think about it this way, then the math makes sense which we want to do is basically want to accumulate enough experience that you get a good distribution of the various different possible actions. Yes. Yes. I can see w but the other way. The w w is all the weights big big V and big w the individual layer rates. So little w is all of the weights in the neural network. V is the hidden layer rates, big w is the output layer rates. Other questions. Okay. So really what I would, what I would want to do is sample enough experience to say okay I was in the same state s a bunch of times and half the time I did action a half time I did action B. And those are really my two possible actions but then I know what the distribution of expected returns or reinforcements would be. So let's just remember the objective function have got to minimize so we have J w given as before. So this is the approximation of the sum and then the gradient as we saw before. So what we'll do is instead of the expectation operator, we'll replace that with samples at time to. Now I can look and see. So instead of the, sorry, I kept calling that error previously I'm going to say expectation I apologize for that. So we have, we replaced these with just a bunch of samples and now I have the return that I got it's time step to plus one, plus the Q value for this sample. So now there's going to be a particular input sample and then the weights associated with that sample, and then minus the Q value at the current Q value times the gradient with respect to those particular weights of the Q value. So what remains then is to calculate this last term. But we've actually seen this before, when calculating the gradient of the output of neural network perspective weights. So remember w is the set of both of both layer weights. So the only change the previous algorithm for if you want to train now a neural network to do a non linear regression is that we're just going to be using the temporal difference error instead of the target output error. So remember previously target output error is basically, I know my ground truth value is T, it makes some predictions why I measured how wrong I am is the difference between T and y. So the temporal difference error is now as given above. So this is going to be the predicted return, according to my approximation, minus the actual Q value at that at that point. So what's happening here, the Q value as it stands is sort of my best prediction. But my, my neural network or my Q table predicts is going to happen. It may be very wrong and may be close to correct. This is my approximation of T. So that is, this is my best estimate was actually going to happen if I take this action, and it could be very close to what the Q table or the Q function actually says, or very far. So what I mean when I say in reinforcement learning you are using and training the network at the same time. So here I'm going to get an imperfect approximation of what I think my full return is going to be. I'm going to use that as the stand in for my target value because it's the best thing that I have I cannot get a better approximation from this, because I don't have a bunch of samples I only have the experience I've accumulated so far. So I'm going to use this as the first question. This is the analog for why that is my current Q value. So where does what does this do. Yeah. No, R is the road road was learning rate Greek letter rose learning rate this this is the learning rate. We've always used row may sometimes use alpha for this we almost always use a row. The little R refers to the reward at some time step TV so our sub T is the reward of time step T. Our sub T plus one is the word of time step T plus one big R is going to be the sum of all those rewards that is the overall return. Okay. So let's review the gradient is an update that we had for non linear regression with neural network so. Britain layer weights be multiply those by my inputs. I apply some activation function H does gives you some intermediate skier intermediate scalar values he I multiply that by my output layer weights w and this gives me my prediction why why is going to be more or less wrong when I compare it to T. So now T minus Y. This is my error term. And so these are the updates that I use to update V writes this incorporates the error and then also incorporates the derivative of the activation function. And then these are going to be the updates format but layer so here's row. We have in this case different learning rates for the two layers that you need not necessarily have that. Now the key change we need to make is in the error. So as I alluded to just now. When I calculate my error over is basically the best estimate I can make at the time. And what my neural network before updating currently says the expected reward should be. So basically my neural network I have two estimates of the reward I have whatever my neural network predicts. And then I have something that's probably slightly different from that based on a little bit more experience. So I have, I've predicted that by my Q function for state where I am now plus moving forward is slightly positive it gets to be a little bit closer toward the goal. I then take a step forward and I fall into a giant pit. Like, oh, that was bad it's not going to be close to the goal. So I have a little bit more experience. And I cannot use that to update my, my Q table, my Q function. So these key changes in the error. So the targets are now going to be the reinforcement plus the next step. And then the predicted Q values so then we will be assembling a bunch of these samples into matrices. So these inputs will be the states in the action of these tuples be collected as rows and x. Those will then pass the neural network will give me Z and then why. Because the prediction of my network is just the output of my Q function. That is its best estimate of the expected return. And those reinforcements will be collected as rows of R. So now I just need these Q of s of T plus one a sub T plus one. And you can think of these as this is just the Q function once I've taken my next action. So this is just rows of Y shifted by one and maybe slightly slightly altered. So we'll do is we'll call this Q and for Q next. So now I can just rewrite the gradient to set updates except we're using R Q and Q. And then X is the same so I have X R Q and Q. All of these except X have one column where X has however many dimensions I have in the input. So now T is equal to R plus Q and right so this is that approximation of best return. And then Z is 10 H or some activation function of X times V as before. And now the update functions are pretty identical except you notice that I replaced Y with Q. Right, I place why with Q because the prediction of my neural network is the output of my Q function. So to make this even more obvious will just replace Q with Y and you can see it here. So now T I have a way of talking that is my return. Plus the next row in Y and everything else is pretty is pretty close to being identical to the standard neural network update. The only thing that's missing is we don't have this one over K that we had there. But because that's a constant. So we just factor that out. Questions. All right. So dealing with infinite sums. So for these tasks that have no final state. So let's say if I'm trying to balance the pendulum and my goal is just to keep it balanced right there's no. It doesn't end if it's perfectly upright or something my goal is just to not let it fall over as long as possible. So for these, the desire to sum a reinforcement is going to be over an infinite future I might say it's going to time out after a certain time my goal is actually to keep it balanced like 100 time steps or something like that. So this is going to grow to infinity unless I force it to be some sort of finite value somehow. So what we'll do here is basically if I'm just trying to keep something going as long as possible. I can accumulate all of my experience from one to infinity because eventually I'm going to blow up. So instead, what I'm going to do is I'm going to discount reinforcements. I'll actually discount ones that occur farther in the future. Right. So at some point, I'm going to say, well, I predict that this action is going to keep me in a good place for the next 200 time steps. And I can't really predict much beyond that. So I'm not going to worry about that right now. In 100 time steps, I'll start worrying about that when my time horizon actually reaches that. So what we'll do is we'll add this factor gamma. And so this is going to be some discount factor between zero and one. So as the further I get into the future, I'm going to kind of scale down how much I take this this reinforcement into account. So need to introduce that into the objective function. So you saw that we put that inside the sum. And so when I rewrite my sum is the approximation of the next reinforcement in the sum of future reinforcements. I will also use that gamma terms is going to be some value of gamma sub zero. And then I'll use gamma sub K in the sum. And then this is just, of course, we can rewrite this as the sum of keep the T plus one reinforcement plus all futures. And, and so then finally, you remember this is just the same as the cube key function for the next state, the next time step. And so gamma will also be out in front of this. Okay. So what I'm just going to do is kind of simply add the multiplication by gamma to the next state action Q value in the temporal difference error. So if I'm using a bunch of updates, these are batches. So what I'm doing non linear regression, we had input and target samples and X and T. And so now one set of samples are collected for one set of weight values that is for one Q function. So after the updates, we're going to have a new Q function, which is going to produce different actions. So what I do then, I need to generate more samples. So I need to do these in small batches so that I don't train too many iterations for each time. Otherwise, if I do training convergence every time, then the neural network can forget everything that they might have learned from previous samples. So I want to say train for like 200 at a time and it doesn't give me a perfect function, but I'm getting closer. And then I train for another 200 samples. It's like, okay, this is, this is getting me closer. So I'm iteratively improving. Right. I don't fully train to convergence. With every batch. Okay. So any questions before I do the example. Okay. So we'll do an example. It's called a 1D maze. What's a 1D maze. It's a number line. Don't let it fool you. It's a number line. My goal is to land on some desirable place in the number line that I would specify. This is the cartoonishly simple example that it does illustrate. So let's take a chain of states numbered 1 through 10. And so I can be in any state I can move left I can stay put or I can move right. And I can't move off the end. So if I'm in one, I can only move right if I'm in 10, I can only move left. And so I want to get to state 5, for example. So let's model this as a cost function. So for every move, it's going to be negative 1 or a negative reward. And then if I end up in state 5, I get a reinforcement is zero. So at this point, I'm just trying to minimize. I'm trying to maximize my reward is the same as minimizing my cost. So modeling the reward in this way will drive the agent to get to state 5 as quickly as possible because once it gets to state 5 the first time, it'll see. Oh, I didn't get dinged for this. So I want to do this more often. So the state is going to be an integer from 1 through 10. So it's approximately the q function using a neural network. So I have two inputs for the integer, so that is the state in the action, six hidden units and one output. So these are the states and actions is just one through 10, negative one for action left zero for action stay one for action right. And then the state is going to be bounded between one and 10 is just taking to be s of t plus a sub t so if I'm in state four, and I move left I should end up in state three. I'm basically just trying to find a desirable place in the number line. So here's my neural network class. This should look pretty familiar to all of you at this point. So I will store this. Okay, actions available stay left or step left stay put, step right. So representatives changes in positions will model them like this. So it's have an array of valid actions, negative one zero and one. Now I need to force exploration by taking random actions, but because this is a neural network. As it learns, I want to decrease the amount of exploration in this case we may not always want to do that depends on how well your neural network actually fits to the environment. This is very simple. So if I've got a really good policy for arriving at state five, I don't know like risk disrupting that by taking a random action at that point. So we use epsilon to indicate the probability of taking a random action bounded at zero and one. So given a Q net called Q net, the current state and the set of out actions and epsilon we can define this function. This will return a random choice from valid actions or the best action determined by the Q net. So that is, if I have a 20% chance of taking a random action. Then once presented with my my my action policy, I will either take the best thing predicted by that according to the state, or I will to after the 8% of the time, the other 20% of the time I will take the random action. And so this is called the epsilon greedy policy. So to find a function epsilon we did it does that so basically this is the important part. If I choose a random number that is less than epsilon, then I'll make a random choice out of valid actions. Otherwise I will take the greedy move. So that is I will run all my samples through my Q net and predict the best action for my for example. So now I need a function. What this will do this is actually going to do the batching. So this makes samples function will collect a bunch of samples of state action reinforcement next state and next action. I can make this a generic function my passing in functions actually to create the initial state the next state and the reinforcement value. So I'll define those functions first. So this is initial state, next state and reinforcement. So basically the initial state is just going to choose randomly from one to 10. So I'm going to get a function that is going to be bounded at 11. It's exclusive. And then new state is going to take in the current state and the action and then add state to the action to give you the state bounded at one and 10. And then the reinforcement will return negative one if I'm not in five and zero if I am at five. Very simple reinforcement policy. Okay. So, So here's my next function, make samples. So I pass in the Q net and then I have a passing these functions for initial state next state and the reinforcement function, as well as my valid samples. And the number of samples and my value of epsilon. So now I'm creating my X, R and Q and matrices. So these are just initialized with zeros. And then I will generate my first state. And then my action is going to be the epsilon green policy over the dictated by the Q net, given the initial state s and then the value of action the epsilon. And then for every, for every step in the samples, I'm going to sample the next state, compute the reinforcement at that state, and then choose the next action using the epsilon green policy, and then advanced one time step at a time. So this is plotting. So we'll just basically draw you a bunch of plots showing, you know, a bunch of ways of visualizing the output. So now for the main loop, we'll create 20 interactions, we'll call them steps for each trial and update our Q net at the end of each trial. Now run for 5000 total trial. So what I'm going to do is when updating the neural network, you function, I'm just going to run Adam for 20 epochs basically perform a little bit of training to try and update my, my, my function. And then I will put in the next set of steps. So I will quick 20 interactions update Adam for 20 epochs, collect 20 new interactions update Adam again. And then this allows me to basically perform these incremental iterated iterated, iterated updates, not necessarily training to convergence every time. Okay, so, and then I will create my neural network architecture here, we have 50 hidden units to think about. So, I'm just doing 50 now I must have changed this description above it said six. Now, gamma this is my discount. So I'm going to scale every future reinforcement by point eight. Right, so the reinforcement at T plus two account for 80% as much as the reinforcement T plus one. And the epsilon decay, basically what this is is I have my epsilon value. I want to reach this final epsilon. Right. When I, when I'm done training, I want the probability of taking a random action to be 0.01%. So I need to start from 20% decay to 0.01% over the number of total trials, and so I'll calculate how much I decay every every trial. So the epsilon decay in this case is 0.99. Okay. So now all this. Here's my initialization of my neural network I need to set the standardization parameters now. So that the unit can be called to get the first set of samples. Before it has been trained the first time so I'll create this function this is going to create my standardizations now. I will set up my standardization with my inputs. So this is going to be 110. And then I will use the following means 2.5 and 0.5 and then zero and one for the means standard deviations. And then the rest of this is is this is just plotting. And now we call make samples and actually train my network. Okay. This is just tracing the value epsilon. This is actually performing the epsilon decay. And then the rest of the loop is for plots. So let's run this and watch it go for a little bit. So this will continue the update. So basically what we can see here is I have the queue for each action on every set of trials. Right. So you can see this updating. I'm getting a bunch of different randomly initialized samples. So for this one, this is going to be the queue for each function. We can sort of start to see it start to take some, some kind of shape. So you can see that I'm getting some sort of peaks around around five for the action stay and maybe getting some peaks that are more correlated with like moving left to right for the other states. So this is going to be the value of X for the last 200 samples. So it's like not easy to see this is basically the sum of total returns as opposed to keeps accumulating so it becomes a big kind of block. Let me see if I can go through some of these other ones, it's not terribly easy. Do you want to. This one is the action. Okay. So sorry sorry for the sorry for the jumps. Make you some, some of you kind of seasick. Let's focus on the action for a little bit so you can see at the state when the state is five, the best action. This is plotted just as line with state at the best action between five at five is around zero, whereas the best action. That's not it's done. The best action for those states less than zero is one that is action for the states less than zero is negative one. This is the value of epsilon you can see it decaying. This is the smoothed value of the total total return. So you can see that it does train and it does start to rise and kind of approach zero. And this is this is just plotting what each hidden unit is doing so this is kind of hard to interpret. And here's a temporal difference error. So ultimately, this is actually what happened here is it basically plummeted for a bit and then after more exploration. The TD error is actually kind of starts to to even out. And so now if I hit. Define run, I will not run this because it says don't run live so it takes about six minutes. So what this is doing, this is going through a grid search trying to find what type of hyper parameters are actually best for this task. So we'll try a bunch of different numbers of trials numbers and stuff for trials epochs architectures values gamma learning rate, etc. So let me skip to the bottom of this. I'll just plotted it in a pandas data frame so we go through each of these. And then we'll talk about the results so this is the number of trials I sorted them by our last two I'll get that in a moment. So you can see for a number of trials steps for trial number of epochs network architecture value of gamma, etc. So the last two columns here are basically this is the total return over the entire training process. And then this is the last two returns so this is over those last two batches, what the returns were. That's the value sorted sorted by. So the take a look at it is there, do you have any sense of what network architectures look like they might perform well. Or, you know, architecture the incongence with other other factors perhaps. The goal is always the same the goal is always the land at five. Yeah, I could. Yeah. So this network would over this this network is over fit to cases where the goal is five. Okay, so if I if I if I change the goal retrain I'll find it. Right. This this network has been trained to arrive at a goal that is five. So I can change the goal to six or something. This train network isn't going to work. But I could retrain the network I might even be able to like take this network and like update it or something and tune those weights to update for goal six. Let's talk about generalization. Do you remember the block stacking example from last time. So what you can do is if I train it to stack two blocks and then I just change the environment to allow it to stack three blocks because also doing is choosing an action on top of the top most block. It probably would do an okay job of stacking multiple blocks. Right. As long as I was what it's predicting is basically here's an action is relative to something very specified in the world. So it's very localized. And then I can basically update what actually what is actually executed in the world like what my agent actually physically does. And they can probably you can learn this actually blocks or it could maybe take that train network and tune it slightly to be to be better at that task. Okay. So, I'm going to be doing that. The action is going to be the state here is basically learning that if I'm in state for the best action is to take one step to the right. If I'm in state six, the best action is take one step to the left. If I'm in state five, the best action is to stay still. So if I change my goal to six, that is I change the environment. If it gets to state five is going to say the best action is to stay still. And whatever reach the goal at six. Okay. So the environment, of course, is going to be as critical. So here, does anybody see kind of what might be something of a discriminating factor in terms of success of this type of network. Is the number of hiddens we got 10 10 here at the top. We also have 10 10 and 10 10 here at the bottom. Yeah. Number of steps for trial looks like it probably does. So like some of the worst performing ones, all of the hundreds steps per trial. Whereas the best performing ones usually have fewer. So one reason for that is that this is actually training less per trial. Right. So it's pouring a very small. So it's like iterative update. So just training for like, you know, 10 epochs or something updating the, the ultimate, the. Sorry, optimizer. Whereas the here, it actually might be training closer to convergence. And then the next trial comes along and it has to forget everything that it learned in order to optimize again. Okay. All right, questions. About neural network reinforcement learning. I'm sure there are many. I can talk about this at some length if you want. So if you imagine a more complicated environment like solving a game level or something. Yeah. No, go ahead. I have to go like. More than we consider now. Yeah, yeah. So choosing activation functions. Let me think about that. So you have a couple of. You do want to be kind of choose the about what activation functions you're using, depending on the nature of the problem. So generally speaking, like. If you use a Rayleigh activation, it's possible you may lose some important information. About say bad actions, for example. But also that would depend on how you formulate your problem, right, because if your reinforcements are negative, maybe Rayleigh was a worse choice there because it's squishing on negative information, but if you're reinforcements are really less positive, so basically, if the worst that you can get is zero. Rayleigh might not be a bad choice. Right. But if you're modeling it like we did here, maybe you maybe Rayleigh would be a non non as optimal choice. Other questions, comments, thoughts. So if you're trying to solve kind of a more complicated environment where there were say. Multiple recurring similar circumstances are trying to solve some sort of game level with something like that. With a neural network, what you get is you can increase the size of the network to accommodate different types of conditions. Yeah, question. Yeah. Yeah, so we did, we didn't talk about that a whole lot, but there's a brief sidebar at the end as I think of the Rayleigh notebook. There are, there are a bunch so that the L view part of Rayleigh was linear unit there are a bunch of other linear units. So like, you know, exponential unit. Gaussian error linear unit parameterized Rayleigh's will talk about some of those like briefly just refresh your memory so let's take a leaky Rayleigh was one, for example. So Rayleigh. Zero until zero and then why equals x well, maybe I want to squish out most of the negative information but on all of it so leaky Rayleigh kind of lets a little bit of it through so might be like. One times x if x is negative otherwise y equals x parameterized Rayleigh was similar to that it lets through some of the, the negative information, but it's not a constant. It's usually a tuned value. So I can kind of decide or I can try to learn the value of like how much of the negative information I don't want to let through maybe I want to let through like more of the less negative information closer to zero and I really want to disregard most of the really different information. Gaussian error linear unit is basically imagine what Rayleigh would look like if it were a nicely differentiable function. So basically we try to put the smooth curve in at zero. The Gaussian error linear unit is the one that's the that is used in most transformers and most of the chat bots natural language applications does are using. Yeah. Other questions. Yes. For like for like grid search. I mean, if you're doing a grid search like yeah this is like the way you would do it on if you're just doing your own. There are a number of libraries out there that you can use now. Hyper opt I'm not sure it doesn't do neural networks very well but basically you know there's a lot of parameter parameter search libraries out there that they will. But one of the best ways to do it is basically, I have a bunch of parallel processors. I will put one instance on each processor and run them all at once. And so that way I can try a bunch of things at the same time get all my returns back and figure out which one of these was best. So, but that of course, you know, it's going to be difficult to do just like on your on your laptop. Yep. Okay. Any other questions before I start on the project. Okay. Let me open that up. See. Where is. Solutions. These are. Projects proposal. And then here is a an example. This is the one for example actually. Okay. So. The project proposal. So basically, you can work in teams of up to four as I've mentioned. The, the scoburger project needs to reflect the size of your team. So, you know, it needs to be more ambitious. The more people you have. The project, as I mentioned to some of you before is really you think of this as an opportunity to maximize your own success. So that is. If there is a technique that you're particularly interested in or an application that you're particularly interested in or like you have some outside area of expertise to apply something to basically what keeps you interested. And then the other thing that you're interested in is what you probably would want to do for your project. If you have an existing research project and you want to put some sort of machine learning spin on it. That is fine. I do not object to that. So there are a couple of options. This is not necessarily exhaustive, but this is what I want you to do is like be inventive and pursue some topic of your interest. So, I'm going to need some ideas. Take a neural algorithm that we've covered in class and apply to some different data set that have interest and do some analysis and draw some conclusions. The analysis has to be more in depth than what we've done in class more than just like a bigger network work better or whatever. You know what I want to see is like, yeah, do the analysis of network strategies and different hyper parameters but also I'm looking for error analysis. So the thing is that your implementation fails on and why. Hypothesize, you can use other outside tools if you want, if it helps with the analysis. You can download code from the internet that implements an algorithm we didn't cover. So we talked a lot about neural networks I will briefly talk about like KNNs and SDNs and stuff at the end. But by then you will be pretty deep in your projects if there's some other machine learning algorithm could be neural could be non neural that you want to explore. And maybe compare that to something that we did do in class that that makes that makes sense. If you have a bunch of coding that you're already doing you're like I can't bite off another coding project. You have the option to write a research report. Or if you do something want to write a research report regardless of the amount of coding you're doing else or you can do that. What that means is you need to study at least five research articles of a topic of interest to you again a machine learning topic. You need to present a report that summarizes each article in detail describing similarities and differences between the papers and then you also need to provide a conclusion section that basically summarizes your takeaways in on that topic. So like, you know, if you look at, if you look at chat box or something you study five papers on chat box and then you would write a report summarizing each approach and then talking about like what the, what our common approach is what the current state of that field is. What you need to put in your proposal is basically just a confirmation that it is appropriate me scopes or remember you start this now. You could probably get approval if you're really fast as early as like the middle of next week. And you could potentially start then but let's assume that the sort of the starting on fire is on April 6. That gives you roughly a month and about a week to get everything done so you need to don't bite off too much. Make sure this is something you can do within about a month to five weeks. In the project proposal, you need to show that they're probably scope for time period and team size and make sure you put effort into both the implementation and the analysis. Talk about what questions you're seeking to answer. And then what hypotheses you can make about the data that you'll be exploring using whatever method you use. You need to explain why you want to do this project and the steps you will use to complete the project describes the method you will use so I need to see the sources the data. Are you going to define new algorithms and implementations. You're going to use them from an online source where you're going to get them from. Basically, do your due diligence show that you have cited your sources and that I can go there and see what you're going to be using. If you are working in a team, you also need to provide some definition of like how the work is going to be divided among the team members. So who's going to be primarily responsible for what this is not like intended to be like a hard, like great wall between the team members. Of course, I expect you will be collaborating. You're going to help each other out, but kind of need a sense that like one person is not doing all the work and the rest of people are just like free writing. Okay. Possible results. So you can speculate on possible answers to the questions you provide me introduction. This is going to be a little bit different between the coding projects and the research projects right the results the research projects. Are basically what you what you're going to be looking for right in your discussion. So what types of contrast you think you might find between different approaches. What do you think you this might tell you about the particular topic that you're researching, for example, possible results for the coding project or of course more on lines of what do you think the likely outcomes are going to be. You need to provide a timeline. So I want to see you like four entries with some dates and describe what every team member will accomplish by these dates approximately your grade will not depend on meeting these deadlines. But this is basically for your use so that you know you can come to me and if you have problems, for example, and I can help maybe try to get you back on track. Okay, so you don't. This is pretty short right doesn't need to be more than like two pages long. So I'm going to put this in canvas. And there's going to be a drop box. So we're going to be last name proposal or you know last name dash last name proposal. And then don't email this because it's going to get lost. So grading of this grading is basically complete or incomplete. So I will not satisfy what I will do is I will send it back for revisions. Once you once you revised it to satisfaction you'll get the full credit. So this is basically 15% of the project grade, and you will eventually get that 15% as long as you've heard it in and complete the revisions. So I'm already, you know, about like getting dinged on like if your proposal isn't is isn't clear or something. I will send it back to you for revision describing what needs to be made. So then also you make sure you're not like under committing or over committing. So like I said, this is teen points. So do this. And, you know, don't give these points up. So basically the grading on the project is going to be 15% for this 15% for the lightning presentation at the on the last two days of class. So what I will do is once I've gotten all the proposals in, I will count how many there are, and then that will determine how long those presentations are usually ends up being about two and a half minutes per each one. So I will go in a more detail of what you want to do for that but really for those last two days. There was every team is going to submit a slide deck that has probably about three slides, maybe more if you have a lot of teams may have longer presentations, but basically say, you know, what's your problem. Not in an aggressive way. What is the problem you're trying to solve? What progress have you made towards it and what approaches you're using and then like what, what is left to do or what have you learned so far or something like that. Okay. Any questions on the project? Yeah. We will. Yeah, so I will do. You at least one lecture on transformers toward the end. Yeah, so, and then oh yeah, I got to do this. You may not use chat GPT to write your proposal or your project. You may not use any line, any chat, but I can't use bar and you can't use all the other ones that came out you can't use like the, the one that the Chinese government put out the other day that didn't do so well apparently. So yeah, cannot use a chat bot to write this I want you to write your own words so basically here's my attitude toward the chat bots right now right now I'm actually literally, you know, in and out of meetings with people that tilt trying to help come up with some sort of coherent positioning chat bots so eventually I do feel we will come to some sort of coexistence and understanding of what is and is not an appropriate use of chat bots in the university. My attitude toward writing is if you're using a chat bot, you're not using your own words. Okay. You may possibly use like a chat bot to maybe help with like the mechanics or something, but ultimately it needs to be rewritten. You need to submit your own writing, and I will be checking against, you know, your written report and say your previous homeworks. I mean we're looking for things to see like has the writing style drastically changed is were you making a grammatical errors and suddenly it's like perfect right I'd rather see, I'd rather see the grammatical error version that I'm not going to be grading for for the next step. So, I think it's like super basic that I can understand. So if you're, if you spell things wrong occasionally you make the casual grammatical error. I don't really care that much because one is telling me that you're actually doing your own work. And two, this is not an English class. Okay. So, I think at also the other thing if you want to study, if you want to use like the outputs of chat bots in your project. So, I think that's a good thing. I think that's a good thing. So, just for some guidelines on like what my positioning is toward the use of these tools right now. Okay. Other questions. But yes, we will be talking about transformers of these, these ones. Okay. Yeah. What? Yes, I will go back. I'll start off as hours of three 30. Okay. Okay. Bye. Thank you.