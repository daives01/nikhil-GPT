 Yeah, yeah. Yeah. Yeah. Yeah. It's really great. It's like the number three bed hospital in the nation or maybe it's even the number one now and. Which is like it's a lot right but you consider how much how much treatments cost. Okay. Yeah. Okay. Let's, let's get guys. So, let me begin with note, mostly to the people who are not here, so I'm just gonna like tell you guys and those of you who are on zoom around this time of semester is when I start to notice that people are not coming to class. And this class is roughly 75 in person and maybe another dozen or so online so like I count the number of faces and it's like a lot less than 90. So those of you who are watching this recording. I do appreciate it when you let me know right and there are some people I can I can see who have already requested clearance to attend remotely and I appreciate that. So, if you're not one of those people and you're just like skipping class. I do start to notice, especially when it's like not negative 15 degrees outside there are certain days where it's like a lot more forgivable to skip class. It is now currently 31 degrees so today is not one of those days. I'm going to give this a warning that I'm addressing to the folks in person who don't need the warning. Not coming to class without a reason is going to affect your participation rate again semester and I assume that most of you don't want that. Of course just keep in touch with me you know if you are sick for a long period of time I understand you know just all the usual things but if you don't let me know I'm going to assume you're just skipping class. So, and then what am I supposed to do. Okay so you guy who was talking to me about the ethics cloak. What's your name by the way. Hunter, so I'm trying to learn folks names and it's hard to do in a class I said. So Hunter was asking me about the ethics colloquium today so if you're interested in hearing me and some other folks talk about chat GPT that's going to be happening in the LSC theater. So basically you go in the LSC and like, kind of as you're going in turn away from the food court down that like green line hallway and sort of keep going. And you'll get there eventually, it will also be recorded I'm told. And so I'm going to get the link from them and I'll post it, you know if you weren't able to attend. If you're interested in watching some of that discussion so it should be pretty interesting. So as a result I have to be there for. So I will have to help to close office hours at 345 at the latest. In case any of you are going to come by so, oh, eventually office hours will begin like the moment classes over if you need to talk to me I recommend just like catching me on the way back to my office and when we get there we can sit down and chat for a bit. But I will need to leave my office at 345 or so to head over to LSC for the event. So we are, I think we are almost done with grading a one I say we when it's sorry doing all the grading. But I think probably the weekend right. Okay, so yeah, so, so she'll give me the grades by about the weekend and I'll just need to spend about a day or so reviewing them so hopefully we'll be able to get back to you by the end of the weekend maybe So, I'll be there at 345 at the latest, which should be in time for you to take any feedback from PA one and and incorporate it into your assignment to submissions. So, if there are no other questions I will start the lecture on classification. You guys got anything you want to discuss. It's me I forget who all I think it's like five people it's from all around the university so it's basically like, I'm the only person in the university who does an LP as a primary research area as far as I understand so like now I'm getting like lots of requests to talk about how I'm handling I'm not sure the university policies. As far as I understand there is no distinct policy yet. My policy is don't. But it's going to be me. And there's like some folks from I'm gonna say like economics or the business school maybe the English department. I don't recall exactly but there's like four, four to six people total, especially a panel discussion for about two hours. So you know me and other people just riffing on the topic I've literally no prepared remarks, aside from what I rehearsed in my head so I would assume so I will say we'll see what happens right I mean I think it should be an interesting discussion I went to another one I did I was a I went to another one last semester. And then they decided to chat GPT so they asked me back. But it's a good discussion, usually it's pretty lively and there's a lot of interesting questions and things we discussed so if you can't make it in person I definitely recommend watching the recording and it just should be pretty informative and I'll probably touch back on some of the material we talked about when I do the last lecture on on ethics and and abuses of machine learning. Okay. So, if there are, would you have a question. I am told it starts at four. So I see the commences at four. So, yeah, I'll probably be there at least 10 minutes ahead. Yeah. Okay. So, let me share my screen. And let's start talking about classification. Okay. So, so far we've talked about regression problems, which is basically taking scale, mapping them to scale or output so my input is some set of numbers and my output is also a continuous number or set of continuous numbers. But many of the AI tasks that you might be interested in are not necessarily just predicting numerical values. Right, some of these what actually want to assign a meaning to things. So that is to say, how can I represent meaning numerically. Right, one is an abstract quantity that kind of resides in our brain and we have interpretations of, and the other is a quantifiable metric that we can actually you know perform numerical measurements on right how do we get these two things to align doesn't seem very intuitive. So we've done regression using linear models, and then also nonlinear models okay neural net so now that we're starting this unit on classification in similar form I'll be basically showing you linear examples of classification, and then nonlinear classifications using neural networks and we can see what is most useful for in which circumstances so let's just take a look and see how linear models are not suitable for classification. So let's we have some data like this, let's say we've got 13 students are taking a test. And yes you could plot the, the grade as an actual numerical value and then maybe it could be solved for using a linear function, but let's say that instead we have this data plotted as x being the number of hours that every student studies for the test. And then the y axis is the letter grade, let's just say ABC. So instead of a numerical value between let's say 70 and 100 is just one of three categories. Right so it doesn't matter so much if this person gets like an 83 and this person gets an 87. This is a big. This is what we care about right now. So, we could still try to do linear least squares on this by using the integers one, two and three for the different classes right so now I could have x being the hours of study, and then just plot one, two and three almost as if this were And you could do that, and then you could linear function to the data. Right. And so you could get a line that maybe looks like this given this data. This is not a great fit. Right, if I were to say, try to calculate the R squared value of this or something. It would not necessarily be very high because if you look at whatever value this is, you know, four hours. It, you would expect the value to fall here somewhere around 1.5 right when in fact the value is two. So, even though this kind of fits the data sort of relatively well, it's not a great idea because you have to convert these continuous y axis values into these discrete integers just one, two and three, 1.5 is not allowed right 1.5 is not one of my classes. So, without adding more parameters I need to figure out like where I have to split these, and I could use this general solution of like splitting at 1.5 and 2.5 and if it's like above 2.5 like I'm just going to round up and say it's a three, right, which would be an A. But if I do this, then rats. Chuck Anderson wrote this part of the notebook obviously because when things don't work for me I use more colorful language. Nonetheless, rats, the boundaries are not where we want them to be, because we can see here that if I'm just if I have my 13 samples and trying to arrive at an even distribution. I'm basically grouping one sample that I know is a B with the C's and another sample that I also know is a B with the A's. Right, so already, there's noise in my output set. So, I can already tell that by doing this, I'm going to have a very suboptimal model. So for example, if I have this, if I say X equals four, it's going to say okay well the predicted output is 1.4 which is below the 1.5 boundary and therefore it's a one or a C. So already, there's going to be at least two samples out of my 13, third is classified. I can do a lot better than this. So one solution would be to represent the class labels, not as numerical values, but to rather decouple the modeling of those values from the actual boundaries. So, using one value to represent all the classes doesn't allow me to, doesn't allow me to have that kind of flexibility. So instead I want to use three different values that are all orthogonal. So now, there's no clear linear dependency between say class B and class C, right. So even though the data may be broadly monotonically increasing, if you plot you know the grade versus the hours of study, it's not a good fit because basically the representation of the different classes is kind of orthogonal to each other. So this could just as easily be three completely different classes of, you know, subjects, maybe that maybe if you're in a study that or if you're a field that requires like on average fewer hours of study per night, for whatever reason, just because the nature of the field, you could use that to predict, you know, this, predict what field someone is in based on their hours of study per night. So class modeling, basically what you want is all your classes to kind of be orthogonal to each other. So one way to represent these orthogonal values are just to think of them as say vectors, right. So if I have three classes, this could be a three dimensional vector where every every class is just perpendicular to all the other classes. Okay, so one way to do this would be just to represent these what we call indicator variables. So if I have three classes of indices zero one and two, and I'll just put a one where it is a member of that class and zeros elsewhere. So indicator variables or one hot vectors, right, are basically the term that we use here. So we can model class one as one zero zero class two is zero one zero class three is zero zero one. If you think of how you might plot these coordinates in 3D space, you basically end up with something that looks a bit like this. So if my thumb is the x axis, my index finger is the y axis, middle finger is the z axis, you can see how if we assume that all my fingers were unit vectors, all these vectors would be orthogonal to each other. Okay. So now I can just let the output be this, this triplet, this, that has you know for however many value however many classes I have it has that many values, and I want to maximize the predictions that I get close closest to the indicator value that represents the class that is the actual true value. So then I'll need to convert to do is take these values and convert them to a class by picking the max. So, for example, if the class is 001, then this should y1 should be one or close to it, y2 should be zero y3 should be zero. If I take the argmax it's going to say okay index zero aka class one is the most probable class. Okay. Yes. Both it should be one. Both it should be one. But we're approximating so eventually we're going to get it as close to one as possible. I mean a little ahead of myself with the, with how we calculate error and classification, and we won't really get to it so much until the next lecture. So for the moment you can just assume that you want class one to be one. Well, if I represent my outputs this way, then I can just have two values on the y axis zero and one, and wherever that the class in question is one if it's one else rate zero. So, if I plot the data like this now we can see that for example all my A's are ones in class three and everything else is zero, and these would be all my B's, and these would be all my C's. So, it would still fit a linear function to this data. Right again it's not going to be a great fit, but it would look something like the following. So then what I can do is I can overlay them to see which one is the maximum value so for example, the, the blue one is basically the line fit to the A data, the red line is the line fit to the C data, and the green line is the line fit to the B data. So these are the boundaries between my actual classes I just look and see for each of these lines in which class, or which, which model has the highest value at that point. Right so the red line is my C's we can see that for these samples, the value is higher than the other two. So therefore these would be in numbers of class one, the green line is the highest for for this set so then these would be members of class one, and the blue line is the highest for members of this set these members of class three. So, what if the green line were slightly too low. Right so now if we look at this sample here, right, this one, which line is the highest at x equals this value, where the mouse is the red one is the sort of the class zero line or the C line if you will. But for this sample that we know is a B it's actually predicting a C because the C line is highest. So, why might this be the case. One reason that we could that could be the cause of this would be a masking problem right we have too few samples of class two. So if my class if only have three samples of class to my data looks like this, and the line that is fit to this data is going to be lower than this green line right is actually going to be much more like this. And so, if I have too few samples of a particular class then novel data that belongs to that class is kind of not going to be modeled as being a member of that class. So there might be no values of x which the second output y two is larger than the other two given a linear model. So that is class two has been masked by the other classes. So if I have say, five samples of class zero and five samples of class three, only three samples of class two, it becomes harder to pick out new samples of class two from new data. This is the same issue, as we would encounter with an unbalanced data set right it's more probable that something is going to be a member of class zero or class one or class three. And so I'm going to get a lower error if I predict those more often. Right, so this is going to, this is going to make me under predict instances of class two. So, let's think of what other shape of function would work better for for data like this. So, you can chew on that if you have any answers, you know you can you can spin them out later. Just hold this thought and let's try an example, real quick. Well, mostly try an example for the rest of the lecture so not really all that quickly but we'll go off to an example and come back to this later. So we'll be using this other data set from the UCI ML archive and particular data set of Parkinson's disease. So what this data set contains is actually, it's a nice bit. So what this data set contains is 147 samples from subjects with Parkinson's disease, and then 48 samples from subjects who do not have it. For these each of these samples, they extracted 22 numerical features from voice recordings so basically they recorded the voice, and then they run you know a bunch of like you know spectrograms and things over it and Fourier transforms And then they run you know different features. So, the feature engineering is kind of already done. These, these features are assumed to be relevant features from vocal analysis to predict Parkinson's, and then they're labeled. So, we just have zero for healthy subjects, and one for subjects with Parkinson's disease is actually from a collaboration with the University of Oxford and the National Center for Voice and Speech in Denver. So we'll just read the data read it in, and then just do some statistics over the data. So, if you look at this, we can see that we have 195 samples that is 147 plus 48. And then 24 indices, so we have the 22 numerical features, plus one for the label, and then the other one is just like an index, so 24 columns. So if you look at all the columns we see here. Okay, so we have name right this is the index, and then we have all these different features. So, there is status in here somewhere there so that their status. This for some reason is not the end but we know that this is the actual value. So we're going to extract that status value, and this is going to be our targets. So now we've taken the 24 features sliced out one of them. This is actually the feature trying to predict, and there are 195 by one values in that array. Okay, remember that other, there's that name column, that's just an index this is not going to be a predictive feature right you can't predict whether someone has Parkinson's if you know their name, or if you just have like some numerical index representing the study that's not correlated. So we're going to drop that and then we're going to drop the status of course because you don't want to have the thing we're trying to predict in the training data, because then what would it learn. Yeah. Or whatever. It would just ignore it could just learn to ignore everything but the status right the status column and the input is obviously a perfect predictor for this for the output so it's like well, here's the answer. I don't know everything else and I don't learn anything. Right. So, one thing that we do in in analysis is just sort of you look, look, like is, is a model. Does the training data effectively answer it obviously. And if it does then that's contaminated training data, but also are the features indicative of the answer right you you had you want to give the the model enough information to figure out the answer, without just giving it the answer. So these are the features that we actually actually use so I don't really know what all of these things necessarily mean maybe some of them mean something to some of you but they're, they're just, you know, features like frequency and volume and vocal quality and things extraction of the local recording. So, let's put the values for each of these so we can see that the ranges are very different, right, we have some that have means that in the hundred and some that have means close to zero and some that have negative means, and then there's a wide distribution as given by the standard deviations as well. So, let's look at the occurrences of the individual values, so we can see here that as, as was mentioned in the description of the data, we have 48 samples of zero, that is, people without Parkinson's disease, and 147 samples of one, that is people with. So this is basically just a binary classification negative positive for for this trait. And you can see that the sample is is unbalanced right we have way more samples who have Parkinson's than without. So, this is going to be one challenge with this. Okay, so for we have the small sample size overall under 200 samples so not huge even though we do have some pretty good data for each one. And then it's very unbalanced that we have like 100 more samples of one class than the other. What we want to do is we'll just force equal sampling from the proportions of the two classes and use that when building our training test partition so we'll use 80% for training and 20% for testing this is a very typical split that you will see. So here we can specify the training fraction so you can run this notebook and message this value to your liking. And then we can extract the class for each of the subjects, and then permute all the, all the data, take the training fraction, and this should keep roughly the same proportion in the training and test sets as occurs in the actual data. So we can see now. So for the training set we've got 156 samples. And for the testing set we have 39, and this should be roughly the same proportion I can verify this by taking the number of class zero divided by the number of class one, and we can see that it's roughly one third. Right, this is not going to be exact. But it's close enough. And so 32% of healthy subjects versus 34% of healthy subjects, or three to one ratio I should say between healthy subjects and Parkinson's is, it's decent, right. And then this compares to the original data set. These numbers are very comparable. This is what we want to see. Okay, so the least square solution would be first will standardize the inputs. We don't need to standardize the outputs because now they indicate the class. Also, the outputs are zero and one. So if you standardize them, I'm going to put something very similar. So what you don't want to do is when you're doing classification as compared to regression, you don't standardize the outputs because they're basically numerically indicators and actual class value. And then there's just some mapping that you use or just convert the numbers to say, a string label or something like that. So, then you just calculate the linear least square solution. So the training function for this would look something that looks pretty familiar. So we have, we calculate our means, standard deviations, we standardize that. We then insert our bias column. And then instead of doing iterative training using SGD right now, we can just do linear least squares. So I can use this, the linear algebra library from NumPy and just do that. And then I'll have a use function that basically will do the same thing. We just, we have the model, we standardize it, insert the column of ones and then multiply by the weights that were in this case we're calculating using least squares. So let's take a look at the data real quick, right. These are those 22 columns that are supposed to be interesting features. And so then I will just take this, feed that into my train function. And then I'll insert the bias. And then because this is a linear model, what I can do is after training, I can actually see what weights are assigned to each of those, each of those values. So, here we go. So of this, right, just take a look at this list real quick and see you know which ones do you think appear to be the most important, right. Remember how we interpret those weight values. So, what do you think is the most important feature? Shimmer? Yeah, shimmer, right. So this is basically highly, well, so this one is highly positively correlated and then there's this other one that is highly negatively correlated. What else? Jitter is pretty high. So, those three probably and then this MDVP, RAP, I don't know what that means, but it also has a fairly high negative value. So like those four features are probably getting us most of the value for our model, right. However well this model does, it's doing mostly on the strength of those four features. The rest of those values are like pretty close to zero. So now let's test the linear model, right. So to compare the target values of zero and one, you need to convert the continuous output to zero or one, whichever one is closest. Okay. And this would be something that you can do with a number of different functions, but we'll have this one function convert to zero one that's basically just computing the distance from the target that is either zero or one, and then just figuring out using the argument, whichever one has less distance. So, if I convert some of these samples where you can see this function at work, so whichever one of these guys is closest to, it will then assign to be that value and you'll note that is not bounded at zero and one, right. We can go above, right. We can have an output that's 1.1 that is much closer to one than it is to zero, so it must be an instance of class one. We've also got negative values in there. So I'm going to have to make things like 0.56, right, which is very, very slightly closer to one, but I must make a choice. And so it's going to choose one. So classification algorithms, just to keep in mind is basically, I'm giving you a fixed set of possible classes and you must choose one of these things. Unless you explicitly have, I don't know as an option, it's never going to say I don't know. It will say there is an infinite infinitesimally greater chance that it is a member of class one. So it must be a member of class one, right. So this is the difference between like 0.50001 and 0.49999. So instead of essentially saying a certain level of confidence, like if it's closer to this amount, I'm confident that it's this. Yes. Yeah. So we'll talk about that when we do classification with neural networks, and it's pretty easy to do that. So you can actually quantify sort of your confidence interval and say this is, I believe it's one and I'm 90% confident that it will, it's going to fall in the range that's going to be classified as one. You can get it to do that. Yeah. So you can usually most models will not do this by default, but you can write some code that will do it for you. Okay, so let's use our, let's use our model. So I'm just going to, I'm using the model over the training data right now, just to sort of test how it's doing over the data that's been exposed to right we see if it's at least a good fit to this model. And then I will do the same thing for the test data. Right. So what do you expect might happen you expect to see a difference between, say the percent correct train and the present correct test and how much right speculate. Okay, 10 Yeah, or, you know, I was just looking for like a lot or not much. Okay, but here we go so let's let's run this and we'll see we just convert all of these to zero and one, and then convert that to percentage. So in this case, actually, they're the same. And this might be, you know, you run this a couple of times with different training splits and you'll see some difference. Yeah, there's a whole lot of difference. In this case because the features are quite predictive, and the test data is just, you know, there's so many features in there. The test data is very very resemblance of the training data so it's unless you get like in a really unlucky split and there's like some peculiarities in the training and the test data. So, you know, random splits are kind of this double edged sword because you can, could you could get a lucky split right you could, I could do this. And I could say, oh wow, I got the same testing and training accuracy and therefore this is as good a model right but if I run it again, and it has a different random split I may not see that. And so, when you're reading a paper or evaluating model. You need to look out for whether they mentioned you know is this like an average of 10 runs or how are my splits done is like the best of and possible evaluations. And when you're writing a paper, you know it's good practice to report that as well. And also think about how you're, how you're presenting your results, you know, are you averaging over multiple runs. Are you using random splits or are you doing like participant wise splits. Yes. For assets that are like, I guess, they want to use the ensemble classifier to kind of alleviate the random splitting. Yeah, yeah so you basically for something like this exactly when it's a small test set right there's a higher chance of maybe getting some slightly odd samples in either training your test set and so if you are unlucky enough to get that, then you either have an overvalued evaluation or like a suppressed evaluation. So you can use like different different models and ensemble them you can run multiple times and average them. You can do your voting, there are multiple ways of handling this, but for for small for small test size for small test sizes and things like this it's generally good practice to maybe like get a second opinion, like this one model is doing really well. How do I know it's not just something weird that this particular model picked up about my training data. So it's good practice to evaluate it multiple times, multiple models, etc. So, let's do some visualization, right so what kind of visualization, could I use to check the results so what I can do is I can plot the true class of the output, and then each training samples you basically have to to series, what the actual class is and then what my model predicted that. So, that may look something like this. So we can see that the blue line underneath is the actual class we can see that these are like my 30 odd zero samples and these are the rest of the one samples. And then those things where the orange dot doesn't hit the blue line those the misclassifications, right. And so we can see, for example, what do you notice like which class has more misclassifications. So, which of the classes zero or one has more misclassifications, so this is zero, right, and that does make sense. Right, if you think about the imbalance in the data set, because you're going to minimize your error more you're going to get a greater So, it's basically losing more or losing less by misclassifying zeros and is my misclassifying ones. If this set were more balanced may not be the case. Look at the testing data we can see kind of the same phenomenon, right, there's fewer samples of course, we can see there's only one misclassified one class and three misclassified zero classes. So, remember we did for convert to one is basically saying, I'm just gonna have a decision boundary at point five, and doesn't matter how far it is from that decision boundary. If it's above it it's one of the below it is zero. So we may want to know for these samples do we have any that were just like real borderline in cases, right, there could be one where it's like, well I classify it as a zero, but just because my output value was like point four or nine for you know whatever. It's like okay. You just easily could have gone the other way if there's like a slight slightly different split or something like that. So, if you're like really borderline maybe don't take that as like absolute truth. So here's the actual continuously predicted output. Right. And so remember, it's not bounded at zero and ones we have values that are below zero and values that are above one, but we can see for example if you look at some of these misclassifications right these samples here. These are the ones that are like just barely below point five. So if you classify it as zeros, they're actually ones but it's like these actual values are probably you know point four or five to point four nine somewhere in there. So you can you can think of this excluding the, the ones that are above one or below zero just think of it as almost, it's 49% likely to be class one, which means it's 51% likely to be class zero so it's like if you're when it's when it's election season, I like read 538 like religiously, and so like the other predictions like saying well X is like 51% likely to win this race or no, and then they lose. What happened well it was a coin flip right it was not you did not have a good prediction for that. So like you're saying it's 51% likely does not mean it's a sure thing. Right. So this is a cognitive bias that people have similar things we see in the testing data, right. So we can look at the sample of samples. The, and one thing we can observe actually is that if you look at the samples that are misclassified as one that are actually zero. Those output values are much higher, right, these are falling in like this is like point 8.6 point seven something like that. This is also an artifact of that sample imbalance, these output values because there are fewer zero samples kind of drift arbitrarily toward that one boundary. Okay. So, what is the shape of the boundary. So imagine that you've got just two variable attributes, x one and x two within our least squares model will create make a prediction for some sample. And it's just going to be our linear model right w zero plus w one x one plus w two x two. And so then, for the Parkinson's problem will just have this decision boundary at point five. And if it's less than point five this output values less than point five, then that is zero and otherwise it's great. It's, it's one. So, the boundary is given by the equation. You know the sum of all weights times the times the inputs equals point five. So, what shape is this actually. So the above methods what we call discriminative, that is, they're trying to they're picking up the features and trying to say, how do I wait these features so that given a set of input features I can discriminate one class from another, right, it's not taking really into account. Things like how many classes, there are, or how many instances of each class there are just like in in the data set or in the wild. So another alternative approach is to basically create this probabilistic model from each class. So that is, this is a generative model so that is there is a underlying base rate for each class. And that's going to influence your prediction. Right, so if, if I'm looking at you have to know something about your data I'm looking at classifying Parkinson's. One of the things I want to know is like what's the overall base rate of people who have Parkinson's in general, but I might also be more interested in like what is what's the base rate of people who have Parkinson's who come into this clinic, because this is where my data came from. Right, so if I'm if I'm looking at data from a particular source. I may be more interested in the class relative to that source as opposed to just like the class in the wild so this is one of those. One of those things we just have to be aware of like your environment so like a lot of studies that are done at universities, of course, there's a bias towards the sample population right so we there's this acronym weird Western educated industrial rich democratic. And so you know, most people who come to a university are going to fulfill like at least three of those five categories. And so there is a bias toward people who come from those societies, and against people who do not come from those societies because they're left out of the data. Right, so maybe more interested in like, what is the prevalence of this class in my capital weird population rather than what is the presence of this class in the entire world because your sample is not of the entire world your samples of a particular group or set of roots. Okay, now before jumping into simple generative models. Let's just do some review of the probability theory that we talked about in lecture two. So, if you want to review this many of these concepts were raised in the entropy section of that lecture. Let's take some boxes of fruit. So, here are boxes, which jars, this red one is a blue one, and they all contain each contains some number of number of apples oranges and strawberries. So, if we count the number of fruit in each jar, we can see in the blue jar there's two apples, six oranges and four strawberries and the red jar. There are three apples green ones. One orange and then two strawberries. Right. So, there's 12 fruit in the blue jar and six fruits in the red jar. So then the probabilities of a fruit from a given jar, that is in the blue jar. There's two out of the fruits to the 12 fruits or apples, six out of 12 oranges, four to 12 or strawberry so there's like a given the, the blue jar, there's a 50% chance that if I pick a fruit out of that it's going to be orange. Right. Whereas if I have the red jar, there's a one in six chance I'm going to draw an orange out of there, because there's only six fruit and one of them is an orange. So, now, let's say that first I choose a jar, and then I choose a fruit from that jar. Right. So now instead of a single event that is, here's a jar pick a fruit out of it. There's two events. First you pick a jar, then you pick a fruit out of that jar. So now we need to know the probabilities of picking the jar in the first place. So let's just say for argument sake, that the probability of choosing the blue jar is 60% to the red jar is 40%. These are just numbers, there's no real reason behind this. You could say these are equal probable, because there's only two jars and you could choose them randomly, but that wouldn't change the probabilities, which is what I'm interested in showing here. So let's just say that these are. So then the probability of choosing the blue jar and drawing an apple out of the blue jar is that joint probability or the product of these two choices. So, 0.6 times 0.167 or 0.1. So with all the multiplications, we can see that the joint probabilities of these events are these different values here. And you'll notice that these don't sum to one. Right. Why would they sum to one? They wouldn't, because they, you're basically picking the jar first, and then you're picking the fruit from within that jar. So whatever value you finally get is predicated on the probability of those two events. And you'll notice that the sums here are not exactly due to rounding, but they're really, really close to 0.6 and 0.4. So basically what I'm saying here is that the probability of picking the blue jar and then any other event relevant to that happening is the same as the probability of picking the blue jar. So if I have a limited number of things that can happen after I pick the blue jar, the probability of any of those things happening after picking the blue jar is the same as probably just picking the blue jar. So it's literally just what's the probability of event X and the probability of literally anything else happening? Well, the probability of literally anything else happening is always one. So just multiply by one. So I'm going to combine these into a two dimensional table and show the joint probability of the different events. So it looks something like this. I'll have the jars on the rows and then the fruit on the column. And they look something like this. And so you can see that here are my numbers that are just the sum of the probabilities of the jars. And then these should be the probabilities of just picking a fruit. So I can just add all these together. Add all these together, they should sum to one. So there's where I get my sum one. So now just symbolically, let's just represent these as variables. So let J be a random variable for a jar and F be a random variable for a fruit. And so you can represent them like this. So here are the joint probabilities just given as P of J comma F. So this is really written. You can usually just eliminate the equal sign here. So you just maybe write this as like probability of blue comma apple. And the assumption is like, you know which one of these refers to a jar and which one of these refers to the fruit. So these can be used in Bayes' rule. And so what we just saw was an example of the product rule. So that is this. So if I take the joint probability is going to be equal to the probability of some event given another event times the probability of that event. So if I have some event F and some event J, then the probability of F comma J is going to be the same as the probability of F given J, right, vertical bar means given times the probability of J. So now, since these two, these are joint probabilities so I can just swap them, right, the probability of F and J the same as the probability of J and F. I'm just looking at co-occurrence. There's no conditional dependency. So we also now know that if I swap these, then I can rewrite this equation as probability of blue given orange times the probability of orange. And so now we know that this equation here and this equation here are the same. So now I have P of blue given orange times P of orange equals P of orange given blue times P of blue. So now I can divide both sides by P of orange. So what I do is I end up with P of blue given orange equals this side of the equation P of orange given blue times P of blue divided by P of orange. And so now the general form is this. This is Bayes' rule. P of A given B equals P of B given A times P of A divided by P of B. So if you want to remember the pattern, it's A, B, B, A, A, B. And you just need to remember where you put your lines. So A, B, B, A, A, B. So in other words, the posterior probability, don't laugh, is this thing here. So this term, this will equal the prior probability. It's the latter term in the numerator. That is the initial degree of belief in A or the base rate of A and then times the evidence. That is P of B given A divided by P of B. So that is the support that B provides for A. So on the right hand side of Bayes' rule, all the terms are given in the fruit example, except for the probability of orange. So I know I can get all of these terms, these initial probabilities and the probability of the jar from the tables. I can just pull those directly out of those values. I have to still calculate the P of orange, but I can do that using the sum rule. And so this would just be the probability of the fruit, the joint probability of the fruit orange and the jar being some jar J and just sum that over all jars J. So in this case, if the jar is blue, the probability is point three and if the jar is red, the probability is point zero six seven. So we end up as point three six seven. So in other words, Bayes' rule can be rewritten by having in the denominator the sum for all events B or sorry, events A. So that's the probability of the joint probability of B and A. So given that, we can then put back the equivalence to joint probability that we calculated earlier in terms of conditional probability. And so for all J sum the conditional probability of F given J times the probability of J. So these are all multiple different ways of representing Bayes' rule, depending on what values you actually have accessible to you. And this is known as marginalizing out and can be done using the joint probability or the conditional probability, depending on what you actually know. So now we can do this in Python. We can actually represent a conditional probability table as a two dimensional array. And I kind of doubt going to get through this notebook today, but I believe I planned on that. I'm a day ahead anyway. So let's include the row and column names as list and then write a function to print the table. So I have jar names and fruit names and I write my print table function. And it prints out a nice little display like I had earlier, including the sums, right. So we got like 18 total fruits. That's what the bottom right here means. 12 fruits in the blue jar, six fruits in the red jar, five total oranges, six total, five total apples, seven total oranges and six total strawberries. And here's the distribution in every jar. So now I can just like sum across the axes to get those sum values. I can calculate the sums of the fruits in each jar by doing this. And now I can calculate the probability of drawing each type of fruit, given that we've already chosen a jar, right. So I take the jar sums and then just divide the counts by that. And so now I can create this conditional probability table that gives me those values that I saw above. And you'll note that when we do this, we actually see sums to one at the ends of the rows. Because what's the probability of drawing a fruit, a given fruit or a known fruit given a known jar, and then you sum all of them right that should equal to one. What's the probability of drawing a fruit, whatever it is out of a jar, it's one right if you're not concerned with the actual characteristic of the fruit is. So we can do a bit more if we code the probability of selecting a jar as an array of point six and point four. And so now I can use this to calculate the joint probabilities just by multiplying those conditional probabilities by the jar probabilities. And so now here's my joint probability table. And now we can see that sum to one only occurs in the bottom right as it did before. So this is this table is all possible results and so this better sum to one. Now we're back and like, what's the probability of an event. So now it happens anything I don't care about its specifics, therefore it's one. So how do we get the probability of a fruit from this table. So we just marginalized out to remove the jars by something over the jars. And this can be accomplished just using NP some over the axis that represents the jars. And here we see those product those fruit probabilities that we saw before. So the probability of a jar, given that you know which fruit was drawn. And so now we can calculate that as P of jar given fruit equals to your fruit and jar divided by a p of fruit. And so now we get this. So, just we can represent all these as NumPy arrays and with some pretty simple operations, just keeping track of your axes, you can get all these joints or conditional probability values out of it. So if you don't know which of these values you need you can use these operations to get it. So now let's use Bayes rule for actual classification. So instead of our fruits. Let's look at hand drawn images. Right. So now I'm trying to say, I pick a image out of a quote jar. And I want to look at it and want to see okay, what digit does this represent zero through nine. So let's let I be a particular image, and then to classify I some digit like four, we need to know the probability of a digit for given an image I right so it's assumed there's some a bunch of samples of hand drawn fours and other digits. Right. And so those are individual instances, and they all belong under a class right that is the digit. And I just want to be able to classify the image as the digit so I need to know the thing that I have is the image. What's the probability of each of these 10 classes given this image. Now we probably only know P of I given digit. Right. But if we assume that I is one over the number of images. Right. So what's the probability of drawing this specific image. It's one out of however many images I've got. Right. And then how many, what's the probability of the digit being for if we assume it's evenly distributed, then this is going to be one out of 10. There are 10 digits and like I'm just choosing them randomly should be 10% chance of getting any single one. So then we can use Bayes rule by plugging in all those values. I'm sorry. In this data set now. So we'll assume that these, these are all unique samples. They look like really similar, but they're all unique. So, Bayes rule will give us the following. So, P of four, given I is going to eventually simplify to P of I given four times point one, like that's the P of digit being four divided by one over the number of images. So, for a simple generative model above we had used linear function as a discriminant functions, there's a boundary. And if it's on one side of the boundary it's one class of us on go side of the boundaries and other class. Now if we had three classes, you would end up with three discriminant functions, and then you compare the values to find the maximum value to make the class prediction. So a discriminant function is this describes the curve that separates points in the data that describe different classes. So if you remember from the neural network lecture I had the one example of like a curve of blue points and a curve of orange points. Right. So if those represented different classes I'm just trying to draw a line between them. Right. So instead of trying to draw a line to fit data, and now trying to draw a line to actually separate individual points. So for n number for any classes the number of discriminant functions is going to be either n minus one, or P, where P is the number of predictors that is important features that are strongly correlated with each, each class, whichever one of these are smaller. So a different way to develop a similar comparison is to define a probability distribution over each of the possible values, which are generative models so basically I have for every class I have a different model. And this is going to incorporate things like the base rate of that class in general. And so then I just run all of my models, and I'll see whichever which one produces the highest result. So these all eventually should generate some sort of probability, or to say like, if I have three classes one two and three it's like okay it's 50% likely that it's a 140% likely that it's a two and 60% likely that's three. Right, it's going to say three, and those classes don't have the probabilities don't have to sum to one. Right, because each of them is a separate model it doesn't know anything about the other models. So how would you like to model this probability distribution or a typical cluster. If you believe that data samples from a class have actually does tend to be close to a particular value, that is that the samples cluster around a point. So if I have some n dimensional representation, all my samples from one class kind of cluster close to each other, right, or at least they're closer to each other than they are to other things from the cluster could be very loose or could be very tight. But the point of a cluster is just that things in that cluster are closer to each other than they are to other things. And we can do clustering algorithms and we will later. And you can also do like a K nearest neighbor to assign new values to one of n clusters. But we want to kind of cluster over some central point of the sample space you want to pick a probabilistic model, it's going to have a peek over that point. So whatever point this is in n dimensions. If I describe the distribution that distribution should peak close to that point. So that is things that are closer to that will have a higher value according to this model. Therefore, it also falls to zero as you move away from that point. So to construct such a model we want a couple of different characteristics so that is the value of that model will decrease as we move away from that central point. So I have a sample that is far from the central point for a given class, it should have a low probability of being a member of that class. And then the value will always be greater than zero. So the least probability that you can have of being a member of any given class is going to be zero. And so if X is the sample and mu is some central points you can achieve this by taking one over the distance between X and mu. So it's magnitude of the vector. So let's take a new to be 5.5 and make a plot. And we can get something like this. Right. If I use the function that I defined previously, then this will give me something on the order of this, but it meets our criteria of flustering around a point, but it goes to infinity at the center. And you can't control the width of the decay in which central samples may appear. So first of all, we can take get rid of that infinity issue by taking the distance to be an exponent so that when it's zero, the result is going to be one. So that is, if I take two to the one, that is, sorry, two to the zero, right, if these two are exactly the same, right, this is just going to be one over one. So let's use base two for this to make things simple. So now we can want to see how we do a calculation with a scalar base and a vector exponent. So let's try to do this. This is not going to work right because I cannot exponentiate an into a list. Data types are not compatible. But if you use a numpy array, you actually can do that. Right. This will allow you to do it element wise. So neat trick there. You don't need to stick into a for loop. All right. So if we do that, OK, so this is better, right. It doesn't go to infinity as we approach this point. It is capped at one, but it still falls off way too fast. Right. So I want things that are like really close to this, but not quite at this. I probably want that that probability to be like point nine something not I don't want that probably to fall all the way down to like point eight. Right. So we don't want to be so exacting that you must hit this this central value exactly in order to actually get a reasonable probability of being in that class. So we want to change the distance to this function that's going to change more slowly at first. But when you're close to the center, but then continue to fall off. So what if we square this right now? I have two to the distance squared. This looks better. Right. So it starts to fall off less slowly. So this is a much nicer shape. And so now we can scale the square distance to vary the width. So if we scale it by point one, this means that the probability of a sample being in cluster X is going to fall off ten times as fast. So this looks. Better. Right. This is a pretty nice, nicely shaped function. So we could just pick a center and scale factor that best matches sample distributions. But let's make a single one more change that won't affect the shape of the model is going to make the calculations simpler. And so that is we're going to change the base of the logarithm. Right. So, you know, I assume you're all familiar with some of the properties of logarithms and we'll see how they come into play to fit a model to a bunch of samples. But the logarithm of say two to the point one times the square distance, or that is we'll just rewrite this as Z. So what's the logarithm of this? We know we can we can convert between logs. So if we're talking base 10 logs, then log of two to the Z would be Z of log two or Z times log two. So this means we can pick the base of whatever we want. So there is a number, as you're all aware, that has some really nice properties when it comes to logarithms, which is E. So the logarithm of the so we use the natural log. So the natural log of E is one. And so now if I take natural log of E to the Z, this will be Z times the natural log of E, which is equal to Z. So this makes things a lot simpler. And so now our model is going to be the probability of X is one over E to the raise of the scale factor times square distance or E to the negative scale factor times the square distance. So this is just going to be some constant. If I plot this again, this is really not going to change the distribution. So the scale factor is a bit counterintuitive. So that is the smaller the value, the more spread out the model is. So let's divide by the scale factor instead of multiplying it. And we'll just call it some variable sigma so we can tune that value. Let's also put it inside the square function so that it's going to directly scale the distance rather than the square distance, which also makes the calculation simpler. So now we end up with something like this. So we're going to end up taking derivatives, should not be surprising. So we'll be taking derivatives of this function with respect to parameters like mu. So then let's multiply by one half. So then we bring the exponent down. It's going to cancel with that one half. Again, this is all just to make the math simpler when we actually go to do the computations. All right. So now the one remaining problem is that this is not a true probability distribution. So this probability distribution must have values between zero and one, and then it must have values that sum to one over the range of possible values. So we satisfied the first requirement, but not the second. And we can fix this by calculating the value of the integral and dividing by that value called the normalizing constant. This is called the Gaussian integral, which ends up being the square root of two pi sigma squared. So if you want to go into more about why that is, check the article. Now we finally have this definition. Right. So we scale by one over the Gaussian constant, and then we take e raised to the negative one half times the square distance over sigma squared. And so now we've arrived at the normal or Gaussian probability distribution, or technically the density function thereof, assuming mean mu, standard deviation sigma, and thus varying sigma squared. So now you know a bit about why we use the normal distribution. And it's so prevalent because it has these really nice properties and desirable characteristics that are useful for constructing probabilistic models. Okay, so now you just hear some more about probability theory from various authors. So you can read more about that at your leisure if you so desire. So now before getting into Python, we need to define the multivariate normal distribution. So we should go to multiple dimensions because we don't know how many classes are going to be dealing with, and we need to have a normal distribution that's going to allow for me to do that. So in order to handle multi dimensional data and not just scalars, we'll go up to multiple dimensions. And so now if we say have two dimensions, that hill we've been drawing is going to be a mound, right, looking like those hills that we drew in the neural network lectures like Lecture 7 or so. So basically we're just going to have a two dimensional base plane and define some coordinate. And then we want to have this distribution fall off appropriately in all directions. So we'll define now X and mu to be these two dimensional column vectors. And we're doing two dimensions here so we can visualize it well. But once you've established the convention in two dimensions, it can be easily scaled up to however many dimensions that you need. So what should sigma be? So you need scale factors for both dimensions. This will allow us to stretch or shrink the mound in both directions, right, because you may not want to fall off equally in both directions. You might be more clustered around in a particular dimension or along a particular axis. So in two dimensions, the difference vector is going to be basically X minus mu. And you end up with these two values, d1 and d2. And so then the square distance is going to be d1 squared plus 2 times d1, d2 plus d2 squared. And so now you can see where the three scale factors go. So we have S1 here and then S2 here and then S3 here. And so this can be written in matrix form if we collect the scale factors like so. We have two instances of S2 because we multiply it by two. Right. So now I can put S1 here in the top left, S3 here in the bottom right, and S2 here along the opposite diagonal. So now you can think about, yes. So think about if you have multiple dimensions. So think about how we expand a polynomial. So if you have just a binomial, you end up with basically a squared plus 2ab plus b squared. So we have a cubic function. It's going to be, I'm really bad at rallying off functions at the top of my head, but like a cubed plus 2a or 3a squared b plus, yes. Then the coefficients get arranged in the matrix. You're going to have like three instances of S, or two instances of S2, like three instances of say whatever S3 would be, and then two instances of S4 and then S5. So now what we can do is we can now have D transpose times sigma times D. Okay. And so now if I do this, if it's the inverse, the identity matrixes would just cancel out to be sigma. So if I have D transpose times sigma times D, then I have D1, D2 times sigma times the original D. And so this will eventually expand out to this where I have every instance of the scale factor, the right number of times. Okay. So it's more intuitive to use scale factors that divide these distance components, rather than multiply them. So in the multidimensional world we can achieve this just by taking the inverse of sigma. Right. So now we're coming back to that inverse matrix. So now the normalizing constant is a little bit more complicated. This will involve the determinant of sigma. That's going to be the sum of the eigenvalues and basically a generalized scale vector. So what's an eigenvalue? You can go to that here. Right. So eigenvalues and eigenvectors. Characteristic vectors of linear transformation. So this is going to be this nonzero vector that changes at most by scale or factor if you apply some linear transformation to x. So you could, you take a vector and you stretch it, you rotate it, you scale it, but that there's going to be a value in there that will let most change by the scale. And that's going to be the eigenvalue, the associated vector would be the eigenvector. So you can skim through the Wikipedia entry on determinants or whatever source you wish. But basically the multivariate D dimensional normal distribution is going to be given like this. So now you can see that this bears certain resemblances to the function that we had previously. So we're basically trying to figure out what the value of sigma is. And so we can see all these terms in terms of D and sigma, big sigma, that are basically defining what little sigma is. So if I have a multivariate distribution, there's going to be a standard deviation in basically all of these dimensions. Right. So I need to figure out what that value is going to, what those values are going to be because my probabilities might not fall off evenly in all dimensions. So I could have different values in the different dimensions and I can collect them all into a matrix and then represent this function in terms of that matrix and those. Okay, so definitely not going to get through this all today. So all this means is that the Gaussian distribution is a nice choice. Its integral sums to one, its value is always non-negative. It has a derivative of a natural logarithm, which is very nice and very convenient. So we can divide p by this kind of nasty function, but it contains all the terms that we need. Right. X minus mu is the distance from the sample to the cluster center. And so now I can take these are distance constants. Right. So now I can multiply them by the covariance matrix. And then I reuse that covariance matrix in the calculation of the normalizing constant. So now if mean mu is some d dimensional column vector and sigma is a d by d symmetric matrix. So in addition to the above reasons for this, this has a number of interesting properties. So that one is the central limit theorem. So that is the sum of many choices of n random variables of 10 to a normal distribution or random variables trends towards infinity. So let's play. Let me just go through like, which can we reasonably get through? Okay. Maybe I'll get as far as QD in the next 15 minutes. Let's go for it. So let's play with this theorem a little bit in Python and we can use this interact feature. So what I can do is I can plot a uniform distribution and then the sum of all the samples and I'll then be able to mess with this value a little bit. And so you can see that as if I start with n equals one, they're basically just almost evenly distributed. Right. Not entirely, but close. And then as I increase the number of samples, it very rapidly starts to approach this curve. Right. So as I get closer and closer to infinity, right, I get a nicer and nicer Gaussian curve. So this is what I'm trying to approach. And I could increase this value. It just take the notebook. It might crash. I'm not going to do that. But you can play around with it on your own. You can see that as we approach greater and greater values, this curve starts to get smoother and smoother. So. There's that. So now how would we check the definition of probability according to what we just calculated? So first you need a function to calculate P of X given mu and sigma. So that is P of X vertical bar mu and sigma. So now if I put in my normal distribution function with inputs X, mu and sigma, where X contains those samples as an N by D matrix, mu is the mean vector and sigma is the covariance matrix. So what I can do here, let me just run this code. If I look at normal D, this now reproduces the the doc string for that. Let me check out the shapes, the matrices and the last calculation. So diff V is going to be X minus mu. This is that distance. So all my distances divided by that or minus that cluster center. So these should be all those D values. And then the normalizing constant is just one by one. And so this is going to multiply by something that is N by D. So and then take the dot part of N by D times D by D. So this should come out again to an N by D. So one by one times an N by D. Then I take an N by D. This also multiply comes out to an N by D. So I can sum across all of the axes or sum across all the axis one. This is just N and then I can reshape it into an N by one matrix. So now if I take this and then I get an answer is one for each sample. Right. This is what I was after. So I have a bunch of inputs and I transform them into a single column vector or column matrix that has an answer for each sample. So let's look at it. Do just an example with some some dummy numbers. So if I create an X, a mu and a sigma and print them all out, here are those values. So now I want to see if it's if I run normal distribution, this will say if you know for given this this mean and this covariance matrix, if my input samples are one, two, three, five and then two point one, one point nine, right. These are the two coordinates. These should be the probabilities of falling into each of those classes. So we can see here that given this this mean and this covariance, this model is not a great fit for this sample, whatever it is, because it has a low probability of falling into all those classes. It will predict that it's an instance of class three, because that is the highest probability, right. Although it's again, if we're talking about, let's say confidence, it might not be very confident in that just might be slightly more confident than that than anything else. So to really see if it's working, let's plot these. So we'll need to make a surface plot in three dimensions to show the value of normal D. So we can do the 3D plotting that we did before. So I'll use the axis 3D. I'll create my mean and my covariance matrix and I'll just create the Z mesh. And this looks something like that. So given those that mean and that covariance value, we basically have a probability distribution for this class that looks like this. So just imagine for however many classes I have, I just get a bunch of hills like this in different locations, right. And they may be wider or narrower, right, depending on how rapid the fall off is. Okay, so finally back to that masking problem. I'm going to zip through this and I will review it next time. So if you were thinking of the radial basis function to fix the masking problem, you're right. I don't know if any of you were actually thinking of that. But if you were, congratulations. But remember what a radial basis function resembles or if you don't know what a radial basis function resembles, you can know that it resembles a normal distribution. So let's say we come up with some gender distribution like the normal distribution for some class K. So I'll just call this the distribution of P of X given class K. So how do we use it to classify? So for each of those classes, we run that model and take the highest value, right. So we can do actually do better than this. Think Bayes rule. So we want to know the probability of the class given the sample, not necessarily probability of the sample given the class. So how do we get this from the probability of the sample given the class? We can apply Bayes rule. So given this, let's just jump to the second line here. So if P of K given X equals P of X given K times P of K divided by P of X, we can get the joint probability, we can get the value of P of X by marginalizing out over all the joint probabilities. So P of X and C. And in other words, because this is equivalent to the conditional probability times the probability of the class, we just now need to sum over probability of X given K times the probability of K. So for two classes, one and two, we can then classify a sample as class two if the probability of C equals two is greater than the probability of C equals one. So now I just write run this, I can rewrite both of these in terms of Bayes rule. So now just the probability of X given C times the probability of C is greater than the probability of the other class. So now all I need to know is the probability of the class and then the probability of the sample given the class, which I should have. I can factor out these because this is just some constant, right. So this is a constant on both sides, it's the same sample. And I can just say that I don't actually need to know what this value is because as long as it's constant, these two will be the proportion will be relative. OK, so using the assumption that our gender distribution for each class is a normal distribution. Now all I need to do is take this narrowly function times the probability of the class in question and this is evaluated for both classes. So running out of time here. So if I do OK, if I do this, then this will this will simplify to the version below. There are a bunch of multiplications and exponentials here so I can make that a bit simpler by using logarithms. Right. If I do the logarithm, it's going to bring the exponential down in front case in point. So if I just take the logarithm of the covariance matrix for the second class, this allows me to bring the one half down in front as a coefficient. I can get rid of the E, bring the one half down. Now I'm back in terms of the square distance and the inverse covariance matrix. So now all I need to do is calculate this, add the log of the log problem of that class and do the same thing for the other class and then compare which one is greater. So now we can define the last the each side of this last inequality as some discriminant function. It's called a delta for class K. And then the new sample, the class of a new sample X is going to be the argmax for all those discriminant functions. And so then the boundary class between the boundary between class one and class two is going to be the set of coins for which the discriminant function is equal for the two classes. Right. If I'm right along that boundary, it should be 50-50, which one I'm in. And this equation has to be quadratic in X, meaning that the boundary between class one and class two is quadratic. And so we just defined something you may have heard of called quadratic discriminant analysis, which is a linear way of doing discriminative or doing classification using generative models. So a lot of terms in the mix here and kind of counterintuitive. It's I'm looking for a discriminative function for a probability distribution, which I'm using to define generative models of different classes. And it is a linear way of doing classification, even though the function is quadratic. So I apologize for that. I, of course, did not invent these terms and their usages. So it's just a lot to kind of keep track of. OK, so we will do we'll go through QDA code and do linear discriminant analysis on Tuesday. Thanks. And I hope to see you at the event this evening or watch it afterwards. Thank you. Thank you. Thank you. Yeah, I mean I I'm looking forward to the day when it's like, you know, passable as real meat and kind of. And what. Yeah. I mean like I don't mind, you know, it's just sort of waiting for the price point to come down until it becomes. Yeah. Yeah, that's fine. Yeah, I mean it's like it's obviously not me, like, never gonna pass for like actual me, but, you know, I'm not huge but you know by you guys remember DZ of my mom's vegetarian and like for, but for like weird reasons basically my mom, some reason, like the post office my mom lives at my house even though she lives in Canada. So for DZ of steakhouse keeps sending her like coupons. For some reason they're like my mother and not me. I probably could it's only because of her name or anything I don't need beef though. So it's like, you know, the steakhouse, they probably have some stuff like that. Yeah. Yeah, I'm looking for it actually looking for like a good restaurant to take like visiting faculty candidates to so I like rare. Yeah. 2020 so like two and a half years now. Yeah, that's good to. Yeah. Yeah. Okay. Yes. Okay, so you're naming all the places that I've been taken like once. So, all right, I should probably stop talking about restaurants. Let's go ahead and get started. As I mentioned, like, if you have good for Collins restaurant recommendations I am looking for them for professional reasons and also because haven't gone out a whole lot since we moved here because pandemic and then baby so you know, tried like the same you know five restaurants and for Colin so send me recommendations if you got them. Okay, so I did post the link to that chat GBT event. If you're watching that not an assignment or anything, an assignment in my seminar. But if you are interested, you have two hours to spare, because I'm sure you all do. You know I recommend watching that we will probably talk about some of those same points when we do like you know ethics and machine learning at the very end so it could be interesting for many of you. Alright so where are we at right now is do a quick share screen and let's take a peek at the schedule kind of get you going on where we're headed next so we are now. We should be back on track, assuming I get through the end of notebook 10 today so I'm going to finish up QDA and linear discriminant analysis and then we'll do linear logistic regression so quite a bit of content, although we got through most of that on last Thursday. So thank you all for getting your assignment one submissions I believe everybody submitted on time which is great. There were a few people who didn't submit although I assume that this folks will probably end up talking the classes to see what happens. So keep that up. Also, so, as I mentioned, it is very much to your advantage to get things submitted on time. So the reason for that is as follows. So I'm going to extend a really great opportunity for assignment one for those of you who choose to take advantage of it here are the terms. I will write them out and send an announcement later. So, any late penalty you first of all you may resubmit your assignment ones for regrade is going to be due a week from today at midnight, no extensions, no hard deadline of February 28. You're not required to do this. There will be a five point deduction. So that is, if you do perfectly you will still get a 95. This is, you know, you're not required to do this. The reason is, you know, I don't want people chasing those last few points if you got like a 99. Stick with your 99 if you resubmit and get 100 still going to get a 95. Okay. If for some reason you do worse on the re grade will keep your original grade. But I know that there are some of you, not too many but some of you who probably would want the opportunity to rectify some mistakes. So this is going to be your opportunity to do that. So I will reopen the box. This evening. Any late penalty will still apply so that is if you turn it in a day late. In addition to the five points will get an additional 10 points off this is why it is very much to your advantage to get things submitted on time. It's also in future assignments, in case I decide to do this again right so the same policy will apply. So, automatic five points so the maximum you can get on this is a 95 although that may be very good for some of you. You can rectify any mistakes will run it through the greater again and will grade your discussion. It is due at the end of the day, one week from today so midnight on the 28th absolutely no extensions, no exceptions, you're not required to do this so if you're perfectly happy with the grade I recommend you keep it. This is intended to be an opportunity to rectify mistakes that is both overhead for you and for us. Any questions on this. This is not necessarily going to apply to all assignments I will decide on a per assignment basis depending on how things turn out better sign it so for this one. This is because it's extended for this assignment. Any questions. Okay, I'll post an announcement with the same details today. So let's get back into the material so where we are now. So first, so I'll finish up, notebook nine, hopefully get through notebook 10. If we don't finish that then we should be able to get through 10 and 11 on Thursday. Next assignment will be rolled out a week from today. So just keep an eye out for that and then a two is due by default on Thursday extensions for those of you who have blanket extensions should already be factored in. Okay, so I mean controls. Let's continue with notebook nine. Font size okay for everybody. Good. Okay. Alright so just to recap, we were looking at introduction to classification. What we ended up doing was we're basically looking for, we want to find distributions in the sample where things items that are closest to an exemplar of a class would basically fall into distribution that is represented here by a peak, a peak, where the further you get from that sort of ideal points in the search space, the more the probability of falling into that class will taper off. Right. So, the idea being that there are distributions where there are clusters in your search space, and you want to find the cluster the probability where a point falls close to that, that peak in the distribution as closely as possible and you want to find that you want to have that distribution represent the data as good as possible. And so what we do is we end up drawing these normal distributions saying I'm going to assume that there are these points where if my sample falls right in this point is basically 100% likely to be a member of this class as we get further from that point we get a lower probability of being in that class and, consequently, a higher slightly higher probability of being a member of other classes. So we model our distribution somewhat like this. What we want is that distribution is really multi dimensional, and it can taper off at different rates and in different directions. And so we're looking at the types of functions we can use to model that we ended up modeling this as a property of Bayes theorem, right so this is Bayes theorem here I have you know p of a given B equals p of B given a times p of a divided by p of B, then p of B is basically the sum of the joint probabilities of B and all possible classes. In this case, what we were calling a. So we went through all the math. And so using the assumption that each class is normal distribution. We can then try to define the boundary between two classes so this is going to be basically a line or hyper plane, where if I fall on one side of that class of that boundary member of some class I fall on the other side of the boundary member of another class. So we went through, you know, all the mathematics is basically a lot of exponents so we use natural logarithms to bring make this function is slightly more attractable. Good reasons for using logarithms include that we can now do addition instead of multiplication so I'm now not multiplying a bunch of small numbers together so my probability of ending and underrun is far, far lower. What we ended up with is basically have this discriminant function that will call delta so basically this is the discriminant function of a class K for sample x. And we just want to find that boundary, where the discriminant of for class, say, class, a of x is equal to the discriminant of class B for access to define where it's equal probable that that sample is a member of those two classes. So this is just the, the two class problem. This equation is quadratic and x we call this quadratic discriminate analysis right because the discriminant function is our quadratic function. So, um, all the math probably make some of your eyes water. So we'll now go through it in Python. So, first, let's just make what we do usually do will make some, some dummy data will define it in terms of D dimension so you can arbitrarily vary the dimensionality of the data as desired. So in this case we'll define our samples x and t, where now there's just a single, a single component to each sample, but this can be you know any size you want, and then 10 samples for each class. So now we define x one and t one is these are samples in class one right we're just going to our trade classes one and two. So we'll just say, there are some samples that are members of class one there's some samples that are members of class two. We define the normal distribution and then the shape that we want to sample that from the normal distribution into just one thing to note here is just for this data we've defined that there to be a wider variance in the samples belonging to class two. So we put all this data together. So what you can see here the way we stack this is we basically take the x one samples the x two samples vertically stack them. And then we take the, the t one samples and the t two samples. These are the targets and Berkeley stack them so now we basically have all our x ones or inputs x ones the next to that our targets t one and t two. And now we stack them together. So this is a standard way of representing our classes where we have the target label says we've done before. So our data setup and pretty much the same way as we have done. So now imagine that instead of the, the actual class labels we just have data right so basically don't know t one and t two. Right. We just have data we don't know how it's generated. We don't know the mean the covariance of the two classes so data might look something like this. Right. So, if I just have these are my samples and I've got some class labels I don't know how I generated these, these classes. So I don't know these means and covariance, co variances. So let me just try and and try and compute the discriminant function for this so let's start as before, I'll just separate the inputs, separate the input column of the target column. So similarly, right so ID dimensional data I'm going to take columns zero through D, and I want to see the last column to meet the target. I will define my means and my standard deviations over my data x and standardize it. So that's done before. Right, we do not standardize t of course because this is now class labels. And so now we're trying to predict basically a discrete value, rather than some continuous value that might have a specific mean and standard deviation. So now we need to find this QDA discriminant function. Here's the mathematics of that again. So, one, the one key term here. Right, let's break down what all these things are so discriminant function for class k of x is going to be negative one half times the logarithm natural log of sigma of k minus the one half times remember x is the sample use of k is the mean of that of the class and questions that's the distance. So this is a set of samples. So each of them is going to be a different x minus the mu for that for that sample should be the same so transpose that times the inverse covariance matrix. Again, times the distance, plus the natural log of the class, probably the class. Okay, so C equals k is just whichever class we're interested in. So we know to calculate all of these things except for that covariance matrix, sigma. So let's consider a couple of ways of doing that so we can define sigma. You know, as some arbitrary values, and then we can say take sigma times the inverse of sigma should give us the identity matrix. So, now I can use this other function pseudo inverse so P inverse. So we should get something that if I use the pseudo inverse function that is very close to the inverse and I'll define this in a moment. So we should get something that is very close to the identity function. In fact, I do. Right, so we have one and then some very very small numbers here. So, effectively zero. So now I don't find sigma slightly differently. So instead of this, you know, 1221 it's not 1212. If I try taking the inverse of this it's going to throw an error, right, this is a singular matrix. So what is a singular matrix we've looked at the senior matrix is basically a non invertible matrix so we have typically the two matrices, A and B, right, if B is the inverse of A, then, A times B will equal B times A which is going to be the identity matrix for whatever the dimensionality of A and B are. So a singular matrix is going to be one that's not invertible right for for whatever reason so we'll see if it's a, you know, a square matrix, but it's it's not an invertible for some reason. So, what can we do in that case well what we can do is you can use the pseudo inverse function so this can can be what we call the more penrose pseudo inverse of the matrix. And so this is going to be the generalized inverse of the matrix, using its singular value decomposition, including all the large singular values. So what this does is this now allows me to basically compute something that's close to the inverse of any of any matrix. So it doesn't necessarily if the matrix is non invertible for some reason, I can still use the pseudo inverse to to get something that's not invertible close to the inverse for that. So, if I now define sigma times the pseudo inverse of sigma remember remember what's what's sigma look like it's 1212. This is going to give me this is at least going to give me an output, right, so this is not the identity matrix of course, but it will allow me to, to, to compute this. So now I can define a function that allows me to use the QDA equation so same similar inputs that we have all of the, all the pieces that we need to put together right we've got the inputs. We've got the means of the district different classes, the standard deviations are sorry, the means of differences is mu, we have the means of the data the standard deviations of the data mu being the, the main points of the different classes, sigma is the covariance matrix, and then prior is going to be that prior probability of the class right so remember this is a generative model. There is a prior probability there as basically how, how common is my class overall than my data, right, so I need to know the base. So, you can see that we're going to take x minus the means times the standard deviation so these are my standardized x values. And now I'm going to take that and then subtract mu, right, this is so this is going to give me the distance of the standardized input from mu, the mean of the class. Then what I can do is I can then compute the determinant of sigma. So now, this is commented out here because it's not going to apply in this particular implementation. If the determinant is zero, then we will raise this error for the same recurrence matrix but instead we're going to do is we're going to actually use the pseudo inverse to allow us to at least get an answer for, for all cases, even if it happens to be singular. Okay, so now we can start putting the pieces together right so here are my negative one halfs right so negative one half times the NP dot log is the natural log by default of the determinant divided by a negative one half times the sum of the product of x times the, the inverse matrix times x, and then I'm just going to reshape that so that all my samples are organized in the right order. And then so this, this here should give me a single, single dimensional matrix. I want that to be two dimensional. And so then I'm going to just add the log prob of the class to every element. So to use this we need to calculate the mean the covariance and the prior probability so we got that right mu mean sigma covariance prior probability. So, what about P of C equals k, this is gonna be the prior probability distribution of class k. So if I have no prior belief that one class is more likely than the other, then it's just going to be the number of samples in that class divided by my total number of samples. In this case, if you look at the data that we use before is 50% because we had five or like 10 members of class one and 10 members of class two. We don't, we're gonna pretend we don't know that though. Right, so we don't know how this data is generated. So we'll just sort of try and figure, try and figure out if we can recover the right answer. So, let me see what all my classes are so I'm just looking at instances of T, where their class label is number one so I can just do this by basically doing T equals equals one right so this will apply the equals one Boolean function over my entire over my entire array. And then I can just reshape it to list all the examples out so now you can see that those first 10 are all true, because T equals one and the rest are all false. So, now I can represent my class one in class two is those indicator variables, right, where it's either zero or one depending on the which which classes in. So I'm going to have K minus one classes that are all zero. And then for that Kth class it's going to be one. So now I can define the mean, and the covariance for all of those for all those different classes. So I basically just pull out which samples in my, which rows of my data are members of class one which ones are in class two. And then for each of those, I can just take the standardized inputs for that class, and then compute the mean for each of each of my classes and same for the covariance. So now what we can do then is I will get the prior probability right so this is going to be, and one is going to be the sum of the class one rose, so that is just how many there are, but same friend to. And so and will just be land of t. How many samples do I do I have. So now I can compute the prior probabilities for class one in class two just by dividing those respective ends by the total number of samples. So, we can see that you know in this case we already know that our data is going to be the same. So we already know that our data is evenly split, but the same code would work. If I have an imbalance data set this will allow me to do that. And this will allow me to then factor in that prior probability. So in case I had a data set where I had 80% samples in class one and 20% samples in class to write the overall probability is some sample falling into class two should be lower and I want to factor that it, it being a generative model. Okay. So let's look at the covariance for sigma one right this is going to be this value. So now we can apply our discriminant function to some new data. So I've been able to define a function that given this information will give me the discriminant function, and then I can actually just compute the output probability for that forcing new data. So now I'll create 100 new samples. And so this is going to be kind of representative of similar data. So what I'll, what I'll define is two, two models right so this is a generative model you've always run k models for k classes. So if I like two classes I need to run two models. If I've got 10 classes I need to run 10 models. So I'll define two instances this QDA function and all I need to do is put in the distribution that I'm interested in right so the distribution in the prior probability so for the first instance, I'll put in those values for the first class and the second instance I'll put in those values for the second class. So now what I do is after I run this, it's going to put output values into D one and D two. And you can see that it should for each of these 100 samples, it should give me a label, basically, you know, true or false or a probability that when rounded would give me zero or one that tells me how likely it is it falls into for the first set into class one and for the second set into class two. So we'll look at it. If you were to run this notebook and increase the dimensionality of the data you would still work right because it's written to, to accommodate arbitrary dimensions. But if the data is more than one dimensional will just plot with respect to the first component. In this case, there is only one component. So to obtain the value of the normal distribution from the sample we've got two choices. So we can either start with the discriminant function value and then transform it into the full normal distribution value. Or we can just use our normal distribution implementation directly. So in this case, we will just define this normal D function, where that has the inputs and then mu and sigma. So print out those values right so I'm going to have different mu and sigma for each class. And you can see already that since this is just a single dimensional sample. The mean value for class one is around centered at point one two whereas the mean value for class two is centered, just shy of point point three or sorry, my, my mistake. That was the covariance. The first class is centered at negative point nine and the second class is second centered at positive point nine, and the covariances are given here and as we mentioned when we're generating the class to data, it's got a wider variance, if you remember that, hence this covariance being being larger. Okay, so what's the, what's the normal D function so x contains samples one pro and by D news that mean vector, so just D by one, and then signals the covariance matrix. So what we can do is basically try to recover the normal distribution by using the discriminant function basically reverse engineering. So if you look at this, this code, you'll observe that it's kind of very similar to the, the QDA code. So, just, I'm kind of doing everything in reverse right and try to get back to the normal distribution from the discriminant function, where previously we had defined the normal distribution, and then we drive the QDA function from that. So let me define that there's our new one in you to data again. So now I can plot what the discriminant functions look like for this data so all I'll do is obviously exponentiate the probabilities, and then I will apply the discriminant function to that. This will give me a nice, nice quadratic curve. And it looks something like this. Okay, so for this data. So you can see where that peak is for the blue curve being class one for the orange curve being class two. And you can see that you know the peaks are different right about negative point. This is on standardized. So, this is the other standardized like negative point nine and positive point nine. So this is probably about negative point nine still but then the peak for class two is probably about 5.5 something like that. And so we can see that there's that wider variance in the second class. Same thing, as we see here. So this is this this first chart shows the discriminant function, you know, the actual value plotted as a function of the input feature, and then QDA here this is going to be the probability from directly from the discriminant function so here you can see more clearly. These are those peaks that we were after. And then that kind of attenuation in the probability and so again here that wider variance in in class two is also evidence. So that here this one is just the QDA using the normal distribution. And you can see that we get a very similar output right so this is what we want we were able to successfully recover that that normal distribution from the discriminant function. Now they're only 10 training samples per class you can expect the results to change a little bit, because it's quite a bit of noise. But what if we have like more dimensions than samples. So for example, I could set D equal to 20 right now I have 20 components for each sample, but I still have 10 results, and I can run the exact same code. And what happens. Well, what happened here, we're not getting results. And sometimes I can run this again, and it may plot something for like one class but not the other. And even when it plots something will often see if it if it possibly for both classes will basically get a flat line here for the probability using the normal distribution. So, something goes wrong each time sometimes a different thing goes wrong. But clearly there's something not right with this. Shoot, not quite with this distribution. I need to accidentally exited. Okay, here we go. Here we are again. So that stigma is very close to singular, which means that the columns of x are close to colinear. So, the determinant of the singular matrix of course is zero, and it can't be inverted. So we'll discuss some ways of handling this in future. But we assume a single normal distribution. As the model of the data from each class. And this does not seem to lead to a huge complex model. Let's say of how many parameters there are if the in the mean of the covariance matrix. If the data is D dimensional for some some value deep. So that means that the me is gonna have D components right so I'm just going to take the samples that have D dimensions and I'll take the mean for each dimension. And so I'm going to have like this D dimensional vector that represents the sort of the expected mean of the distribution of that class. So the covariance matrix would have D squared components, right, because it needs to be D by D. So if it's one, if it's just like a single dimensional sample, we saw that our, our Sigma's just had a single value, right, one value by one value is one value. If we had 100 components then the covariance matrix would have 10,000 parameters. So in reality, the covariance matrix is symmetric. So it just has basically D over two plus D or sorry D squared over two plus D over two, or D times D plus one over two. These unique values, we can compute the remaining values just to pre know those values, but still quite a lot right and it grows. You know, not quite exponentially I guess but polynomial. And so we have one for each class. So the total number of parameters including mean is going to be k for the number of classes, D plus D times D plus one over two. Right, so a lot of a lot of samples. So if the data distribution is then under sample that class boundaries going to have a lot of errors in it, right, because I'm using. If I have 10 samples with 100 components. I'm not going to necessarily be able to find a very good mean for each of the classes that I have that I'm interested in. So, we're going to basically overfit to those few samples and if things. There's basically a lot of variants, the more, the more components you have that could end up on either side of that class boundary because you didn't do a good job of finding that. So we need to remove some of that flexibility from that normal distribution model. So, when we can, what we can do is we can restrict all the covariance matrices to be diagonal. And so, then we'd have these basically ellipses that you can draw in the data, these will be parallel to the axes will come back to this point when we do dimensionality reduction. This wouldn't really work well if our features are well correlated to each other. So now we can force all the classes to have the same covariance matrix by taking the covariance matrices for all classes and averaging them element wise. But this is doesn't seem like a great idea on the surface. Why not having single covariance matrix for all classes seems like it might. So no. What's the answer. So why is why is averaging all the micro variance matrices a bad idea to get a single covariance matrix for all classes. Yes. Yeah, basically that's it right we look at the variance in different classes could be radically different. And also the more components you add the more dimensions you add the variance in individual dimensions could also be different so even if I have a three dimensional sample. The variance in one class in dimension three could be way less than the variance dimension three for another class, while the variance in dimension one is a lot bigger right so averaging that you're basically losing a lot of information about in how much when I move along a certain dimension, my distribution starts to fall off right so this is not like a great idea. So what we can do then is we can use the average covariance for each class, and then weighted by the fraction of samples for each class. So why do we do this. When you can set it for and similar. Okay, you're off the hook again. They run around thinking as soon as because he skipped class last week. So yeah, basically, we, we can assume that if my classes are like uneven unevenly distributed, then if I'm averaging all the, the co variances, it might not be so detrimental this one class that like has a different sort of a different variance in it, in some dimensions but it's it's under sampled in the overall data set right so if I've got one class of which I have 10 samples in a data set that has 10,000 samples like if I neglected this somewhat it might not be the end of the world right because it's so rare. I don't know if fitting model to the means and standard deviations defined by these data that I have for that class is actually representative of what other members in the class would actually be right so it doesn't make doesn't make a whole lot of sense to overweight your lens too much credence to just a few samples that might be really noisy or whatever. So, we can actually do this and see a better result than, than using a unique variance matrices. So now to remember our, our discriminant function. So, this is the discriminant function. And so we're basically just trying to find values for where d of some k is greater than or don't have some days greater than some other k, and then use the same covariance matrix for every class. So now instead of sigma sub k we just have a sigma right so now we can we can use the discriminant function, plus the natural log of the probability for each of those classes. So this can be simplified as follows. So you're not going to go through the math and basically just like multiplying out all the terms, so that I am able to have a simpler functions and I determined function is now becomes something like this right So this is the function of x transpose of k of x is equal to x transpose times the inverse singular matrix or covariance matrix times mu sub k minus one half of mu transpose times the inverse covariance matrix times mu sub k plus the log prob of k. And so now you can see that unlike the previous function might not be obvious because I'm not sure you don't you probably don't remember the whole previous function we go back and look at it. So this is going to be linear in x, as opposed to taking a bunch of square roots and then trying to bring the exponents down in front. So now this can be rewritten as delta sub k of x is equal to x transpose w sub k plus constant. I'm sorry. It looks it looks a whole lot like that linear function right. So remember, we did linear regression we did neural networks we basically have some inputs x multiply them by some weights w, and you add a bias B which is usually just sort of subsumed into the weights and you train you train the way for that bias, but it's So this is the normal distribution. Right. If we started with y equals mx plus b, where m is a slope, and b is a y intercept, b is a bias right this is just going to be some constant value, and then the slope in this case is multi dimensional, just defined by weights is the coefficients that I'm going to weigh each each each input by. So then using the normal distributions as the generative models and restricting the covariance matrix. This gives me a linear boundary. So, this is now called linear discriminant analysis. So QDA and LDA are based on these normal distributions by modeling the data samples in each class. So I can say for some sample. What would this look like if I tried my best to model it using a normal distribution and trying to find that boundary between my classes. And so, QDA has this flexibility. And so LDA is actually often better in practice, in particular cases where we have under sampled data or high dimensional data, right, for reasons that we saw before, because we don't necessarily want to have that full flexibility of trying to define a covariance matrix for every class. When in reality I can model the data in a more flexible way, or in a more practical way with fewer computations using linear boundary. Alright, questions for the example. Yes. So yeah, if you have a data set and it's not very comprehensive, I'm going to put words in your mouth, and I'll say like, yeah, sort of, just give a sparse, sparse data set. So, yes, very likely you'd want to use LDA, because you can. The intuition is basically, you don't really have enough information to be very confident in the sense about what your covariance is going to be. So, trying to trying to fit a covariance matrix to like every data set you're going to probably overfit to any peculiarities of those samples. And so instead I can define a little more general. Would dimension reduction also help with this? You certainly could. Yeah, so let's say you have like some high dimensional samples, but you can figure out that like most of that variance is actually captured in say the first two or three principal components, right, then in that case you might be able to reasonably fit like a QDA model to that, where it's where it performs a little bit better, because you can basically infer that the risk of having that large variance in those higher dimensions is actually really low, right, because they're not actually capturing all the information so it could just be you have some sample that for some reason, you've captured 100 components of, but just the first few components are actually where what's really important for making some sort of classification distinction. And so those other say 97 components just adding noise. Right, so you could get rid of those and yeah you definitely could so just keep in mind just all these machine learning techniques are individual tools in your toolkit. And so, most of your job, you know if you apply this in your careers or in your research is going to be trying to figure out like what's the right combination of tools that I want to use for my data. Right, so do I want to do PCA on the data itself or do I want to do PCA for like visualization or something, you know, it may be helpful for one but not for the other. And do I can I do some dimensionality reduction technique that allow me to use a technique like QDA because it's faster than say, trying to fit a neural network to it because these extra 90 plus parameters are just like not really that relevant and if you figure that out, that you've probably solved that problem already. Okay. Other questions, comments. Cool. Yeah, so remember we had that Parkinson's data as we had samples to class to class problem. They basically have features extracted from the voice. And then you do the samples labeled as has Parkinson's or does not. So we're going to go back to that and then classify using QDA. So first let's calculate the means and the covariance matrices so I'm just going to use the same splits that I had before. So if you remember, we had like 170 or 195 samples I guess, or something like that. And then we split them into a train test split using I think 8020. So what I'll do is all 50 is generative models to each class, right, so two classes just zero and one. And so then I will standardize my trains, and then I will compute my means and my co variances for those two classes. Then same as before I'm going to run these two discriminative models over the samples from class one and samples from class two and see, given what I know the labels are how correct am I. And then the same thing over the test set right so again, I'm going to be using X test I'm going to be using the same. The same computed means and standard deviations for the data and then also the muse for the two, two classes and the co variances right. And so now, then I'm the last term here is going to be the prior probability of the class so my class zero is healthy, and then I'm going to be using the same numbers as Parkinson's that is going to take the total number of each of those classes, divided by the total number of samples. And then I can return that into some percent correct. And so you can see where we're going already but in this case, using QDA, right, the train percentage is like 98%, correct, and the test percentage is about 87% on this split, if I run it again I think it's slightly different numbers. But you can see that there's a significant underperformance of the test accuracy compared to the training accuracy. So, what we can do now is we can write this function is going to do it multiple times we're going to try different splits and run it multiple times so again as I think I mentioned, we want you want to typically you might want to try to average like over a bunch of different splits just in case you got a lucky split once right and you don't want to report those results, because perhaps someone trying to reproduce the work wouldn't be able to and they're going to be like well I ran your exact code and I got a different results so what gives, but what gives is that there's a different random seed or something. And you gotta you just have to get a lucky split of the data that no one else can actually ever reproduce. So, basically this function is going to do what we just did, just a bunch of different times. And so we can see that I will make a, you know, a split of the data run my two discriminant functions over the train and test data print out the percent correct. And so now what I can do here they can basically do this run Park, I put in the data file and my training fraction. So, give me that number, or do it again, right I get slightly different numbers you can see like now we're getting 92% test accuracy. So, the training numbers pretty much stay the same, right, this is about as good as I'm going to get using QDA on this data, but for different splits, you know, I may get sometimes radically different percentages so we have a range here from about 84.6 to 92.3. Right. And so we can see here, we compare these two in this case, the test accuracy is identical. Right. And this is not necessarily because we have the exact same split just happens to be that you identify the same number of samples as the incorrect or correct. So, um, let's just you know for for your review you can just consider how would you get the values for these for these different things using Bayes theorem, if you need some practice so you can just look at these. Look at these points, and then go up in the notebook and to see how we would get these these different values. So now what do we need to change where we're just doing this with QDA So what do we need to change for all this to run it with LDA. So let's write this LDA function and see if the same classifier, or the LDA classifier which assumes all the classes have the same covariance matrix does better than QDA on the Parkinson's data. So we showed that if we assume the same covariance matrix by weighting it. And by the number of classes. Then our discriminant function becomes as follows. So then what I can do is I can write disk LDA, that's implementing this function instead of the quadratic function over the same data. And so then I can redefine run Park to use this function disk LDA, instead of the QDA function so here we're going to run the QDA function that I run the LDA function, and then we can see how they compare. So, if I run this right now we can see, here's my QDA result and there's my LDA result and you run it five times. And then you all take a look at this and see, tell me, you know what you observe. How does, what's QDA doing versus LDA. Right, yeah. So a lot of this is based on that on the split right so we can base it based on which 20% were holding up for tests can have significant effect. But we can see that you know the the QDA percentages are like routinely north of like at least 95 often up into 98 99, where the LDA numbers are lower 89% 93% on the test fraction can sometimes be significantly lower actually for LDA but also sometimes a bit higher. So for example, here's, here's one case where basically the LDA is beating the test only that's the even beating the LDA train accuracy, and it's also significantly beating the QDA test percentage so we can see you're the QDA probably over fits and is a little bit more generalizable often, but if you run this again for example, you know, run it a few more times, you'll get some different data right so here's here's cases where for most of these numbers are pretty pretty identical to the test so he's like say this sample. Same test accuracy, even though the LDA train accuracy was quite a bit lower. So, then what I can do is I'll just like write this out into a file. I'll call it QDA LDA. And so then if I just run this, then we can actually see the probabilities. Right, so now if I look at this. Here's a sample that's class one is predicted as class one, and then the probability is actually, you know, 20% or something. But the second class is just point 001%. So the number of these classes don't sum to one is just looking at which one is more probable right so even if this is only 20% likely to be a member of class one 20% is still a lot more than close to 0% so that's going to be the answer. Yes. Yes. The one both. So you can basically see, there's no way to definitively tell that it's overfitting but it's very likely for seeing my train numbers being 98% and my test numbers falling significantly below that. So, it could fit really closely to this data and there's enough resemblance between the test data and the train data that it's it is lifting it up and it may actually get the higher than a, than the LDA model. Not necessarily usually going to produce a lower result but it shouldn't produce a result that's like so much lower than the train. Okay, so it's really, it's much more about that discrepancy between the train accuracy and the test accuracy if it's really overfit to the train data it's like your train data is going to be like close to 100% it's really good thing but the training data. If your model were good. You would expect to see a similar number on the test. Right, so if I'm seeing 99% train accuracy and 82% test accuracy for a problem that is this simple. So it's probably seeing overfitting. There are more complicated problems you know some like very complicated like objects or action recognition problems that the state of the art on the test set is like 40% or something, just because the problem itself is so hard. Yes. Yes. I think we assume that. So we will be covering the test data. Yes. The second is, we have both at 87 and 89. But, it's a 9891. Yeah. So how can we say that it's overfitting or underfitting. So, you, you're going to look at the discrepancy between the training data and the test data. So here for example this one right for LDA, the training test accuracy is a really close. So, this model is trained well enough to get 89% accuracy in the train data. It's also generalizable enough to get similar accuracy of the test data. Okay. So, if I look at this one for example, my train data is like 98%. I love to see that the test accuracy is a lot less than that. Right, so there's something in this way this is fitting to the data that it's like, not so flexible and it's able to generalize quite as well. So you're the ideal model is one where you just get like really good train and test accuracy. But in most cases you can't really expect to do that. What you don't want to end up with is a model that is somehow fit to some sort of peculiarity in the training data such that when I give it new data. It just kind of falls apart or just doesn't do as well. Yeah. But do we want to lower accuracy better fit more than. I mean it does it does depend of course. It depends what the use case is right do I want to, if I'm just interested in like just fitting to the Parkinson's data, maybe IQD is great, right, because actually in some cases it's doing better so like I could stick with that. But if I wanted a model that's like okay I just want to be able to handle an arbitrary two class problem, and I don't know where the data was generated from and I don't know those, I can just calculate the means and covariances but I don't know kind of the distribution the data was sampled from, I might want to err on the side of some that's more flexible. So it sort of depends on like what am I trying to use it for. If I were running a test that's like, I just really want to fit to Parkinson's data of this form specifically, then you probably just want to do whatever is going to give you the best time, the best test accuracy but it's like, I just, I mean we're running a bunch of different samples maybe we're going to allow every trying to do to class classification like a bunch of different diseases or something, just as like a filtering to send people to you know a specialist or something like that. You probably want something that's more generalizable. Whether or not, just keep in mind like, just because you can doesn't mean you should right machine learning may not in fact be the best tool for this if you think of like a medical environment that stakes are pretty high. So you may or may not want to use a technique like this. This is just a demonstration of like you know if we have data set up in this way this is how you can model the problem. So RMSE well I mean this case we're not measuring RMSE because it's a classification problem so RMSE is error on us on scalar values. Right. So, if you're having, if you're predicting continuous values than the RMSE will indicate you the cow. How close are you to predicting the correct value. For classification right your metrics are different, right so here we're talking about accuracy. You might also do like precision or recolor f1 like we talked about in. In the second lecture or like you know area under the receiver operator curve. There's a bunch of different metrics that you can use and part of your task is to pick the right metric for the task at hand but like if you're dealing with a classification problem like RMSE would not be the one to do because you have to it's basically squared error over some units what's the units of classification there aren't any labels. Yes. Yeah. Yes, yeah, yeah. Basically what I'm saying is like, if I look at like this class right this is a misclassification. So, that's the sample member sample one of class one is predicted to be a member of class two, because the model, when factoring in things like the prior probability of the classes in this case it's equal probable but then also like the features. It's saying okay there's an 8% chance that this is a member of class one there's a 9% chance or 10% chance is a member of class two. Neither of these is objectively good. Right. Wouldn't put money on this result. But these are the only two things these are only two models that have got. So, I must choose one. Right, you can handle the problem in a way that's like, if I don't get a value that's like above 50% or something I'm just gonna say I don't know. Right, I'm not, I'm not, I'm gonna, I'm not going to use this result. But if I handle my problem in this way if I set up the formulation this way, whichever one's higher wins it doesn't matter if that higher value is actually objectively low. Yes. So, the last this classification. That's the spot on. Yeah. Yeah. Why is that one was. I mean we would have to look at the exact inputs for that sample. Yeah. It happened to be that way but there might be you know, there may be something that's like a typical about that sample. So, you know, we're talking about like voice features for Parkinson's so like maybe that person had a particular timbre to their voice already, right, maybe they had a naturally shaky voice or something like that, or actually, maybe this is, I think, class, class zero. Maybe they just have like a really solid voice that even the shakiness that comes with Parkinson's doesn't really change that part of the vocal signature all that much. Yeah, yeah, yeah so it really could be I mean this is this is not a severe outlier but if I had a case where it's like this is 95% likely to be a member of the wrong class might be an indicator that it's not. All right. Okay. So, what I will do now I will start and run this notebook on your own. I'll start the next notebook here which is classification with linear logistic regression so we're continuing with linear classification, and we'll just follow up, kind of with an alternate method for a discriminative model rather than a generative model so just as a point of point of fact so a discriminative model and basically looking for, for the individual features. What is most relevant to this class. I don't know about the prior distribution, so much, I don't factor that into my model, an unbalanced set can have repercussions for how you fit your model. Because there are fewer samples to be pull your decision boundary in a particular direction. But I'm not going to be using like prior class probabilities and making this decision. So, this linear model use for classification we can have this masking problem where if I have to, too few samples of one class. This, this can result in masking so we had these different membership functions that other other than linear functions. So first we use these generative models to model data from each class, and then convert that to probabilities, using Bayes theorem, and then drive those quadratic and linear discriminant functions. So now we're going to instead of doing that instead of having like two model that's going to give me a probability for being in one class and being in other classes, I'm just going to directly predict the probability. And what is a consequence of this if I'm doing this, then I should end up with a probability that it is a member of one class that if it's greater than 50%, I'll classify it as a member of that class because there's no other comparison to make. I can't say it's 50% likely to be member of class A but 30% likely to be a member of class B there for class A wins. I'm basically just saying, is it a member of class A well it's 50% likely or more so therefore yes, or it's less than 50% and therefore no. So, in this picture of the problem was that this line for class to the green line this is too low. And in fact, we'll see that in the middle of the range all the lines are too low, right. So, if I'm looking here. Yes, the green line is the highest for the set of samples, but none of them are particularly high, whereas for the ones on the edges. We see values for these two functions of the red line, the blue line are quite high and so you could probably be reasonably confident of that. So, one thing we can do is reduce his masking effect by requiring the function values to be between zero and one, and then requiring them to some sum to one for every value of x so these sound like probabilities, these are properties of probability so we can actually represent the probability function as basically there's some function, some predictor function for x parameterized by weights w for some class and I'm just going to sum that or divide that by the sum of the outputs for all functions, all such functions for all the classes. Right. So if I assume that f of x parameterized by w is greater than zero. We haven't discussed exactly what f looks like yet, but we can see the w represents those parameters that you're going to be tuning to fit the training data. So now we're back in trying to optimize weights. So, we know that this expression will give me a value between zero and one for any x. So now we also have probability of C given x expressed directly, as opposed to modeling x given C for every class and then running all my models using Bayes theorem. So, this is going to be an arbitrary function, so let's just give it another name was called g for now right so g is the probability of C given x, which is given by the output of f for function k or class k divided by the sum, the sum values of f for all classes in m. So now we need to choose something for f and whatever that is we have to have some plan for optimizing its parameters. So, what's our plan. So now what we're going to do is we're going to try to maximize the likelihood of the data, so that is to say I've got some data. And I know that all my samples in these classes in my in this data belong to some set of classes. So we need to try to maximize the distribution of classes such that the likelihood of seeing this data is maximize. Okay. That makes sense. So I've got some data, and I want to see what classes do I need to define what distributions, should I infer, such that the likelihood of this data is the greatest that I can get it to be. So, if you have training data consisting of samples x one through n. And then these indicator variables for classes one through k. Remember these are all going to be ones or zeros, where it's one where it's a member of that class and zero otherwise. So, each row of this matrix should contain a 01 and a single one. And then we can also express my samples as an n by D matrix. But for the following examples will be using single samples more often. So the likelihood is going to be the product of all probabilities for the class of the n sample, given that n sample for that sample. And so a common way to express this using indicator variables would be this. So here's my indicator variable raised t sub nk that's the indicator variable. So the probability raised to this value right is either going to be is either going to be raised to zero, or raised to the one. So let's say I've got three classes, and the training sample and is from class two. So the product is going to look like this right so it's going to be the product, the probability of one given x raised to the t sub n one times the probability of C given x raised to the t sub n two times the probability of C given x raised to the t sub n three. So of course if I raise anything to the zero becomes one. So now, only one of these terms is going to remain. So let's say it's a member of class two, this is going to reduce to the probability of C equals two, given x. So now this shows that if we use indicator variables as exponents we can now select the correct terms to be included in the product because basically looking at what actually is relevant here. My class of interest is class two, I really only be looking at the probability that it's in class two. It's not I'm back to computing a single probability. So, if this is the data likelihood what we do to maximize the data likelihood. So again, I'm going to be finding some weights, w. So, that maximizes the likelihood of actually encountering this data. So, if L of w, right so if this is previously likelihood of the data b. So now I want to find the w that maximizes that data so basically just looking and trying to solve likelihood of w is going to be for, for all n and all k I'm going to take the product of those probabilities of that sample falling into that class. So now I'm finding the derivative with respect to each component of w, that is because it's a derivative in high dimensions and now back to computing gradients. But there's a whole bunch of products in here and what happens you multiply a bunch of fractions together. It approaches zero, right and so the more multiplications I have the closer it's going to get to zero. So I'm going to make it easier by working with the log problem, the log likelihood. So I'll just call this ll of w. And so now I can do things like convert all my products into sums, and then bring my exponents down in front. So now I take sum for all n of the sum for all k of the indicator variable times the log prob of the class. So now this becomes a lot more tractable, and I'm just trying to find, this is going to be a negative number, always, unless like, well, it has been a good number. I'm just trying to find the least negative number. So now unfortunately, still trying to solve the gradient. Of course the gradient of log likelihood with respect to the weights is not linear in x. So we can as before, simply set the gradient, the result equal to zero and solve w right so now if you're paying attention this sounds a lot like our neural network problem, right, it's now no longer a linear function we've turned by my linear function into a nonlinear function. So we'll do a similar technique. So we'll call this gradient ascent. And if you're wondering why we're doing ascent and not decent. Just think about the properties of a logarithmic curve that make this appropriate to think about the shape of logarithmic curve. So what I'll do is I'll initialize w to some some value, and then I'll make a small change in w in the direction of the gradient of log likelihood, with respect to the weights. So, should be back in familiar territory this is starting to sound a lot like neural networks, or just, you know, linear regression even with SGD. So I'll repeat this, this step until I seem to get to some sort of maximum value in the log likelihood. Right, so this is a form of convergence and it's just going to see my, my value does not seem to be increasing very much I probably reached about the maximum on this gradient I'm ever going to get. So, what we see here is now what's the value of w I'm going to take the previous value w plus some, some value alpha times the gradient of the log likelihood, with respect to w. So, alpha is going to be that that constant that affects the step size which sounds like learning rate, right. So, again, you know, also sometimes labeled alpha. So remember that w is going to be some matrix of parameters, let's say we'll have some columns that correspond to the values required for each f, of which they're going to be k minus one. So, we can work on this update formula, one column at a time. So, here we have this. This is for each column, and then I'll just combine them at the end. Right, so this this weight is going to be weight. The weight plus alpha times of the gradient for all of those individual weights those individual components. So now let's remember that we have some function was called h. So, the delta, the derivative of the log log of h with respect to x is going to be one over h of x. And then we have this function this probability function we've just labeled g right so now I'm trying to figure out what g is. So, I can now rewrite my log likelihood function just put g, g of x in place of the probability. So, now my, my gradient with respect to weight J of log likelihood is going to be some for all n for all k of t sub nk divided by g sub k of x sub n times the gradient of w sub j times g sub k of x of n. So, if you're wondering why the above works just remember what the derivative of the log of x is right so driven that for log is one of our x. So we can actually do that. So, now it would be really nice if the gradient includes the factor, g sub k of x of n is going to cancel with the other one in the denominator. So we can rewrite the function to get this. So if we define f of x sub n parameterized by weight sub k as he raised to w sub k transpose times x right so again we see this thing that should look familiar double weights times inputs. So if we rewrite this such that, g of g of case of x sub n so g sub k of x sub n is equal to f of x sub n parameterized by w sub k divided by the sum. So now we can work on simplifying this right so we take this function that defined here. I'm just going to rewrite it in terms of this new definition of f right so we get this. And so now, by take the gradient of this end up simplifying to something that looks like this right so if I have the gradient of w sub j times. The sum for all k of e raised to this quantity over one over that times, you raised to that same quantity. So now if I take a look at this. What's going on here so I have the gradient of W sub j raised to e. Sorry, times, he raised to the W times x. So now remember what our, what our indicator variables were doing right so if it was a member of that class I get some value. It's not a member of that class it's always one. So taking the derivative. I should get zero, if it's not a member of the class of interest. Otherwise I get an actual quantity. So, therefore, what we can end up doing after all this math is basically saying, for this value, what I can end up doing is I can just define this function, where it's going to output, some, some value if it's a member of the class of interest and zero otherwise. So now if we take this substitute back into the log likelihood expression, we end up with something like this. So we basically have okay so the change in W sub j of the log likelihood is going to be the sum for all n for all k times the indicator variable over our function g divided by or sorry times the gradient of W sub j times the times g. So now what I can end up doing is I'm just going to take this change in delta, delta sub j k minus the my function g times the input. So now this gives me this update rule. Right, so we see what we had before so previous or previous value of W sub j plus alpha, my learning rate times the sum for T sub n, sorry, T sub n j minus g sub j of x sub n times x sub n. Let's focus on this term here, like so went through all that very very fast. Let's focus on what's going on here so if we look at my indicator variable, either a zero or one. Right. This is going to now be some probability value. And so, if I have my samples that are set up basically saying it's got a probability that's zero or one or zero. Right, let's say it's a member of class two is 010. Now I want to predict something that's going to give me a meaningful error. So let's say about three classes where my indicator variable is 010. I want to subtract from this, something that is going to be of the same dimensionality that have to be three terms. I also want to be a meaningful error. So, what's a meaningful error in this case basically let's say that we've got three classes, whereby indicator variables are 010. Think of those as probabilities instead. Zero percent 100% zero percent. So now if I can have a predictive function is going to output probabilities that can be directly subtracted from a value between zero and 100. It's going to give me a meaningful error. So if you imagine that I have some function that says, okay, got three outputs and it is point three point six and point one. We can now subtract that from, we can take that and subtract it from 010. So we'd have zero minus point three, one minus point six, zero minus point one. So now we're back into the kind of traditional error formulation of how wrong am I? So I'm trying to predict my probability. My ground truth is saying, well, there's a zero percent probability it's a member of these classes and 100% probability it's a member of this one class. I want my output values to approximate those values as close as possible over all of my samples. So this function, whatever it is, and you may be thinking of names for this, is really just another way of representing error. It's just this time it's representing as an error in probability instead of an error in some scalar value. So let me finish this, I think. So just to summarize what we've done. So I have my probability of my class given some sample and some data likelihood we want to maximize. So what I did is I took my probability function and I'm trying to find some function that's going to model this appropriately. So for right now I'm just calling it J or sorry, G. We want G to have the following properties. It should be bounded between zero and one and it should sum to one for all possible outputs. So this value should be raised, this value raised, e raised to the w sub k times x sub n. Remember, this is just my input x times my weight t, or my weight w. And so this should raise, equal this value if k is less than k. Otherwise it'll be one if k is equal to k. So now what I can do here is then for the likelihood of w is going to be the product for all n and all ks of these probabilities raised to the indicator variables. Remember, we're writing the probability function in terms of as this. So now the gradient of the log likelihood with respect to w is going to be something like this. So I now take the gradient, it's going to be the log likelihood. So I'm turning all my products into sums. Because I'm taking the derivative of the natural log, I can now bring the indicator variable down and then also divide it by my function g. And now I'm going to multiply this by the gradient of my weight g, or sorry, my weight w times the function g of just k of x sub n. So now what I end up with is the simplifies to for the sum for all n of x sub n, my input, times my error. Right, so I get some output here. And I subtract that from my indicator variable is the can be thought of as a probability. And then I multiply that by the input. So that's the gradient this now allows me to turn this into an update rule which tells me how much I need to move along that gradient in order to optimize those weights. So, last few minutes, questions about this we'll pick this up again on Thursday. So the function of the physical alphabet. Yeah, it says function of f of x sub n. I thought you mentioned that k is less than, it's not equal to capital K, then it should be zero. So k is the number of classes, so big K is the number of classes. So k here is an individual class and basically it's going to be there's a class of interest. Maybe it's like 012. And so it's going to be the, the probability of falling into the individual class. When for all classes, right, this would probably actually confusing. This should probably be like a summation I think. So yeah, I think I'll fix that it's a bit of a typo. So, remember here we have the output of this is going to be what's the probability of it being in this class, and there's all this probability should sum to one. Okay. Alrighty. Yeah, so I will go back to my office. Come to office hours if you want to be there until 430, and good luck on getting a two completed. or. Thanks. Alright. Alright guys, let's Alright. Alright guys, let's start. So I appreciate you start. So I appreciate you start. So I appreciate you hardy souls who came out today. I actually was not expecting this many people. I actually was not expecting this many people. I actually was not expecting this many people. I actually was not expecting this many people. I actually was not expecting a dozen to 20 people in the room which is twice as much as I would actually be here. So, yeah, thanks for, thanks for the rat and we wait for him to catch. There we go. All righty so what we're going to do today is I will just go ahead and do the assignment. So, assignment to is due So, assignment to is due today for most of you so if you haven't requested if you need an extension. And your default due date is today I'm sorry it is now too late. So, please make sure late. So, please make sure that you get your assignments submitted. So, so well, let's do today. So, so well, let's do today. We were assignment one if you We were assignment one if you choose to do that are going to be the last two days. So, we're going to do assignment three. So, I'd say around So, we're going to do assignment three. So, I'd say around now is probably when things start to get pretty, pretty intense to get pretty, pretty intense, as far as the workload for this class. So, it's about it So, it's about it as far as class announcements. So what we're going to do today is I'll finish up the, the classification with logistic regression lecture. And then I believe we are exactly on track for where we need to be is the 20 second, and then we'll get to the third. Okay, let me get through this and I'll see I think that notebook 11 is like pretty, pretty short. So, see if we can get through that today because we are now a little bit behind. Okay. No. All right. So, just a refresher on So, just a refresher on last time. So, we're doing So, we're doing classification of course and so we talked about that in terms of probabilities. So, we basically have a bunch of classes. And then for example we want to predict the probability that it falls into each of your any classes or your classes. And so then what we have to do is we basically have to maximize the likelihood of seeing the data that we actually have according to the class distribution that we've got so the data of course in this case is going to is going to include the samples and whatever numerical features define each of those samples and then a label for each for each class and remember the label is now going to be an indicator variable or a one hot vector where it's all zeros except for a single one where in the index corresponding to the class that the sample belongs to this can also be thought of as a probability right so it is a 0% chance the ground truth is basically you know sample sample and is K or is not K for whatever class K. In other words, for the correct class K it is 100% likely to be that class and it's not going to be anything else so think of this as probabilities everything is 0 or 1. I'm now trying to minimize the distance between my prediction and the ground truth which is now just a probability distribution right and being a probability distribution it has to sum to 1 so we have the data likelihood we want to maximize now so this is going to be we have some function G that's basically going to be my prediction function for my sample parameterized by the weights K for every class K and then divide that by the sum of the prediction values for that for every class M right. So we define the gradient with respect of the log likelihood with respect to W and we end up with update rules for each individual weight W. This should look kind of superficially familiar to what you're what you already know from doing progression with neural networks that is I'm trying to update an individual weight and so I need to take the previous value of that weight and then add something to it to move it in the right direction that something is going to be some constant learning rate let's say alpha and then this is going to be the sum of all my errors for my samples right so WJJ corresponds to class I want to better optimize the weights corresponding to that class I need to effectively figure out how wrong is my prediction for every sample in my data set for every sample N. So then what I'm going to do is I'm going to have some target value this is going to be that indicator variable where all zero is the single one in the right place for sample N for that class J so basically what this means is that now for this sample N at index J there should be a one and there should be all zeros other one. So then what I want to do is I want I want to subtract from that my prediction right so this should be this function G I'm going to technically have a different function G for every class J and this will tell me the like of my class I just need to do this for every class and then of course I have the other term there being that input so pretty standard optimization function at this point in that I have some learning rate I have some error term and I have some input right and these are the three things plus the weight that I need to use to optimize the value of that weight to the correct value. So of course we're doing this in going to be doing this in Python and so we want this to be some some level of optimized for speed so if we have my update rule for W sub J where J is some class as given above we can see and we check out the well the expected shapes of these arrays are going to be right so remember we're dealing with these weighted sums of inputs and so we're going to be adding this constant one to the front of each sample so what's the dimensionality of X sub n it's going to be D plus one where D is the number of measurements or features for that sample plus one which is that's my bias column and then we're going to add this that's my bias column and then by one because I'm really dealing with a single sample right now W sub J should also be D plus one by one which is just a single class and so then T sub n J minus G sub J of X sub n is going to be some scalar so again this is my this is my error measurement how wrong am I and so I need that to be expressed in terms of a scalar value. So this all works but you'll notice that the sum is over n and then each term in the product is going to have n components so that means we can rewrite these as matrices and then do it all as a dot product to basically do all my computations across the entire data set at once. So just to work this out we'll kind of do something similar to what we did when we worked out the update rules for neural networks using matrices so that is we can remove the sum and basically just replace the subscript n for all samples with just an arbitrary placeholder which is use the star so I can remove this sum right here and so now for whichever value of n I'm just going to be updating the associated value of T sub J for that sample. So consider remember we want to set up all our data as these big matrices where my rows are the samples and then my columns are going to be the targets. So in this case the target is going to be you know a n by k array for k classes. So what are the shapes of each piece here? So this is going to be T minus G this is going to be n minus one so or sorry n by one so for every sample I should get a scalar value saying how wrong this prediction is and so now X right likewise I remove the n and I'm just using this kind of placeholder star so this should be this is going to be basically my big X my big collection of all my inputs and so this should be n for n samples by d plus one where d is the number of features of the sample plus one for the bias and so then W sub J as above is going to be d plus one by one. So this is going to work now if we transpose X and then just pre-multiply it and then we just define G as a function that accepts X. So if you look at this and then here below they're basically just identical having removed the sum not only all I need to do is transpose my big X to make sure that my shapes align. So now what we've got is we basically have an arbitrary update rule for all of my X for a single class J. So now we've been able to effectively get rid of the summation over n and so now I want to get rid of my other subscript which is this J. So in other words we're going to try and make this expression work for all the W's. So now we've successfully gotten rid of n let's do the same thing to get rid of J we'll just replace J with our placeholder and so likewise every time I see a J here I end I just replace with the star so now I have nicely I end up with sort of T sub star star which if you remember how we handle this in the intro to neural network lecture this is basically just saying I'm going to replace this with a big matrix of all of my T's. So here the star star means I can account for any row and any column so now this is just going to be big T matrix of all my target values. So if W star which is basically now just W this is going to be D plus one times K. So now think about the what we're trying to map to is going to be probabilities over K classes right so this output should be of dimensionality K what are we trying to map to those K classes? Come keep your size D plus one for the bias so this is going to be D plus one by K. T sub star star is just big T you remember so this is going to be n by K. How many samples do I have? N. So this is going to be each of them could be one of K classes so n by K. And now G of X is going to be n by K minus one so this should this basically predict my samples and then for each of them is going to predict some class. So actually why is it pulled on? This is incorrect. My mistake. So this is going to be N by K. So times there we go. So G of X is n by K. So now T minus G of X should also be n by K of course we have you know n by K minus n by K equals n by K just element wise. And so T or sorry X transpose times T minus G is now going to be D plus one by K. And so now we have this T minus G plus one is D plus one by K. And so now X transpose times T minus G plus one is D plus one by K. And so now we have the update equation for all my W's being for any W I'm going to take the previous value of that weight plus learning rate times the input times the error return. So far so good. So A will be some constant or alpha constant. T is my n by K target indicator variables and then G of X is the prediction function over X or over the values X. So this what does what does G what does G look like basically? So we defined for K from one through big K. We said that this function will just take it to be the shape of E expi to the W times X. So we do the exponentiation because we basically need to end up with a subtraction where we need to be able to do T minus G. But previously, if you remember, we worked in the world of logarithms. And so if I can't necessarily easily subtract, there's nothing that I can take the logarithm of a subtraction very easily, right? If I take the logarithm of a division right now, I can I can subtract the logarithms, but actually want to be subtracting the scalar values itself. So to get around that, I'm going to do is just going to exponentiate both sides. Now I can actually do a subtraction. So if G sub K of X is going to be my prediction function for that class K divided by the sum of the prediction outputs for all of my classes, then I can change these to handle all the samples X. So now instead of for a particular class K and for a particular sample N, I'm just going to take F of X parameterized by W, right? So now I'm back in my familiar territory of I have some inputs X that I'm trying to map to the right outputs. My job is now to optimize for the right weights that will let me do that. So I can just say that F of X parameterized by W is E raised to the X times W. And so now G of X is going to be F of X parameterized by W divided by basically the sum over all the rows for X parameterized by W. So I basically have one one class and then I divide by the sum. So given training data X, which as you recall is N by D plus one and then class indicator variables T that are N by K, we can then perform the following expressions, perform the operations with the following code. So first what we need to do is we need to create this function to get indicator variables from the class labels because you might find that your class labels are not set up in these one hot vector representations. In fact, they're just like labeled ABC or 123. So you need to translate them into an appropriate indicator variable format. So basically I'm just trying to go from this, you know, 12213 to 100100101001 for however many classes you got. You might discord notifications. It keeps beeping at me. Oh god, you have to do that. Okay. So everybody just ignore that please. So we're going to do is we're going to find this make indicator variables function. This will take in the T column matrix. So this is just going to be like a two dimensional matrix. It's N samples by one. And then what I will do is I'm just going to pretty much just reshape this into the into the into a two dimensional matrix if it's not already and then just take those individual values and map them to the appropriate index in a vector that's otherwise all zeros. As a demonstration, let's take the above sample right here. Reshape it, right? And then I will run it through the make indicator variables. And this gives me the same thing. Right? So these are now the above indices as indicator variables. So now for how many things am I trying to predict? It's going to be however many classes I've got. And what I want to predict is now the probability that my sample falls into each of those classes. And by dividing by the sum of all predictions, I can ensure that I'm going to get a value that's a basically a percentage out of one. So I'll define G that does all of that here. So now I have G of X of W. And so I'll have my my F, which is going to be going to exponentiate X times W. Then I'm going to take the denominator and then I'll return my G's, my G values, which is going to be that F's value divided by the denominator. This G function is also sometimes called the softmax function, as you probably have heard of. So softmax function is this common final layer activation function. And now as you see, what it does is it takes your scalar values and maps them all into some probability distribution that sums to one. So like the sigmoid, right, which converts things into a value normalized between zero and one, the softmax does that too, except it does it makes sure that that that sum is normalized so that all values sum to one. So as you can see, the sigmoid is useful for binary classification, right? If everything is going to be between zero and one, then if I have a class of interest, all I need to do is sigmoid some value and it's going to tell me the probability between zero and one that it falls into that class of interest. So this is useful for two class problems because software because sigmoid squishes everything between zero and one, which means that to get the probability of the other class, I just do one minus the sigmoid. Softmax, of course, I can't use the sigmoid function for multi-class problems because if I have a class of interest, all that's going to do is tell me what's the probability that this is not a member of that class, which is a binary classification problem. If I actually want to know which class it falls into, I have to be able to normalize all the probabilities such that they sum to one. Turns out, maybe we'll review this after spring break, the softmax is a generalization of the sigmoid function for multi-class problems and it's a fairly simple derivation to show that. That is not, I believe, in this notebook, but we can get to that. So softmax function is basically as you may have seen, you know, you take your final output, aside, prior to the softmax, exponentiate it, then divide by the sum of all exponentiations. So now the updates to W can be formed with code such as this. So I'll make my indicator variables, I will define the weights in the appropriate shape, define some learning rate, and then for every step in my specified number of epochs, I will then take the softmax of X and W, right, in this function, we're multiplying X and W, and so then we can use my update rule to better optimize the weights. So this, in a nutshell, for a very simple linear classification problem is how I can use the softmax function. And so when I'm doing neural networks, what I'll do is I'll just have the appropriate insertion of hidden layers to handle non-linearity and then do backprop in a very similar way. Questions so far? I'm curious, why is everybody gravitating to this side of the room? Even when people are here, that side of the room is much sparser for some reason. Maybe you all come in through that door and it's just closer. Anyway, okay, we have an unbalanced distribution in this class, right, if I just want to predict like where someone's sitting in the class, it's almost always going to be on this side because, sorry, this is very empty over there. So here is code for applying linear logistic regression to the Parkinson's data. So we're still in the world of linear equations here because I don't have my hidden layers to allow me to insert non-linearities. So I can still use the softmax function, of course, to squish everything into the appropriate probability distribution, but what it does is just taking a matrix multiplication of X and W, which is still a kind of standard linear equation that we have been doing since the beginning of class. So we'll do the Parkinson's data set again. So let me read in the data. So this should look familiar, 195 samples with 24 features each. So then I will do the same type of data cleaning that I did before. So of course I don't want to include the status, which is my output label in my inputs, and neither do I want to include the names. It's not a helpful feature. So I drop those and then I slice off the status column. This becomes my targets. So now I can see that I've got 22 input features in X. These are the names of all those features. I have a single output that's status. Standardization function, as you've seen before, so I'm just going to compute some means and standard deviations and standardize the inputs. I'm now also going to import the QDA, LDA implementations that we've used in the previous lecture. This will allow me to compare the performance of logistic regression to the two previous methods that we use, namely QDA and LDA. Okay, so now to generate our training data and our validation and testing partitions, what we're going to do is we're going to partition data into folds on a class-by-class basis. So this is kind of similar to what we did earlier in that we're going to define how many folds I want, and then I'm going to use one of them to be testing data and then one of them to be validation and then the rest to be training data. The only wrinkle here is that I need to make sure that I have approximately the same proportion of samples from each class. So this function does this. This is called generate stratified partitions. So what this will do is it's going to generate my sets for training, validate, and test for both my inputs and my targets. Or if I don't want validation data, I can set validation to false, and it's going to give me a dictionary that's keyed by the class label. So if I want to retrieve, say, the training and the validation sets for class zero, it's going to be set up in a pretty intuitive way. So what we do here is I'm just going to basically shuffle all of my rows, and then I'm going to pull out the appropriate set for each fold, and then I'm going to partition my folds, or I'm going to, having partitioned my data in my folds, I'm going to segment off which collection of folds I want to use for training, validation, and test. So basically this just does all that. So in this part here, this is just going to make sure that I'm roughly balanced across all of my classes, and then this part will generate the test and optional validation fold, and then return the rest as training. So if you run this, what we'll do here is I will print, these are each of my folds, and then we'll see that the first number, this is like my number of training samples, and then my number of validation samples, and my number of testing samples, and then this will be the number here is the partition, or the percent of the partition that is class zero. So if you remember that we have the healthy samples and the Parkinson's samples, and it's not a balanced data set, but we want to see roughly the same proportion of each class across all of my folds. So we can do this in this trick using np.mean, right, so this is going to be computing the arithmetic mean across the specified axis, and what this does, t-train equals zero, is it's going to turn everything to either true or false, and so if this is true, it's going to be a one, if it's false, it's going to be a zero, and np.mean will handle that automatically. So these are the individual samples. So what that means then is that np.mean of A equals equals zero is going to be the proportion of samples in A whose values are equal to zero, and just replace this constant here with whatever thing you're interested in. So now what we can do is you can write a function that's going to iterate all the possible ways of making train validation and test sets from n partitions that I specify, and that'll also train my QDA, LDA, and now my logistic regression models over this data. Okay, so what I'm going to do is I'm going to save time just using the break statement to stop execution after just one run of cross validation, and then to the end of the run, I'm going to have to calculate the mean errors or accuracies over all cross validation runs. This is what you're going to be doing in A3. So does everybody know the term cross validation? Who's not familiar with this term? Okay, somebody that's funny. So basically cross validation, I have a data set, and maybe it's small, or maybe I just want to make sure that I'm not just getting a lucky split. What I'm going to do is I'm going to basically partition it into these folds, and I'm going to hold one out as the thing to be tested on. Now remember, maybe that particular split is lucky for some reason. We talked about how you can just sort of, depending on your random seed or other factors, you can get a particular testing split that just happens to perform well. If you hold that testing data out, the remaining training data is just very indicative of that testing data. There's nothing really in the testing data that is maybe unusual from the perspective of the training data. So in that case, it's quite likely that you would get a very good test performance. But we don't know that you just segmented out the 20% of the data that had that property. So if you stop there, you could report a really high test result, but then someone else who runs your code partitions a different split, or they have a different random seed, they get different rows in that split, and all of a sudden you're showing 96% test accuracy, and they get 68% or something. They're like, whoa, what's going on? This is way lower. So what you want to do, or one thing that you can do, is to take that test split and rotate a different one each time, and then average all those results. So for example, if you had some test split that was like, okay, you got 96% on this one, and then 76% on the next one, and then like 86% across the remaining three, just to make my mental math easier, you end up with an average test accuracy of 86%. And so that's much more realistic than either the high end, the 96, or the low end, the 76. And so in that way, you'll get a more accurate picture of the actual performance of your model trained over these different splits in the training data. So clear. Cross-validation can also be a useful technique when you have a small data set. So for example, it may take, let's take this sample. We have 190 samples. This is not huge. It will do fine for these linear methods, but let's say you have a more complicated problem that requires a neural network. Often it requires more samples to actually converge to an appropriate model, and so you need some, you know, you need like 90% of those training samples to get your model to converge and you hold that 10%, which in a sample that has 195 samples to begin with, you'd only, you'd have like 19 test samples, right? And the results might not be all that indicative, right? Again, you have the sparsity problem, so you could end up with a with a lucky split problem. And so one way you can kind of ameliorate that is to cycle through these different testing splits. So a number of reasons why you might want to use cross-validation. We will do that here. And so you're going to have to implement this in A3 using similar methods. So we'll have this this version of the runpark function. It's going to be called runpark logreg for logistic regression. We'll also include the outputs of QDA and LDA. So this will output the prediction accuracy using all three of those. So I'm going to run my generate stratified partitions. I specify how many folds I want. In this case, I'm not going to use validation, so I'm just going to generate train and test. And now I compute my means and my standard deviations. I standardize my X trains and my X test using those means and standard deviations. And then I will do all my preprocessing upfront. I'll append my column of ones. Now the new stuff for linear logistic regression is now I have this make indicator of ours function, right? So this t train and t test now gets transformed into t train i and t test i. So now I should have some one hot vector representation or indicator variable representation of all of my target samples. And now the rest is as we've seen. So here, to this point, ignore this likelihood list for the moment. We just initialize our weights, specify some learning rate, specify some training time. This is my forward pass because it's linear. I'm just running through the softmax. So you can imagine this being just a neural network with no hidden layers and a softmax on the output. It would be the same thing. And I do my backwards pass my weight of the weights and my backwards pass my weight update as we saw. What I'll do is now here I will convert the log likelihood of likelihood. So if you remember, we were calculating the log likelihood of the data to make the mathematics easier. We need to convert that back to the actual likelihood because that's what we're trying to maximize. And so then what I'll do is I will append this likelihood per sample to this list so I can plot it. So now what I can see is after I plot, I see the likelihood of the data as training proceeds. And then I will print the percent correct using the logistic regression method. Then I'll do the same thing using the QDA and LDA code that we did before. And the rest is plotting. And then I will define this percent correct to turn my results into a coherent accuracy. So let's run it. Okay. So this is the percent correct for each one. So the logistic regression is 89.2 train, 78.9 test. QDA and LDA have really high numbers. Or actually, wait a second. This is the same because, right, I forgot. This is the issue. So sorry. Sorry about that. Okay. So here we go. Run this again. Okay. This is more like it. So we see logistic regression and LDA have kind of similar performances, although we actually see the logistic regression has a higher test accuracy this time. QDA, we see the same thing that we saw last time. We received this really high training accuracy. And the test accuracy is not as impressive, suggesting that QDA, again, may be overfitting to this data. These are the weights. And you'll see that basically the weights of the second column are just the negative of the weights in the first column because this is a binary classification if it's not one, it's the other. We're using the softmax because that's generalization. We could recast this problem into one that uses the sigmoid because, again, one minus sigmoid of X will give me the probability that the sigmoid of X is not in the class of interest. Now if you look at a single sample, these are the individual values. These are standardized. So now the samples times the weights, right, this is going to be that first sample times W. This gives me some scalar value, right, 1.23 and negative 1.23. We can already see what the output's going to be. We already know which class it's going to be in. But just to run everything to completion, I'm going to exponentiate this. So now this is basically e raised to these values. So now they're no longer negatives, right, 3.45 and 0.3. I sum the exponentials. This is now 3.7. So now I normalize these. 0.92 and 0.8. So now these sum to one. I just do a sanity check here, right, this does sum to one. And the argmax is going to be the first element or class zero. So this is a, the sample belongs to class zero. Once we got to this point, you could probably tell what the answer is going to be just looking at the scalar value because this is a binary classification problem. It's going to be one or the other if it were a multi-class classification problem, it would not be necessarily so easy to see that. And also the softmax has the nice properties of making everything consistent with your error term relative to a probability value that you do need. So you can sort of see what the answer is by the time we get here, but you actually need to softmax in order to get a meaningful error term that I can use to do things like weight updates or back propagation in neural networks. So finally our charts here, this is the likelihood of the data according to logistic regression. We can see that we end up with just like from 0.5, you go up to probably about 0.8, most of that. And now here this plot, you can see the outputs for each individual classification. What we will find is in most cases, LDA and logistic regression are performing the same. There are probably only two samples where LDA says one thing and logistic regression says the other. QDA, again, we have some overfitting in the training data so it's not performing quite as well. So we can run this again one more time. So we can run this a few more times. You see similar trends. And generally what we find here is that at least for this data, logistic regression can usually achieve a higher test accuracy than LDA even though maybe the training accuracy is the same or slightly lower. So this may be a problem where logistic regression is more suited. So run this a few more times and you can see similar trends as we have seen before. Okay. So now the code above, this is using SGD in the gradient of the log likelihood. So do we have a better way of doing gradient descent? And remember, gradient space ascent, not descent yet. So we can try Adam. We've demonstrated that Adam often converges faster with fewer training epochs. But first you need to define your error function to be minimized and as gradient function. So previously we were trying to ascend the gradient, but the function to be optimized, we actually want to try to minimize the gradient so we can use a general solution using any kind of gradient descent algorithm. So in this case, what we're going to do is we're going to create this function as the negative of the log likelihood. So this allows us to minimize the function and then the gradient function also needs to include the negative. So here's the definition of log likelihood above and its gradient below, also written as matrices. Oh, did I stop? Oh boy, I hope I didn't interrupt something. Just stop the girl. See what happens. Okay, didn't break. Sorry about that. I think I stopped accidentally. So I'll define this second version of the function that will be using Adam. I'm going to import the optimizers class that I have before so I can specify which optimizer I want. And now what I've done here additionally is below the softmax function is I've now defined the negative log likelihood function and then its gradient. So now I can use these to get the output of my gradient descent operation at each step. And so now this is the thing that I'm trying to minimize. And then what you may want to do is convert this back into the likelihood of the data. So here what I'll do is I will take this to likelihood. So I have the negative log likelihood. What I'm going to do is it's going to take the negative of NLL or the negative log likelihood, exponentiate it. This gets me back into regular likelihood. So now I can define this likelihood trace that is going to be, that's going to instantiate an instance of Adam that takes the negative log likelihood as an input and its gradient and the number of training epochs learning rate. And then I just define this error convert F function to likelihood. And so this error convert F is a member function of the Adam optimizer. And so it's just going to call this here that I've defined. So it's automatically going to convert the negative log likelihood back to the just plain likelihood. Then the rest is the same. Right. We just do just a regression QDA and LDA. Print this. And so now here this is written error is a little bit misleading just because of the way that the neural network class is written. This class is written to originally based off of the regression problems and trying to minimize error. Here all I've done is I'm now printing the likelihood of the data instead. So just a note when you see error here, you'll notice that it actually increases sometimes. And that's because we're trying to maximize the likelihood of the data, which is what's being printed out here. OK, so maybe a more we could rewrite this to actually print the negative log like we didn't see it decline. But then it would sort of go opposite what the chart actually shows. So here we see the error or likelihood climb to some threshold of about point eight nine. And then we also see the the training and test accuracy. And you can see here that using Adam, at least in this case, train accuracy is only eighty eight point nine or test accuracy is actually ninety six, which is significantly better than the test accuracy for either QDA or LDA. So run this again. And then we see similar different split here, but we're still we're still getting basically better test accuracy than the train accuracy using using logistic regression with Adam. So just throw that out there to some of you. What would you do to change this to run SGD instead of Adam for the linear logistic regression? Just think about how the code is set up. We'll go back to where we instantiate that, highlight it a little bit. So what would you do to use SGD instead? And the answer was probably a lot simpler than you think. Well, so I always use softmax for all classification problems. Right now I'm just I'm asking this is this is a lead into the next lecture, just just to sort of telegraph where I'm going a little bit. I need to change a word, a single word here. What do you think? What do you think of the back? You agree. Yeah. OK. So basically, we have this optimizer class that that contains implementations of Adam and SGD. So to do that, all we need to do is change SGD to Adam and then change the change the type signature of the function in whichever way it demands. So which brings me to the next lecture, code reuse for the next lecture, code reuse through inheritance. Before I do that, any questions on logistic regression? All right. So OK, 11 code reuse by class inheritance. This will be pretty useful to you, and I think this is relatively short, so I'll let you go early, which would be cool. So what you have done, say, in A2 so far, what we had you do, for example, is you complete a neural network implementation and then we say here's a cell, copy your whole neural network implementation into that cell and make the following changes. Right. And some of you doing that are probably thinking this seems like a very inefficient way of doing things. Why am I doing this? Well, it is not a great way of doing things, obviously. Instead, what we can do is you want to make relatively small modifications to the existing classes and functions. So what you can do is you can add arguments to select for the new behaviors, like what type of activation function you want to use or what type of optimizer you want to use. Or you can use class inheritance to extend the original class. So I assume that all of you are familiar with doing class inheritance in some way, right? Maybe in Python, maybe not, but presumably you've at least taken Java here and so you've done class inheritance. So we'll do just an example here that uses the neural network class. So to allow, say, the tanh or the relu activation functions, you had used this keyword to specify tanh or relu, and then in that copy of your neural network class, you had to add the additional relu behaviors and the gradients and everything like that. So instead, let's use class inheritance to do the same thing. So first, I'm just going to write out the optimizers file. So this is right here. And then I will put it in its own Python script. So now to use this class, you need to import it like we've done. And here is a... I'm thinking about giving away the answer to 82. Maybe I should just end class here. What do you guys think? Do you want to get like 30 minutes back? I'm going to stop here. No. I feel like I shouldn't let this like... Yeah. What's that? We were right where we need to be. And I think the due dates slip every year a little bit. I have to calibrate them appropriately. So I'm going to call it a day here, actually. So I don't give away the answer. Okay. So you all get 30 minutes back. So I will see you next week. All right, you already going. Okay, so everybody should have turned in assignment to, which is good, I can pick up on the notebook that I left off on last time. We will try to get this back to you within about a week. So today I'm going to go finish this code reuse notebook. This is short. Then we'll start classification using neural networks and then I will assign assignment three. So I'm going to assign assignment three today. You have like a solid three weeks to work on it. So I didn't want to give you something to do during spring break. So this is due the Tuesday after spring break. So you have time. I do recommend that you start early, especially if you want to have some some spring breaks. I'm not going to be here. And so of course, I'm not going to be holding office hours. So recommend you start early and to give yourself time to to work on on this. So I guess without further ado, let's do this one for real. I hope you enjoyed the extra 30 minutes I gave you on Thursday. So these these notebooks are now available again. Note that we're not going to do a reread on assignment two because basically the answer is now given to you or an answer is not given to you. So You know this this one is not going to be eligible for if you do want to resubmit assignment one reminder that those are That's a hard deadline. So we will not be accepting anything else on assignment one after midnight tonight. So As you recall, you know, if you want to modify your neural network implementation or any other implementations. You can either add arguments to parameterize the different behaviors or you can replicate code from the original class using class inheritance. So first, let's just show our neural network implementation not going to go through the code because you've already seen most of this before. Here's optimizers and has an admin implementation there. So now we will import optimizers and then here's a version of the neural network code, you'll notice the familiar functions that these have all been filled out. So you're this is basically your implementation or something close to it. If you if you got full credit on assignment to That what we had before was we had a division function argument and it you have to change this to specify whether you want to use 10 H array loop. That was that was part of your assignment and this had to be added in the neural network instructor calls in that in it function. So now what I can do using this is I can now define two different instances of the neural network a 10 H version and radio version. That have the same architecture. Otherwise, simply by just passing things activation function. To the to the constructor. So now if I just print this, it will show me that I've got an instance of neural network that has An input size of 10 100 hidden units followed by 50 hidden units and an output size of one uses the 10 H activation function. The other one is the same just uses the radio activation function so The same two kinds of neural networks could actually be implemented just using two separate classes. Right. So we can say we can have two classes, one called neural network 10 H and one called neural network radio, but You would observe already. I think that most of the code in these two classes is going to be identical. And so if I duplicate all this code in two separate classes, you are effectively by yourself a future headache because one day you may decide that, oh, there is a bug or I want to change some functionality and you do it in the 10 H version. You have to replicate it exactly in the radio version or your code is going to work differently. So you make one mistake. And all of a sudden you have diversion implementation. And so, you know, one of them might not work or may not work the same and suddenly your results not directly comparable. So instead what you Inherit most of the functions for one class to create the second one and only change those things in the second class that would directly override things in the first class. So this would be going through these in real time, or I can find my typos. So this would be readable if you just define two functions. So we'll call the activation function activation function gradient. Of course, the things that I want to change about this, this new version of neural network is going to be the activation function. So If I just find these in the first class, then I can overwrite them in the second class with a different functionality and everything else can remain the same. So it makes sense to define the second class first and we'll just see how this looks in terms of code implementation. So if we assume that neural network 10 H is already defined, then I can just define my neural network ReLU by passing the neural network 10 H class as an argument into the class definition. So now we have neural network ReLU Will inherit everything from neural network 10 H except now I have these two functions for the ReLU activation and the radio network. Gradient that have the really specific behaviors here. And so presumably they would just overwrite any equivalent implementation in 10 H. So we run this. And it didn't give me an error because I sorry there was an error there previously, but then I ran the rest of the notebook. So if I if I cleared the kernel and run this get an error. That's because the neural network 10 H function. The first time you run this notebook has not been defined because I ran the first time. The first time you run this notebook has not been defined because I've already run the notebook completion once it has been defined. So it doesn't give me that error. But if you got that error, what you need to do is you need to find the original neural network class redefined to use this activation function and activation function gradient functions and then I'm going to rename that to neural network 10 H that it matches the definition that the neural network ReLU implementation is expecting. So if I define neural network 10 H you will observe that it looks pretty much the same as the current neural network implementation, except when I get down to here, I now have instead of say the 10 H function or the ReLU function as you defined it, I just have a generic activation function and activation function gradient. Now, of course, these have to be when I define this, these basically have to be initialized in memory with some behavior. Right, I could just say pass, for example, I could just ask you were here that would basically say do nothing. But then, of course, if I tried to run this then I would not have that that requisite behavior. So I'm going to define some default behavior. And for the purposes of this demonstration, I would just assume that my default behavior is going to be using the 10 H function. Right. So if if not defined your activation function is 10 H. So you'll see that we now return the 10 H and then we return one minus y squared, which is the same as the gradient 10 H. So now forward pass same functions as before. So now I can define my second class. So now I have an instance of neural network ReLU that inherits from that 10 H version but overrides the activation function as gradient. So now you observe that this version here, instead of having this activation function argument that specifies in a string form what activation function I want to use, I now can use this one. Right, let's compare these. So here's neural network using the activation function argument. And then here is the differences, different classes. Where I don't want to specify that argument. So there are different advantages and disadvantages to this. So like one advantage that in the small example is not that significant, but might be if you're using this at a large scale is that like I don't have to specify the activation function name in a string. This prevents me from say making a typo in that string or something. So in that sense, it's a little bit cleaner, but there are some obvious disadvantages and in general, we'll see one of these techniques is preferred. Yes. So you mean both the 10 H? Well, that's what we're doing in this first version here, right? Because then you have, if I have all these functions available, whenever I run the neural network, I have to specify which one of these I want to use. Right. So that's what this activation function argument is doing. That's what you did in A2 effectively, right? So like there's both of these are valid strategies and we'll see at the end that in fact the strategy that we were using is probably on balance preferable. But it's important to understand how you can do this. So when we do, I think it's assignment three, you have to basically take your neural network implementation or A neural network implementation for regression and redefine it to classification, which is you if you've been paying attention, there are some subtle differences there, but you don't have to change the entire neural network architecture. So it makes sense to kind of inherit from a generic regressor neural network and then overwrite certain functions to make it classifier. Yes. Like an example of what we might do, but this is a very serious for us. No, this is not necessarily for assignment. This is a this is an example of how to use class inheritance in an example that's simple enough to illustrate in a few slides, but it's probably not what you actually want to use this for. Nonetheless, let's assume that I'm just going to be using this to define different activation functions. So now if I print instances of this, you can see that now I've got these two separate instances of what are different classes. Although this one neural network relu inherits from the first one, they have the same architecture. So now you can see that this approach, you know, worked pretty well. But now imagine if I have like a bunch of different activation functions, right? We talked about tanh, we talked about relu. Some of you may have implemented the switch activation for A2, right? There are a bunch of other different activation functions talked about, you know, jlu, elu, etc. So I don't necessarily want to define like a different class for every single one of them when I'm just overriding like two functions. So, you know, you have to implement, you know, let's say if I have three variations with each with two choices, right, I'd have you know, two to the three, eight different classes. So even this bit here, just doing it for these two, these two variants seems a little bit excessive. So you'd have to remember like all the class names. So on balance for this for defining the same type of neural network for this for a similar task where I'm trying to do, say, change the hyperparameters. On balance for that approach, this type of thing where I have an argument that I can specify is probably the better approach that allows me to implement the different functions that I need all within the same class. I don't remember different class names. And I just have to specify which arguments I want, because here we can effectively define a neural network with these four arguments where NH is a list. And it is to specify the input size, the network architecture and the output size and the activation function for arguments for this function doesn't seem excessive. It might be more so like if I were trying to define radically different neural network architectures in the same function and instead of specifying, you know, feed forward versus convolutional versus recurrent in the actual constructor itself, I have a like type argument where it's like CNN and then I specify all the other things I have to set up my neural network as a convolutional neural net based just on this argument. That's not necessarily the best way forward for that. So when deciding whether I want to use class inheritance or arguments for specifying neural network parameters, it's kind of important to consider what exactly you're going to be editing. So for activation functions, I would argue that probably using this type of argument based method is preferable for different types of network architecture or for different types of network classes entirely. So feed forward, regressors, classifiers, the convolutional nets, recurrent nets, etc. Then it makes more sense maybe to have some sort of base class that has like all the functionality, you know, is common to all these things. And then I just have to modify certain functions. That's what we're going to do next. So we can take, you know, create different types of neural networks and then use say the same implementation of like your optimizer, your training function, those things that maintain their same basic shape and don't change a whole lot even when you may radically change the particulars of the network architecture. Using class inheritance is probably the better way to go. And so, for example, that torch.nn module that we used briefly kind of provides you with all of this basic functionality. So when you do assignment three, for example, you're going to be using PyTorch and you need to refer back to the autograd notebook, notebook eight. And so if you see things like, you know, loss.backwards, right, single function called it basically does all the backdrop for you. That's part of that neural network module. And so I wouldn't have access to that function if I didn't inherit from the neural network module. So these lower level things with regard to the functioning of the neural net is probably a better use for class inheritance, whereas some of the more hyperparameter tuning like things that you're going to be doing in most of your neural network usage probably can be specified in argument structures. Alright, questions on this? Alright. Let's start classification and I will try to get through all of this. So I have time to talk about assignment three. If we have to leave a little bit of this to Thursday, it's probably okay. So previously we talked about regression, right, so we did linear regression, nonlinear regression, all of these are predicting continuous values, right. So we can use the neural network as a way of introducing arbitrary nonlinearity to effectively construct a universal function approximator for predicting continuous values. So the principles are that we have some nonlinearity that's applied to the output of some hidden layer. So that would be say I have X times W gives me some set of scalars, I apply some tanh to them and this then the output layer is going to be linear in W and that just gives me my my prediction. So now when I'm optimizing my neural network, the values of W are going to be optimized effectively with the assumption that there's going to be a tanh or just some other nonlinearity applied to it. So of course, if I optimize the weights for a tanh network and this like clone them into a ReLU network, we're going to get drastically different performance of course because the actual outputs are going to be different. So if we have this scalar output, then we want to classify categories instead, right, what we did is we basically want to classify these things into probabilities that they fall into one of a set of K classes. For that we used in the previous notebook number number 10, we used logistic regression. This included the softmax calculation given here. So what this does, you know, just exponentiate basically the output of the output layer and then sum up the output layer and then sum up the output layer. So this is going to take the i-th prediction of X, it's going to turn this into a probability and that probability represents the probability that sample X sub i falls into whatever whichever category is denoted by these weights or more properly, since all our weights are going to be stored in a big matrix, each column is going to be associated with a particular category. And so weight sub k of that multi-class weight matrix W. So the softmax, this part here, right, this is the prediction, this is the output layer in the sense that this is the output of that final multiplication by those output layer weights. So this is going to be again still some scalar value. Then by applying the softmax function, this allows me to turn it into a probability and then turn the linear model into a logistic model. So when we previously introduced neural networks as this way of arbitrarily introducing nonlinearities into regression problems, so now we want to do classification. So what might be some thoughts you might have about how to do logistic regression, as you may remember from notebook 10, but in a nonlinear way. So this is the first thing that we're going to do. Yes. You can have neural networks that have multiple layers and activation functions. That's our way of arbitrarily introducing nonlinearities, right? So if we just had that, which you end up with would be a nonlinear regression function, and then we discussed that how we can turn a regression function into a classification function is the application of the softmax. So this is going to turn those k predictions that I want to make about some sample into k classes and the probability that it falls into each one. Right. So basically what we're doing is we're now just putting together the pieces of things we've already learned. We talked about softmax last week, and we've been doing neural networks now for a couple of weeks. And so we're just going to apply the principles of turning scalar values into probabilities using softmax to the use of neural networks as universal nonlinear function approximators. So let's just review some of the math. So remember when you're doing classification, we're basically trying to arrive at the set of weights that would allow we to maximize the log likelihood of the training data. We're trying to maximize the likelihood of the training data, but we work in logarithms because of these nice properties that allow us to add and so multiply, so we don't get these very infinitesimally small probabilities. Instead what we get is just a set of negative numbers, and you're basically trying to figure out which of these numbers is the least negative. It allows me to maximize the log likelihood. So just recall we're going to be using more or less the standard variable definition. So big W is going to be the whole weight matrix, and then X is going to be all the inputs where big N is the number of samples, therefore X of N is the Nth sample. Big K will be the number of classes, and therefore little k will be an individual class index. T, these are going to be the target values, so remember that the target values are now class indices. And so these are indicator variables where I have a string of zeros and then a one at the index that represents the appropriate class, and so therefore T sub Nk is going to be whether or not sample N falls into class K. So you can just think of this as being basically binary for K columns, right? So if it's a member of, it either is or is not a member of class zero, it either is or is not a member of class one, so on and so on until class K, and then by N rows for each sample. So now I basically just have an N by K matrix representing my outputs. So now P of C equals K given X sub N is going to be the probability of class K, right on the left side of the bar, given sample X of N. For simplicity's sake, we usually just say P of K given X of N. We can also rewrite this as some function G sub K of X of N, and in fact we'll just shorten it further to G sub Nk. And then we talked about how this arbitrary function G that we arrived at is actually the softmax function. I went through a bunch of math involving exponentiation and logarithms to basically show that if we perform a certain set of transformations, we can then derive the probability for a set of classes. So if we have the likelihood of W, this is going to be the probability for all N for all K of the probabilities of some class K given some sample, exponentiated to the indicator variable. So what that does is that this value is basically only exists, is only not one when T sub Nk is one. So if I raise anything to the zero, it becomes one, and then the product of course that just cancels out, just multiplies out. So similarly, if I then take the log likelihood, all I'm doing is I'm now just turning all my products into sums, I'm bringing my exponent down in front, and so now the logarithm of one is of course zero, and so the same is true if this T sub Nk is zero, then that also goes away. And so therefore, the log likelihood is going to be the sum for all N over the sum for all K of the indicator variable times the log probability basically. So now the gradient log likelihood will be given as follows. So if this is our log likelihood function, and then we have G being realized as its softmax function, and let's just define Y sub Nk as being that prediction for the output of sample N for class K. So in other words, this is going to be the weights of that class multiplied by that sample. So therefore, we've now arrived at the same definition of softmax function we had before. And so now if I take the partial derivative of both sides, I end up with the following. So now I'm just taking the sum for all N overall K of T sub Nk divided by G sub Nk, this is the actual target, and this is going to be the actual the prediction value, and then I multiply that by the partial derivative of G with respect to W. So if the general gradient looks something like this, so now we have effectively the partial derivative of the prediction with respect to the weights times T sub Nk minus G sub Nk, so this is going to be my ground truth target minus my prediction. So once again, this part is my error term, right? So this is the same as T minus Y in the linear regression problem, this is basically what's my ground truth minus what did I predict? How wrong am I? So again, T minus softmax is going to be how wrong am I, it's just going to be how off are my probability distributions, right? So if I'm very correct, then for that ground truth, it should be very, very close to one, because in the target values, my only choices are 0 and 1. And then I just do that, and then I take the sum for all classes, and then for all samples. So for linear logistic regression, Y sub Nj, that is the product of the weights times the input. So therefore, the derivative of that will only exist when J is the class of interest. So I'm going to do this for each of my classes, but for all of my classes, except for the one that actually is the truth value, I'm going to get 0. So I just want to optimize away from those and toward the thing that I actually have a value for. So therefore, the partial derivative of the log likelihood of the weights with respect to the weights is going to be equal to the sum of your error terms times X sub dJ, where d is going to be that particular input. So the nonlinear version is in some ways a little bit simpler if you remember how our neural network operations work, because effectively, all I'm going to be doing is the same neural network operation that we've been doing just with the softmax at the end. So let's remember the general form here. So we just have sum weights W. So to include the nonlinearities, we have to have a hidden layer. And so we'll just call this V. And so then the log probabilities of the K classes given X are going to be H, some activation function, over X times V times W. So remember the quantity here, X times V, that's that hidden layer output before the activation function. So previously, we call that Z or maybe A. And then I'll apply H over A. Maybe I'll call that Z. That will be the output, the final output of my hidden layer. Then I take that and multiply it by my output layer. And that gives me my pre softmax prediction. So because we're not still dealing in the world of logarithms, so we're going to take the top part of the softmax function, E, exponentiated to Y sub nK. The log of that, of course, this will just cancel it out. So we'll assume that all logs here are natural logs. So now I just have Y sub nK. So now Y sub nJ, where J is just whichever class index I'm kind of focused on at the moment, is going to depend on V and W. So that's going to depend on V and W. So that's going to depend on V and W. So therefore, the log likelihood of V and W with respect to V is going to be just the same as above, except now I have the partial derivative of Y sub nK with respect to individual weights in V. And I do the same thing just with respect to W. So now here at the end, the only thing that changes is going to be just what happens in the denominator of the partial derivative. And so now the log And so now the log likelihood of V and W with respect to W is going to be just the sum for the error terms times the partial derivative of Y sub nJ with respect to W. So we've already calculated these two things, it turns out, in the previous neural network lecture. So we already went through the derivation of how we calculate the derivative in order to do backprop. So this is in the training by gradient descent section if you need to review. Again, eventually we'll see this in Python, so this is all getting a bit sorry for you, I don't think you need to worry. So when you compare the above with the derivatives of the mean squared error in regression problems, you can start to see the parallels. So here E, this is going to be our error term, this is the thing we want to minimize in a regression problem. The distinction between that and log likelihood is we're trying to maximize the log likelihood, but the operations are broadly the same. So this is the formula for squared error. And so then below that we see the derivative, the partial derivative of the squared error with respect to each of those weights. So you now start to see the parallels between these. So here we have my absolute error term, and I square that to get my root mean squared error. And then here I just have sort of the class relative error term, I'm not sure that's, you don't really call it that, but this is the error term in terms of my probability, and then I take that and subtract myself next. Yes. Is there a reason why we are not creating some of all the classifications of the large partial derivative of the- In this one? Yeah. The partial derivative of that is pretty much the same than- Right, yeah. So what we do here is we're basically taking for, this is going to be some class, specific class K. This is just going to be for a arbitrary class of J. So I'm just trying to compute the partial derivative of Y sub nJ for this particular class, which may not be the actual class of the ground truth class of the output. And so I'm just trying to compute the derivative of the log likelihood with respect to the weights that define this class. Same for the regression. So we can just compare these to the actual likelihood, and we basically see that there's a strong parallel between the regression version and the classification version, just by recasting the error in terms of log likelihood, where now my prediction is basically the softmax distribution over different classes. So the previously derived matrix expressions for neural networks, we can just use those as we have been. All we need to modify is the output calculation. So this is squared error, right? So standard first two lines are my kind of standard neural network operations. So I take my inputs times hidden layer weights, apply some activation function, take the output of that times the output layer weights, it gives me Y. So now I have all my T's minus all my Y's. I'll use that to compute my error term. And then the derivative of the gradient of the hidden layer weights is going to be the error term times the weights. Then I'm going to multiply that by the derivative of the activation function from that layer, then multiply that by the input. And then the gradient for the output layer weights is basically much more straightforward. All I have to be concerned about is the actual error. So now the changes needed for nonlinear logistic regression follows. So T, I is going to be the indicator variable version of T. So remember, this is going to be some n by k matrix where there are all these one hot vectors. So the first two things are identical. So take inputs X, multiply them by hidden layers, take an activation of that, then take the result of that, multiply them by the output layer. This gives me some scalar prediction. So far the same. So now what I do is to exponentiate that scalar prediction. This is going to give me some other scalar value. And then I just sum across all the columns and then divide by that sum. So these three lines, that didn't work out well. F, S, and G, that's the softmax calculation. So I take the exponentiation and I divide that by the sum of all the exponentiations. And so then the log likelihood is going to be the sum of the indicator variables times the log of the softmax. And now these two, or these two lines that are partially highlighted are effectively the same as above. The only difference here is instead of the scalar prediction Y, I have a softmax prediction G that is a probability distribution. And I still use the same activation function derivative to calculate the gradient. And then the only difference with the output layer is I just have my softmax probabilities and subtract those from the indicator variable. So yes. Just in the very last layer. So you can think of it as like neural network is proceeding as normal. And then at the very end, I just decide, well, I don't want a continuous value. I actually want a probability distribution. So I perform this softmax operation that turns that into a probability distribution. So now what that means is that the error term is literally just, it's still a distance metric. It's just now for every, the ground truth for every indicator variable is basically going to be a bunch of zeros and a one. Think of that as probabilities instead. It's basically 0% except in one case where it's 100% because we know this is the class for that sample. And so now I'm just trying to maximize or trying to minimize the distance between my predictions and that ground truth. And that only makes sense if I'm also predicting probabilities. I need to have some functions going to turn my output into a probability. When you graduate, they will call you begging for money. Okay. So how do we do this for two dimensional data? So let's just try to create some two dimensional data and then try to separate the distinct segments using a nonlinear logistic regression. So I'm going to use kind of a similar example to this one. So if you remember from when we did regular activation, I kind of had this chart where we had these sort of two curves. And I gave it as an example where you can combine some nonlinear activation functions to fit a curve to it. So if you get a tanh function, you kind of want something that rises and then peaks and starts to fall. For ReLU, you're going to get a more piecewise curve, but with enough ReLUs, you can try to fit this pretty approximately. So let's do something similar except not trying to fit a curve to this data. I'm trying to fit a curve between sections of data. And this curve would be an example of what kind of function. Decision that's not a bad choice, but wasn't quite what I was going for. But when you make a decision, what are you doing between choices? Getting closer. Like the first five letters are correct. Discrete, yes. We call this a discriminator function. Yeah. So basically what I'm trying to do is I'm trying to find a line that's going to keep as much of the blue dots on one side and as much of the red dots on the other side. So I'm trying to find a line that's going to keep as much of the blue dots on one side and as much of the orange dots on another side. And it's not going to necessarily be perfect, but you can see sort of as I just trace my mouse kind of like that, this might be an example of like a suitable discriminator function. So I'm going to make some two dimensional data this time that has similar properties. So we have this thing, looks like a Tide pod. Don't eat it. And so now obviously just by looking at this, you can tell that like there's no way that I can fit a linear function between these areas. There's just no such linear function unless I add a third dimension where each of these are like distinct along some z-axis or something. And in fact, it would be pretty difficult to just fit an average kind of deformed linear curve to this. What single curve can I draw between regions? It's going to separate all my points, nothing that works very particularly well. So what we'll try to do is we'll try to classify this data using a five hidden unit neural network with nonlinear logistic regression. And my goal here is to basically separate the portions of the Tide pod. All these individually colored points are taking the instances of a different class. And so I need to be able to classify them accordingly. So you can think of this now as clearly a problem that has to happen in multiple dimensions. I've got three classes. Just by looking at the data, we can see that if I'm restricted to working in this plane that you see here, I cannot do this, even using like the most nonlinear function I can compute. So we need to have multi-dimensional data. So what I will do is I will now define a new class called neural network classifier. And I'm going to do this by subclassing the existing neural network class and making relevant changes. So this is where code inheritance becomes very useful. I'm no longer trying to just change the activation function of something. I'm actually trying to change the purpose of the network from a regressor to a classifier. And that's going to require more than just specifying some arguments. I could do this. I could say like neural network and then pass type classifier, but that it would end up probably duplicating more code that is necessary. When in reality, as we've observed, kind of all I need to do is change what happens at the very end of the network. So I'm going to need to define a softmax function. I'm going to need to define my back propagation functions to use the output of that. So I will import a neural network class. And then what I will do is I'll define an implementation that it's already in this neural networks implementation here. Neural network classifier, the subclasses from neural network. This allows me to specify the input size and then the hidden layer size in this case, just a single hidden unit with five or can layer with five units and then three for the classes. And so now I train, I specify my number of epochs and my learning rate, and then I can plot my outputs and then I can plot my likelihood function and my training over time, et cetera. So let me just run this and it'll take a few seconds to run. Like last time, because I'm subclassing from neural network and I changed the functions necessary to turn this from a regressor into a classifier, I didn't change the print functions. So it's still printing error, but you notice that number goes up when in fact, as we've done before, we're just actually printing the likelihood. So this is the likelihood of the data. I'm getting a pretty good number. And so here is my, this is my training line. So I'm going to use the number. And so here is my, this is my training likelihood plotted versus training epochs. You can start, you can see how we started below 0.7 and end up getting quite close to a 0.95. And here is the data again. And here's what it predicts. So you can see that it's doing a pretty good job. It's fine. It seems to have identified that that outside region belongs to the blue class. And this kind of top region belongs to the red class and the bottom region belongs to the green class. And it's also doing it in a way that is effectively capturing kind of the shapes in this data. Not too bad. Obviously it's going to make some mistakes like here. These green samples seem to be predicted as members of the red class, but overall it's probably doing a pretty good job. And then there's some white space here that is just sort of filling in somewhat arbitrarily, but it just, it doesn't have data for that. So what we can now do is we can plot this in multiple dimensions to get a better look at it. So this is the distribution, probability distribution for the red class. Right? So you see when I have samples that have these XY coordinates, the probability of being a member of the red class is given by this function here in three dimensions. Similarly for the green class, you can kind of see how this one and this one sort of fit together. Right. And then there's the blue class, which has this big ring on the outside. And then it's the, it descends to zero probability in the center. Okay. So now you can see how with this two dimensional data, we are basically fitting a three dimensional function to model this highly nonlinear class distribution. So if you want to plot the outputs of the hidden units, you can actually see what it is learning. So you can actually see like for a given input, what one unit is going to output. And you'll kind of see how those curves, which also could be visualized in three dimensions when combined, should allow you to predict this probability distribution. Okay. Questions so far? Yes. There is a reason why you just show this one hit in there. Yeah. In this case, this problem is like simple enough that that can solve it. So you can, well, you can't really run this. Let's, you know what, let's just do it. Right. Let's just change the hidden size to just try 10 for now. And let's see if it does better. Right. We can already see that we're doing slightly better at maximizing the log likelihood of the data. So here's my prediction. Maybe it's like a little more symmetric. Okay. And here's that. Right. So same things. We can try, one thing that might be interesting to note here is actually, you see there's a bit of a dip there. It's kind of hard to see, but we've basically got some place where the probability of falling into the green class is not as certain as it is in some other regions, but it's still higher than anything else. So if you look at this part here, it doesn't, this chart doesn't show the probability, but it's probably, it's at least greater than 0.3, 0.33, but maybe it's not that much bigger than that. We can try, you know, making this a bigger network. And this might end up, you know, fitting even better to it. So it looks like it is so far, right. Getting pretty close to fully maximizing the likelihood. So again, this is probably the best fit we're going to get to this data. And then we look at our shapes, right. And this is like close to as exact a fit as you're going to get. And so in this case, that single hidden layer was like just fast enough to train on this machine, even on the CPU. You can have a slightly bigger network and it's, you know, fitting slightly better to the data. So that was just a function that is nonlinear enough to capture this effectively, but also just really fast to run. Okay. Any other questions? Okay. So let's try some actual data. So let's finish the toy problem, move on to some real data. So we have this human activity data given in from accelerometers. So basically, there's a bunch of data where people had some accelerometer, it might've been like a smartphone or a smartwatch or something. And then based on how people are moving over time, the goal is to classify what kind of activity they were participating in. So am I climbing stairs? Am I playing tennis? Am I running? Am I jogging? Am I, you know, just resting? Am I eating dinner? So basically, this, the name of the files, just accelerometers, it sort of obscures what's being done. But basically, based on how I am moving, can you tell what I was doing? So the classes include things like walking, playing with Legos, playing Nintendo Wii, climbing stairs. So X is the motion data and then T are some class labels corresponding to the activities. So what, how do we define class labels for the data? How would you do that? Yeah. Each class can have a different integer value, right? Yeah. You have to encode that in some way. Does it, what's your intuition about, like, does it matter if I put like more similar activities, like numerically closer together or not? Does it, yes? Who thinks yes? Okay. Who thinks no? Okay. So generally, at least for these, for classes, classes problems like this, generally it doesn't matter that much, because you think of them as vectors. The vectors are kind of orthogonal to each other. Say I've got 12 classes, you can think of just like 12 dimensional unit vectors, each is orthogonal to each other. And so basically what it is, is I am, if I'm not doing one thing, I'm doing something else. There's no real overlap. So that is, I can just decide arbitrarily, like walking is zero, playing with Legos is one, eating dinner is six. And it doesn't really matter because the probability distribution, the ways should optimize for that label set. Of course, if I change the label set, then those ways are going to be completely wrong, right? So it's going to be very dependent on the label set that you choose when you train. You have to make sure that you're evaluating against the same label set in the same order. Where this falls apart, of course, is problems where similarity matters in the final output. So in particular, in like language problems, it doesn't make a lot of sense to have the word puppy be equally orthogonal to the word dog as it is to the word truck, right? Because obviously two of these things are much more similar to each other than either of them is the other thing. So in problems like that, you have to have more sophisticated ways of representing your classes. But in this case, we can just assume that they're arbitrarily chosen. So here we have 225,000 samples, each of them has four outputs. And so then data looks like this. So we can see what do these look like inputs and which of these look like outputs? Yes. First column looks like an output. We just talked about how these are integer class labels. So this looks like integers. So I'm going to guess these are the class labels. And then these are continuous values. So it seems like I can take my first column, turn that into my T, and then the remainder will be my inputs. So here's a function to generate k-fold cross validation sets where we talked about cross validation. Anyone remember what that is? If you do, it's going to help you with assignment three. For each of the part of your sample size, you can actually test the model that you created on the right. Yeah. So I'm going to do this for k times. So I'm going to have each time going to hold out a different set and then rotate that through so that each time I'm training on k minus one fold is evaluating on the k-th fold. And this will tell me a decent average picture of how my model can be expected to perform on arbitrary unseen data. So here's a function that does that. I will randomly order x and t, partition them into folds, and then I will basically return each one for x train validate and test. So if I do x dot shape, I end up with this number of samples times three, three dimensions for each one. I'm using this yield keyword at the end. What is that? So this is basically something that suspends execution and then returns the current state back to the caller, but it will retain the state information so I can continue what I left off. So the return keyword will just exit the function entirely at that point. Yield will basically say, here's what I've got right now. I'm going to return you some values, but if you want to keep executing, I'm going to maintain my state information. This is kind of a functional-like operator in Python. So it's sort of like the continuation operator in Haskell, for example, if you know anything about that. Just a demonstration of how that works. I can have some function here that is just a times two function. It's going to return i times two for, you know, in range of i. So now if I turn, if I print out list of times two, it's going to give me i times two for zero through nine. But if I just return the result of the times two function, actually gives me this generator object. So what's that? So the generator object allows me to basically call this next function over it to prompt it to return whatever it's going to yield at the next step. So remember, we're keeping the state information. So it's basically, was the last thing that I returned, and then I'm going to continue where I left off. So if I call, if I return my generator function, it's basically an instance of the output of times two. And so now it actually, to actually get into that, I have to call next over it. So if I call next z the first time, it's going to give me the first item that times two would return, which is zero. Then if I call next z again, same thing, right? I did not change the syntax of this call at all, but now it's returning two, because z is an instance of this function that preserved the state information because I use the yield keyword. So using this in my K4 cross validation sets means that I can then just call next to basically get the next cross validation set. It's going to segment everything just right, return my train val and test partitions. So now the size of these things, you can see I have 75,000 samples by three and then by one. And then these are equally partitioned right now. So I now have a train validation and test partitions, all of equal size partitioned into different folds. So now I'm going to call the NP, we're going to use this unique function. This is going to find the unique elements of an array. So if I run unique over t train and with this return counts keyword, what this is going to tell me is what are the different elements of t train and how many times does each of them exist? So now we can see that I've got 10 classes, they've got labels on them one through 10, and I have roughly 7,500 of each. So this is a decently balanced sample. So there are 10 classes for each class K and then there are K prime instances of that instance in our data. We can also control how many digits after the decimal point are printed for a NumPy array. This will be important for doing the confusion matrix. So I can set my precision to five. And so now if I just divide the counts by the total shape of t train, so basically this is accounts for class divided by the number of samples, we can get the percentage of the data that falls into each class. And so you can see that's roughly 10%. So this is a well-balanced sample. We can do this also for tval and ttest. And we can see we have roughly the same distribution across all of them, which means that for cross validation, we can expect that this result is going to be relatively accurate. So all these steps are very important to do if you are performing an evaluation on an neural network. So let's see where we at right now. So what I will do now is I'm going to construct a neural network classifier where the input layer is of size n, where n is the number of parameters in every input sample. In this case, that's three. At step two, a miracle occurs. And at step three, we get the answer. So what we do here is I'll just define my neural network classifier that we did before. I'll call train on it. All that stuff happens. And then I will just see how long it took and I'll plot my training likelihood versus the training time. Any questions on this while it's training? Yep. So the reason why we're not using one-hot encoding is because we're using the neural network. Is that right? Sorry, I guess I sort of elided this because I didn't show you the neural network classifier code because you have to write that yourself. One of the things that it does that you will have to do is it takes those integers and turns them into one-hot vectors. So you'll see that if we look at the definition of the class, so what's the input size is going to be that second dimension of x dot shape. So in this case, three values for each input. This is the hidden layer size. And then n classes, this is the number of outputs. This is the number of things I want to predict about each sample. In a regression problem, these would be n scalar values pertaining to something about that sample. For a classification problem, the thing I want to predict is for every class, I'm going to predict the probability that it falls into this class. So in this case, this would actually have an output size of 10. So that did that. So it looks like it is pretty, it's decently maximized the training data likelihood. So for classification problems, you want to see the percent of samples that are classified correctly. And then it's also interesting to see which ones are misclassified. So you can see if maybe there are some classes that are being confused for other classes more often than not. So you think of this as basically a grid where you have along either the rows or the columns, you have your predicted classes. And then on the other one, you have your true classes. And so those things along the diagonal, which for a perfect model, you basically have 100% along the diagonal, as we'll see. So this is called a confusion matrix. And so this is just like a table of classification percentages for all the target classes and predicted classes. So in our version, the row is going to be the target class, the columns are the predicted class. This is not universally true. I have seen cases where people have flipped it. So just be careful when reading confusion matrices. This is typical. I believe this is more common. And so then the diagonal is going to show the percentage of samples that are correctly classified for each target class. And you want it to be as close to 100 as possible. Yes. So this training, it seems like it's been a lot longer than what we've seen in the measures before. Is that due to just how many samples we have or is it more due to the neural network? In this case, it's probably due to the number of samples because there are 75,000. So the data here, it's three dimensional data. So yeah, it's like an extra dimension than say the Tide pod example we did before. So it takes a little bit longer there, but mostly it's going to be the number of passes through the data. So you have to pass through the entire data set to 75,000 samples versus, I don't know, a couple of hundred probably for most things we've done before. So you wouldn't expect it to be if the samples were the same sample size. Yeah, no. I mean, if you're dealing, yeah, so the thing about the number of numbers that it has to pass through, it's going to be n by d. So the longer the input dimensionality is, of course, the longer it's going to take to go through each sample. And then the more samples you have, the longer it's going to take to go through the entire data set. Yeah. Okay. So we can then just run the use function and then we'll end up with a basically an n dimensional array showing all the percentages. So this will convert the predictions into percentages. I'll just spend that to table. I'll print the table. Of course, this is not very useful to look at. It's just a bunch of numbers. If you can mentally conceptualize how it's being arranged, you can see there seems to be a good percentage for that first class and maybe a less good percentage for that second class. But let's arrange it in a way that's actually easy to read. So if I put in a pandas data frame, I'll put some headers on it. So now we can have the class names and their indices. So now you can see the class one is rest coloring, Legos, we tennis, we boxing movement at various speeds and climbing stairs. So now we put these into a data frame. I can print this out into a nice grid. It's now you can see by stepping down along the diagonal, which samples are, which classes are well classified and which ones are not. But of course, this is still not like the best way to look at it, right? It'd be really nice if I could get an immediately intuitive grasp of what this is showing you rather than having to analyze all the numbers one by one. So first of all, let's convert things to percentages. So you can look at this tutorial on pandas styling to help you with your data presentation. If I can convert things to percentages, now all of a sudden it is quite a bit easier to read. I'm no longer staring at so many decimals. Now the above function call doesn't save the style and the confusion matrix. So if I run this again, it's going to give me the decimal version. So I can add colored backgrounds to provide a quick visual comparison. Using CMAP equals blues is going to give me this nice, this color scheme where I can immediately look for the dark cells. Now, one thing you'll notice here is that it basically the darkest cell in each column is the darkest one. And then everything is graded relative to that. So what that means is that this one, we tennis, where it's only classified correctly 3.5% or 3.8% of the time, but that is the highest value in this column. So everything's normalized relative to this. So this is the same color as rest being classified correctly 96% of the time. So now we can combine these two styles in an object-oriented fashion. So now if I run this one, now we get the percentages. So one thing that we will want to figure out is like how to normalize this. There's some things you can do in Python to do that. So now I'm going to try a bigger network. I'm not going to run this live because it'll take a long time. So I'm just going to start from here. I'm going to kind of just skip through this. But you can see over a bigger network, I now have two layers with 100 units and 50 units. This is the data likelihood. So you can see that as we train this bigger unit has some fluctuation in that likelihood. So if I'd stopped training here, maybe it wouldn't perform as better, but eventually it kind of stabilizes. So now if I run this again, pretty confusing matrix, you can now see that at least the correct class is the one that is classified correctly most of the time for all classes. They're not all equally classified correctly. For example, stairs is only correct 30% of the time, whereas rest is still correct. 96% of the time, coloring is 85. So now let's check the validation and the test sets. So the validation percent correct is about 57%. But we see kind of a similar distribution for all the different classes. And this way you can see which ones are commonly confused. And this makes quite a bit of sense. It may be difficult to tell from accelerometer data whether I'm moving at 1.75 meters per second or 2.25. So I'm going to check the validation and the test five. Blah, this is going to depend on my height, my stride length, maybe the type of shoes that I'm wearing. So by looking at a confusion matrix, you can see which classes are commonly confused and maybe consider ways that you might process the data to make that easier. So confusion matrix is kind of the most common way of presenting multi-class evaluation for most machine learning problems. Okay, so then same for the test percent. Basically, we're seeing very similar trends in terms of overall accuracy and also prediction between different classes. So we've got some issues here. So for example, obviously some of the really easy ones like rest and coloring and even Legos, it's getting right, you know, most of the time. Wee boxing, there's probably like a relatively distinct movement pattern that is picked up in the data. But kind of the walking, jogging ones and even climbing stairs, the overall accuracy is just not that great, particularly for like class eight and class nine. So what if there's a different data representation that we could use to represent movement? So we can use this thing called a continuous wavelet transform. And here is some code that's going to apply a CWT to this accelerometer data. I'm not going to go into this, but basically what it's doing is just converting this into a waveform that's going to approximate the frequency and amplitude of the motion. So imagine if I'm walking really fast, right, imagine that accelerometer like hitting the floor or the sidewalk, you know, every time my foot touches down, there could be like a pulse every time I take a step, right? And so the frequency of that pulse is going to be correlated to say like how fast I'm running or whatever my pace is. And then the amplitude is going to be like how hard my foot is hitting the pavement. Whereas if I'm sitting there playing Legos, you know, sitting on the ground, I'm going to get a different pattern of movement that's going to have a different frequency and a different amplitude. This is a useful technique for transforming this raw accelerometer data into something that's maybe a little bit more intuitive. So if you are familiar with CWTs, you can look at this code and apply it maybe to some of your own data. Okay, so if we just look at some of the different properties, basically we can see, you know, we can define a max frequency and then we can see for the different samples, you know, what some of the properties are. Might be easier to look at this in terms of a visual chart. So now what I can do is I can actually plot some of this and you'll only see like three distinct regions here because there's actually 10, but it keeps repeating the same colors and they're kind of all plotted on top of each other. So if we look like real close, you might be able to see some other ones, but not really. So if we now look at the individual samples, luckily they're ordered. We have about 7,500 of each one. So you can see, okay, these are the samples of class one and class two and so on and so on. And so now I can plot the frequency for these three dimensions for each of these. So if you look at instances of like class, what, five, I guess, I think this is like one, two, three, four, five. So this is we boxing, right? And so these, you can just go down below this and you can see the amplitudes for that. So this particular class seems to have quite a bit of distinct frequency and amplitude compared to the other things, right? And now let's look at the ones that got confused, right? So these were sort of fast movement, maybe running or jogging. If you look at these segments here, there's not a lot of difference between them. So it kind of makes sense maybe that these were getting confused. You'll notice that like there is a slight difference, maybe the amplitude in Y for this class is slightly less than that for the next class. So maybe this representation will allow us to tease that out a bit more. So just think about how you would move in terms of doing each of these activities. And that's reflected in the frequency and amplitude of the signal. Okay. So now I'm going to take this representation and I call it my CWT net. I'm not going to train again because it takes like a minute. And then I will run a prediction and then I'll see how well this is doing. So already you can see, if I look at test percent correct, 92% compared to like 58. So clearly this data representation is a good way of representing this for this task. Now look at the confusion matrix. Wow. So really nice numbers there. One thing that is maybe not coming through very closely is coloring is often misclassified as rest, right? Because if you look at it, it makes sense because of if I'm converting my data to frequency and amplitude and maybe it's going off of like the accelerometer data from my phone in my pocket, there's no real distinction in frequency or amplitude of motion associated with coloring versus rest that would be picked up by an accelerometer. And that CWT transformation may be squeezing out data that was useful for that. So we had some cases where like coloring, that percentage actually goes down right now. It's like 50 something before it was 64, right? So maybe we lost some information here. Okay. So that was classification. Basically key points. Everything gets converted to a probability distribution, but when using neural networks, that all happens at that very end of the operation. But also the way you represent your data, it can make a big difference. And this is true for all kinds of neural networks from just how you're encoding the classes to how the inputs are represented. And so you have to think pretty hard about how you want to represent your data for a neural network classification operation. Okay. Questions. Okay. So before we go, let me go through assignment three. So the purpose of this assignment is to build an implementation of a neural network in PyTorch. So you're going to be doing a lot of the similar things that you did to assignment two, but you're going to be doing it the PyTorch way. So what you will do is once you've done that, you're then going to conduct some training experiments on some data, and you're going to be doing this using cross-validation. So notebooks, you're going to want to look at eight for the PyTorch stuff. The cross-validation notebook whose number I don't recall, I think maybe it's seven, though it might be nine. And that should cover most of it, I think, so basically you're going to want to review PyTorch implementations and cross-validation. So we have this N net class. What you're going to need to do is you're going to complete the train and use functions the PyTorch way. So you cannot just copy and paste your A2 code, although the principles remain the same. So basically you've done it in NumPy, but NumPy is limited by the CPU. So let's move to PyTorch so that you can make use of the GPU for more substantial operations. So you're given definitions of, say, the activation functions. You can do tan h array that's done for you. What you need to do is you need to complete the train. So you need to calculate the forward pass, calculate the mean squared error, take the gradient. Remember in the PyTorch method, most of all of these things just happen in single lines. So you'll find, if you find yourself working out the math for things, you're probably on the wrong track. You've done that already. So now you need to do this using PyTorch's built-in functions. So what you need to be careful are things like zeroing the gradient, making sure that you're detaching things from the computation graph, et cetera. Same for the use function. So same operations, right? You need to standardize x through the forward pass and un-standardize, but again, you're going to need to do this the PyTorch way. This thing here, if you run into errors about the computation graph, just review that line, save you a lot of trouble. So for an example, we'll give some data like we did before, run it through your implementation of NNET, and then calculate the RMSE. If you do it correctly, your plot should look like this. So then you then need to perform experiments over actual data after performing stratified cross-validation. So we give you the complete code for generate k-fold cross-validation. So you don't need to change anything with that. And then here's some example data using just some dummy data using this function. So you can compare your outputs to this. And now you need to train, create a function that will train the neural nets and then average the RMSE over all the ways of partitioning. So you have to do all your cross-validation, take the average, and then report that. So you have to find this function that's going to do things like define an instance of neural net and then generate k-fold cross-validation sets, and then retrieve the output and report it. So basically, this is the same as you did in assignment two, except you're calling the cross-validation function and you need to call your PyTorch neural net instead of the NumPy one. So the application is going to be to this airfoil data. You can go to this website and download it. And then it gives you these parameters like frequency, angle, chord, pressure. So you need to apply your run k-fold cross-validation function to the airfoil data. X will be the first five columns and T will be the last column pressure. So basically, trying to predict pressure using these other five things. And below, you'll find an example run over some real data. So you need to, again, collect your outputs into a data frame where you report the architecture and the RMSE for train, val, and test. And then you can plot the results. Okay. So this is a much more coding-heavy assignment. So you'll score most of the credit if the train use and run k-fold cross-validation functions are defined correctly. And then you can test this with the greater, same as before. So unlike this one, you need to complete this one individually. So A2 in the final project, the only thing you can work with a partner on. You can earn 10 extra credit points in this assignment. So there's significant advantage to doing that. One is to add a keyword argument that will allow you to use the GPU. You have free access to any of the CS machines. Then if you need to use those, if you don't have a GPU in your own. And then also you can find another data set and apply this to that data set and report on your conclusions. So you will get five points for doing one and five points for doing two. You can elect to do one, but not the other, if you have the time or lack thereof, or you can do both. And this is due March 21st. All right. Questions? Okay. All right. We'll see you Thursday. Yeah. Yeah. Yeah. Yeah. 11 and 12. Try again I think I had to. I had to revoke access because they show the answer to a two. So I put I should have put that back. Try. Try reload the page also try a different browser because then as it saves it in your cash. All right, let's go ahead and get going guys. So let me just first brief announcement about the schedule. So it looks like I'm going to be gone. This, or is it these two days, so 28th and 30th of March. So I'm going to be in California to conference. What we probably I had to do this once before. Last time is because I went to Korea and I had no choice this time might be able to do remote lecture. So I think it's only be off by one, one hour I could, we could just do like all online those two days. The other option. And this would only really be the case if my paper presentation happens to fall at the class time, and I can also ask them to move that. But there's also possibility just like me recording the lecture and you're watching it at your convenience and then I'll put like a discussion board up for that lecture so you can post questions that worked pretty well in my grad class last semester when I had to go to Korea so one of those two things will be the case. So it depends like 30th I may be traveling so like possibility of me just giving lecture like from the lounge in the Denver Airport. It's gonna be interesting. And then 28th, I, there's a small chance that my presentation will will coincide with class up just a heads up so basically don't bother coming to the room on those days I suppose that's what you need to take away from that right now. I will, I will, I will, about what the plan is. So just luckily I'll be able to introduce reinforcement learning in person and we'll be able to do the the two clear learning in person as well so like what is these two, the two middle lectures will be online or asynchronous I'll just do it anyway. So, hopefully that works for everybody. Make sure that we get get through the content on time that I still have to darn I have to go give a conference presentation and get to travel. So that was the only real announcement right now so I will start convolutional neural nets if it's. Well, I don't care whether you want me to or not I'm going to start convolutional neural nets. I don't care about your opinion whether you want to or not this is what we're doing. Okay. So you probably heard the term convolutional neural nets. If that brings to mind computer vision tasks you're not wrong that's what these things are mostly used for release where they sort of first prove themselves so first I want to be able to introduce you to this data set called M list that you also may have heard of. So this is just like a bunch of handwritten digits that's because that's what it is. This is the modified National Institute of standards and technology database. So basically what it is, is, this is actually a variant on the original quote NIST database so MNIST modified NIST with it's basically just a bunch of handwritten samples of the digits zero through nine obviously. This is commonly used for training various kinds of image processing algorithms so the first kind of neural perceptron image processor that was developed I believe at MIT. I'm not sure if you use exactly MNIST digits but it was a digit recognition task. So this has been pretty much a common factor in neural machine learning from the very beginning like even when neural nets have to be run under giant mainframe and really weren't very powerful. And then when we reached about 2010 2012 and we got the hardware processing power and the data to make these things actually scale, then MNIST suddenly became sort of your go to go to set. So even now, you know, vision paper is well sometimes publish you know a demonstration on MNIST and then also it's very common for say, kind of the more like NURBS tile, like optimization papers will demonstrate on MNIST because it's very well known and simple problems you can kind of demonstrate your optimization on these sort of saddle optimization problems you try to find, you know, an optimal point in a highly nonlinear function. And then they'll go demonstrate things on MNIST clustering algorithms, they demonstrate an MNIST. So we will also be making quite a lot of use of MNIST in this class because it's a very nice introductory database. So the original MNIST database was taken from employees of the American Census Bureau. And then the testing data was taken from American high school students. So basically you have you train on, you know, government employees, and you test on high school students so you can you can kind of see like how the testing data like should probably resemble the training data. Because by that your handwriting is more or less similar than high school. And so then you have adult handwriting as the training data you evaluate on maybe young adult or youth handwriting, but not like, you know, kids handwriting where they're like drawing with crayons and holding the crayon and like a full fist. So what MNIST did is they basically took the black and white NIST images and they just normalized them to fit in this 28 by 28 window. So 28 by 28 is 784 I think. I didn't just calculate that in my head I have been memorized from before and I might be wrong. So we have seven basically 784 pixels, and then the anti aliasing did. So just to zoom in, what I mean by anti aliasing is you kind of see how the lines are not jagged right there's there's a little bit of blur there so the anti aliasing is basically just a thought smoothing technique. So, this turns a black and white image into a grayscale image where around the edge of the image there is some not fully black and not fully white pixels. And so then that and give us this, this modified MNIST data set. So, like I said MNIST is this kind of default teaching convolutional neural net data set various types of computer vision systems. In this particular lecture, we're not going to go so much into the training, but into the intuition behind the convolution, what that operation is, and how we can actually extract features using this type of operation. So, let me see. Talk about the ways I want to see if I can actually talk about sort of the fundamentals of the vision system in the notebook text looks like I don't. So I'm just going to riff on that for a little bit. So, the convolutional neural net of course is basically this kind of common computer vision backbones let's talk about how humans do vision, and how that intuition translates to a computational setting or how it doesn't so basically your, your eye does not have to be fixate on a single point. That's actually a very difficult thing to do. There are two common types of eye movements in humans and other like predators, and there's basically saccades and smooth pursuits. So saccade is basically if you're looking at a fixed frame effectively, then your eye tends to kind of make these involuntary movements across the frame to pick out relevant features. So if I look right ahead, my eye will probably naturally drift towards things that look like faces so every even though I'm staring at the wall right now. There's nothing really interesting there. My eyes naturally going to pick up, you know, your head your head your head your head because these are the things that show up in my periphery that are likely to be relevant. So let's imagine that I am instead a hunter on the African savanna, and I'm going after a gazelle right so what I would do then is, I basically fix the point but that point is moving. And so this is the smooth pursuit this is the other type of movement that you're that your eye tends to do so I don't necessarily have to move my head to follow this this thing that I want to track my eye will do it for me. So what that means is that for mammalian vision systems and things like birds are different with talk on mammals for the moment. The eye is naturally drifting across a scene, just involuntarily, trying to pick out relevant information. So just think of the optical flow of your eye there's just this whole firehose of information coming at you at any given moment, you can't pay equal to anything that's coming in even if you even if you were able to represent things as pixels. You automatically do this filtering, you try to you pick out things that are informative. And so the idea here in convolutional neural nets is if you want some mechanism that the computer will be able to use to pick out important information at a low resolution. So if we look at say, how many weights we would use to classify this using a fully connected net. So let's say if we have, you know, 20 hidden units in one layer and 10 units in the output layer, then each pixel is going to have a weight in the unit. And then each unit has 24 by 24. So that's so many for us or 20 20 by 20 is a 704 plus one for the bias that many weights. So the hidden layer is going to have 785 times 20 individual weights, and the output layer is going to have 21 by 10. So what does that equal? Well, it equals 15,910 when you add them all up. Of course, if I had a bigger network, that would be that would grow exponentially more. So this is a lot of weights to train to try and classify it, you know, 28 by 28 image. So you can imagine if I flatten this all out into a 784 plus one vector input vector. This is still that's like that's like a whole lot of weights to classify this one sample. So let's think about how this number can be reduced. So large numbers of weights require more samples to train. So if I have 15,000 weights, I'm going to need a lot of these 785 dimensional inputs to actually achieve reasonable performance. So what if we could follow some intuition from the vision system and basically just provide every hidden unit with a part of the image. So now imagine that behind my eyes, I have some my cortex. It's got a bunch of connected neurons in some fashion, we'll just assume that I'm going to use the brain and the computer is basically analogs at this point. So then I have all my optical information coming in, and I can have individual units that are effectively specialized to pick out different things. So for example, I could have things that are specialized to pick out like certain colors and maybe here I'll be able to recognize the contrast if you're wearing like a dark jacket against a light background, then I may have things that optimize like pick out human faces, right, because this is a common feature that I need to attend to. So in all the things that come at me, this would be my training data set, I need to have different specialized units to pick out those individual relevant features. And then I can combine them into things. So okay, I see a human face and then I see human limbs and legs and feet. So this is probably a human, I could have different neurons that effectively activate on seeing those different things. And then you put them all together, in this case, just using some sort of linear sum, and it's like, okay, well, this satisfies this checks all the boxes of features that I need to identify something as a human, so it's probably a human. So that's basically the intuition that we're trying to go forward the convolution neural net is trying to be more efficient than just processing the entire optical flow effectively one pixel at a time. So we can take part of the image, let's say a little patch, maybe 10 by 10. And then we can assign each unit a random patch. So then we would have 100 plus one by 20 plus 21 by 10. So that reduces our number of weights down to 2230. And so now we're about one seventh of the original number of weights. Now the problem is that if I slice my image image up saying to 10 by 10 patches, and I stack them side by side and they have 10 by 10 pixels that are being fed into one unit, and the adjacent 10 by 10 pixels that are fed into another unit. What if that split goes right down the middle of some interesting feature, and the left half and the right half. When viewed in isolation are not actually all that relevant, or you can't, you can't detect through all that relevant. Right. So it may be that just seeing the left half of something on the right half of something is not going to give me meaningful information I need to see both of them together so I need some sort of overlap I need my 10 by 10 pixel, or 10 by 10 patch my other 10 by 10 patch, I need some other patch that kind of straddles them. So instead of just stepping, you know, discreetly across my entire image. I really want to do kind of a slide. And so this is now replicating something closer to that psychotic movement that the eye does in that the eye is darting around but it's not just fixating on like an individual square in your visual field. So in that moment that it slides is taking in that information. Right. So if I'm looking at someone's face, and I'm kind of looking at one eye and then I moved to the other eye. At the same time also to get you know information about say the shape of their nose or their forehead or things like that. So now I want to try and replicate this. So, you know, you can add more units to cover more parts but that the way the number of weights starts growing again. So now we have this optimization problem of, I need to cover all parts of the image, and I want to do it without growing the number of weights. So we have the sort of dual constraints that we're trying to try to cover here. So, can we figure out a way to cover more parts of the image without increasing the number of weights in each unit, and the number of units. So obviously, should be no surprise the answer is yes. So what we can do is we can take a hidden unit that will receive these 10 by 10 image patches, and then apply this unit to all of those 10 by 10 patches from a given image. So now we have this lens that just seems like a 10 by 10 patch. So this can be shifted around the image sort of like looking through a pinhole camera, and I can move it around the image and pick out individual features as it as it's going. And each of these outputs can then be reassembled into another image, and that quote image should have some level of interesting features about the, about the original image. So if we have our 10 by 10 unit, this would result in basically the 28 by 28 digit image will have a two by two set of 10 by 10 non overlapping patches, and then eight pixels left over on the right, and at the bottom. So this is a hidden unit to these four patches and this produces four outputs that can be arranged into two by two matrix. And then each of these outputs represents how well that pattern matches the intensities in each patch so now I have basically this filter that has some weights in it that will optimize or that will activate at a certain level when it encounters certain types of features. And so if I see a part of the image that just really kind of aligns with whatever those weights signify, then it should have a high activation and if I see something that's the contrary to that, or just doesn't activate it then it's going to have a lower activation. So now we can see that I have weights that will then be effectively, I'll, I'll use the word designed for now so I'm not going to get into training in this notebook but I have weights that have values that are intended to activate on certain types of input features and non certain types of others. So, this process of shifting the focus is this convolution. So this convolution operator is basically this shift of focus across an input, and then using those weights to basically process the entire in the entire each segment of the entire input in the same way, and then determining which regions of that input are actually most relevant to this, this particular weight matrix or what we'll call a filter. So weights and hidden unit often called a kernel or a filter I'm going to use the term filter for the duration of this class just with the same consistency. But these two terms are used more or less interchangeably. So you often see the word kernel using place of this kind of in the analogy to like an SVM kernel or something like that. So in this case, we was filter just think of it this way. I'm trying to filter out the parts of the image that are relevant, right and my filter is going to be some numerical weight matrix that allows me to do that for certain types of features, I'm just going to have a bunch of different filters. And so, them all put together should allow me to select the different relevant types of features for a bunch of different classes. So I have 20 units in the hidden layer I'll have 20 much smaller images produced, and then the weights in one unit, they could be values that say result in a high value when that image patch has like a vertical edge, so that filter will be optimized to activate when it sees a vertical edge. Then the second unit might have like weights that optimize for another type of feature like a horizontal edge or a curve or a diagonal or something like that. So if you're using a lot of different calendars features like that it should then output a larger value, whereas that vertical edge filter if it sees like a curve like the bottom of a three. That's not very well aligned with what it is, what it is designed to activate on so it's not going to output a very high value. So, this allows me now to effectively say for different parts of the image this filter is designed to activate on certain things And so, I get a high value here so this is sort of a vertical edge filter. This is a simplified example because in reality with MNIST there's sort of limited numbers of features you can use to combine into images where you have curves you have straight lines horizontal lines diagonal lines, etc. So in actual image classification, you're not going to have individual filters that activate on like very specific say entity level features it's not like this is a dog filter. It's more like this is a filter that responds when it sees like things that are triangular right that that's going to be that's going to respond when it sees things that like the ears of certain kinds of dogs. Often you'll actually have filters that are optimized for multiple things you may very well end up with say a filter that will have a high activation when it sees a dog or part of a dog but also when it sees part of a car. And that just happens to be, it's optimized for multiple things and the other things other units that get activated are also very important for determining what the output of that, that network is. So in the simplified case with MNIST, we now have this network that has 2230 weights, it can process an entire image and really instead of increasing the number of weights we just have some slightly additional processing time for the convolution, and some impact on storage for storing those smaller outputs. So, this digital computation and storage costs is generally pretty small it's much more desirable to have this than to try and have a fully connected network where you have like one weight for every input, which would be every pixel. So now we come to the question of overlapping. So I want to like shift this lens, I want to instead of just like taking my pinhole camera, I actually want to move it instead of like pinpoint pinpoint pinpoint. I want to move it all the way so that I get a continuous picture of the entire input. So what I can do is I can take this thing called the stride is basically just a length, and this is like how much do I want to shift the lens of this unit. So I can do that when calculating my patches. So I might shift it by like one pixel or two pixels or something. But if I say a 10 by 10 image, I want that stride lights to be less than 10 so I capture some overlap in between the different patches. If I just do one with my 28 by 28 image and my 10 by 10 patch, then 28 minus 10 plus one that would be 19 patches left to right, similarly top to down, we'll assume all square images. And so then each unit will be applied 19 times 19 times to produce a new image that is 19 by 19 or 361 output features. So now each hidden unit will produce 361 values. And so when we weren't doing the convolution, each unit produced one value and the output layer received 20 values, one for each unit. So now the output layer receives 361 times 20 or 7220 values. So for this slightly elevated cost of computation storage, I'm basically able to get, you know, in this case, 361 times the information in that final output layer. So instead of trying to make my judgment about my category over 20 values, I now have over 7000 values, and those are bound to be much more informative. Maybe I should be able to find, get a lot more information out of those values. Okay, questions on the intuitions. All right, so let's move on to some actual examples. Effectively what we're going to be doing is we're working on the hand crafting the process of creating an image, making patches, performing convolutions and we'll see what types of activations or outputs we actually get. So, I'll import the MNIST data set. Let's look at the, look at the shapes here. So I printed out X train, T train, Xval, Tval, Xtest and Ttest shape. So please look at these numbers and kind of decode them for me. So how many samples are there? How are they represented? And what are the output classes? How many samples are there? Somebody said 70,000. So there's 70,000. So we have 50,000 in train, and then 10,000 in validation and 10,000 in test. So we're going to split them using this train val test. So how are they represented? Yeah. So they're 28 by 28 arrays. These are flattened now. So they're just, the way this is stored is basically just flattened the pixel values. So they're pixels. So what do you think? How are they numerically represented in those 78 element arrays? Yes. Or normal or a normalized version of that basically. Yeah. So these are, these are or 0 to 255 or 0 to 1. So these are grayscale values. So eventually they're either already standardized or they will be. So effectively they're going to be squished down to between 0 and 1, where 0 is fully black and 1 is fully white and everything else is some shade of gray. And then what are the output classes? Yes. So there was a nine, right? So this is the MNIST data set. So it's for digit classification. My goal is to take this thing, a bunch of these and see, you know, which of these classes is it. So we might look at a sample like this one here where my mouse is and say, well, that might actually be a nine, right? Even though it's in the row of four. So this would be labeled with four, but our classifier, you know, when we train it might actually not do very well. So, okay, so there's our data set. So these images are grayscale. So how do you think if they were RGB images, how do you think the representation might be different? So you have three different values. You have three different values, right? Yes. You basically, and they would still be 0 to 255 or 0 to 1. It's just one would be the red channel, one would be the blue channel, one would be the green channel. So now when you consider that, you just have to change the representation when feeding into the net slightly to account for basically that third dimension that is the color. So that's pretty simple with grayscale images for now. So if you look at t-train, the first 10 output samples. So what is what do you think this means? What's the what what what is the first image in the data set? The five, yeah. So these are not ordered, right? We don't have all these zeros, like at the front of the data set and then all the ones and all the twos. So these are in some sort of random order that they presumably shuffled it and they saved the data sets when you open the pickle files in this order. OK, so if we look at this, like the seventh sample, right, this is going to be an instance of a three. Yes. Yeah, these are the target samples. So t-t-t is what we're always using to denote the target output labels. This is a classification problem, so they're not scalar values, they're they're they're class labels. So in this case, I think the class labels correspond to the actual digits. But remember what we said about the orthogonality of classes. So the classifier could have all the threes under label five for some reason, right? Wouldn't be very intuitive, but you could do it because the computer doesn't have any interpretation over like the number three. It sees the number three and it's just a collection of pixels. It doesn't know that actually corresponds to the number three in its internal structure. The way we've ordered here is just happens to line up because that is, of course, a very intuitive way to do it. And why wouldn't we do it that way? OK. So let's now take this and plot it. So I'm going to use the IM show function. So this allows me to basically display data as an image. I can just put it in a pie plot. So I'm going to make this draw image function that will put in the image and label and it will draw the image and it will put the label above it. So I put three, then there's a three and it'll give me this class here. So the MNIST data set is natively like this with the background is all zeros that is fully black pixels and then it's basically white on black. That's not, again, not a very intuitive way to to view pencil drawing. So I'm going to create a function that will actually invert this and draw it like a pencil image. So I'm going to kind of draw the inverse image. So that looks like this. So now instead of 10 by 10 patches like we were talking about, let's use seven by seven patches, the 28 by 28 image. So this will divide in very nicely. So to do this, you can use two for loops and you step across the columns left to right and then an outer loop that steps down the columns top to bottom. And then I'll collect each patch into this patches list. So the 16 patches in total, right, 28 by 28, and that divides into or seven devices that very nicely. So with my seven by seven patches, I end up with something that looks like this. So first thing you might notice is like some of these patches are all black. And this is well, why do you think this patch is all black? It's all zeros. And actually the way that we've written this function is basically doing the negative of the image. Negative of all zeros is still zero. So there's basically this assume there's going to be a range there. And in this patch, because it's empty, there is no actual range. So I will modify this function to include Vmin and Vmax. So basically this is going to enforce a range from negative one to zero. So that way when I invert it, they're actually it does it does force a range on that and makes the all the all black patches all white because the inverse of negative one is one. And so that's going to draw us all. So draw that. OK, now it looks nice and uniform. So all intensities are zero. So I specify this min and max value. I can I can switch that to to all white. So if I want some overlap, then I'll want seven by seven patches that shift by, say, two columns and two rows. I'm going to use a stride length of two. So instead of shifting by seven each time, I want to get that overlap. So now what I'll do is I will then go through my patches and then add more patches that include those intervening intervening pixels. So now I have one hundred ninety six patches. So basically, 14 by 14. So if I plot this. Takes a minute and this is what it looks like. So this is sort of your your strided version of this three image, and it gives you a pretty nice sense of what it what your what your patch is focusing on as you move it across the entire image. Couple of issues with this, you notice here on the on the right side in the bottom, you have these odd sized patches because basically my stride has run off the side of the image. There's nothing left there. So we need all the patches to be the same size. Conversion neural nets like fully connected nets still require fixed size inputs. So so far, you basically have to specify your input size. If you deviate from that, then it yells at you. So we're still in that territory. We talk about recurrent nets later. We'll get some instances of when that is not the case. But for the moment, we're kind of stuck in this world. So what we're going to be doing is we're going to just discard the ones on the right in the bottom. This is just the most simple technique to use right here. Usually more desirable is to actually pad. So that is, I want to basically take this last one and pad it out so that when as I stride, I'm not I get to the end, and I'm never going to run off the edge. That's a little more complicated to implement. It doesn't really affect anything you want to do here. If you want to see that I have code that that does padding for you. But for now, let's use the copying solution. So basically, if the row where I'm at where my stride starts plus seven, my stride length is greater than the size of the image or the same for the column, I just stop here. So now instead of 196 patches, I have 121 patches. So 11 by 11. And so now it looks like this. So you can see that we still keep the entirety of the image that we're interested in, in this like there's no pixels being cut off. And we also have the advantage of having these nice square patches all the way through. Okay, so now just, we have this additional storage, but how much storage are we actually using so these patches. Are they actually using lots of storage that got my storing every single one of these as a separate place in memory, or are they just views on the original image race remember review is a shallow copy, where if I change the view I change the original. So hopefully they're just views and so I can test this by modifying the original image, and then seeing what I'm seeing redrawing the patch and seeing if it changes if it shows a modification. So let me look at the first four rows and columns, they're all zeros. So now I can take like this upper left column of that first patch. So if I change all those two one. And then I print the, the first five you can see that I've changed those first four to one and then there's all zeros. So now if I draw it again. So now that those first four pixels in the top left, have all turned to black. And so therefore these are just views. Right so I changed the original image I didn't change anything about that patches list. But then, having done that when I when I when I draw those patches, the changes can apply. Okay, so don't want this to screw up our actual processing so let's just reset those pixels. Okay. Well the the trimming here is what makes them uniform so they're all uniform by default. So all of these these are all uniform squares. Right. And so then once I once I get beyond that my stride starts running off the edge of the image and so I'm just like well let me just stop here. Yeah. Yeah, so this is, this is why padding is generally a slightly better solution, because if there is interesting information at the edge. You don't want to lose that on MNIST is such that all the images are designed to be nicely centered and so like, I can be pretty cavalier by sort of throwing things away at the edges because I am reasonably sure there's nothing interesting there. So just keep in mind that like, this is a nice task to demonstrate how this works and it's pretty intuitive but like, just because you demonstrate something on MNIST is no guarantee it's ever going to work at anything else. Other questions. All right. Okay, so now let's talk about weight matrices as a kernel or filter. So how would we apply. How do we create a unit as a filter and apply to all patches. So if I want to multiply something by these patches which is seven by seven weight matrix, I need to have something that is of a shape that can also multiply by those patches. So, it's just going to be another seven by seven weight matrix. So I'm going to have a patch size, and I'm going to have a filter size, as long as those two are compatible I can now do things to that patch with that filter. So, we'll start by just kind of hand crafting some patches and see how they react to different types of features. So, let's make a path that should detect diagonal edges from lower left to upper right. So, this is, I'm just going to kind of craft this filter we see that we have a bunch of negative ones here, and then a bunch of positive ones here. So, how this detects edges is basically filtering the image patch at the same size, so they will multiply together. I want to have some negative weights that should kind of deemphasize things on one side of this edge and some positive weights that should emphasize that, or these just sort of preserve the information on the other side of the edge. So, if you imagine an edge that's like black above and then white below or something like that. And then that quote negative side, you would want to maybe deemphasize some of that information on the positive side you might want to bump it up a little bit more. So, if these two were to align perfectly you'd basically say well this is a perfect match for this shape that I was looking for. And I should have, where I have like a bunch of ones, they're going to multiply by these ones, and they're all then you sum together you get a pretty high value. What about the other things? Well, if these are all zeros then these negative ones just all multiply by zero becomes zero. And so you basically have an output that almost looks almost exactly like the inputs. In this case, if it was a perfect match. So, what we'll do is we'll apply this patch to a bunch of different kind of handcrafted images, and we'll see how they respond. So, if the patch is all zero, and what I'll do is I'll store all these experiments for plottings. So then if the patches is all zero, remember this is a negative. So, this in reality it's actually black. So, in this case, what we're going to see is that the black pixels are the positive values and the white pixels are the negative values, just because we're using our our inverse drawing function. So, here's this patch. So, it's blank. Let me multiply them by the weights. And that's, it's still all blank. Right, because it's all zeros, everything multiplies to zero. No, and then it's it's it's matrix multiplication, but you see you saw everything, but everything is still zero so everything comes out of zero. So, seven, seven by seven array of zeros times seven by seven array of literally anything is going to give you a seven by seven array of zeros. Okay, so now if the patch is all ones. So here's our patch so now it's fully black. What do you think, what do you expect this might look like when I multiply that by my handcrafted edge filter. Yeah, might look something like the edge filter knows that's a reasonable supposition. What happened. A couple of things happened. One is that there is actually in reality there's some grayscale in these values but the the inverse drawing kind of eliminates that but you do see this sort of vertical line. Remember how matrix multiplication works. So I take every value multiplied by every other value in a row or column and sum them all together so basically a single value is incorporating information from the equivalent row or column in the other matrix. So, what I'm getting there is not necessarily the, the, the exact patch, but I'm actually getting effectively a graphical representation of kind of how much information is relevant to this patch. So what you're going to see is effectively, the more the greater the distribution of higher values doesn't necessarily match the actual physical configuration of the individual values, but the overall distribution should be higher. If the patch is a good match for the filter. It's pretty intuitive with this version because anything multiplied by all zeros becomes all zeros, but because of the the matrix operation and the sums across rows and columns. We're not going to necessarily reproduce the exact patch. So what this means is that when you multiply a patch by a filter, you end up with something called a feature map that is sort of, it's kind of like a low resolution low resolution like gist of the input, and that it's got things in the input that kind of resonate with this patch, and the output will reflect that numerically, but it's not necessarily going to reproduce exactly like something that's reminiscent what was in the input. So now it's had this checkerboard pattern. So, alternating zeros and ones. And so now when I any guesses what this might look like. I guess we scared you off of making these kinds of predictions. It look up it'll kind of I mean we'll have some features of this it'll look sort of like a chessboard in that it'll, you'll, you'll see you know some of these. Some of these filters here. I can just draw the draw image I think this will sort of show it in more light so here. Annotate this real quick so you saw these this part here was black. It's just because there's a threshold there about point five and so that negative drawing is where it's below point five it turns it all white if it's above point five it turns it all black. So that's just an oddity of the way that we're drawing things right now. So, okay, so what if the patch actually contains an edge from bottom left to top right, and I'll just use the, just imagine that the image is inverted. What do you think is going to look like. So this is a very plausible. Well, this is this is this is the actual image, so that this is the actual patch. So it looks something like this. So yeah you do you do see places where it like responds very well. Right where in this bottom in this bottom right here. So this is a value that's like pure white because when I saw them over the rows and columns I get a very high value there and then it's been it's normalized, but then all went on the on the left side, where it's basically not a good kind of match for that. For that filter, you're going to get a low, low output. So, what about the reverse. So I have this. So, predictions okay let's just see it. So we get something like that. So it sort of looks like we take this image, and this image it's sort of like, it's been rotated and inverted. Right. Again, and this is just because of that matrix multiplication. Alright, one more weird pattern. So I got this thing kind of looks like something on like a rug or something. And then, so it's feel it does have some of those features that I'm interested in right it's got this horizontal edge, but it's also got a bunch of stuff like above the edge that maybe I don't want to emphasize quite so much in my output. So that's going to look something like that. So now you can see that it's similar. This one here is similar to the one above, right visually, and that they have similar features. But this other information here is basically like this is part of this patch and this, this filter is only partially relevant to this patch so I don't want to have as high an output. So, again, so here's all of them, side by side. So here's this is sort of what was intended to show but without the negative image Oh, ignore, ignore this one is because I ran that cell twice. So, get rid of that. All right, so in this case, effectively we start by multiplying something. So remember, because we are now, this is not using the input imagine this one is all white. So, if this is all white this one on the right should be all white. This is all black, multiplied by my filter it gives me the sort of odd gradation. This is the checkerboard I see kind of some features reminiscent of that checkerboard like it now it has this combination of features of the input, and kind of the visualization of the pad of the filter if you were to do that. And then here we see some oddities and that the, the visual representation is like not very intuitive in terms of reflecting either the input or the filter, but it does if you combine to the entire thing. It will actually sort of correspond to how much relevant information was in this in this patch relevant to that filter. Any other questions. Yeah. So, So, I mean, for the whole training process is basically to automate this. So, what we've done here this this what I've tried to show here is, this is the operation that's happening inside a convolutional network once you've arrived at those filter weights. But it wouldn't make a lot of sense for me to sit around and hand craft all of my filters, I have to have a very deep knowledge of my data set it might be impossible. Because I don't know everything every type of visual feature I might encounter. Instead, what we would want to do and we'll get to this next time is we're going to want to take a bunch of data and basically train the weights in these filters to better optimize for that data to tease out those individual features. So this is how we end up with these convolutional filters that can be optimized for both parts of a dog and parts of a car. So if you think about image net which has 1000 different categories, much more complicated than MNIST only has 10. How much space do you need to store all the information about those about those different categories well it's less than you think if you're very efficient about packing information together. So I could have a filter that part of it you know it will respond to this dog type feature and also respond to this car type feature. And then I put it together with other things that some of which respond, some neurons respond to dog features, but not car features and those respond to car features not dog features, and that both neuron would be activated along with one set or the other depending on what the input would be. And so the precise values that are trained into those convolutional filters can be kind of opaque, and you don't really know exactly what they're responding to and that's actually an open topic of research. Yeah. So those common images we do you can use them in sequence tasks, because of the stride operator. So for example, you can, they're, they're not like necessarily great and natural language tasks with that you can use them in that if I want to compare going to learn something about context, I can have a window, it's going to look at say, my center word and say, one or two words on either side, and then I move my window and say okay now I'm focused on this word and these are the other things on either side. So it's pretty much anything that you can use this, this kind of striding operator where you can use a convolutional format for whether or not it's really optimal for that is a different question but it's possible. Yeah. Yeah, okay. Yeah, that makes sense. Yeah, so similarly, sequence type data, you know where you want to look at say, you know, a DNA base or something in context to things not either side of it, it could be useful there. Okay. Okay. All right, so that we've kind of denoted this is just like patch times weights. And so this is something that's called the feature map. So if you see that term it's basically saying these are the inputs times some weights is going to give me some representations. So basically the the feature map is not really meaningful in and of itself which hopefully you've seen here, when when the output is like not obviously you know kind of shares some echoes of both the input and the filter but it's like not, you know, it's not the element wise multiplication of either. So it provides information about how much response, do I the filter get out of this input that I just received. So basically you can leave it was like, how excited am I by this thing how much of a high number of me going to put out. So if I'm, if I'm a filter and I'm optimized for particular type of feature, I get really really excited when I see that feature not put a high number, and if I see something that's like irrelevant to me, then I will do much. So basically the more positive values in the matrix that's created in the feature map. So in this case, since I inverted the colors is actually the more white in the image in the right column, then the greater positive value that patch will will output when you multiply by that filter. So for example, if the filter defines a top to left, left to top bottom left top right edge. So then only one of these features this one I'll give you the answer. Looks like it's reduce a very positive value. And so that's actually this one. So, because these are normalized. It's maybe may not be entirely clear like what the, whether the top one of the bottom one would output a higher value. That's just because we're normalizing all the values into it is into a specific range. If I were to print out the actual numerical values. This one should have like objectively higher values. So, for that. Alright, so image. So, no, because there we go. So, this patch has this well defined edge and when you multiply by the edge filter, it will output a very positive value. So now let's look at our three patches again. This. Alright, so take a look at some of these patches and you know do you see patches that look like they might respond well to that edge filter that we handcrafted. So there are features in this that probably share some information so that maybe like this patch here perhaps perhaps this patch here this one there. So there may be some, some relevant patches in here that this filter would be able to detect. So we can apply this filter to all the patches. And so then to do this we just need to multiply the intensities in a patch by the corresponding weight and then sum it up. So, we can do that here this will just show me the outputs, the actual numerical outputs. This of course is hard to read. So let me make it into an image, and then drive. So, the dimensionality this image is going to be 11 by 11. So if you remember from before, the way we broke up our patches seven by seven and then discarding all those in the edge we went from 14 by 14 patches to 11 by 11 patches. And so then each of these. When I multiply that and then sum it up should give me a single value. So I have 11 by 11 patches. So now I can do this. So now what does that look like kind of looks like the three sort of looks like the three if like you squint a whole lot. Like if you do that is like yeah you can see the three. So basically what this is then is this is not the inverted one so just where black was white now white is black. So now we have those, say these high values here. These sort of fall kind of in like the crook of the three the arms of the three, which has something that sort of resembles that edge right so if you look up back up at the three image. We're talking like these, these parts here. Right, so that has kind of that that edge type feature that we're interested in. So now you can apply all of your weights to filter all the patches in a single multiplication, which makes things even faster. So we have my new images like 11 by 11. So 121 patches, and then a seven by seven patch. So, the shape of the weights then should be also seven by seven so they multiply together. So then to do this I'll just reshape my patches array past negative one. So what we'll then do is we'll take that seven by seven and turn it into 49 right so now I have 121 samples, each of which has 49 individual values. So what are those. So remember let's count up. Let's count these up so there should be 11 by 11 patches there's 121, you just flatten the image row by row. So now I have all of this row, all the all the second row and so on and so on. And so now each of those will basically have 49 values associated with it for each patch. Okay, so then if I reshape my weights into a 49 by one and from a seven by seven. This allows me to multiply this new reshaped array by those weights. So now this image is going to be 121 individual pixels with one value each. And so now this allows me to get the exact same thing. So this can be the sequence of events can be basically a neat way of optimize or speeding up your computation. So fewer for loops, fewer opportunities for mistakes. And then you also get to use the vspeed inherent in NumPy. Okay, so now the idea is to come up with a number of these weight matrices. So if I have my three. It's only so useful to have that certain type of edge right I also want to be able to detect say horizontal lines, vertical lines, curves, things like that. And so I need to have all of these things optimized in in my filters so think about the MNIST data set we see all these different types of features in there. So it stands to reason that if I have you know some some network, right, let's just say you know I've very bad neural network here. And just imagine that I'm not going to draw those lines between them. But you know for some input. You know if if this weight has a high activation and then this weight has a high activation and then you know this is great as an output layer. Then, if I see these right should correlate with another with a different class compared to if like I see this one, and that one. So again, think I use this metaphor before you can think of this as like one of those pachinko machines where you put your coin in at the top and it bounces off a bunch of different rods on the way to the bottom, and some some it comes out at say one of 10 slots at the bottom. So the goal is I'm putting in my input. And depending on those weights it should sort of effectively diverted its path through the network in terms of which nodes have the highest activation. They're all going to output something, but I'm going to be interested in those things that have you know that that really high activation value. And then that is going to kind of when I run the softmax function squish that into probability distribution, where it's got a peak at some class. And that's going to be sort of where your sample comes out at the bottom of the, the pachinko machine. So I need to have these different types of filters in my, in my neural network. And so each individual node is going to have some sort of different filter that lives inside of it. What is that right like you were saying we do not want to have to sit there and handcraft all of these for it maybe for MNIST like wouldn't be too infeasible to do that. There's a limited number of features and you probably actually could sort of handcraft an appropriate number of filters, and then have a neural network of the right size such as a different filter in each in each unit, and maybe you wouldn't get bad accuracy but does anybody really want to do that when you could just learn it all from the data. Doesn't seem like a very productive exercise to me. So then what we would do is we want to learn these weight matrices. So basically every, every unit in the hidden layer is going to be this convolutional unit with this n by n weight matrix, plus one weight for that constant bias, and then we just like back propagate some error from the output layer to the convolution layer, and I'll play the update those units weight so for example, same prediction applies, we have a softmax layer that will output in this case, 10 values that the probability that each of the sample the samples falls into those classes. And I can compare that to my one hot vector where one of those elements is effectively 100% and everything else is zero, and I can see exactly how wrong I am so if my sample is actually a seven, and by randomly initialize convolutional that predicts a four. Well then, among other things, the current weights are activating in a way that will output a four I need less of those and any more of the units that are going to activate in a way that's going to output a seven. And so then I can I can back prop this error, and then update those individual weights. So this is basically just what we've been doing all along, but it's kind of at a different scale, and it's a little bit tricky. So every unit is going to be applied multiple times to all the patches. So you think of my, my three image. Every single one of these patches is going to go through every unit. So I need to kind of get an error metric. That's that represents the overall picture. So, for example, if I have like a four and a seven, they both maybe share a diagonal line type features something like that maybe it's in a different place in digital got that feature. And so I want those. Maybe I want to preserve like those some of those diagonal line filters, but remove or do or like optimize away some of the other ones. And so, my back propagation operation demands an error. That's kind of this holistic view of how wrong were all of my patches, all of my filters and apply to all of my patches. So what we do then, and I'll get into the math later is we're basically going to sum up all the resulting weight changes that result from applying each unit, a unit to each patch, and then that some is going to be part of that error term that I use to update the weights. So before we get into the code for doing this, let's just revisit the method for dividing an image of into patches. We had to use these two nested for loops. So we all know that for loops are slow and they introduce opportunities for error so since convolution is this very common procedure numpy has this add stride as strided function that will kind of do this for us. So I'm going to use this stride tricks library that will import, and I'll define this make patches function. So, here what we do is we pass an access is my input, and then I specify what size patch that I want and what stride length that I want. So then I will flatten this. I'll take my square images flatten them into a continuous array. And then I'll just look at how many samples do I have the image size is going to be just the square root of the second dimension so that is the number of pixels. And so then I can compute the number of patches that I would have given this patch size, and this image size. So then I can this will just automatically use the stride tricks as strided function to reach to give me this array that has everything reshaped into the right number of patches. So if I pass in my X train, and I have my 50,000 samples each of which is a 28 by 28 images and 784 pixels passes through make patches I'll look at just the first two samples. And I'm going to have a patch size of seven so we'll assume square patches seven by seven, and a stride length of two, like we did before. So now, the number of patches is going to be to the number of images by 120 121 patches for each image, and then 49 pixels for each patch. And so now I can actually plot this so just running this make patches function will give me the following. So the first two images in the data center five and zero. And here are the fully strided versions of that. So, pretty neat trick that you can use to automatically patch your images. Okay. All right, so then the weights in my filter that I defined a seven by seven. So I'll reshape those into 49 by one. So, there's two filters. One of them is going to detect edges in one direction and one in the other direction. So then what I'll do is I'll take my weights I'll just copy that into two identical columns. And then if I print out the first one you can see these are still, there's that patch that we that filter redefine now redefine the second column to represent the new filter so you can see here that we now have negative ones kind of on the bottom left side and negative ones on the top right. So I have basically two kind of opposing edge filters. So now here's that second filter that we define to see now compare this one to this one and we have edges going in opposite directions. So then, I'll just take the entire all my entire set of patches, multiply it by my array of weights. So, I now have weights representing basically two columns representing the flattened version of my two different patches. So I can take my output, which is this case two images and multiply them by that all at once. So now this is going to give me two images by 121 patches and then basically two values for each right and then those two values should allow me to to to plot. So, here are my original images and then these are the, the feature maps for each of them, using those opposing filters so I've got my five, and I've got my zero, and you can see with one type of filter we're clearly getting a higher outputs on parts of the image, where we're getting the exact opposite output on using the other filters. So if you compare say the top line of the five using the first filter and the second one, they're pretty much inverses of each other. They're not going to be exactly be so, depending on the specific pixels in each individual patch, but clearly using an edge filter for like one direction versus an edge filter for another direction to basically get like these complimentary feature maps out of each stage. Questions. All right. So one thing that is not in this notebook that I want to talk about that it is actually in like the NLP version of this is pooling. So let me pull that up real quick and we'll just talk about that because it's one thing that I didn't mention in this. So, CNNs. Okay. Alright, so where's, here's the pooling party. So, we kind of do the same experiment that I showed that I showed you guys right here. So if we start with our feature map. In this case the the pack size is five by five both pretended seven by seven. So the feature map ends up with 49 individual values. I need to get some single representation out of that. And so I could just summit right that's sort of your, your default strategy. So this feature map is going to record information about the precise values and the input. And so basically small changes in the locations of these could really change the output. So for example, if I have my three, and I strided differently or the image is offset by like one pixel, given a fixed convolutional filter, then the output that's going to be pretty I may not make that much of a difference right I may not be all that interested in whether or not the part of the path that part of the images like shifted by one by one pixel or not so one of the key features of convolutional nets is there basically invariant to translation when trained properly. So that is it doesn't really matter where in the image. The thing I'm interested in is my convolution neural net shouldn't always kind of respond to that in more or less the same way for with MNIST we sort of allied to this because all the images are nicely centered, but for robust image processing you want something on this kind of more overall picture. So that is you want to preserve as much information as you can while reducing the size the input to subsequent layers. So we can obviously we can change the stride length to do this but also more robust approach to this thing called pooling. So pooling will also rely on the stride across the feature map. So basically it's going to take something like this, and then kind of break this up into its own patches and perform some operation over those patches. So common pooling operations are averaging or max you'll hear average pooling or max pooling. So if we take this image here, it might correspond to these numerical values. So now I can define this pool function that's going to take in the feature map and a specified method. And so then what it's going to do is it will take this end by end patch over the feature map, and then either compute either the average or the max of the values in that window. So for example if I were interested in this in this feature app and I had like a two by two stride, then it would look at say these four. These four values, and it would take either the average of the max of these four values and say this is kind of the gist of this part of the feature map, and then the next two values it would look at and then we take either the average or the max. So, if we take this feature map on the left and compare it to the average pool average pooled version on the right. Do you think that this is sort of like a reasonable approximation of the things that appear in the feature map. Yeah, right we see darker lower values in the bottom left on average lighter values on the right and maybe some middling values in the upper left. Right so adding this extra specific information might not get me a whole lot more than just looking at this. So, we can look at what average pooling versus max pooling often does. So pooling layers often occur after activation so you'll have some nonlinear function then you do the pooling. So, as I note here, CNNs usually use ReLU but when I wrote this function I felt lazy and I use 10h just because I didn't want to define a ReLU function. You know what, it happens. So if we have the following feature maps, and then we pull the features using average features or max features, you're going to get slightly different results. So in particular if you look at say, the two that the one that we were just focusing on, right, if the right side is average pooling and left side is max pooling, we maybe get a little more information in the average pooled version. So we're in the max pooled version. Effectively these two segments come out to roughly the same value. So these are different ways of further reducing the size of the input to the future layers. So we can also do say pulling over the sample image using that edge filter. And so that's going to, if we have like sample image in this case this is just a square, and I have the same edge filter defined here, and then I can, So basically here is the original feature map. So just the raw feature map. And then here is the average pooled version, and here is the max pooled version. And so all of these roughly represent the feature map in approximately the same way. But instead of having say 49 outputs, you have four. And so this allows us to further reduce the input size and maintain the speed of the convolutional operation, while still maintaining the information about the input. Okay, so that is kind of intro to convolutions in the large, alluding to how we actually need to train these things of course. And so we're going to get into that next Tuesday. So we'll actually do convolutional neural network training next week. All right. So if there are no questions, then I will let you go. All right, let's start. So weather is absolutely awful, and like a bunch of people are feeling sick. This time of year. And today is my birthday and so my present to myself is having a short class. Thank you. I mean, I'm turning like 36, which is like, mathematically, it's actually kind of interesting, but legally it means nothing. And it just means that you're like that much closer to 40. So yeah. Okay, so does anybody need to come to office hours like after class? All right, yeah, that was gonna be my other present to myself was canceling office hours, but I sort of felt like kind of a jerk doing that. But if you don't need to, then I'll do that and I'll hold office hours in the regular time on Thursday. So all right, a one re grades are back for those of you who submitted them on everybody did at least slightly better, even with the five point deduction. So that seems to be paid off nominally for most of you. And Sarah has finished the age two grades, I just need to review them and I will post them. So I will endeavor to get to that in the next 48 hours at least. And then under about a three is due the Tuesday after spring break. So you know, if I Thursday would be the last time to talk to me about it in person, I will be online if you need to reach me over break. But I may not be able to respond like very promptly on when I'm traveling. So just F.Y. you do have some time, but it's because of the interveying implications for accessibility. So just make sure that you stay on top of that. Any questions or anything? Yeah, I am not here the twenty eight and the thirtieth, I think. And there's in the end that I'm going. Yeah, yeah. So I'll be here for that intervening week. And I'll be accessible. It's just I'm probably either do a remote lecture or recorded lecture those weeks that I'm on. OK. OK, any other questions? All right, cool. All right. So last time we talked about a common convolutional neural network. So this is effectively just a network that includes a weight matrix called a filter. That way you multiply it by things that, quote, match that weight matrix should output a high value. That's like this high activation. But we use these handcrafted filters. And so, of course, the question is, you know, why do we waste our time hand crafting the filters? Isn't the point of machine learning to basically look at a bunch of data and figure out what matters automatically? So this lecture and the next will actually be about performing the training operations on convolutional neural networks and how they differ from, say, the standard feedforward and what CNNs are able to do that feedforward networks are not able to do. So just as we recall, we take our image, we break it up into these patches. This allows us to, among other things, kind of simulate the scanning continuous motion of the eye over an image. And then we can take each of those patches and try to tease out what the important information is in that patch for any particular class. So this is a classification problem. You can use CNNs for regression problems. We're not going to cover that here, but we'll just treat this as an image class. So this is sort of this canonical computer vision problem that we're going to be talking about here. So what we're going to do is we're going to be taking our neural network classifier class that we had from before and turn it into a convolutional version. So the classification part is the same. Right. I have K classes and then that means I have K output nodes and I run my softmax function over the values in those output nodes that gives me whichever one is going to be the most likely class. I take the argmax and that's the class output. So, again, I have indicator variables and my error is going to be the probabilities for all of my classes. And then we can take that and subtract that from my indicator variables, which is a bunch of zeros with a one in the place where the class is the correct answer. And that makes it 100 percent probability minus whatever probability it predicts for that class. Do that for all the other classes. And my goal is now to try to get as close to that distribution of all zeros and a one as I can for all samples. So in this example, we're going to be just using a single convolutional layer to demonstrate how the mathematics works without getting too complicated. And then we'll assume that our samples are these two dimensional arrays. We'll just assume black and white images. So we'll have them be square and have the same number of rows and columns. So and then maybe not this one, but the next one, we'll talk about how to do this using pipe torch. And using multiple convolutional layers. So our neural network classifier CNN class consists of pretty much the same components as the existing neural network classifier class. So we have our init functions as the constructor. We need two more arguments in additional to in addition to the usual ones that we have in the CNN or in the neural network classifier. So that is we start with the things that we've we've had before the number of inputs, number of hidden per layer, the number of outputs, and we have the activation function. So the first three and this fourth one, those are the things that we've got already in our existing neural network classifier to make it a convolutional neural network. We have to add the patch size and the stride. So the patch size is given my input image. What's the size I'm going to break it up into in patches and the stride is going to be how much overlap do I have between those patches? That is how many in this case, pixels am I moving from patch to patch? So if my stride, my patch size, like seven by seven, my stride were also seven. This would mean I'm going to take a seven by seven patch. Then we're going to move seven pixels to the right. There would be no overlap between my patches. So we don't typically want that. What you want is a stride like that's going to be lower than the patch size. So if I had a patch size of seven by seven and a stride length of two, once I clip out that seven by seven patch, I move over two pixels, clip out seven by seven from that location. And keep going. So these are the two in addition to the standard four arguments. So the patch size that's going to be the same as that kernel or filter size. So we want our filter to match the size of the patch that they multiply together so I can actually get a meaningful scalar value out of that. So to make the right number of weights, previously we had this kind of make weights and views function that would automatically take all your weights and squish them into an array. And then in our constructor, we can create the appropriate size of the different matrices that allow us to perform these operations between layers. So this becomes a little bit more complicated in that we have to deal with the convolutional layers in a slightly different way. So first, I'll initialize the weights and I'll just build this matrix of all the weight shapes. So I'll have my inputs. I'll build this list of shapes. And then first, I'm going to build the shape of the matrix for the convolutional layers. In this case, we're just going to have one. So right now, we'll just assume there's going to be one convolutional layer followed by N fully connected layers. So shapes will be initialized with the following. So you're going to have self dot patch size times self dot patch size plus one. And then it's going to also have the number of hidden layers in there. So let's just focus on what this means first. So this is going to be self dot patch size. Let's say it's a seven by seven patch. This would be actually taken five by five just for simpler math. So if this is a five, we'll have five times five. That's twenty five plus one. And this one is our bias. So this should be a twenty six by whatever the hidden layer size that I'm projecting into. So, again, remember what happens when you create a neural network. I have an input size to that layer and then an output size of that layer that's going to be projecting from N dimensions into N prime dimensions, whatever those values are. So in this case, because the convolutional layer happens on the input, the input size should be the input to the entire network. So this is going to be my patch. So I'm going to take the patch size plus one for the bias. I'll take, say, a five by five patch, string it out into twenty five individual numbers plus a single one more number for the bias. And that's the input that goes into the convolutional layer. Then the convolutional layer should, for each of these inputs, map it into dimensionality equivalent to the size of the next layer. This should be the number of hidden units. So I'm going to take, say, a ten hidden units. I'm going to take my twenty six dimensional input and I'm going to somehow map that into ten dimensions. That's going to be the size of that convolutional unit. And so then the input size would be the square root of the number of inputs. So if I if my number of inputs is seven hundred and eighty four, then my input size would be twenty eight, because this should be a twenty eight by twenty eight image. And so then and in would be the input size minus the self minus patch size divided by self dot stride plus one. So what this does is it's going to basically tell me for the next layer how many inputs I should expect to have into each unit, assuming that I'm segmenting my image according to the specified patch size and stride length, allowing for making sure that I'm taking just the floor division to make sure that I always get an integer. And then I multiply that by the number of units to make sure that I'm getting this that many samples that go into each of those units. And so then for each hidden layer in my hidden per layer, except for the first one, I'll then append the number of inputs plus one and then the number of hidden. This is going to be the shape of that next transformation. So from this. This part highlighted, this all deals with the convolutional layer and then this deals with all of the fully connected layers. So because I've just specified kind of by Fiat that there's only one convolutional layer, I just deal with it all here and it's specified once and then I can use this for a loop to deal with all the the full connected layers. Yes. I think this is just basically a reflection of what's in the existing code. So probably this doesn't necessarily need to actually be here. You probably just get away with deleting, with removing it. But this is currently what's in the neural network classifier code. It's just leftover. Other questions? Yeah. Yes, absolutely. Yeah. So for for the task that we're going to do, which is basically classifying basic shapes, one convolutional layer is definitely enough for more. Let's just take RGB images, for example, you're going to have three channels going in. So you're going to want to, for example, have something that's going to be able to select features, relevant features from each of those channels. It gets a good deal more complicated. I want to get, say, my pooled or gisted version of those input features, then feed it to another layer that's going to tease out like higher level features. What you typically see happening is like for actual images you do, you can see in the earlier layers, often things happen that are like detecting edges, your features that kind of do things like those diagonal edges and basic shapes. Then you combine those into more structural features. So the deeper you go to the network, the more structure you get. That's how you can actually recognize real images of people and things and real pictures in the world. OK. Other questions? All right. Good questions. So we'll define this make patches function. So what we're going to need to do is we're out to have some way of automatically converting that input matrix into the patches. So this is going to be very similar to something that we saw in the previous lecture. So what I specify is just my input and then the patch size and then the stride length. So these are the things I need to actually segment the image. What the trick here is that I am going to be passing in X as a flattened array, right, because I need to have some fixed input size and then one initial term for the bias. And so then I need to turn this back into something that can actually be segmented horizontally and vertically. So for X, for the shape of X, I will take the square root of that. And that's going to tell me what are the dimensions of this image on each side. So we will assume that all our images are square at this point and even more complicated convolutional nets. Usually you squish the image into a square shape. It just makes the math easier for arbitrary image sizes. So I'll compute the number of patches. This is basically going to tell me how many for this patch size and stride length, how many images I should expect to have. And so then I can use that stride tricks library that I showed you last time to basically compute the actual image patches for every individual patch. As we shape that into an appropriately sized array, and then I'll return that so that now I can move through all of my samples and then have all of my patches accessible for each sample. So now that we have the make patches function, we need to modify the use function. So what I do here is similarly, I just start with the standard unstandardized X. I'll standardize them. I then convert those flattened samples into patches. So remember that this X at this point is still a single dimensional array representing the pixel values. Run this through make patches. So now I have X patches. And so then I'll run that through the forward pass, which we'll see next. And this will actually perform the convolution operation over the patches and then return the the predictions. I then run my softmax over that last element in the prediction. So remember, Y is here is basically the output of every layer like we've done before. It's accumulating this list. So I take the last element of that list. This should be the output of that final layer. Run the softmax layer over it. This should give me the actual probabilities for every class. And then I just take the argmax to actually tell me which which class index is predicted. So, again, if we have 10 classes, if one of them shows up at 11 percent, that's the highest, even if it's not very high, is going to predict that. So always good to look a bit more at kind of at your your actual probability distribution rather than just relying on the output label. So the forward pass, we actually have to monitor it a bit to handle the input as patches. Though this also has to flatten the image from the output that we get from the convolutional layer to feed it into the fully connected layer. So a common tactic is not necessarily globally useful, but one that is very common is to have some number of fully connected layers after the convolutional layer. In many cases, this has been shown to improve accuracy, but it does so at the expense of compute time. You don't really always need to do this and often just having convolutional layers, depending on the task, can be enough. Nonetheless, it's very common to see these fully connected layers appended to the end of convolutional net. So in the forward pass, what we'll do is if I'm in my convolutional layer, so this is all kind of the same, right? This is the forward pass. I have my regular activation or my my 10-H activation, depending on which activation function I specified. And so now, if I'm in the convolutional layer, which just in my example is known to be the first layer, then I need to find each sample into a vector to output that into the following fully connected layer. So that's what this does. And then for the other layers, once this is flattened, then it just gets fed into the next the next layer and then the same operations that we've seen before occur. So I finally get like the last weights. And so then I can append the outputs of that last layer times that last weights plus the bias. This was going to be that final prediction. OK, so train is actually not that difficult to modify. So the only thing we need to do is actually just create patches from X, because the way they make patches function in the structure, it's going to have them already in the right shape to feed into train. And so then I'll use that as the input matrix to the optimizer call. So now the way this is written, we have this function arguments for the optimizer. Whereas previously it was going to be X and T for regression problem, X and T, I or T indicator variables for a classification problem. So now this is still a classification problem. So we still use that again. We still use T indicator bars. But now the inputs is going to be like the patched version of X. So the way that this is written, this is actually a very straightforward change. Error F, we actually change nothing. So straightforward. So why do we not change anything in the error function? So what's the error in the classification problem? Disclassification. It's missed probabilities, basically. So I'm just trying to compute a sort of a distance metric, how long I am, how wrong I am. This is going to be one of these terms is going to be 100%. And I take that 100% minus some probability distribution. If it says it's like 93.4, then it is, you know, the 6.6 off. And all those other numbers are going to be some number off as well. I'm trying to minimize that distance. In the convolutional net, has anything changed about the classification aspect of it? It's the same, right? So the error term is the same. What changes is the backdrop part, right? This is the gradient. So error is the same in that that final error term is just the difference in predicted probabilities, but how we actually backprop through the network to account for those convolutional layers is quite different. So I'll show you how to do this here. Once you move to PyTorch, you don't have to do this because we still can just use autograd and do, you know, loss.backwards. Nonetheless, I think it's very useful to understand exactly what's going on here. So the the backprop loop is going to step backwards with the layers, and you have to add this special case when you reach the convolutional layers, in this case, the first layer. So going back to the fully connected layers is the same as we've been doing. But once you get to this convolutional layers, you have to understand where exactly the error gets back propagated. So if you consider that in a fully connected net, I've got some weight values that sort of live inside those nodes. I want to optimize those in the convolutional net. There's a weight matrix, right? That filter is the weight that sort of lives inside the node. One, there's multiples. Two, it gets applied to every patch of the input. So it's not just like a single component. And so you have to allow for all of those differences. So here, basically, the way we end up with that, that delta that's backprop from the fully connected layer is going to have different values for each convolutional layer. Those different values result from the application of those convolutional units to each patch, because when I apply the same kernel, same filter to different patches, I'm going to get a different value. And so then the delta that corresponds to each of the applications is going to be slightly different. So what I'll do is I'll sum those delta values by multiplying each one for each convolutional unit by the values in the patch. So because the patch is different, the values in the patch are going to be slightly different, and the patch is different, it's going to result in a different output. So I'm going to weight that error differently. So in order to do this, I'm going to reshape that delta matrix to the right form, done like this. And so then I need to reshape the convolutional layer input matrix to a compatible shape so I can multiply them together. So I'll take that input. So this is that input term that you multiply by the error to actually get the amount by which you update the weights. So the only trick here is that because I have now a matrix of delta values, I need to have an equivalently shaped matrix for the convolutional inputs. So now you can calculate the derivative of the error with respect to the weights with the convolutional layer with a single matrix multiplication. And then the fully connected layers just work as they have been. So the only real trick here is just understanding that you have multiple applications of each patch of each filter to each patch because the patches are different. The resulting values, those feature maps are going to be different from the same filter applied to the different patches. And so then the the error that you're going to get out of those features that come out of those different patches multiplied by that filter is going to be slightly different. I have to account for that. I do that by collecting everything into a matrix and then making sure that my inputs are reshaped into the shape that is compatible with that matrix so I can do the whole thing in a single operation. All right, questions about the components of the CNN classifier. So what we'll do is we'll in my code that's hidden from you, I'll define the neural network classifier as a new class that will extend the neural network classifier. And so then you can use it as in the following. So what we'll do is we'll make some simple images. These are either squares or diamonds. So I'll first define my square so you can see here I've got a bunch of zeros and then some ones that define the shape of a square. Similarly for the diamond. So you can see here if you look closely and see those ones, I'll draw those images by defining this drawNeg image function. So now you can see I've got an example of a square and I've got an example of a diamond. And these are centered within my within my image frame. OK, so this works fine. But right now, if these were my only images, it would be my image frame. So if I'm only using the images, it would be my net would be really good at classifying squares and diamonds in this exact position, which is not very useful to us. Right. If I'm trying to detect zebras, I want to detect a zebra, whether it's on the edge or in the center. So one of the key benefits of CNNs is that they're invariant to translation. Global Trans-CNN will basically be able to pick up features corresponding to an image class, whether it's in the center of the image, whether it's off from the edge, even if it's from the camera or closer to it, as long as it's not so far away you can't see it, of course. OK, so what we're going to do is we're going to create a function that will generate a bunch of images like these, but we're going to randomly shift them left and right and up and down. And so this would challenge the fully connected nets, but not the convolutional net, because remember, the convolutional operator is sort of like having this pinhole camera that scans over the scene and says, oh, I see a corner that looks like it might belong to a diamond there. That's important. It doesn't matter the exact place that it's that it matters that I see it. So I'll define this make images class. I'll make 20 or 400 black and white images, the diamonds or squares, and I'll sample a few of them to show. So this is going to create training data that has sort of randomly arranged squares and diamonds for classification task. So I'll create, in this case, 100 of each class, and then I will split that into training test. So we get something like this. Right. So each time I run this, I'm going to get a slightly different set. And so we can see here, there. Yeah, so now I get a different set of images. You can see here that we've got big squares that are nicely centered, squares that are in the corner, some squares and diamonds that are like tiny and shoved off to the edge. So at least this should challenge a fully connected net and allow me to demonstrate when a convolutional net is actually able to perform. Yes. All of these squares are fully within our brain. Yes. If we were to introduce ones that were cut off, so I swear that it's important. Yeah. This network could be a more advanced network. It might challenge this one somewhat. I suspect it probably wouldn't challenge it a whole lot, because if you consider if the square is cut off in the corner, you still have very square features. If my two classes are square and diamond, if you imagine a square and a diamond that are cut off in the corner at the equivalent position, I'll have a square that looks like this. And then there's the frame in the corner. The diamond might just be like a slanted line. Right. And that could be enough. Basically, what we would end up with is possibly filters that are optimized to detect, say, not the whole diamond, but just the slanted line. But that also then connects to something in the output layer that has a high activation when it gets for the diamond class, for example. So for this example, probably not for a real example. Probably. Right. Especially if you have things that are like rotation. You know, I'm trying to classify animals from different perspectives or even chairs. There's a you've probably heard of the ImageNet data set. It's a common thousand class computerized data set. You know, some researchers showed that because the images are nicely cropped and framed, you're sort of getting your chairs and cups and bottles and things. But they're all like this canonical pose. And so if I take a picture of a chair from like up here, all of a sudden that doesn't show up in ImageNet. So an ImageNet trained network doesn't recognize that as a chair because it doesn't it doesn't have the features of a chair. So cases like that, you want more you want some combination of more sophisticated network and maybe just like better training data. But in this case, if that were in the training data, it probably would be OK at that. You can. There's a couple of ways to do this so that you can have an additional channel that say you have depth data, for example, that actually operates with the depth data. So some like gesture recognition stuff can be trained here at CSU. They use the depth channel for that. One of my students actually defended his master's thesis yesterday, did a thesis on convolutional nets over 3D geometries, so actual meshes. And there's some very interesting properties of meshes that you have to account for to make them invariant for a CNN. And they're still it's still challenging. And then also there are 3D CNNs. So they're actually instead of a 2D convolution, you actually have a 3D convolutional operator. And that can be used for both 3D data and also say like multi-channel like video data. You can have the RGB channel and then a third channel that is like time. And so you can actually look over like multiple frames. But these are way more compute expensive, of course, the moment I add another dimension, I'm basically going from like X to X squared to X cubed. And so every additional pixel basically have to process that three times. Yeah. What if you inverted the pixels on your picture? If I trained it on black on white and tested it on white on black, it probably would have a lot of issues. Yeah. So again, we run the same issue that your training data must more or less resemble your testing data. So if I trained it on images that look like this and then I inverted this and used that as the testing data, I suspect we'd have a lot of problems. If you train it on both, then it probably would be able to accomplish classification of both, assuming that the network size is big enough to accommodate the filters required to optimize well enough to accommodate both of those features. So I suspect, again, if we had like the square diamond task, if we did that sort of bi-color version, might have to increase the size of the hidden layer a little bit, but it probably wouldn't be too much of a big deal. Other questions? Good questions. All right. So now we've got, we've created our CNN. We've made the requisite updates to the patching, the forward pass, the use function and the back propagation. We've created training data that would challenge a fully connected net that is probably not going to challenge a CNN. So now we can actually try to train this. So our net has been defined to accept these two dimensional input matrices. We need to flatten each image. So first, what I'll do is I just look at this, look at my train samples. So we can see like I've got 10 squares here. Let me take a sample input for train and test. I've got 200 train samples, 200 test samples. These are 20 by 20 images, so each of them contain 400 pixels. So what I want to do is flatten those. So what I'll do is I'll try two units of the convolutional layer followed by one fully connected layer of two units. I'll use a patch size of five and a stride of two. So what are my classes? I can just run np.unique over t train. So I've got two classes, diamond and squares. I'm going to have two output nodes. And so now what I will do is I will import my neural network. I will track how much time it takes to train. And so here this is X train dot shape. This should be that 400 number. This should be my flattened input size. These are my two hidden units and these are my output size. And then I specified my patch size and my stride length. So I'll train for 2000 epochs of the learning rate of 0.01 using Atom. Let's go for a minute or so. Not even that. And this is this is our results. So it took four seconds to train. Perfect accuracy on the training data and pretty close to that on the testing data. So what I can do now is I can look at my individual samples and see what the probabilities are. So these are the this is what I predict. So class zero, these are squares, class one, these are diamonds. This is the probability for each. And then the blue line is the actual ground truth labels. So we can see there's basically one sample. In fact, it looks like the very first maybe not the very first node is the very first sample. It appears to be the very first sample. It got it got wrong. But it got the rest right. So of the 20 testing samples, you got 19 correct. And one of them appeared to classify a a square as a diamond with slightly above 90 percent. But in all other cases, it got it correct. And the most the one thing that got closest to being wrong was this one where I thought like, it's about 19 percent likely to be a diamond. And it's actually a square. So pretty good performance overall. It's a simple task, but this demonstrates what a convolutional neural map can do. All right. So now let's see what our units actually learned. And I can do this by drawing images of the rate matrices. The first I'm going to look at the shapes. We have 26 by two. So this this should be a five by five patch plus the one bias weight. So I'm not going to visualize the bias weight, not least because I can't turn this into a filter. I can't turn a 26 sample array into a square. So I'm going to lop off that bias weight, reshape it into two five by five arrays, one representing the first unit, one and the second unit. And we'll see what it actually learned. So what do you notice here? The square looks funny. Do you think there seems to be something that looks like a diamond, right? Do you think it's actually reflecting the fact that it learned the shape of a diamond in that configuration? Not really. Why not? Yeah. Right. Visually, and especially in this case, because remember, the diamonds are like are randomly distributed across the frame. So this is just sort of coincidental. It might be because of the way the weights were initialized or other factors of the training. But let's not be misled into thinking that because we see the sort of diamond shape here, that this is this is necessarily like the unit that has a higher activation when it sees a diamond. It might be. And maybe there are a higher proportion of diamonds that are nicely centered from our maybe just kind of unlucky with randomization. But it's comparatively unlikely. Sorry, who's calling me? Let me again. Like this time today, like my my undergraduate university, they called and asked for money. So anyway, CSU will do to you, too. I'm sure. Anyway, so what we can see here is that although we see a diamond, it isn't necessarily indicative of the fact that that filters like learning the diamond features in this configuration. So let's print the weights. So this is the the first unit. This is this is going to be this one. So you can kind of see, let's take like this first value. This is like pretty low. This is negative point six, whereas these two on either side of it are point five, three and point five, eight. So you can see that this corresponds to the values we see there. So each hidden unit contains this five by five pixel filter. And then the individual pixels represent the value in that matrix. So unlike the last lecture where we had this very specific kind of edge, these units had been trained at the same time to optimize for both squares and diamonds of different shapes in different places in the image. And so then also, it's the it's a five by five filter over like a twenty by twenty image. And so as the as we move across those those patches, these individual values are going to reflect some sub segments of the image. OK, and so they do pretty well at this task, but they don't really resemble a square diamond features visually. So if we train the CNN to detect dogs, would you would you expect the filters in that network to visibly resemble dogs? Not really. Sometimes you will see when you say take the filter, the inputs times the filters, the feature map, not the filter itself. Sometimes you actually see like high activations on like features of a naturalistic input image. So you can see if there's a filter that appears to activate more for certain types of edges, you'll kind of see the image of like a German shepherd or something in places where maybe there's like a diagonal edge for a certain type of filter that activates with that type of feature, you sort of see that it matches those those features in the image. But as you get deeper and deeper into the network, those visualizations become like more and more obscure and opaque. So kind of it's easier to visualize maybe for those more intuitive features in the early part of the convolutional net, but much not not so much later in the net. Similarly, these images, these filters can be optimized for multiple things. So both of these filters are optimized for squares and diamonds, you know, maybe some more than the other, but it doesn't both at the same time. And this is generally true for all filters in a convolutional net. They're all optimized for all classes to a certain to basically varying degrees. So you have some that are going to be more optimized for certain classes and then less optimized for certain classes, but they should all activate a bit, you know, at least like a non-zero amount or mostly non-zero amount, depending on what they're most optimized for. Okay, so now let's repeat. We'll use four convolutional units and two fully connected layers after the convolutional layer. So here we have, these are my four units and then I'll have the two fully connected layers with 10 units each. And this time I'm going to train for 1000 epochs instead of 2000. I'll use Adam again at the same learning rate. And so if you remember the last one took four point something seconds, this took 5.8 seconds, even though I trained for half the number of epochs, right? This is because it's a, there's one, there's twice as many convolutional units. There's also these two fully connected layers. So it's a bigger network. There's more back propagation due, there's more operations. The test percent is still good. It's 98.5 instead of 99.5. So if I visualize those probabilities, it looks something like this. So there's some samples. It still seems to get that first sample wrong. In fact, you got it more wrong in terms of the actual probability of the incorrect class. And then there's two samples of diamond that is also getting wrong. So I'm going to use the first sample, which is the first sample, and then I'm going to use the second sample, which is the second sample. And there's two samples of diamond that is also getting wrong. So the training converges faster, but the generalization of the test data is actually not as accurate. So we might say that, you know, perhaps this is like overfitting to something in the training data. And that's quite likely in this case, because the task is so simple that the bigger your network gets, the more likely it is to learn kind of spurious correlations in the data. So we will visualize what the four convolutional units learned looks like this. And then once again, you know, we can see that there's no clear correlation between the actual physical shapes of the input images and anything that's being learned in these convolutional filters. So you can see, you know, where they're like, for example, in this filter, these two pixels here represent in the right hand side. Individual values of that filter that tend to have high activations kind of regardless of what they're looking at, at least relative to what's in this data. And then this one has like a high activation with or is as high value in the filter, just like right in the middle, but not a whole lot of visual correlation. So let's see how a fully connected non-convolutional neural net would do. So let's look at sort of the non-patched version. So I got my two 200 samples training test. So instead of using my CNN classifier, I'll use my normal classifier. So you'll notice that I don't use the patch size as an input. I don't use the stride length. And also because this is now an instance of neural network classifier, not the CNN in that function, it's not going to call like the make patches or anything. So the way you've written this code is that in the CNN, it's set up in a way that will automatically do the patching for you if you instantiate this type of networks. So I don't need to do that pre-processing of the data before I instantiate the network. It does it for me by virtue of the type of network that it is. So if I have, you know, let me run the convolutional version first and then I'll run the fully connected version. So this is four hidden units and then two fully connected units, 99 percent test accuracy. If I do a fully connected net with the same network architecture. So one, you'll notice it trains a lot faster, right? Because the convolutions mean there are more operations that have to be done because I have to apply every patch, every filter for every patch. So trains a lot faster, you know, two seconds versus five ish. But test fraction is only 83 percent as opposed to 99. So we plot that and it looks like kind of a big mess, right? There's a bunch of squares that are being misclassified as diamonds and a decent number of diamonds that are being misclassified as squares. So what did our filters actually learn? So we get this. So now take a look at this. So what do you think it's tried to do here by looking at these filters? So remember what our inputs look like, right? We've got squares and diamonds randomly scaled and randomly moved about the frame and it tries to optimize for all of the data at once. That's what a neural network does. So let's take a look at some of the things we might observe. For example, we might see, you know, there is sort of a line here of high values. There's also one at a similar position in this other filter. So it might be that, for example, there could be a lot of squares that are in this region or there's lots of horizontal lines in this region in the training data. And so it's learned that, well, there's probably something here at this location in the input. So if I optimize to detect that, I will tend to get a higher accuracy. So it may have learned effectively to look for a certain type of feature at a specific location in the input. Similarly, you might observe that there's a diagonal line like here and maybe another one there. So you can see some sort of diamond esque features here. And so this might be another indication that a lot of diamonds happen to occur at approximately this location in the training data. And so it tried to optimize for that because by predicting that it got a better it got less error and a better better performance. But if these things don't occur in the test data or don't occur in as significant a proportion, then it's going to incorrectly predict things that may be a square that has a pixel at this location is going to go, oh, well, this is correlated with being a diamond in my training. So I'm going to predict that I'm going to predict wrong. So the invariance to translation is one of these key features of the convolutional net that a fully connected net is not going to pick up on because it's trying to learn everything it can about the individual pixels in the data. And it generally just doesn't do a very good job because maybe if I had a bigger network, it would do a better job with a simple task like this. But generally speaking, it's not going to there's not enough space in the network to represent the information at a per pixel level where the actual position of the pixel actually matters. Any questions? OK, so the the fully connected net tends to overfit to the straining data and so but it tends to it fails to perform as well in the testing data. So there are basically a few things that a convolutional net requires. So it still requires fixed size inputs like your fully connected net. But the reason it can do invariance to scale is because there is in there's an imposed ordering on the pixels in that the neighborhood of a pixel is the pixels on either side. So we can actually learn basically relations between pixels at certain points and pixels around them, regardless of the absolute position inside of the image. And so then there's also this or this there's this implicit ordering and then there's the neighborhood around the actual pixel. So this allows us to use the stride the striding operation, the convolution and that filter to segment images according to relevant information relative to each other, regardless of the actual position inside the image. OK, any questions? Yeah. In the images, are we not considering the lower values like the black spots on the left side of each image? They're pretty similar. Yeah, they're pretty similar. And they're probably close. They're pretty close to zero, I think, if I'm looking at like the edges. And in this case, this is also probably learning correlation with with the data in that we. We actually don't allow images to go off the edge, for example. So it's very likely that that first pixel on each side is empty in most samples. So it learned effectively that there is zero correlation or close to zero correlation with the edges and the class, because at most you might have like one pixel or one row or column of pixels if it's a square in that edge. But otherwise, in the vast majority of samples, it's probably an empty, empty space. Other questions? All right. So to summarize, convolutional nets use fewer overall weights, but they learn more generalizable matrices or filters. These filters don't necessarily match the input features visibly, but we can train them to recognize multiple different types of features in sometimes very diverse input sets. And we can recognize them at the same time. So these can be squares or diamonds, straight lines, diagonal lines, curves. As the network size increases, the capacity for learning more types of filters also increases. What we need to do is we need to take the image segmented into patches, stride over those patches. We get a significant amount of overlap between the patches. And so now these filters can learn the difference and similarities between neighboring patches that have slight differences between them. So after you standardize your input values, so these these just like intensity or in RGB, they're just color. So you're going to standardize them into a distinct range, convert that entire matrix into patches. We have a function that does that for you. And in both training, you usually do this so that your input is of the same format in both functions. So now the input to the forward pass is the patched version, not the raw input. And so then if the output of a convolution layer is input to the fully connected layer, you flatten that into a single vector. So now it just becomes basically a feature representation that goes into the fully connected layer. So the hardest part is back prop. So because we have multiple applications of each individual patch and each individual filter. And so those delta values are going to differ depending on the different features in the patch. And so they're going to have multiple values for every convolutional unit. And so that has to come from the application of each unit to each filter. So we sum that from each unit to each patch, we sum that for all the delta values, take the whole delta matrix, reshape it into k by n, where n is going to be the number of units in the convolutional layer that you want to back prop. What's the other thing we need to multiply? So we have the error, we've got the features, then we have the input. So then we reshape that input matrix to n by p, where p is the number of values in the patch. So now k by n and n by p will multiply together to give me k by p. This k by p will give me the gradient of everything but the bias weights. So this is going to be the gradient associated with k classes or k outputs and then p patches. So then I'll sum every column that reshaped data matrix, delta matrix, and get the gradient bias weights. So you compare this to back-propping through a fully connected layer where all you need to do is multiply the inputs to the layer by the delta. And then we have no delta matrix to reshape, it's just all been single values. OK, so final questions. OK, I'm going to take it at your word that you don't need to see me in office hours. If you do, I will be there, but I'll be writing. But I will give you back 30 minutes and I'll see you on Thursday. Okay, let's start. All right, so messages again to people who aren't here if I excuse you from one day doesn't necessarily mean your excuse from the next day or if you're still sick or whatever you gotta let me know. I understand the weather is kind of crazy right now and this seemed like people do get sick with the sort of weather change so just please make sure that if you remain sick, you let me know that you remain sick. So if you're are not here one day and you're still not there the next day I'm going to start wondering why. So, it's just, where are we right now I'm going to do today is I will do convolutional neural network training with pytorch. So we're getting pretty far ahead in the content of like what compared to where you are with the homeworks right now right now you're doing a cross validation, and then we're going to do and this is going to be the fully connected that you won't do convolutional network training for an assignment or assignment five. So now, sort of apply versus what is what your class is going to start to start to diverge a little bit on we are back on schedule what I'm going to do today is I'll go through this notebook which should be on the shorter side. And so then, after spring break. What you usually have is kind of kind of like a review session. So I'll take any extra time today and just answer questions about CNN softmax some of the various components that we've had to put together, you know, in the last couple of weeks. So I will have office hours, we end early for some reason I'll just go over there. And so, now would be the time to ask me questions about assignment three. And also if you started thinking about project ideas, I want to discuss that. You can also do that during office hours today. So after the break, I will formally announced the project. This proposal, you see if to write down what you propose to do and I'll either approve it or ask you for revisions, then you have about six weeks or so to complete the actual project. Yeah. You are allowed to you don't have to you can work in groups up to four. The catch is that if you work in a group project I expect to see a commensurate amount of effort. So like if you work in a group before it better look like it took four people to do your project. Other questions. So, this is going to be in terms of the amount of work there would be compared to be a lower bar to clear, because it's one person worked for six weeks as opposed to four people. I do expect to sort of see quality work but it's going to be you know I'm going to qualify that based on like how many people are on the project is not going to be like, you know, you have to run every exhaustive evaluation on your own, you have four people you can probably run a lot more evaluations. That's the sort of thing I'm looking for. Anything else. I guess the point of that is that it would behoove you to start thinking seriously about your projects. And I'll go over this again after the break but if you already have something you're working on. Maybe start thinking about how you might be able to apply that to like put a machine learning spin on that. That would be an acceptable thing to do for this class so my goal is not to give you extra work. If you already have some active line of research and you want to make it so it applies to this class that's perfectly acceptable you have to convince me that that's what you're going to do and you're not just like completely piggybacking on something else you have to do some dedicated effort for this class, but you're allowed to use existing resources and things you know your, the goals for you to use your expertise in in a way that's relevant to the course material. Okay. So to share screen. All right, can you can everybody on zoom you can see my, my notebook window. Sorry, can you can you see the zoom. All right. So, you've done intro to convolutional neural nets. So basically the core mechanic of a convolutional neural net is this filter. This is just a weight matrix that effectively needs to be optimized to output a higher value when it encounters things that quote match it better. Those things of course can be multiple different combinations of features. So these are handcrafted filters just to illustrate how the concept works in terms of what type of activation and scalar value you're going to get out of performance linear some operation from a filter to a patch of the image. Of course you want to be able to actually learn these filters from real data so pretty much the training process of a CNN is trying to optimize the weights in these filters you have to specify things like what the size of filters are, and then how how you patch up your images. And so that's going to learn weights that are optimized for whatever types of data encounters assuming those hyper parameters. We did the NumPy version. And now of course you want to make use of as much speed as you can because as your network start to grow, they start to take more compute. So let's go through effectively similar operations, except using PyTorch this time. So, just a point, if you were going to use PyTorch on the workstations in the CS department, you want to execute this command. So basically what this does is it's going to add the site packages library where PyTorch should live to your Python patch. Sorry, Python path. Can't speak today. So you can run this every time that you log in but a better solution would be to add it to the start of script in .bashrc. So that way it's always there when you log in, and you don't have to worry about this. But if you have things like, if you run import torch and you can't find it, very likely it's missing from your path. So just a point of fact. So this is taken from this example of implementing a CNN PyTorch. So this is kind of stripped down version of that. You can look into. Okay. So we're going to do our usual sets of imports, right, so we're going to import NumPy. Of course we still want to use NumPy to do things like you know processing and plotting our outputs, because remember PyTorch operates as a tensor, and you can't do certain operations for things like next visualization directly on tensors. Nonetheless, the main thing we're going to be using is torch and from torch we're going to import the autograd module. And then we of course going to keep track of time and Jesus and Pickle are to to open our data. So in our NumPy version of a CNN, this looked, our constructor looks something like this. Hold on. So we specify our patch size, and then we initialize an instance of this neural network classifier CNN so 28 by 28 that's going to be our input size. So remember, fixed size inputs, and we're going to assume square images. And then this two and two this is going to be our list of that defines the hidden layer architecture and output size would be the number of unique labels you have right so for MNIST this should be 10. So you're going to have unique values in T or whatever your class labels are. I set my patch size remember that this is a square, so I say patch size five. This is effectively a five by five. Right. And then stride to similarly that's also a square I'm going to take two step two pixels over, and then two pixels down and I reached the end of the row. So this is also like a two by two in both directions in the PyTorch version you're going to see a lot of the same things, but it's slightly slightly slightly different. So we're going to use of some of the PyTorch functionality to more automatically define things like our network architecture. So, first of all, just like take a look at these two definitions, and what do you notice that is similar about them what do you notice that is different. Start with the easy one what's the same about these definitions. First parameter right both 20 by 28. That's the input size. Yeah. Yeah, so the output size too so it's the same data, right so it's going to have this is MNIST they're going to be 10 classes still. And our inputs are going to be 28 by 28 images. Okay, so now what's what looks like it's different about this. Yeah. Yeah, yeah, right so I'm separating out, you know, the convolutional layers and the fully connected layers. So here. Remember when we did the version last time we basically said okay I'm going to just decide there's a single convolutional layer, everything after that is a fully connected layer. Right, so I could write the NumPy version so that is more like this. The PyTorch basically does this will allow you to do this automatically, as we'll see so right now, like, what you see here that I'm highlighting. This is not a PyTorch command specifically this is just a Python call that instead shades an instance of this conduct class. The net the conduct class will be written using PyTorch. And so now I can take these, these parameters and use that to basically, you know, construct the individual PyTorch layers one by one, exactly as I want them. What else do you notice that is different about the PyTorch definition. Yeah, so this is well this is the stride. It's, it's too. What's the name of the argument that's there. Per layer right so I can do, I can do something I can have a little more control here in that I can specify different size patches for a convolutional layer and also different size strides for convolutional layer. Let's think a little bit about why you might want to do that. So remember, in our sort of single convolutional layer example, we have let's say 28 by 28 image, and I might specify a seven by seven patch. And then I stride over it say two pixels at a time this is going to give me, you know some number to be like 11 I think 11 by 11 patches using cropping. So I'm taking a 28 by 28 image turning it into an 11 by 11 little sub images, each of them gets multiplied by a filter that should be the same size of the patch so also seven by seven. From that I get a linear sum right or I get some sort of value so the easiest one is just get a linear sum so single value. I can also do pooling, right, we're just going to take those initially seven by seven values, or whatever the size of the patch times And then I can down sample that. So I did like a two by two examples I take the seven by seven down sample it to a two by two, or just take a linear sum and down sample it to basically just a single value. Let's take the pooling example, because that makes it friendlier to more convolutional layers. Let's say I take my seven by seven feature map down sample it to a bunch of two by two feature maps. All of those let's say there's also 11 of them. So 11 by 11 of them 121. All of those, we have 121, but effectively like four pixel images that way. Those go into the next, the next layer. So if I have a bunch of input this effectively like pre patched already there two by two, and I have a patch size or stride length, that is three, or let's say even two, what's going to happen. I have a bunch of two by two images. I have like 11 by 11, and I stride over them with a stride length of two. So I'm not going to be able to really overlap between my patches. No. Let's say my stride length is three. No, I'm also going to miss things. So effectively, the stride length is going to be dependent upon what's the resolution of the thing that's actually going into the layer. If I have a patch size that is five and a stride length of two, this is going to down sample my five by five and I pull my down sell my five by five feature maps into some lower resolutions. This makes it easier to to compute right it's faster this list less information. Also if I do my pooling right I might get a pretty nice representation of what's really important about those feature maps at with less computational cost and less memory cost. But I'm also going to be feeding in a smaller quote image into the next layer. And so, if my stride length is too big I won't be able to patch my images effectively. So it's going to basically just throw an error. So, it can be, it's usually desirable in fact to have different patch size per convolution layers because you want to get important information from maybe a very complex image down sample it something that's like you think is truly important about that image and then see that in the next layer so we can do a similar operation over those what we'll call gisted feature maps in order to get what it then thinks is really important using filters that are probably optimized for somewhat more complicated features in the end. So when you're using your convolutional neural nets you want to be careful that you're not an image that's of too small resolution to be useful. So, for task like MNIST, one convolutional neural net or one convolutional layer is probably enough because it's very simple task for more complicated things real images, you know, multiple convolutions, multiple convolution layers, but you got to be careful about these things like stride length. Okay. So, that all all clear so far. So first just let's look at a backdrop from our previously existing neural net lectures. So if I'm trying to optimize the weight or the value of the single weight. So remember we're going to have some weights that just sort of reside inside these hidden units all apply some activation to them. And we can't we showed how we can optimize the weights one at a time so if I have this is a layer V, and I have an individual weight I'm trying to optimize the value that this gets optimized to is going to depend on among other things. The error that results from every unit that is connected to this. So, just consider there's like one neuron right here, and it's connected to in this case three other neurons so whatever value comes out of this first hidden neuron is going to have some bearing on the output that comes out of those three other neurons. If those are my predictions, then it's going to be some level of wrong compared to what the actual truth is. And so I want to optimize this weight in this quote parent neuron over these are all graphs in a way that's going to take into account how, how wrong all the children are. So, it's like you know if you, if your children misbehave you punish the parent. I guess that's what people do. My children are very well behaved so not an issue. One of those even born yesterday hasn't had a chance to be hasn't had a chance to misbehave. Anyway, what we are trying to do is like if this is slightly wrong right if y y one minus t one is like a little bit wrong. y one minus t two is like really wrong, and then why t y three minus t three is, you know, also really wrong. It might be more beneficial to perform a greater optimization on this weight because two out of the three things that it connects to have very large errors, right so we can we consider all of these. So basically in plain English what this will show is that we've got k output nodes in this case three. And so this should output, you know, k number of values for any input sample. So for this regression example it might be continuous values, so I want to predict properties of a car from other properties of a car. And to classify this is an output probabilities. So again the error term is really the same it's just that I have to make sure that my quote, not units but kind of units are the same right you'd be subtracting MPG from MPG horsepower from horsepower probability from probability, and just modulo, whatever standardization you have to do. So, all these are, there's, these should all be apples to apples comparisons at this output layer so then the way to update will be follow these red lines. So if, depending on how wrong these things are, this is going to be updated based on values that are in part derived from these, from these values. So I see parsley dried here just in the colloquial sense, not in terms of the partial derivative. So, the partial derivative is kind of the key component of the actual mathematical operation of backpropagation. In this case I'm just saying, a portion of this error is going to inform how much the connected way it gets updated. So, we updates in the hidden layer are going to depend upon the way updates, to some extent in the output layer or any other layers that are connected subsequently to that hidden layer. So, we're going to have about like say two units and W, and the upper layer and just a single hidden unit and be so we'll just have you know one hidden unit and then two output units connected to it. So these are the derivatives that we use that you may remember from, from the first year of network lecture. Right, so the partial derivative of the error with respect to weight one is going to be negative two, or in this case to constants you can factor it out, times the input disease is going to be the input that that that output layer, and then the partial derivative of the error with respect to the hidden layer weight is going to be similar except because there are two output weights, I have to sum the, the error times the value of that weight. Right, and then, because I'm also going to be applying this activation function before the output of layer V goes into layer W, I have to account for that so if my activation function is 10 H, this is going to be one minus z squared. And then the last thing I need to do is I need to multiply it by the input to the layer, and in the hidden layer that it would be x. So, the update on w one is going to be proportional to the product of the error and Z, which is the input that layer, and then the update to V will depend on the sum of both of those errors in output layer W times the derivative of the activation function with respect to Z, and times the input to the hidden layer. So, same thing is happening in my classifier network. So, what was the point of all this you know if you've been paying attention you understand how weight updates work. So let's actually see what happens when we apply it in convolutional neural nets so if you just remember from Tuesday. I have the same filter that's going to get multiplied by a bunch of different patches. And that's going to contribute to that final prediction. And so, I need to best optimize this particular filter to account for it. And so, that's going to be a good prediction for everything that it might encounter. Right and good prediction in this case might mean that high activation for for certain things at a low activation for other things is just trying to best optimize what those things are that when this filter encounters it, it's going to best produce the most predictive values and over all these working conjunction there are a bunch of different hidden units, and just trying to do this all at the same time. So, in order to out to perform the actual back propagation operation in a hidden layer. What I need to do is I need to collect all those delta values from all applications of all filters to all patches. And then let's turn this into a big weight matrix, as you can do, do my multiplication operation all at once, just to make sure that it reshaped the inputs into a compatible shape. Okay, so we'll do our kind of square and diamond task again. So here I'll define my diamond. So there's my diamond. And so now what I will do is I will define a patch size and a stride length and I'll patch it appropriately. And then this is going to give me something like this. So there are 64 patches according to this patch size and stride length that I specified, and this patches look like this. So now, this filter would be applied to each of these patches. So as you're going to generate a feature map that is the product of the filter and the patch where in this case C is going to be the weights of that of that filter. So this is the sum of all those values. So if prod here is basically just a list of the values that result when you apply this filter dispatch, a sum of all those, this becomes a scalar value representing how responsive this weight is to the values in the patch. And then this value thing gets propagated through the network. So right now I'm taking a single value, I could do pooling and just down sample to a lower resolution image. So what our standard neural network operation looks like now is very similar. So the only thing here is that if z equals h of xv, v are now our convolutional units, and there is an application of each one of these to every patch. So I can remember my patches are now just considered them to be input samples. I just flatten them and I feed them into my, well, I guess I flatten the image. And then what we do is we just reshape everything so it's a nice square that multiplies together to get our value. Then y is my prediction. This is just going to be z with my bias times my output layer weights. And then we have the softmax operation. So what I do is I exponentiate y. So y is just some scalar value. So I need to turn this into something that can be turned into a probability distribution. So I'll exponentiate that. I'll then sum over all values of f for each class. So this is basically summing across the columns. And so then I will take that and then take the f for each individual value class divided by the sum. And this is going to give me a probability distribution. So now what I'll do is I'll take the sum of all my indicator variables. So now my indicator variables are just these, again, one hot vectors. So think of them as a probability that is all zero except for one case where it's 100. And then times the log of the actual probability that I predicted, which is g. So if I do that, then what I can do is I can take the gradient in v. And then what I'll do is I'll take my inputs, x input in layer, and then multiply them by t minus y, except now these are as probabilities. So I have the indicator variable version minus the probability distribution g. And then multiply that by the weights and then multiply that by the derivative of the activation function. So these parts are pretty much the same. It's just that I've got the indicator variables minus the predicted probabilities rather than a scalar minus a scalar. So now what I need to do is for the convolutions, I need to sum all of the partial derivatives of the error for every delta value in the gradient. So then for every convolutional unit, we need to find the delta for every scalar value. And that's going to come out from every feature map when that filter is applied to each patch. So let's review the back propagation of weights in the CNN. So we'll assume this is kind of a recap of the one from last time. So we'll assume this only has a single convolutional layer, and that's the first one. So what I'll do then is I'll reshape this backed up delta matrix into the right form. And then I'll reshape the convolutional input matrix to something that's compatible. Now I can take that input times the delta. So this is just doing this part here. And then what I will do is I'll just calculate the derivative of the error with respect to the weights for the convolutional layer with simple matrix multiplication. And then in the fully connected layer, it's the same as we've been doing. So the trick is basically just collecting all of my values for every application of every filter, every patch into a big matrix. This allows me to effectively perform the same operation as long as I can ensure that my matrices are in the right shape. Okay, so this sounds like a lot of work, right? So you got to remember, okay, I've got to collect all of my delta values into a matrix. I have to make sure that my inputs are appropriately shaped. Then I can do a matrix multiplication. But let's check the PyTorch definition again, right? This is a single convolutional layer. In the PyTorch definition that we had, we can have one multiple convolutional layers. Each one has a different patch size and each layer has a different stride length. Here they're the same, but I could very easily change them to be different. So you can see that this is rapidly going to grow out of control if I'm trying to do this all manually using NumPy operations. There is a very good reason why we limited the discussion on Tuesday to having a single convolutional layer. So it seems like a pain. If only there was like something we could use to automatically calculate the gradients. So if you're thinking, boy, I wish I had something like that. I have good news for you. Let's remember lecture eight. We got this thing called autograd in PyTorch. So recall what autograd does in brief. We construct our neural network as a computation graph where everything can calculate its own value and the partial derivative of its own value. And then you connect those things into a graphical structure such that operations, the outputs from a single, from one node can then be fed into as arguments into any sort of child node. And so then I can calculate everything. Let's say the partial derivative of every leaf and then back propagate them with a simple, with a single one line call. So this definition of ConvNet using this is basically what we're going to execute that Python call and instantiate. So first you'll observe we're inheriting from torch.nn.module. This allows me to make use of all of the PyTorch functionality for doing things like creating neural networks with simple commands. So now I can specify those things that I want, right, the input size, number of convolutional layers, and new stride lengths, patch sizes, etc. I'm going to do some things to make sure that we can use the GPU, but I specify the activation function. Now that I've done this, I can now create all of the convolutional layers. So here I'm going to create this thing called module list, and then I'll add things to that list that represent those individual layers and their properties that I want. So first, this argument is going to be the number of channels for each pixel, this first argument here. So the input height and width is going to be the square root of the number of inputs. So again, if it's 784, its height is 28 with this 28, so the square root of 784 is 28. And so now I can create this module list for the convolutional layers, and I will just kind of zip up my number, desired number of hidden layers, patch size for that layer, stride length for that layer, and then every trip through the loop I'll add something to the convolutional layers list that contains those properties. What that is, is this is what you're probably going to see when you read other people's PyTorch code, is you instantiate these items, it's basically like torch.nn.layertype. And this could be .dense or .conf2d or .conf1d or .lstm or whatever type of network you're trying to write. And then you specify exactly those things that we just talked about, the input size, the number of units in this layer, and then other in this case for convolutional nets, we have the kernel size, I have stride length for other types of layers, these would be slightly different properties. So you'll see, I'll create nnet equals torch.sequential, and then I'll add these things one by one. So what I'm doing here is I'm feeding in a list that automatically specifies what I want the properties of this layer to be. And then because I'm assuming this is all convolutional except for the things that I specify like number of hidden and convolutional layers, I'll create the convolutional layers that way. And then I'll also create these linear layers for the fully connected. Okay, so this is just your standard way of just creating your your pytorch neural network is effectively just a list of what layers I want. And then when I do like, you know, model of input, it'll run my input through all those layers in sequence, I can specify very cleanly. I want a convolutional layer here, I want another convolutional layer, I want to do some pooling, then I want a linear layer, then I want to do like dropout, and everything that I could possibly want to try. Okay, so now forward all outputs is going to be basically just like one pass through my network with inputs x. And so this forward, the forward pass will basically call this. So you can see that it's handling the convolutional layer and the fully connected layer. So what I can do is I can just say for conv layer in self.conv layer, so for every element in that list, which is an object that represents the layer itself, I can actually just use that object as a function over the input. And so now I can just say that, okay, specify the convolutional layer, it's got some number of hidden units, each of them have x weights in them. If I just call a conv layer of inputs, it'll run that convolutional layer of over the other elements. Train is pretty much the same as we've done before, so I specify what method I want to use all this should look fairly familiar so I just remember set requires grad, I specify the type of loss function that I want to use in this case because we're doing categorization I'll use across entropy loss and then the rest of this stuff is pretty much just for plotting and training. So to calculate the loss, I'll actually run my CE loss function that I just specified over y, which is my predictions and the targets for this batch. And then loss.backward will actually perform back propagation and then call your optimizer.step will actually perform the update of the weights. And then lastly remember that we have to zero out the gradient every time otherwise it's going to be accumulating the gradients because that's just how PyTorch works. Now we have the softmax function in PyTorch. This is the same as we've seen before, I'm just using PyTorch tensors. So here is a trick here to avoid overflow or actually calculate the max. But then I exponentiate and then I take the denominator, I divide the exponentiated version by the denominator, that's our softmax operation. And then use function is use function. This is pretty much the same I'm just using the Torch version instead of the NumPy version. Last thing, especially if you encounter problems with the GPU, I recommend you refer to this notebook here. So you need to detach things from the computation graph, move them back onto the CPU and then optionally convert them back into NumPy to do processing with them. So if you leave things on the GPU, you will encounter difficulties. All right, so I will set the device that I want to use. In this case I'm running on my lab machine so I have access to the GPU, I will hit yes. So it's now running on CUDA. I can check nvidia.smi, I forgot to tell my lab that I'm using the machine for class. However, it does not appear that anyone's really using it. It's fine. So you can actually see with this nvidia.smi command the usage of a given machine. And so here I can see like, I don't know, someone's, I'm assuming this is probably me running Python 3.8, no one else really appears beyond this right now. All right, so now that I specified my network, I can actually try to run it. So I'm going to open up MNIST. Right, so this should look pretty familiar. So what I'm doing, I'm just splitting it into train and test. We must reshape the input matrices for convolutional networks in PyTorch. So what I'm going to do is here I'm just going to take the number of samples, one dimensional sample, 28 by 28. This actually requires a two dimensional input to be fed into the convolutional layer. It will do whatever flattening is required. Yes. The parameters of torch.module. So did you see that somewhere? Oh, here? Like these? Yeah. So basically this is going to say what needs to be updated. So remember self is an instance of a neural network. So it inherits from torch.nm.module.parameters is going to be one of the actual weights of this. So this is W. So what am I updating? This is my collection of like W, and V, and V prime, and whatever. It's basically, you remember how we would collect all our weights into like a single array to do say atom optimization back in Lecture 6. This is just a version of that. So I'm going to have a neural network that is a convolutional net of a certain size with a certain number of layers and a certain number of units per layer. So this is going to in turn mean that I have N weights that have to optimize. This self.parameter stores basically the address of those actual weights in memory. So when I call torch.optim.optimizer, I pass in, what's the address in memory of the things I'm actually going to update whenever I make this call? Okay. So this is never explicitly set because this is a member of torch.nn.module. So because my ConvNet class inherits from torch.nn.module, it has access to this. So when I call self.parameters, because self is that instance of ConvNet, which then inherits from module, it has access to that member variable of module. So when I create the convolutional net using that single line call that in my case, it's stuck inside of a for loop, it's doing things like assigning, you know, it's initializing values into self.parameters. Any other questions? All right. So, all right. So we open up MNIST. We can see that it's pretty much the same. So you can see that I've got by default, the data, the numerical representation of MNIST is flattened. So it has just single 50,000 rows, 784 values. So PyTorch requires that we actually reshape it to that two dimensional shape to feed it into a 2D ConvLayer. So you'll see that before reshaping, we got 50,000, 10,000, et cetera, by 784. And now I have 50,000 by 1 by 28 by 28. So I've got 50,000 samples. And then each of them is basically a single dimension. And then each of that is 28 by 28. So now I can have things like I can specify a batch size that I might want for that second argument. So I can say, OK, I want to feed in 100 of these samples at a time because it's slightly faster than just feeding them in 1 by 1, 100 times faster than feeding them in 1 by 1 and optimizing the weights for every individual sample. OK, so then what I will do is I'll instantiate my instance of my convolutional net. So now I call ConvNet and it is instantiated using 10 hidden units and 20 hidden units and then a final fully connected layer of five units, output size of 10 because there's 10 classes. And I have a five pixel stride or five pixel patch in my first layer, a seven pixel patch in my second layer. And I stride two for each one. And then we train and we can see that we achieve our loss after just 10 epochs is going down significantly. And then finally, one thing you can do is you actually just print the network structure. So it actually will print out this nice little display of what your network looks like. So you can say that I've got a conv layer one and two. And then it takes in, say, a one sample and 10 hidden units with a kernel size of five by five and a stride length of two by two. The second one is going to basically map from 10 to 20. And then it's going to have a kernel size of seven by seven, striding two by two. Fully connected layers. So you're wondering, OK, I have to flatten the output of my convolutional layer to feed into the fully connected layer. How many actually dimensions is that? So we can just print this out and see exactly it's 180. So this fully connected layer will take in 180 inputs and then map them down to basically five dimensions. So in features, number of dimensions on the input, out features number of dimensions of the output. So that's five. And then this last thing, this is the softmax layer. This is going to take basically a five dimensional representation of every sample and then distribute it into probability over those 10 classes. It's going to say, OK, we've got you me five numbers and I will tell you what the probability that it falls into any one of these 10 classes is. All right, so we trained. Looks like it went pretty well. So then I will use my use function. So I have my test set and what it will print out is going to give me the classes and then I'll use that to calculate the percentage that's correct. 90.58 percent correct. So we can try a few things like we can we can train for longer, for example, if I train for twice as long, then. So you get 95 percent accuracy. Right. And so you can see that just by just by increasing the training size or the training length, we get a significant boost. You can also try, you know, messing around with the batch size, adding more hidden layers. You got to be careful when making sure that your your patch size and your stripe length are appropriate for the layer. But you can see what kind of accuracy we get. So let's examine the effects of one of the layers weights over on an input. So first, let's get the hyper parameters of that first layer. So that first convolutional layer. Let's just take a look at what those values mean. Right. So it's got 10 hidden units. This is the kernel size five by five and the stride length two and two. So now let's view the outputs of each convolutional layer. So I'll take one sample and this here's how to take the 20th sample from my test set. So I will then turn that into a torch tensor and now I'll just run that directly through the forward all output. So I have a single sample. What I want to do is I want to grab every the output of every layer and accumulate them so I can see what's happening to the sample in the first layer, the second layer, et cetera, et cetera. So now I'll have like layer one weights. This will give me the weights data for this list or for this for this layer. So if you look at like CNN net dot children, this is going to say in this graph structure, I can find the children. So these are like the first child to be this conglayers set. Second child to be the FC layer set and then inside conglayers I can get like, OK, the first convolutional layer. That's what the zero zero is. And then in that I can do dot weight and actually get dot data, which is going to give me the actual weight values. So now what I can do is I can plot this. So I'll take the layer one weights. And then I will multiply this input by those weights and we'll see what each layer is actually what each unit is actually predicting for actually outputting. So this is the output. So take a look at these. So what you'll see here, that's the original image. This nine on the left, you will see these are just the plotted values of the weights. Right. So these are the filters. And then this on the right, this is going to be what happens when I apply this filter to every patch of this, this nine. So take a look at this. And what do you what do you see? What are these different filters? What do they appear to be doing to this image? Yeah, I mean, ultimately, yes, but let's take let's let's look like one at a time. Let's take this. Maybe compare like this one and this one. Yeah. Yeah. Right. So and they can remember what these numbers with these colors mean. Right. So if it's, I think we're using the negative version here. So if it's darker, it's more positive. And if it's lighter, it's less positive. Right. So this filter, for example, this first one is probably activating more on like the outer edges of the nine. So maybe it's like responding more to like lower pixel values or something. Whereas this one, it seems like it's mostly not it's sort of it's gray on the outside. So sort of ambivalent. Right. Maybe it doesn't have very strong, either positive or negative correlation either way. Inside the interior of the nine, it seems to be mostly negative or low values. But then all those edges, it seems to be having higher activation. So this one, this filter, at least for this sample appears to be kind of optimized to detect this type of this type of feature. And in fact, you know, this is sort of one case where this actually might be reflected in the plotting of the weights itself. So you notice like, take a look at where you see the darker values in this filter and compare that to where we're seeing the lighter values in in the feature map. Right. So sometimes it's not so obvious this one kind of seems like there might be some sort of correlation there. So, what can we say kind of generalizably about this? We have different filters that are optimized to kind of pick out different features of an image. Right. So some of these are maybe optimized for things more like certain types of edges. Maybe these two kind of seem to result in a similar feature map. So maybe these two, even though they look quite different, are maybe optimized to pick out similar features. And these are all normalized. So there's no guarantee these values are actually the same. But within that normalized range, they appear to be similar. And so does that one. Then if you look at like these two here, they also, you know, again, the filters themselves have pretty different weights. When you apply them to different patches, you'll find that they're maybe selecting for like the outer portion of the image as opposed to like the inside. All right. Questions. Right. So what do we observe here? We just did that. So basically what we can take away from this is that these different features, we have 10 different filters, and each of them is optimized to pick out a slightly different part of the image. So you put all that together, you can imagine that for the different things that occur in MNIST, like, okay, yeah, these filters can pick out like the edges that I'm interested in, the curves that I'm interested in, maybe like the outline of certain shapes. And so these are all useful things to identify in hand drawn digits. So this 95, 90 to 95% performance we get on MNIST seems to make sense. So now let's take a look at the second convolution layer. Right. So again, I do index at one. I'll get the second layer. So this then takes in 10 inputs, has 20 hidden units, kernel size is 7 by 7, stride length is 2 by 2. So layer two weights is going to be the weights from this, from the second layer. And so now I'll see what the outputs are when I actually input some samples. So I'll take the same X, that same nine image, and then plot the outputs of this. And this looks like that. So what's going on here? Right. This is not very interpretable at all. So you're taking layer one as an input and then saying something. It's saying those feature maps that come out of layer one. So basically the inputs to this layer two is all this stuff. Right. So, okay, we can see that this sort of, you know, this is reminiscent of a nine, especially if you know that a nine is what you're looking for. But then I take these values and those are a quote image. Right. This has already been down sampled. This is no longer 28 by 28. This is, you know, I don't know what it is, five by or seven by seven or something, whatever. So this is already lower resolution. And so then I put that into the second layer. It gets chopped up into patches. And then a bunch of these weight matrices that, you know, if you plot them look something kind of like these as well, random looking patterns and then you multiply that by those patched, chopped up, patched low res versions of the nine that have already been kind of processed by some of these to such for some of these features. And it comes out looking like pretty incomprehensible. Right. So this you can't really impute any real meaning to what these things are actually detecting. It's more useful to see what those outputs look like numerically. So this is going to look something like this. And you can see that maybe this first row here is a kind of all the same for this, that one output. So it's hard to say which one that is. So basically every seven by seven filters going to apply to a 12 by 12 input. And that results in a three by three output. So this is, these are the actual these are the inputs for layer one. And then these are the outputs for layer two. Right. So this is three by three. So this is the second layer we get. In this case, I think maybe there's one fully connected layer. So this case, this is this would be flattened into a single array, and then fed into effectively just like a nonlinear classifier. But then then you get your final output, then you softmax, then you get your final output. And once again, that fully connected layer is like not required. It just often is used. But, you know, sometimes it just like adds extra compute for no real reason. So we're just sort of doing it here to show the difference between convolutional layers and fully connected layers. Okay, so we've got this, it's a 12 by 12 input, not 11 by 11. That gets fed into the convolutional layer. It gets chopped up and patched. That's seven by seven filters applied to each one. And then that ends up resulting in a three by three output. So, what this will do is I will then look at the indices that 12 by 12 image. And so what I'll see like, how many intervals of size seven can fit an interval of 12. And you'll see that it's three, right, we've cropped one pixel, because we run off the edge. But this is how we get from seven by seven applied to a 12 by 12 input to get a three by three output. Okay. So just think about like, how many times can I apply a filter of size n to an input of size m. And that will tell you. And so then you assume either crop or padding to make sure that everything fits. All right, so now what I'll do is I'll grab the first 10 samples from some random place in the test set, and I will plot them. And so, this looks like what this will show is this will show the sample and this was going to show the probability that it is a member of any given class. So for example, here's a one, and it's got a pretty high probability, probably close to 100 that it is a member of class one. There's also kind of a low probability that's a member of class seven. Right. Here's a seven, and we see the reverse. Right. That makes sense. Right, because sometimes depending on how you write a one that can look a bit like a seven. If this if this had like a little line there, you could it could be somewhat ambiguous so yeah. Yeah. Not necessarily I mean in this case it does just because we think about think about the number the digits you know zero zero through nine. The one that looks most like a one is seven out of all the other choices that you've got. So like, it's likely that like the second place choice for a very obvious one like this is obviously a one no reasonable person even, you know, confuses for a seven. Yeah. It's likely it's not necessarily globally true so there's probably some samples in here where it has like the European seven with us with a stroke across. And in that case, it might be really, really evident that it's like this is seven like nothing else even comes close but it's just check something out real quick so. Okay. High probability seven non zero probability of one. What's that. So, this network even though this doesn't really look like a nine in any real way and you know you don't really confuse sevens and nines. Our network has determined that there is maybe a slight probability that this is actually a nine. Right, so I could rank all the output probabilities in order. This one is even like a very small probability that thinks this one is a five. So a lot of these really small values come out from just doing the optimization usually from randomly initialized weights. It's like, it does pretty good in predicting like the top one accuracy because the choices are pretty obvious if you go a little bit further down it's like yeah the third place choices thing is making is like really make a lot of sense. At that point it's just almost like residual distribution of the probabilities. So we can find examples maybe that are somewhat more ambiguous here's another. So here's like one of those F sevens right we actually sees a similar thing where we see almost identical plot, actually. So in this case the network has seems to have optimized you for this type of distribution, maybe it makes sense with the nine because like you could you know if you close this gap it might look a bit like a nine. So let's see if you can find some other examples that are maybe more ambiguous may have to run this couple of times. Let me try a different set. So, okay here's here's one. Right, so here's a nine. And it's pretty obviously a nine but the actual value here might be like it's 50% likely to be a nine and that just happens to be the highest of all of my choices. But, you know, it also thinks there's at least probably a 20% chance that it's a four, and also a slightly less than 20% chance that it's a seven. So, our network right if we were to plot the distribution of our test samples, we'd probably see our sevens and our nines kind of close, because for whatever reason, it seems to have noticed that like one of the nearest neighbors to a nine is probably some instances of class seven. So, in this case, this is one where maybe it's not quite so confident that this that this is a nine, even though that is the highest probability. So the individual samples they the probabilities have to all sum to one obviously, and you usually see similar distributions like if you look at these two threes. Right, there's roughly similar probabilities of it being an eight and maybe even a zero. So, there's kind of similar distributions across classes, but there are cases like if we look at this nine and this nine, you know, the distribution is roughly the same in terms of like you you can sort of like linearly scale one to the other. But this one, this particular nine seems to have a much lower confidence on the probability of being nine. So, you can compare it to like this other one, like this nine looks a whole lot more like this last one. Right. Other observations other questions. Okay, so just briefly review. This is going to be important for assignment four so good to start thinking about this now. What's the key difference between calculating your error for a regression problem versus a classification problem. Yeah. Probabilities is the key part I mean not necessarily calculating the probability that you're incorrect you're calculating. What, yeah. That's what you output that's what the softmax is out there so we talked about the error so if I have my prediction, minus my, or my target minus my prediction. So, if I have my prediction, minus my target, I would say, in assignment one for regression problems. What's different about classification problems to represent this probabilities and therefore So, the target is zero and one right and so the targets are represented as like the ground truth values are going to be represented as what one of them is going to be one. So it's a what it's a what kind of variable. So we have the indicator variables right indicator variables are a bunch of zeros, except in the correct class it's a one. So if that if these, if you think of these as probabilities instead of numbers. What does that one represent 100%. So this is the ground truth. I'm saying, I know that this sample is a member of class nine. Okay. In other words, there's 100% probability that this, this sample is a member of class nine. If these are probabilities, they all sum to one. What is my network outputting in terms of probabilities for classification problem. So they also all sum to one right, their probabilities. And so, if my network is is predicting that for some sample. It is 30% likely that it is a member of class nine, and my target value saying it's 100% likely this is a member of class nine, then what am I subtracting Yeah, so you're directing the different the probabilities right so and then you're doing that at a scale that includes all the classes. Right. So if I have a free class problem. Let's see if people can see on zoom will try and draw this. Okay, so basically if I have a three class problem, and my sample is a member of class to and this work doesn't work. This might be better. Okay, so if my indicator variable is basically 010 saying that my sample is a member of class two and I can barely read that. Then my, my network will be output like okay point 1.7 and then okay point two. So, this is my target T. Wow. We try the red one. That's even worse. Whatever. This one is T. This one is why, okay, put a minus sign there, we're going to end up with is zero minus point one, one minus point seven, zero minus point two. This gives me an error for every probability I can then use that to optimize my weights. So what I want to get it I want to, I want to approach this thing. And I'm going to get this thing I want to minimize the distance between this thing, and this thing. The only difference here is that these are probabilities whereas in regression as predicting scalar values. So the quote units here are just like percents, whereas in a regression problem they actually represent things like whatever the units are the values you're trying to predict. Other questions, comments, thoughts. Would you need to have more data. Do we need to add more layers to it. Yeah, so as a general rule of thumb yes more convolutional layers will allow you to get better accuracy or get a better performance, but also a bigger network. You may be more likely to overfit. So, and this is not a difficult problem. So you know, throwing four convolutional layers at it. You're not going to get a whole lot of extra, like extra mileage out of that. For more, for harder problems, you know, image net action recognition problems. You know, type of video classification, whatever, the more complicated your problem is, the more likely you are to benefit from, you know, additional convolutional layers as long as you're not just kind of throwing them at the wall, you know, randomly. So how many layers we can have? How do we know like, at this point it's for the whole of it. I mean it's going to depend entirely on the data set. So there will come a point, if you know your data set there will come a point at which adding more convolutional layers is just not going to give you anything besides taking longer to compute. And usually that point is pretty apparent. You have to empirically try and verify it. So you know, you try like three layers and like, okay, I get 97% train accuracy and 80% vowel accuracy or whatever. You add another convolutional layer. It's like, I'm not getting much better than this. And it's just taking me a lot longer to train. So you're going to try and find that sweet spot between where you can actually train it in reasonable time versus getting accuracy. And as you add more, you basically approach the law of diminishing returns. You get to a point where it's like, I'm just not getting any more from adding this extra compute tower. One more question. When we get train and test results accuracy and train has higher accuracy than the test, what does, I mean, you might have explained this but what does that tell us about it? Does that mean that our train went well but when we actually tested it, it didn't perform well? It depends on like what the gap is. If it's like my train accuracy was 97 and my test accuracy was 94, that's fine. You're actually doing very well. If your train accuracy is 97 and your test accuracy is 79, that's much more of a problem. Something like that would suggest that there are probably peculiarities of the training data that it might be overfitting to. And that might be because like you have too many convolutional layers or something. And so it's like it's able to, if you got a train set that contains a disproportionate number of nines that sort of look like this one, right, it might be learning something about this like weird hook at the end or other features that are kind of spurious correlations to something in the training data that doesn't appear as much in the testing data. So I see this and it doesn't necessarily match anything that I train on very well so I don't know. Is it because, is it kind of to be also because of how we did separate the whole set into 80 and 20? It can be, yeah. So remember this is also why if you do, it's important to do stratified cross validation. So you can say like I'm not just getting a lucky split or something and my test data just happens to be very nice and perform like my training data, my test split or such that. They train very well and they test very well. Whereas if you try a different 20% all of a sudden it falls apart, right. So what you really want to do is you want to try these averages and see like if I sort of hold out a different portion of my data and then train on the remainder and validate and test on this portion and then rotate that portion I can see how well is my network can that be expected to perform on a random new sample. And so that can take more time obviously is like it's hard to you know train a big network like this. So often most of like the modern tasks will have a curated train validation and test set. So you say like I evaluated on like this, the test set of this data set and everyone knows that they can go and get the same test set. So presumably the test set is assumed to be kind of curated such that it's friendlies to network but not too friendly if you train on that training data. All right. Anything else. All right. So I'll call it a day there I will head back to my office. And you know you guys can just stop by anytime between now and then for 30, and I will have a good break. And I will you in a week. Okay. Welcome back everyone. Hope you enjoyed a good break. So, I, let me start with the schedule, where we're going from here. Okay, so what's going to happen, like I mentioned before break I'm going to be gone next week. What I'm going to try to do is actually get us a day ahead. Mostly today, because we have a review of softmax and CNNs that I kind of always build in after spring break, but usually we don't really need to spend the whole class on that if any. And so I'm going to try and basically get through notebook 16. So I can move 17 to Thursday and then do 18 remotely next Tuesday, and then, ideally, we can all actually take the 30th off, because I will be traveling, and you all can have a free day I guess if we, if we manage to do that so hopefully we can that's my goal. So, okay so then next Tuesday, I will be in California, but I will just do lecture remotely so you can come to the room and attend on your laptop is really wanted to. But you can just you know, watch from your, your house or, or wherever, or if you can all come in here and someone who rejected you have a watch party for lecture. Huh. How am I supposed to know where you. You do that and then you like write a reflection on how this was a different experience for you and two pages minimum then you get some extra credit. Okay. Yeah, and you can't use chat GPT to write it for you. So, um, a three is due tonight I. So here's one thing you should know about a three that I discovered. People have mentioned there's some, you're having some issues with the automated greater and some things like you're seem to be doing it right and maybe it's giving you some different answers so in some cases, I discovered, how many of you are using an Intel or an M1 Mac to do your homework. Okay, in some cases and this might be the case with other types of newer processors as well there's some slight differences in the architecture of the chip set that actually caused some slight discrepancies in the way it ends up performing grades and operations. So, I sorry I'll send you all the discussion that we first kind of found this last year, and we, we seem we think we can't put the way to address it. So this means that it might take a little bit longer to get your a three grades back because we have to sort through some of these issues. But it's going to warn you so if you are worried that the auto greater is maybe not is giving you things that you can't track it down I recommend you come talk to me or send me the the issue first like and maybe review but also know if you're using things like a new Mac. Sometimes if you're using like Python 310. It's maybe a slight issue. So I wouldn't necessarily get like too upset about that if you're like losing points on one of the issues and it seems to be like there's a slight discrepancy. Yes, in the back. I think we are grading using 3.9. So if you want to just be assured that you're writing into the same version that we're grading on us 39. That's not necessarily going to change anything with like the architecture of the chip side or something like that. So that's also an issue. Unfortunately, you know, well, unfortunately technology continues to evolve. And so our notebooks that were written back in like 2019 or whatever, are not necessarily like fully automatically compatible with everything that's out right now. Probably not. No, if you're getting full points the auto grader, especially even if you're using like a newer computer, most likely, you'll be the same for us. If you want. One thing that may help is like, you know, if you're concerned you can submit along with your assignment take like a screenshot of the auto grader and make sure that the output is like, present in the notebook. A lot of people don't do this never clear your output before you submit because then we have to rerun the whole thing and we're not going to see what you actually run. So like, you know, run the auto grader, show us that your output is, you know, you're getting full points. We'll run it again, of course, but if you notice some of these issues then that at least tells us that like we can check with you ask like what your, what your system settings are. Check it against a machine that has those same settings or as close as we can get. So yeah, apologize for that that is an issue mostly with like the GPU part of the code. But it's one of these things with this was just newer machines, and the way that they handle certain types of instructions. All right, so today, I'll lecture in whatever form it takes, which is hopefully mostly just being an also assign assignment for assignment for tends to be the trickiest one for a lot of people, fair warning is a lot of different moving parts to plug together I'll get to that when we, when we assign it. And then on Thursday project proposal so I know some of you come talk to me about your ideas. So, if you have an idea and you want some preliminary feedback on and I am available. The proposal. I'll assign that and then you will have, you know, little over a week I guess to write your proposal, send it back to me, and I'll have time to give you feedback and request any changes or clarifications. So, that being said, does anybody have any major issues that you want to review regarding classification convolutional neural nets or the softmax function. And if you do, let's take a few minutes to talk about that before I go to lecture. If you don't, then I will start reinforcement learning. So, I feel pretty good about softmax. Remember it's just like you're just computing error in the same way. You're just now you're subtracting probabilities from probabilities convolutional nets. You basically have a patched version of your image that allows you to simulate kind of the scanning function of the quote I over the image, and then you allow you train your filters to have higher outputs when they put match certain things in the input, and then with a set of properly optimized filters, you're able to do classification for things that have different types of image level features. Any questions as we go through the homework, of course, we can come back and review this material in office hours. But hopefully I think everybody's at least somewhat confident on this. I have no way to say no. I just want to stress that once more. If you're not confident about it, there's probably other people in the class who are equally not confident, but less brave than you. So if you speak up, you're probably doing your classmates a favor. But I'm assuming from the silence you all are completely expert at the softmax function and convolutional nets and you're going to have no problem with assignment four. So I have no choice but to take you at your word. So I'm going to start to lecture 16. That's assignment four. There we go. Okay. So this is the beginning of our unit on reinforcement learning. So we'll have about four lectures on this. So the introduction we're going to do is just sort of the simple, we'll call tabular reinforcement learning. So no neural networks involved at this stage. On Thursday, we'll show how you can use neural networks. So reinforcement learning can, for some people, be very intuitive and for some people it can be very non-intuitive and it might be kind of how you've been conditioned to think about machine learning. There are a couple of different ways to think about reinforcement learning. One is that you are training and using your model at the same time. Another kind of similar way to think about this is this is sort of like supervised learning, except you're not getting all your samples at once. This is basically you're in a quote world and you have to explore the world and that process of exploration is what gives you the actual samples. And so this is in some ways it's like learning by trial and error, which makes sense because a lot of human learning, especially when we're say infants learning to manipulate things, is done by trial and error. So the example that I gave last year was when my daughter was like kind of learning to eat with a spoon. And so she likes the taste of applesauce. And so if she loads up the spoon with applesauce or reloaded the spoon with applesauce for her and she sticks like the handle in her mouth, she doesn't get the applesauce. But if she sticks the spoon part in her mouth, she gets the applesauce. She likes the taste of applesauce. So basically, the agent has things that make it quote happy or sad. And it wants to be the most happy it can be or at least the least sad it can be. The way you specify your problems is analogous to that. So this is sort of trial and error learnings like this agent world model where the agent explores the world and learns about what types of inputs give it positive reinforcement or positive rewards and what types do not. And it tries to maximize the positive rewards or minimize the negative rewards. So, for example, I assume we all know how to play tic-tac-toe. So if you look at a board setup that looks like this, this is the current state of the board where X and O has made two moves each and your X and you're trying to choose the best action, right? You have a number of different possibilities. So this is not, I guess, this is not totally exhaustive. Or actually maybe it is. So basically we have five open squares on our grid. And you can place your X in any of them. Of course, depending on where you place the X is going to be a better or not as good move, right? So if you look at this, this is sort of a qualitative judgment. But if you look at the ones that are outlined in green, these are good moves. So for example, if I put X here, as X, I now have two paths to victory, right? So if O puts their blocks by going here, I can still win by going there. Similarly here, right, there's one path to victory. So maybe it's not as good as this one. Maybe this is like obviously the best one, but this one is still pretty good because like O could block me there. But then I still have like a couple of other places that I could go. This one maybe is not as good, right? So this is not really, it's not really obvious like how I set myself up for victory here because I put X there and I have to fill out like at least two more squares before I can win. And I don't really open another path to victory without an additional move. And like maybe these are marginally good moves, but like not this one. This one is actually very good. I'm not sure why this circle is dark green. I would say like this one and this one are like obviously the best moves. And like these two are OK, but maybe not the obvious best. And this one is probably like one of the worst moves you can make. So basically these may not, these moves may not like immediately lead you to victory, but you can see how they lead you to victory in like within the next move. Or you can see how like this one, for example, it's not immediately clear how in the next say and moves you're going to, you're going to be able to win the game. So all these can be represented as quote, so we have a set of possible states which is S and all these can be discrete. So, for example, in tic tac toe, that is the cardinality of S is going to be less than infinity. So we have tic tac toe game positions, right? So at most, even the board is empty. There's like nine possible moves that every player can make. Position in a maze, so assume that kind of a grid world, I can move like one step in every direction. But depending on the size of the maze and how many squares are blocked off is going to be a finite places, a set of places that I could go. Sequence of steps in a plan. So there's a defined goal. And I'm trying to figure out how do I get from where I am to the goal. There's going to be, and there's a set of discrete steps. I can only choose a finite, one of these defined things. So if I can't really do multiple things at once or something like that, then this would be a finite set. These could also be continuous values, right? So these could be joint angles of robot arm or this could be the angle of an inverted pendulum page, for example. Or you could have the position velocity of a race car, right? If I'm trying to control a vehicle, I have continuous values that I can pull at any given time. And they're not necessarily integers or they could be just like fractions to a specified decimal. You can also have parameter values for a network routing strategy or something. So any type of basically planning problem, planning or control problem, can be realized as a reinforcement learning problem where you have to define the states that you can be in. Now, when you're in a state, you must decide what action you take in that state. That action is going to move you to other states and depending on the correlation between states and actions, an action could keep you in the same state or it can move you to a different state. And the actions can also be discrete values, right? So the next moves are where I see the X's in red. And so this is going to, if this is my state here, each of these actions is going to put the board into a different state, right? And that's going to then dictate which actions are more or less advantageous in that state. So in the maze, where I am is part of what I need to know, but also I need to know where could I go, right? And so that's going to be from where I am now, what possible moves are available to me. It could be there's a wall on one side so I can't move there, right? That might restrict the set of actions that I have available to me. And then like rearrangements of a sequence of steps in a plan. So, for example, there might be multiple ways to get to the goal from where I am, right? It doesn't necessarily matter which one of these things I do first at this state, as long as I do some subset of them that gets me to a place where I can then move toward the goal. But these can also be continuous values, right? So if I'm my inverted pendulum or my robot arm, there's a torque that you're going to apply to like the pendulum or the joint. And that's going to be a continuous value. So, you know, I think in terms of this project that we're working on to control inverted pendulum using a joystick to the joystick, right, that would apply a torque. And that's going to be a continuous value. And then in the race car example, right, so if I'm trying to actually control the car, if I know where I am, and I'm trying to like move, learn how to like drive on a racetrack, I'm going to have to steer the car and apply a certain amount of acceleration in order to move the correct way along the track and not say, you know, drive off the racetrack or drive off the road or something like that. And then like in the network routing strategy, you like there are certain parameter values that you're going to need to set in order to route traffic through your network. And so these are going to also be continuous. So what we want to do then is given a state and a set of possible actions, I want to choose the action that's going to result in the best possible future from the state. So in order to do that, quantitatively, I have to have some way of representing that future outcome. So, for example, what should this what does value represent? So in tic tac toe, I might want to determine if I make this move, what's the likelihood that I'm going to win the game from this position? So, for example, we can see if you know the rules of tic tac toe, if I am if I make this move, and I'm X, I should be able to tell that by doing this, there's 100% chance that I'm going to win the game. No matter what O does at this point, I'm going to have at least one path to victory. Whereas for like one of these, maybe it's more likely than not that I would win the game, but it's not certain. I might have say 67% chance of winning and a 33% chance of it being a draw or something. So I have to quantify that somehow. In the maze, it's going to be like how far am I to the goal? Right? So I know where the goal is, and I want to be able to figure out like what the number of steps is going to take me to reach the goal if I am in place P and I make move M. So, you know, in planning, basically, this might be like efficiency and time and cost or something, right? There may be multiple ways of getting to the solution. I probably want to choose the one that's going to be like the most efficient, right? Or has some added some benefits. So maybe if I'm like playing Pac-Man or something, right? I might want to choose the path that's going to allow me to pick up the most pips or something like that. There are multiple things that you might be wanting to balance. Robot, I might need to, I might have some energy constraint, right? So I want to choose the move that's going to allow me to conserve energy the best. Race car, right? Time to reach the finish line, but also taking into account that if I make certain moves, I could say flip the car over, right? And that would, of course, delay me getting to the finish line, if ever. Never frowning, maybe your metric is throughput. So all these things, what you decide to measure is going to be, again, critical to determining what type of strategy you actually arrive at. So I'll show, if time permitting, I'll show an example at the end where basically like by measuring certain things, we can learn a different type of task, right? But of course, that's contingent upon the ability to measure that thing. If you can't measure that thing, you have no way of quantifying your progress toward that particular goal. So with these correct values, these multi-step decision problems can then be reduced to the single step decision problems. That is, I'm just going to look at a set of possible actions. I'll pick the one that has the best value. So this is going to be guaranteed to find the optimal multi-step solution. So the dynamic programming problem is basically, I've got some multi-step problem and I'm going to be looking for the optimal sub-solution by solving for the optimal, sorry, the optimal global solution for solving for the optimal sub-solution in every step. So that is, if I know where I'm at right now and I can find what the optimal solution is for my current situation, then I can be guaranteed that whatever I choose now is going to be part of the ultimate best solution that I arrive at. And so the cost of a single action is going to be what we call the reward of that action from that state. So then the value of that state action pair is basically the expected value of the full return. So I have a full sequence. I have at each step, I'm at some state, I take some action. For each of those state action pairs, I'm going to get my reward or my reinforcement. And then at the end, I'm going to get the full return as the sum of all of those rewards. So for example, if I look at this example, if I start in one of these states and then I get small r for every reinforcement, and then the return is going to be the sum of all those. But if you look at this, just if you take a look at this, you'll see that these numbers don't add up left right. So if I find me here, it looks like my reinforcement is 0.2, my return is 0.9. So that doesn't really make sense. But if you add it backwards, that is right to left, it does. Why is that? Well, that's because I'm trying to get to the end here. And I use that by trying to predict the sum of all reinforcements that I'm going to get when I get to the end. So that is, if I'm here, then maybe the optimal action that allowed me to get here would be going from this previous state. Let's assume this is just like one path through a lattice that we'll see in a moment. But basically, if I add this up right to left, all of a sudden, I can see that if I follow this path, then I'll get the total return of 0.9, according to the sum of all these little reinforcements. So let's say that I'm in some state, S sub t at time t, upon taking A sub t from the state, I'm going to get the one-step reinforcement and then the next state. So if I'm here and I take this action, I'm going to get into this next state, I'll have the reinforcement in this case of 0.3. And so now I can continue this until I reach some goal state k steps later. And so then the return R sub t starting from state S sub t should then be the sum of all of my reinforcements. So that's why this works. So effectively, what I'm trying to do is if I'm in any of these steps that precede the goal, I want to be able to predict what the best action is. So I can do that by estimating the sum of all possible actions that I can take from this state and try to figure out the best sequence. So then I'll use the best returns to choose the best action. So this will become a little bit clearer if you look at this example. So if I have this lattice here, my goal is to get to one of these end states. And let's assume these states are actually all the same. So there will actually be a closure here, but we're not showing that. So basically, if I'm looking at this, if I go along this path, then my return is going to be 0.9. If I go along this path, my return is going to be 0.8. If I go along this path, my return is going to be 0.6. So which of these is the best path to take? The first one. You think the first one? You all agree? What if I'm trying to minimize costs rather than maximize reward? So the last one. So it really depends. Are you maximizing or minimizing? That is, what does your reward or cost function actually represent? So for example, if it's the race car and you're measuring time, you probably want to minimize time. Whereas if you're playing Pac-Man and you're measuring the number of pips you collect, you might want to maximize that. Or if you have like, are you trying to conserve some resource or are you trying to gain some extrinsic reward or something like that? So if it's like energy that you want to minimize, then you might want to take like this bottom path. So let's say that this is actually a cost. What is the energy required to move between each of these states? And so this is interesting because in this case, the first reinforcement you're going to get is actually the highest value. So if you want to minimize the total return, it doesn't necessarily make intuitive sense to take the path that has the highest reinforcement if the reinforces actually a cost. But you can see that from this state, you then actually can get to a sequence that allows you to take a bunch of really kind of small cost steps. And in one case that has no cost in exchange for taking a big cost step at the first time step. And so this actually ends up being the best the best path to take if you're trying to minimize, say, expenditure of some resource. Right. So conversely, if you're trying to maximize, if you're in this state, it might make sense to maximize. It might make sense to take this because you get like the largest reinforcement. If you think that's a reward that might seem like the best action, but ultimately you're going to get the lowest return. Right. So you may want to take one of these sort of lower reward first steps, because you can figure out that you're actually going to get a higher reward later on in the sequence. So to do that, we need to know these values. Right. So you get the same sense of what's going to happen if I'm in some state and I take some action. I need to try to estimate this for basically all combinations states in action. So where do these values come from? So we have a couple of options. Run one is I can write the code to calculate them. So this usually isn't possible. Or if it is possible, it's not really a problem. You need to solve the reinforcement learning. Right. I can kind of brute force my way through it or use some other type of solution. And you can probably do this for tic tac toe because it's a fairly constrained problem. And there's a relatively limited set of things that you can do that will and you can typically account for all of those. You could use dynamic programming. This is also usually not possible because it requires knowledge of the probabilities of the transitions between all the states for all the actions. And if you don't have that, then it becomes a lot more difficult to infer what those rewards would be. So you can compare this to things like the hidden Markov model. This is a type of model that we use in natural language processing to try and figure out, like, what's the probability of, say, moving from state X one to state X two or from state X two back to state X one. And then we also have these things like what are my observations that I've ever stepped on. You can use certain types of dynamic programming algorithms to solve certain types of sequence problems. But again, if you can do this, your problem is probably constrained well enough that you don't need to be doing reinforcement learning for this. So a true reinforcement learning problem is one that you are looking for examples, basically looking for lots of examples of both solving the problem successfully and not solving the problem successfully. So that is I want to see for my inverted pendulum, if I'm trying to balance, I want to look and see like given a an angle and velocity of the pendulum, what moves the same trying to balance the gear, try to balance like a pen on your hand or something like this. Right. So I'm trying to keep this balanced. I do like really bad job. And then over time, maybe I become better, better and better at it. And I learned that by figuring out if I can feel see the pendulum tilting a certain way, I know that I shouldn't move and I should maybe make a certain movement that's like small or large, etc. And I do this a whole lot. Right. So in some sense, you know, learning to walk, you know, is some sort of reinforcement learning or at least some kind of reinforcement learning feedback happening between the brain and the body. So basically, we can teach AI agents to learn to quote walk by running them through an obstacle course, and they sometimes come up with these really weird walks like you know they're not necessarily by people and you can have like a three limb agent that sort of has this walk. And so it learns actually will come out through, through the environment by this, by this example sampling. So basically these examples are going to be represented as these five tuples so that as you have the state, where I am the action, what I do, the reinforcement, like the return the the reward that I got from making that action that state, and then where I end up, so that is the next state and then the next action. Yes. Assuming you're trying to maximize. Right well again you have to formulate your problem. So if you're assuming that you're trying to maximize the return, then yes the one that has the highest return is typically going to be, you know, at least an instance of a good plan, possibly the best plan, it gets replicated as you have like more continuous scenarios, but generally yes now if you're trying to minimize of course then you're basically just inverting that it's going to be like the lowest returning. So there are a couple of different techniques, broad families of techniques we can use. And so I'll will focus on temporal difference and also talk about Monte Carlo I'll do a demonstration at the end, time permitting. So basically, in Monte Carlo sampling what I'm going to do is I'm going to take an average of the observed returns, and then assign that to every state action pair so that is I have basically some value function that takes in my state and my action. And this is going to approximate the mean of the return from that state and that action. Right. So, in order to do this, I need to actually wait until the episode concludes that is I either reach a goal or I fail, like I timeout or something. I need to wait before I update my state action pair variable. So for example, I'm trying to solve a multi sequence problem. And I semi randomly explore the world, and then I reach my goal that I can see that I got my goal and I can see oh hey this is actually a really good sequence of events. I want to do more like this in the future, or if I say I got a time limit of like you can take 10 steps and then you're done right you get maybe 10 attempts to solve the problem I try to move through the world, and in 10 attempts I do not reach the goal. Right so then you can see that and say well, this was not a good sequence of events. I should do less of this in future now. The trick is that in maybe a suboptimal sequence of events maybe I made the first three steps of my 10 were actually optimal right from where I started, those took me directly toward the goal and then I moved off track or something right so doesn't necessarily mean that all of the everything in that sequence was bad, just means the sequence overall was bad. Similarly, if I do reach the goal. I might have started off that, and then I stumbled upon, you know, a path toward the goal. So again, just just like the opposite. I do not necessarily want to adhere to everything in that sequence, because there might be a better way right maybe I reached the goal in like seven steps. But I actually the best path could have gotten me there in for right so maybe I don't want to continually just exploit the strategy that I happened to stumble upon, because there might be a better one. Okay now temporal difference is the other strategies to basically what I'm going to do is I have the value function I'm going to evaluate that over the next state next action pair, that is, given where I end up if I take this action that I look at all the possible actions that I can take in that in that state, and I'll sample each of those actions in turn and then put that action and that then my current my next state into the value function. So I'm going to use that as an estimate of the return from the next state, and then we're going to update the current state action values that is value of s of t as of t. So that is the value function of s of t as of t is going to be approximately equal to the return from the next. The Sorry the reinforcement at the next step, plus the value function at the next step which is an approximation of the entire return. So that is this is small r, this is an approximate approximation of the big R. And now what this does is then I can update my state action pair immediately after taking the action. So I can basically see I took this action from the state, and I have a pretty good estimate of how good or bad it was, so I can see if this is going to get me closer to the goal or not. So if we take a look at this, this little graph here, I'm trying to estimate the return R from state B. So this is where I am right here. And so basically I've got a situation where I can move from one of these states through state C, and then from state C you can either move to state W or L, W versus when L is lose. So pretty straightforward scenario, I want to end up in W. And I have some examples. So that is I have, I start in A, I go through C, and I go to L, and I do that 100 times. So basically 100 times I go from A to C and then I lose. So I have 100 examples of failure. And then the 101st time I go from A to C to W. And the 102nd time I start in B, I go to C and I go to W. So I'll do this as a contrived example. Right. So I have one example that shows me what the estimate of the return from state B is. So I'm trying to figure this out. What this would look like for these two methods is basically for Monte Carlo, every example starting in state B leads to a return of one. This is trivial, because I have one example starting in state B, but it leads to a return of one. Right. So every example in starting state B leads to a return of one. The mean of this is one, which is a prediction of a win. Now the temporal difference method would show me that the reward from C to B is zero. And then from state C, I have 100 rewards that are negative one and two that are one. So in this case, what this would end up being is a value of negative 0.96. So this is a very likely loss. So temporal difference takes advantage of the cached experience given in the value learned for state C, whereas Monte Carlo is going to take just those samples that have the entire sequence, including my start. So basically, what do I do in this situation? If I go from B, if I start in B, if I'm trying to win, what type of method would I go with? Monte Carlo. In this case, it's pretty trivial, because my Monte Carlo method gives me the only prediction of a win starting in state B. So this is not necessarily how you would actually choose. You would have a better distribution of your actual samples. So any questions so far? Let's see where we are. All right. So let's take a very simple maze example, like a stupidly simple maze example to the point that you look at this image and you think like an image failed to load. But here's our quote maze where G in the corner of these walls represents the goal. So I can be any position here. We'll assume there's a grid. And I need to decide whether to move up, down, left, or right. So to do this, we need an estimate of the number of steps needed to reach the goal. This would be big R. So I'm going to go from A to B. So I assume there's a grid and I can I need to decide whether to move up, down, left, or right. So to do this, we need an estimate of the number of steps needed to reach the goal. This would be big R, the return. We need to formulate this in terms of the reinforcements, that is the R. So first of all, what reinforcement do we get for making a move? So for every move, it's going to be one. Because we don't really know if any of these moves are going to get us closer to the goal. Then big R is going to be the sum of those to boost the number of steps to the goal from each state. So now you can see that if I have a bunch of cells in this that represent where my agent is, it's going to look sort of like one of those little minesweeper grids. There's like a number associated with how close, how many steps it's going to take me to get from here to the goal. So this is the first step. I want to basically traverse the shortest path. So the Monte Carlo sampling will assign this value as an average of the number of steps from the goal to the goal from every starting state. And the temporal difference will update the value based on one plus the estimated next value. So the next step is whether we do Monte Carlo update or temporal difference. Let's look at this comparison on this maze problem. So we've talked about this value functions that is the V of S and A. So how do we actually represent this function? So the simplest way to do this is actually just as a table. So you can imagine that let's we take this maze example. So we're going to take this state that would be represented as some cell, say numerical cell, and then action that's going to be one of those four things up, down, left, right. And then the value of that should be some representation of how many steps it should take me to get to the goal from the state, given that I take this action. So we're going to take this other function and we will actually write this function called the Q function as this table. So this state action value function is basically you take in both the state and the action and the value is the prediction of the expected sum of future reinforcements. So in the maze problem, those future reinforcements are basically a sum of a bunch of ones, each one representing a step through the maze. So terminology Q comes from this thesis by this guy called Watkins from the University of Cambridge. And so what we're going to do is we're going to select our current belief based on the optimal action is in the current state by taking the argmax of the Q function or the argmin of the Q function, depending on if you're maximizing or minimizing. So again, what we will do is I think for the maze problem, I believe there's some typos here, like it says argmax, we actually mean argmin because we're trying to minimize distance. But typically, you can think of another way to do this is you can just realize your cost as negative rewards. So now if I think like it costs me to take a step and therefore the return for taking a step is negative one, all of a sudden it's turned argmin problem into argmax problem. So typically I'm going to be talking about in terms of minimizing, but usually most people are going to are trying to maximize. And if you have this problem of you've got a cost function rather than reward, it's pretty easy to just like invert your reward values and suddenly you turn your rewards into a cost. So we're going to represent the Q table for this maze world, which is going to be so I have the Q function of S sub T and some action. I'm trying to find the argument for a that gives me the best value. So if we have argmin of these possibilities, let's say this is the goal and S represents the state. So I have effectively a set of Q values for this state and each of the possible actions up right down or left. And so I'm basically going to evaluate this Q function for each of these combinations and then choose the value of the action that is the right hand side of the second argument that gives me that in this case minimum value or maximum value if you are treating them as costs. So that is we can let the current state be a position in X, Y coordinates and the action would be integers one, two, three or four. And so therefore I'm looking for the element of one, two, three or four that's going to minimize the value of this function where my state is represented as these two values X and Y. So if I assume a grid, then I can have say zero zero one zero one one or something like that. You know, you can start from the bottom left if you want. So let's try to do this in Python. So first, we need to know how we can actually implement this Q function. So we know what the arguments are already. It's going to be a tuple consisting of an X, Y coordinate and then a single integer representing the value. So we know we can enumerate all the states, which is going to be a finite set of out of 100 possible positions. And then we can enumerate all the actions. So that is also finite, which is four. And so then we can calculate the new state from the old state in an action and then represent all of these state action combinations in finite memory. So a fairly nice compact table that we can use. So we'll just store the Q function in in table form. So this case, we're going to use three dimensions X and Y. You could in principle use two dimensions. Right. So I could just say enumerate these 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, next row 11, or next row 10, 11, 12, so on, so on. It is maybe just a little bit cleaner, easier to visualize if I keep them separate. So that's what we're going to do. But it wouldn't be too difficult to switch between these two representations. So we'll have a three dimensional Q table where I'm going to have one dimension is X, second dimension is X, third dimension is X. And then the third dimension is the action. So you can think of this as kind of this cube where I've got each of these states on the X, Y plane. And as I go every level deeper, that's representing what the Q table is going to look like for each of those different actions. Now, the actual value is going to be the steps toward the goal. So we're going to be minimizing. And so for the images above, in order to get X closer to G, I'm going to be moving either right or left. So we have some intuitive representation of what correct Q values should look like, which allows us to calculate them. So how can we represent this three dimensional table of Q values in Python? So if we have X or X and Y have 10 possible values and there are four actions, so we can create this table, basically three dimensional numpy array that is a 10 by 10 by 4 array. So now we can represent this three dimensional table as a numpy array. So how should we initialize this table? This above line is going to initialize all the values as zero. So think about what effect this is going to have for the Q values for actions as they're updated to estimate steps toward the goal. So what's going to happen is basically we're going to have a table that's going to be a numpy array. So this is going to be the actual goal. So what's going to happen is basically all of my actions are going to have the lowest possible Q value that is zero. And so this is going to force the agent to try all of the actions from all the states, because it has no notion of what might be better than the other. So it's basically going to start like here. Every possible action that it could take is going to be zero. It won't know what the possible return is until it takes that action. So going left is just as good as going down. Right. So this is lots of what we call exploration. OK, questions. Right. So now updating the Q value using the temporal difference updates will look something like this. So I'm in a state, I take an action, I get some sort of return, and then I'm in a new state and I can see which possible actions are available to me. So then I can calculate the TD error. So that is the return at this this next time step, plus the value function that is the Q function. And then I'll use this. I'm going to take the Q value for the next state, next action pair, subtract the Q value for the current state, current action pair. That is where I was and what I just did. And then I'll use it to update the Q value that's stored for S&T. So it's going to be some cell in this three dimensional array that has the value in it. That's by default initialized to zero. And I'll update that with some new value. So if I, for example, start here and I move to the left, then this should give me I'll have the return, which is one. Right. In this case, my return is always one. And then I'm going to have the Q value for this, which is going to be different steps toward the goal, which in this case, let's assume like a seven by seven group, just count them. So we'll have one, two, three, four, five, six, seven. Let's say I get Q value for the next day, next action at seven. Compare this to where I was one, two, three, four, five, six. OK. So now I have one plus seven minus six. So then that's going to be that'll give me the value that allows me to update the Q table there. And so then how much do I update it? Well, I don't necessarily want to update it just with the raw value. I'm going to scale that by something. So I'll have a constant here. Row is going to be some learning rate. So I could say, well, maybe this is not like an atrocious thing to do, but it's suboptimal. So I want to maybe discount this one a little bit. Point one point zero one, whatever this row is just a scale factor between zero and one. And then I take that and then I add it to the current value of the Q function at that at that point. And so that's going to give me the new value. So in Python, it looks something like this. So let's say the position is going to be represented as a two dimensional array or a two value array. And so then we return this here. Reinforcing this case is always one. So I'll take my Q old, which is going to be basically the Q value using my old state. This is a two two elements array and then my old action. And then Q new is going to be the Q value of my current state and my action. So remember, what we're doing here is this is after I've already taken the action. So I'm updating Q new as my current state. This is where I am now. Yes. What I just did, what I just did to get here. Yeah. Yes. So keep in mind, like. The Q new here is the values at time step T plus one, whereas Q old is at the values of time step T. So you think ahead and kind of one step in time. But I want to update the values of the table where I just was, which I think of as being the quote current time step. So I'm projecting forward, seeing where will I be if I make this action? How good or bad is this? I'm going to use that value to update my values where I am now. Yes. What do you want row to be zero? Yeah, actually, you know what? This should probably be non-inclusive. You wouldn't want to have a row to be zero. Non-inclusive. You wouldn't if row is zero, you would never learn. Right. So you would have no update. Yeah. So this is probably more accurate. OK. Other questions. Right. OK. So here we are. And so now how much how much do I update? So my TD, my temporal difference error is basically this part here. Right. So again, I'm trying to figure out what my expected return is compared to the Q value of where I currently am. And then I'm going to take my current value and then update it by my learning rate times this error function. So now this should start looking pretty familiar. Right. I have an error term. I have a scale factor of how much I update it. And then I have some value that is being updated. So this is going to be performed for every pair of steps that is T and T plus one until you get to the final step. Of course, that has to be handled differently because there is no next state. Like once I get to the goal, I'm done. And so then the update given as previously is going to become basically I'm just going to update my current Q value with instead of taking some error from the subtracting the Q value for the next step. From the current state, I'm just going to take the next return minus or the next reinforcement minus my current value. So in Python, this becomes adding a test for the goal. Yeah. In this case, yeah. In this particular example. Yeah. So it's not not necessarily always going to be one. But I think we change to the goal. Yes. Well, because we're taking steps through a maze. And these are discrete. So basically, imagine if I had a two dimensional maze where sometimes I have to jump on top of a box. Right. That might have a cost or something of two. Right. Or if I have a continuous, I can decide how much I move. Right. Maybe the me were like playing mini golf or something. Right. And you can either hit it hard or softly. Right. You go to like, what is it like on Mountain Avenue? They have like this mini golf courses. That I suck at. And so like you're trying to figure out like how much do I need to putt? Right. So they could be I have a force of like one Newton or whatever. And then I have a force of like point two. So these you always have to think about what your what the problem we're trying to solve and how that is represented in this case, it's discrete steps through a maze. All I can do is step one square in any direction. So it's got a cost of. It is not necessarily always known, but you basically get that from the environment by making the action. OK, so I'll I will show you a block stacking example at the end and I'll explain how that's going. So I'll try and get through this thing. OK, so now I need to check and see if I'm at the goal. So in this case, the maze is represented as a character and I have a letter G at a goal position. All I'm going to do is just check and see if where I'm going has the letter G at it. And if it does, then I use my my kind of goal update rather than my normal update. OK, and so now to choose the best action for some state X, Y, I just need to look at the argument for the Q function at the row that contains where I'm at, given my given my current state. And then if we store the available actions as this array, then I can just update the state based on the action by looking at a where a is the argument of the Q function. So this is this agent environment interaction loop that is the agents in the environment. It does something. It gets some sort of feedback from that and uses that to decide what is best to do the next time. So we have to implement the following steps. That is, I initialize Q, I choose some sort of non goal state can be done randomly, and then I repeat the following. So I'm at the goal, then I update my Q function with the temporal difference error that is one minus the Q function or R minus Q function, and I take a new random state says I'm at the goal I'm done. I reinitialize and I try to solve the problem again. Otherwise, I'm not at the goal, I select the next action. If I'm not at the first step that I'm going to update the Q old with the temporal difference error within this case is one plus Q new I'll shift the current state and the action to where I where that action landed me, and I'll move those to the old ones, and then I'll apply the action to get to the new state. So in the 10 by 10 maze is going to look something like this in Python. So I'll initialize my Q table, choose a random starting position, and then for however many times I want to train. I'm going to execute the following. So if I'm at the goal, then I'm going to update my Q function using one minus Q old, and then I'm going to reinitialize right so this is now re initializing my state. Otherwise, I'm going to select the next action. If, and then I'm going to update my Q old using this formulation temporal difference error, and then a shift my current state of my current action to my old ones, and then apply my action to get to my new state. So in order to solve perform an RL problem you need to know the following things. You have to have the state space that is, in this case is the size of the maze, what is the extent of things that I could that I could possibly where I could possibly be, then the action So these are the things I could possibly do. So in the in the maze example this is going to be moves you can make in balancing an inverted pendulum the state space is going to be you know have the angle and angular velocity would say of the pendulum and the action space is going to be like how can I move my my hand or what torque and I apply to my pendulum to keep it balanced reinforcement for every state action, or at least a method to calculate it so this can be shoved off to another function. And then you need to have some method of extracting this so you can get very sophisticated with this like in the example that will show you actually draw that directly from the simulator. So you don't have the, you don't have any knowledge of it ahead of time but you basically use like a representation of world physics to figure it out for you. And so this can be either a cost or reward so usually we just call everything reward and you inverted if it's a cost. And you also need to know whether you're minimizing or maximizing returns. So depending on this, you know you can just apply like a negative reward there are some subtle differences in behavior with more complicated problems but generally this holds. Okay, so the Python solution to the maze problem looks like the following. So we'll start with text file that specifies this 10 by 10 maze looks like this. So now I'll print the maze so here's my representation and I have some walls here, and I have my goal so basically I'm going to start somewhere in the white space and have to find my way to G. So I'll convert this into a machine readable format of course. So I basically flatten this into a list. And I'll turn this into a 12 by 12 array. It's 12 by 12 because we have a 10 by 10 maze with walls on all sides. Right, so we're just representing this is basically places that the agent cannot go, but this is mostly just for for readability right I could represent this without these walls here. It would just be pretty confusing to look at. All right, print it out again to make sure we didn't screw it up. Great. That looks good. So, I'll need some functions, I'm going to create one that's going to draw this queue surface over this two dimensional state space. And then one to select an action, given the queue surface. So that's what these are. This is really just a drawing functions not going to go through that for time. So I'm going to construct these arrays that holds the queue table and updates it using temporal differences, and then we're going to use one that will update the queue table using what's Carlo. So remember the difference between temporal difference in Monte Carlo temporal difference I get to update my values every time I make an action and see what happened. Monte Carlo I have to wait until I get to the end. And then I can see, based on an average of how good every action was I'll update that. So I need to have four possible actions for each position. These are represented as changes in rows and columns so they have plus one and minus one. And then I'll set the Q value for the invalid actions to infinity. And so an invalid action is going to be basically where I run into a wall. So if I end up here, right going right should be an invalid action because there's a wall there. So, by doing that I can basically say that this is by saying this has a cost of an infinite cost I'm never going to take this because I'm not I'm not allowed to. So now for some parameters so we'll run 100,000 interactions, that is 100,000 updates all that my learning rates be point one and then I also have this epsilon. This is a random action probability. So what's the random action. Well let's imagine that I explore, and I end up at the goal. So I could take one step forward one step back one step forward one step back one step forward one step back and one step down that step down lands me at the goal. Right. Doesn't mean I need to take like three forward back steps and not a dog trying to find a place to nap. And before I, before I step down. Instead, it's a possibility that maybe I took seven steps and I could have taken one. Right. So there's a random action allows me to basically pop out of some sufficient strategy that's still suboptimal. And so I can do that by taking a random action it might get might allow me might maybe take like some random actions that never get me to the goal, but there's a chance I'm going to get a better, better solution. So I'm going to keep history trace of positions and reinforcements and then I need to use that to update the Monte Carlo version of cube. So I'll store this trace of x, y and a. And so now this is just for initialization display. So now let me initialize. So here's my start position. And then this is my, my first action that's just the index of where I'm going to look for my for my actions. I'll keep track of the trials and the number of times that I've hit the goal. So now we can see okay if I found the goal I'll perform the goal update, if not I'll perform the regular update, and then the Monte Carlo update is looking, I'm going to look in the trace, and then I'm going to update this based on an average of all of those, all of those Then if I'm done, I randomly start new position. If I'm not at the goal, then I pick the next action. This can be done randomly. If I choose a random value that's less than epsilon otherwise I'll choose the best action from the cube function. And then every 100 steps I will print. So, let's watch this run. So, here's the TDQ policy. Here's the Monte Carlo Q policy. This is what it's done, the most recent trials that red dot obviously is the goal. And then this is going to be the number of steps per goal at each trial. So, just take a look at this and see you know what do you think is better at solving this temporal difference or Monte Carlo. So, if I'm here I'm trying to get to the goal, the arrow represents like what the best action is for this cell. So, let's say I start here. Follow the arrows. Great. Let's start in the same position in Monte Carlo. That's this one. Right. Okay. Oh, oh crap I got stuck. Pretty clearly for this this type problem the temporal difference is a better solution. You can see, obviously in the Monte Carlo I find plenty of places where if I start at some trajectory, I end up, you know, in a place where I just sort of go back and forth if I adhere to the Q policy. I might be able to pop out of this by doing a random action, but there's no real guarantee. Okay, so now let me show you a quick example of Monte Carlo learning for different tasks. So let me. Alright, let me run this environment so this is basically a unity simulator of block stacking. So, when the sometimes it crashes hopefully won't crash. If it does crash because I show a video or something. So I'm going to train for 300 steps okay so we have two blocks. My goal is to stack the. There he goes. Okay, my goal is to stack the C block on top of the B block and what it's doing is it's selecting an action relative to the surface of the B block. I mean you can see that there are times where it's basically jumping around and moving you know it's not even touching the surface of the B block. And sometimes it does. So we can see here in these updates every so often, one of these, one of these rollouts will pop out, showing me like what the reward, the mean reward that it's been getting so this is using Monte Carlo learning. What this is doing is for the first 100 steps. It's really just randomly exploring. So trying to see if I randomly explored the space, what looks like it's a viable solution versus what's not, and then the actual learning starts. So what you can see here is kind of out there exploring the space is really just sort of verifying that there are no good moves out here. And then eventually it might sometimes it takes a while to converge. So I'm not sure it's going to get there in 300 steps. But eventually it will kind of going off spinning off in space and see it doing its thing there. Eventually it might find. Oh hey here's a good action this actually gives me a good reward. I should try more of this. I'm not sure if I'm going to get there. I did not. Okay, let me try again. Usually I run for like 500 but sometimes that it as it starts to succeed it takes longer, and this one just takes like a bit more time. So it's still learning. Okay. Not learning a whole lot, or at least it's learning what bad moves are it found something. So it should try to do more like that in the future but right now it's still in the random exploration phase. So let's see if it. We do 100 steps of random exploration, still there. Okay, so now it's found it may be found a couple of actions that might be pretty good or at least closer to optimal. So you can see now it's starting to stack. So now it does some more exploration. It tries some some more stuff. We can see that it's getting mean reward of roughly 37 or, and in this case the reward is like 1000 for stacking the right the first time and then at the discount of 100 each time. I'm not sure it's actually going to learn anything here. There we go, starting to get something. So now you can see it's starting to kind of learn a bit. And then it's. We'll see what the reward is. So here now mean reward is like 32, it should start to get slightly higher rewards here so 31.2. So now it's like 68.4. So now it seems to kind of start to converge on some sort of viable solution. So this is an example of Monte Carlo learning because it has to terminate the episode in this case it gets 10 attempts, or it stacks successfully. And so once it reaches the goal or a times out we'll basically look at all the actions that it took and see how good or bad they are. This is using a particular type of method called a deterministic policy gradient so it's actually using a neural network to optimize the weights. In particular, there are two neural networks one called an actor and one called a critic. So as you can imagine, the actor chooses an action, and it also predicts how good it thinks the action will be. And then the critic basically says, yeah you're great or boo you suck. Based on the action. And so the actor tries to make better actions, the critic tried to get better at predicting how good the actors actions are. And so then both of those losses flow backwards into those respective neural networks to optimize those weights. So this should terminate relatively soon, and we can see what the final word was, and we can try and evaluate this briefly. So this probably could have done with training for like a little bit longer. So you can see had a mean reward of 169, so it kind of. Yeah. In this case it's 1000. So basically if you were ever perfectly stacking the first time you would get a reward of 1000. So if it's, if it, the way the reward is set up here it's 1000 for stacking perfectly the first time, minus 100 for every additional attempt so if it stacks right the next time it gets a reward of 900, and then like 800 the third time and so on. Then it gets a reward of negative one for missing the block entirely, and then a reward of nine for touching the block, but not stacking successfully. So, so again we're using the world physics here. So basically if it, if it stacks off center but it stays stacked. That would be a reward in the hundreds depending on which attempt it was on. Perfectly yeah yeah. But of course you're more likely to stack perfectly the first time if you put it exactly centered. Right. So, if it trains long enough you know it should it should approximate that. Okay. Questions. So, what does the reward actually tell us about the 169. So in this case, we think about if we know the maximum reward is 1000. Right. So we can think that on average. This would probably given what you know about the reward shape. Alright, 1000 for the first attempt 900 for the second attempt. So basically between this number falls between what would be a good stack between the eighth and ninth attempt. So, if you average the model as it stands right now would probably stack successfully somewhere between that eighth and ninth attempt, maybe a little bit better, because there's a lot of noise in this and so when you actually evaluate the model sometimes it performs better than it appears to at the end of training. So, it's all about you have to understand like what the values in your reward actually signify. And so this in this case the reward value were chosen quite deliberately to kind of encourage it toward a solution with the appropriate amount of exploration like once it finds solution, it gets a very high signal, saying I should be trying more of these things. Would it be right to assume that the reward is no, it's not very efficient learning? That may be the case. So you can you can use like these reward shaping strategies to specify like what types of things you want to encourage the agent to do. But you could just as well train this with say a reward of negative one for failing to stack and just a reward of one for stacking. It might take like a little bit longer to converge possibly, but you might actually converge at like a better solution. So for example, with like the off-center stack it might get a really high reward and that's as good as it ever gets. So it's like, oh, well, this is a really good solution. It's not like perfect, but I'll keep exploiting this. Whereas with a more with kind of a rewards of lesser magnitude, you might encourage it toward like a more like a more perfect solution. Though it might take longer. Yeah, so you're you because you store the action at every step. So if you see the action is represented in zero to a thousand. Basically, in this case, values that are closer to 500 mean it's closer to getting it centered. So in this one we have 578 and 530. So the reward is nine. So it got it on the block, but maybe it didn't quite stack it fell off. And so you keep track of all of those, each of the rewards and each of the states, and you can say for this action that I took I got a reward of nine, which is like not terrible but also didn't get me what I want. This action got a reward of like 600. Right. So this is better than this other one that got me only a reward of nine. Yeah, until the until the end. Yeah. Other questions. Good questions. Yeah. Yeah. Yeah. In this case, yes. Yeah, so this case, if it just sort of plops the block out there in space and it never touches the destination blocks that word of negative one. So when you saw like the block kind of moving around in space is really kind of exploring the space and learning like there are no good actions here in this region. So eventually I want to try and move out of this region. Okay. Other questions. One last question. Okay, go ahead. So, yeah, so we get the actions over there. The one which has minus one reward and the one that has three more. Yeah, they won't pretty much have like about 500 as the quarter. Yeah. And if I remember I mentioned that if they're above 500 they get very poor from there pretty cool. Well, so I mean, keep in mind that like, imagine a large space where the block is in the center. So the block is defined is bounded at zero and 1000. So values very close to zero or just like way off in space, very close to 1000 or way off in space values close to 500 are going to be close to the block. But the way that I constructed the action space here is that the block is like really small, because it's, I'm not sure about like the span of the block in the in the action spaces but it's very very small. If you if I picked 500 500 exactly, it should stack. We also add. We also make this more difficult actually because we add a little bit of physics noise to it. So if you imagine this is a virtual environment, I can be hyper precise. So that but whenever you actually stacking things, when you release it there's a slight motion. Right. So basically add a little bit of a jitter or push. So sometimes, even if it stacks well. There's a bit of noise there that actually like sort of simulates this release and it actually ended up falling off. So there's a number of things we've done to make this problem kind of harder and more realistic. That makes it you know, at least, it's still pretty easy for reinforcement learning to solve but like, you know, at least somewhat challenging. All right, I better talk about assignment for before we adjourn. So, what you're going to be doing here is classification of hand drawn digits so you're going to be given a solution to assignment to similar to what you've seen before. And then you need to extend it into a neural network classifier class. This is using MNIST but it's not a convolutional net yet this is still fully connected net will do convolutional nets next. And then you're also going to need to find the confusion matrix function. So here's the neural network class should look a lot like what we've seen before. So, what similar to some of your A2 solutions. Then we have we got our optimizers we got an instance of neural network. So what you need to do, then, is you can test using these functions. You need to extend the neural network class. So again, you'll have access to all the same functions, but you need to override the train function the error f function the gradient f function and the use function. In addition, you also need to define make indicator bars and softmax functions. All of these are given in some form in previous notebooks. It's just this you're gonna have to pull from. So, you're going to need to do that in 9, 10 and 12 probably, I will double check that. But there's a couple of different places you're going to look for some of these different things. So, you want to look in the implementations of classifiers that we've done and look at how we override the train function and the error function gradient function and use function, and then create versions of the make indicator bars and the softmax function You can do just create a sample test of this new class. So basically what this is what this is doing is just classifying random numbers into instances of class 01 or two depending on whether they're less than 25 between 25 and 75 or between or greater than 75. So basically, you can test that function using using this. So this would look something like this. And these values are just offset so you can see them both so we just we show t plus five just so they don't overlap perfectly. Then for the hand drawn digits. So you can download MNIST. This deep learning site goes down a whole lot but you can also get it from here. So we've, there's a page at the Washington CS department. If you have trouble with this I have MNIST pickle saved so like if some for some reason either these sites work, just let me know I'll put it up on canvas. Pickle file is already partitioned. So open it. You've seen this before 50,000 training samples 10,000 valid test samples. So these are these these training classes these are the digits. If we look at those 784 columns we now know that those are the pixels right so if we if we split all of them up, you'll see that we just plot the values and you have a bunch of things between zero and one. But these are actually pixel intensities for the image right so you have pixel intensity of zero, and then suddenly we start to see actual pixels that are non zero. So you can see that these are the pixels that are laid out in rows. That's why I had this periodicity in this plot. So you can rearrange them into a square. So let's reshape them look at the numbers. We print this image we can see it's a five, give you a function to turn it into a grayscale color map. And then you can pop the negative image. So we have about 100 images and plot them using the labels as titles so these are the first 100 images. So you want to check the proportions of each digits see the roughly even, we found that they are roughly 10% belongs to each, each class. And so they're all very, very class close to point one. So let's do an experiment. So training neural net five hidden units in one layer and a small number of epochs, and so you can train your neural network classifier using this, these, these settings and see what the final training error is. So now you have to run some longer experiments so first you have to write code to do the following. So you have to try five different hidden layer structures doesn't really matter what they are. So you can choose you to train that for each one training network for 500 epochs, collect the percent of samples classified for the given train validate and test partitions. So create collect these into a pandas data frame. You want to log the times this is going to be how long the network took to train in seconds. So we've done that before you see you start a timer, and then subtract the end time from the start time or the start time from the end time sorry. And then what you're going to do is you're going to retrain a network using the best hidden layer structure as judged by the percent correct on the validation set. So basically you've done a network search using the percent correct to the validation set use that to figure out which of your network architecture is is the quote best one. So you can use this network and then use this network to find the find several images where that it gets wrong. So as you look in the test set and see where the network's probability the correct class is closest to zero. So that is these are the images which your best network actually does the worst on. Right. So then you draw these images and you discuss why your network might not be doing well for those images, for example, are there a bunch of fourth of look like nine just something like that you know depending on how your network is performing. So you could write a confusion matrix function. So this is going to return the confusion major any classification problem as a pandas data frame. We've shown this in lecture 12. So this needs to take in two arguments the predicted class and the true class, so it should look something like this, where you take in why classes and t test and output something like that. If you want to do some of the coloring, you can use what we showed in lecture. I think to do that. Okay, so 50% for the correct code 50% for the experimentation and discussion will have the greater I have not put this up yet I will do that this evening. Same, you know, same procedure, so put this in your, in your folder with your, with your notebook and run it. Finally, you can do extra credit for combining the train test and validate partitions into two matrices, and then use Adam And then you use Adam ReLU and then a single value of learning rate number of epochs compare several layer architectures by applying the crossword validation as defined in lecture 10, and then show the results and discuss which architectures, you find works best and how you determine this. Okay, we're out of time, so let you go. I'll go back to my office and have office hours in a few minutes. Okay, let's go ahead and start. So here's where we're at, we are still a day ahead. So this is the 23rd, we finished lecture 16 already, so I'm going to do 17 and then 18 next week remotely. And then hopefully we'll be able to take the 30th off. It'd be great for all of us. So today I will do reinforcement learning with a neural network and then also discuss the project proposal. So this is going to be due, we have two weeks to do this, but I recommend that you get it in sooner because then I'll be checking for those to come in and the moment I sign off on it you can start even if it's not the 6th of April yet. So I'll talk more about this at the end. Before I start, I just want to like reinforce a couple of points about office hours ethics. I wish more people were here. Just a note that we need to be able to assist everyone who's up fairly. And so I would request that if you come to office hours, my policy, this is like pretty much open door and as you observed you come in. If I have you do something, you are welcome to stay and try it, but I recommend, I would ask you that you like move to a different chair or if it's going to take a while, go out in the hall and work on it because there may be other people coming in. So just, I don't, we really can't have people there like monopolizing our time. Office hours for me start effectively right after class most days because I can start talking to you guys if you need, but they end at 4.30. After 4.30, I've moved on to other things. I'm not there for office hours. And I think the same is true for lab ops, right? There's a specified time. I come there for class questions. After that time, we are not obligated to discuss class content. We both have plenty of other things to do. Start your assignments early. Again, if there's a flood of people coming to either of us, you know, insisting on help at the last minute, very likely the answer at some point is going to be no, you should have planned your time better. So just, it's not like a global problem, but it's been prevalent enough that I need to make this clear again. So if you start coming at the last minute asking for help on your assignments, very likely, you know, I might help you a little bit, but at some point, you know, it's on your own. You need to be responsible. You all are adults, as I said at the beginning of class. Part of being an adult is learning to manage your time. And this is the best time of your life to learn how to do that. Frankly, the habits you develop now are going to stick with you the rest of your life. So learn to manage your time now and I promise you it will serve you very well. Okay, so hopefully that is clear. So please be respectful of us. Please be respectful of your fellow students when it comes to making use of office hours resources. All right, so any questions? Very hopefully you have, if this hasn't scared you into starting your assignments, you will your assignment after this class because that is going to be due in two weeks from Tuesday. So please, please bear that in mind. All right, so I will go into the lecture if there are no further questions. I had the controls. Okay, oh, good. Sorry, this one, this is big. And sometimes when I reload it, it takes like forever to render all of the all of the formulas for C is already loaded. Okay. So if you recall, last time we introduced reinforcement learning with this thing called a Q function with Q function is basically the function that you're trying to predict given a state and an action. How good is this going to be for for my objective of achieving my goal? And also, where am I going to end up if I am in this state and I take this action? What state am I going to end up in and what actions can I take from that state? So previously, we had filled this out as a Q table, right? So the Q table is going to basically have dimensions for the elements of the state and then for the action. So in our case of solving like a maze, we have a two dimensional state, basically xy coordinate in a maze and then an action, which is one of four things move up, down, left or right. So we were looking for the sequence of action that either maximizes or minimizes the sum of reinforcements along the sequence. So depending on whether you're you're trying to minimize your cost or maximize your reward, what we're going to do is we're going to be looking at lots of examples and looking at those reinforcements and returns. That is, if I'm in an action, if I'm in a state and I take an action, how good is that according to some some measure of goodness? Right. So again, I gave you the example of if I'm trying to solve a maze and I'm in state s and the the goal is right in front of me and I take one step to the left or the right, one step to the left, one step to the right, one step to the left, and then I move forward. I don't want to just repeat that sequence because there's like six unnecessary steps in there when I may have discovered that from this point, if I take one step forward, I reach the goal. Right. So I don't want to just find one sequence that works and continually exploit it. So we thought the Q table as this function that takes the state and a possible action and returns some value for that pair. But we this this table is actually a function. So if I have a function, we have certain types of mechanisms that I can use to find the value of that function. So now let me give you something we've talked about before. If I have some inputs and some outputs and I assume that there is a function that maps from those inputs to the output, my job is now to find the values that parameterize that function. So I need something that can say approximate that function. What type of mechanism might I be able to use? In the title. Yes. A neural network, a neural network, universal function approximators. So if it is a function, I can use a neural network to approximate it. I can, for example, if my function is y equals x, I can approximate that function with a neural network. I don't know why you would do that, but you can do it. So I can do with a function of arbitrary simplicity. Of course, I can do it for a function of arbitrary complexity. So if q is just a function, let me just optimize for the value of that function. So the objective of a reinforcement learning problem is going to be reduced to the objective of acquiring this q function that's going to predict the expected sum of future reinforcements. So now I'm trying to find this big R value. This is going to be the thing that I'm going to try to predict with a neural network. So we're now back in a pretty standard neural network paradigm. I'm going to take in some inputs and predict some outputs. My output now is going to be the sum of future reinforcements or the total return, and my input would be some representation of the state in the action. And so the correct q function, once it's optimized, will best determine the optimal next action. So now my objective is to take this approximation and make it as accurate as possible. So here are the components of this. So for every step, I want to compute the return for the sum of the reinforcements from here to the end. So little r, this is basically I'm trying to predict what big R is from this time step to the end. I'm going to do that for all possible steps. And that's going to be the value of the q function. So to break this down a little bit more, this is usually formulated as a least squares objective. So that is going to minimize the square of the expected value. So because the sum of reinforcements is some sort of scalar value, I can still use a mean squared error metric to approximate this. So this is no longer a classification problem so much as I'm trying to optimize the values in my network to predict the sum of future reinforcements. So now I'm back in the territory of trying to minimize error. So I have an established q value and I'm trying to minimize the error between that and the predicted sum. And I'm going to square that. So the expectation is going to be taken over a couple of sources of randomness. So there's the action selection. We talked about how I can I have a set of actions that I can take. But sometimes, let's say I don't want to I don't want to exploit a correct but suboptimal solution, such as moving back and forth left and right before moving toward the goal. So I want to have some level of randomness. I'm going to take a random action. So that random action that that's that epsilon from the previous lecture, that's the thing that allows me to determine with some probability, I'm going to take a random action instead of exploiting what my best strategy is right now. So if I find a strategy that works, but it's not the best one, I'll keep exploiting that where in fact, there could be a much easier solution. But I have no way of finding that because I've kind of found this rot in the search space. And I just keep doing that over and over again. There could also be some randomness in the state transitions. It may not it may not be deterministic what which state I enter given that I take an action. So if I'm in a state and I have two possible actions, I take the left fork or the right fork, there's some probability distribution. Maybe they're not evenly distributed and that I'm going to take either of these actions. And then finally, the reinforcement received. So if this is a continuous, continuous problem, I can approximate what I think my expected reinforcement would be. But it might not be exactly that due to other factors, depending on the complexity of the environment. And then, of course, the key problem is that we don't know we don't know this. We don't we don't know this term because this is asking me to perform a sum over infinite elements and I can't do that. So what I'm going to use is I'm going to use the Markov property. Basically, does anyone know what the Markov property is? They heard of this term before. Kind of any sense in what context you read this term? I've heard of chains. That's a good place to start. What's the Markov chain? Like it's the last out, right? Yes. And does it use it to repeat for those in Zoom a Markov chain, you take the last output and that is the input that determines the next output. Does a Markov chain use the previous output before the last typically? No, I mean, if a Markov chain, if the first order Markov chain, which is by default a Markov chain, it does not. So Markov chain is kind of something that we might call a drunk walk. So if I have a bunch of grids and I am in one grid and I take a step forward, well, then the grid, the square that I'm in now has influence on where I go. I could go to any of the adjacent squares, but it doesn't know that it just came from the square behind me. So I could just as easily just take a step back. So now I do this a bunch of times. I'm staggered around like a drunk person, hence a drunk walk. So you can maybe you might stumble upon a path that you actually do start moving forward and then suddenly you'll verse to the left or you step backwards or something like that. So the Markov property is basically this memoryless property that is where I go next depends only on where I am now. It does not really take into account anything previous such as where I came from. This doesn't sound like it would be very useful for solving a sequential problem. But in fact, it actually is because we can now approximate the sum for all k of r sub t plus k plus one as the sum of r the next return. So r sub t plus one plus the sum of all returns from that point. OK, well, I just kicked the can down the road by one by one step. But if I can use this highlighted up here to approximate, let me draw this out a little easier. So if I can use this part here to approximate this part, now I have to approximate this part. What can I do to approximate the part that's just in the box? I just shifted again. Right. So now the sum from k equals one to infinity of r sub t plus k plus one can be approximated as r sub t plus two plus the sum from k equals two to infinity of r sub t plus k plus one and so on and so on. And so at some point, this goes on to infinity because I don't know how many steps it's going to take me to reach the end. But at some point, I will reach the end and it's no longer infinity. I'm just looking at the return that I just received. And so if I approximate it this way, then I should end up with a relatively close approximation of the entire return. So. So, basically, because we have realized the Q function as this sum, then I can approximate the entirety of the Q function as the next return plus the Q value of the next state and the next action. Everybody clear on kind of how this is working so far? All right. And so now I'm back and trying to figure out what the actual value of this Q function is, approximating that with a neural network. So we'll just assume that the Q function is some function of an arbitrary level of nonlinearity. So I just have to find kind of a good network size is going to allow me to capture that level of nonlinearity and approximate the values in that in those ways. So the gradient of the objective. So now our minimization problem is like this. So Q is going to be some function parameterized by weights W. And so now I need to minimize the gradient of this function. So if I have the error gradient, J sub W or J W is going to be equal to the error of the expected return minus the value of the Q function for this input parameterized by these weights. And I'm just trying to solve for W. So in other words, because we now can break, we can break this down into the steps above, this can be written as the return plus the Q value at the next state minus the Q value at the current state squared. So if this function represents some kind of gradient, what kind of operation could we find to get the lowest point on the slope? What might we do to that gradient? We could descend the gradient. Eventually, if we descend the gradient successfully, we will find the lowest point in the gradient. OK, so this is now hopefully back in some level of familiar territory. So we take the gradient of J with respect to W and do gradient descent. And then we'll see here Q appears twice in the expression. So a pretty common approach is to take the respect or take the gradient with respect to Q at the current state and then we treat the next the next value of Q as a constant. This isn't necessarily correct, but it works well and makes the algorithm a lot simpler. So it requires less compute. So now I only have to compute the Q function once, given its current values rather than twice. So in more recent methods, a second Q function called the target Q function has been used to calculate the Q value at the next state given a set of different parameters. And then at certain intervals, we're going to copy my source weights to my target weights and then update the target Q function to basically keep these two roughly in line. OK, so just in principles, let's recall that the expectation operator is going to be a sum of possible states weighted by probabilities. So if D is the value on top of the fairy die, the expected value of D is actually three and a half, which doesn't make a lot of sense because it's not a possible output. But the gradient of the expected value is going to be the expected value of the gradients. So the gradient of J, we can just take the gradient of the above formula. And so this is going to be two times the return plus the value at the next step minus the value of the current step minus the current gradient of the value. So the expectation operator requires knowledge of the probabilities of all those random effects. We don't have that. So what we'll do is sample from the world. So this is now because I don't have this, I still need to see for my states and my actions, what kind of returns do I get if I take those actions in those states? So that is more probable events will occur more often. And then if we just add updates to W for each sample, you will get the expectation. So if I'm in a world and there's a distribution of events that can take place, regardless of the state and the action, the state of the action that I take, things that are more probable will occur more often. So, you know, you can think of like continuous problems when balancing my inverted pendulum. If I'm already over the marker here, I don't use the eraser. So if my pendulum is already tilted over here, it doesn't really matter what I do. I'm very likely to continue to drift to that one side. So given the state and this action, the state of the world, you can keep the markers, OK? Given the state and this action, then it's very likely I'm going to end up in the same place regardless. Whereas if I'm more balanced, then maybe there's a wider distribution of possibilities that could occur. So by sampling the world with the current state, I can basically get a decent doing that repeatedly. I can get a decent survey of what types of things are likely to happen given the current state of the world. OK, any questions? OK, so first, let's look at the gradient of the Q function as a table, and then we will look how and see how we can update that using a neural network. So when the gradient, when Q is a table, we first have to decide what the W parameters are for a table. So these are actually just the entries in the table. So if you think of what I'm if I have a state that is two dimensions and an action that is one dimension and I have my three dimensional table, I basically just go, OK, X, Y, Z at this cell in the table. This is the return. This is what I what I expect. And so what I just I just hear what I can do is I can take that input and multiply it by those weights instead to get the return. So you're not going to get like the identical values, basically just get an alternate way of saying, hey, instead of just retrieving this value, you actually want to take what's in here and multiply this by your input. And this is going to give you the expected value. So since W also weights, you can formulate the selection of this correct parameter as a dot product. So if we have X of T as a column vector, this will be equal to the number of table cells and values. And these are all zeros with a single one. So that one designates the cell corresponding to S and A. And so then Q of that is going to be that that input times those weights. So this looks a lot like the prediction step in a neural network. So therefore, the gradient of the Q function will basically just be the gradient of X of T transpose times W. And that's since this is taken with respect to W, this is just equal to X of T. So now, if I want to actually update the the weights, I need to define the temporal difference error. So in this case, this is the formula we've seen before. So delta sub T is going to be R sub T plus one plus the Q value at the next time step minus the current Q value. And so if we substitute the gradient of J in from above like this. So now this we can just replace with delta sub T. And so now the gradient in respect to W of J is going to be two times the error, delta sub T minus the gradient of the Q table at the current step. And so since we've established this is going to be X of T, then this ends up just being negative two E of delta sub T times X of T. So, in other words, for because our input except T is represented as a one hot vector, where one that one indicates the cell corresponding to S of T, A sub T, then this value. So this is delta sub T times one. And so now this value ends up just being negative two times the error of delta sub T and then elsewhere is zero. So now we can replace the expectation with samples and then we'll get the temporal difference update for the Q table. So this is just the standard weight update. I take my current weight weight value and then I subtract the gradient and store that in the new weight value. And so if the gradient is delta sub T times X sub T, I just multiply that by some learning rate row. And this is the update. So W times row times delta sub T times X sub T. And so this is really the same as what we've seen before. So I'm updating the Q table. Q of S of T, A sub T is going to be Q of S of T, A sub T plus row times the temporal difference error. And this is the same as what we did in the previous lecture. So previously, the update to just the cell was implicit. So this is also the same as the weight update in neural network. That is, I update a weight W based on an error, in this case, delta, a learning rate row and an input X. So same components as we've seen before, just written differently and kind of arrived at using a different type of formulation. OK, questions about that? OK. So now the neural network version. So we've shown how it corresponds to a table and that the tabular update is effectively interchangeable with doing a weight update, if we assume that those weights are effectively just entries in the table. But using a table for Q has limited use. Reasons why maybe the size of the table might be too large to store in memory. So imagine if I'm trying to solve some sort of complex environment, I might have a continuous function or even if I don't, I might have too many possible state action pairs. So I might not be able to store the entire table in memory. Learning could be slow. So I could learn something from if I have two similar situations in different locations in my environment. I could learn something from doing something at one location and eventually find my way over to the other location and I'm not able to solve it. It's like if you learn by reinforcement learning that red means stop and green means go, but you learn it at that traffic light like on Meldrum north of the parking lot and then you go to another traffic light and you have to learn that all over again. That would be very inefficient and also very dangerous. So instead, what if I have learned that red means stop and green means go and now I can see, well, I've gone somewhere else and I see also red and green happening here. I might have learned something previously that's relevant to this. So instead of having to represent every single state action as a set of cells, like every intersection in Fort Collins being a distinct cell, I actually can learn from features of the environment and then be more likely to reproduce good actions when I encounter those features elsewhere, even if I learned about it in a particular location first. So we can use this universal function approximator or a neural network. So to do this, we have to make a couple of changes in the derivation of the gradient, but they're going to be things that you've seen before. So we need to approximate this Q value parameterized by W with a neural network. So we already know the values of W are the weights and all the layers. So if you have two layers, then W is going to be composed of hidden layer weights V and big W. So it looks like the neural network should have a single output and inputs for S and A. So to find the best action, I could input S, try all possible actions A and then calculate the output of the network for each one. Then I pick the action that produces the best neural network output. This is not how you would actually want to do it. But if you think about it this way, then the math makes sense. What you would want to do is basically want to accumulate enough experience that you get a good distribution of the various different possible actions in S. Yes. I can see W, but the other hidden layer is the hidden layer. W is all the weights, big V and big W are the individual layer weights. So little w is all of the weights in the neural network. V is the hidden layer weights. Big W is the output layer weights. Other questions? OK. So really what I would want to do is sample enough experience to say, OK, I was in the same state S a bunch of times. And half the time I did action A, half the time I did action B. And those are really my two possible actions. But then I know what the distribution of expected returns or reinforcements would be. So let's just remember the objective function we're going to minimize. So we have Jw given as before. So this is the approximation of the sum and then the gradient, as we saw before. So what we'll do is instead of the expectation operator, we'll replace that with samples at time t. So now I can look and see. So instead of the E, sorry, I kept calling that error previously. I'm going to say expectation. I apologize for that. So we have we replace these with just a bunch of samples. So now I have the return that I got at time step t plus 1 plus the Q value for this sample. So now there's going to be a particular input sample and then the weights associated with that sample. And then minus the current Q value times the gradient with respect to those particular weights of the Q value. So what remains then is to calculate this last term. But we've actually seen this before when calculating the gradient of the output of neural network with respect to the weights. So remember, W is the set of both of both layer weights. So the only change the previous algorithm for if we want to train now a neural network to do a nonlinear regression is that we're just going to be using the temporal difference error instead of the target output error. So remember, previously, target output error was basically I know my ground truth value is t. I make some prediction why I measure how wrong I am is the difference between t and y. So the temporal difference error is now as given above. So this is delta. This is basically going to be the predicted return, according to my approximation, minus the actual Q value at that at that point. So what's happening here? The Q value as it stands is sort of my best prediction, as in this is what my my neural network or my Q table predicts is going to happen. It may be very wrong and may be close to correct. This is my approximation of t. So that is this is my best estimate of what's actually going to happen if I take this action. And it could be very close to what the Q table or the Q function actually says or very far. So this is what I mean when I say in reinforcement learning, you are using and training the network at the same time. So here I'm going to get an imperfect approximation of what I think my full return is going to be. I'm going to use that as the stand in for my target value, because it's the best thing that I have. I cannot get a better approximation from this because I don't have a bunch of samples. I only have the experience I've accumulated so far. And then I'm going to use my prediction. This is the analog for why that is my current Q value. So where does what does this do? Yeah. No, R is the row is the learning rate. Greek letter Rho is learning rate. This is the learning rate. But we've always used Rho. Sometimes you use alpha for this. We almost always use Rho. The little R refers to the reward at some time step T. So R sub T is the reward of time step T. R sub T plus one is the reward of time step T plus one. Big R is going to be the sum of all those rewards. That is the overall return. OK. So let's review the gradient descent updates that we had for nonlinear regression with neural networks. So for hidden layer weights V, we multiply those by my inputs. I apply some activation function H. This gives you some intermediate scalar values Z. I multiply that by my output layer weights W. And this gives me my prediction Y. Y is going to be more or less wrong when I compare it to T. So now T minus Y, this is my error rate. So I'm going to use my prediction Y. So now T minus Y, this is my error term. And so these are the updates that I use to update V. This incorporates the error and then also incorporates the derivative of the activation function. And then these are going to be the updates for my output layer. So here is Rho. We have, in this case, different learning rates for the two layers that you need not necessarily have that. Now the key change we need to make is in the error. So as I alluded to just now, what I calculated my error over is basically the best estimate I can make at the time and what my neural network before updating currently says the expected reward should be. So basically my neural network, I have two estimates of the reward. I have whatever my neural network predicts. And then I have something that's probably slightly different from that based on a little bit more experience. So I have predicted that my Q function for state where I am now plus moving forward is slightly positive. It gets me a little bit closer toward the goal. I then take a step forward and I fall into a giant pit like, oh, that was bad. It's not going to be closer to the goal. So I have a little bit more experience and I can now use that to update my Q table, my Q function. So this key change is in the error. So the targets are now going to be the reinforcement plus the next step and then the predicted Q values. So then we'll be assembling a bunch of these samples into matrices. So these inputs will be the states in the actions of these tuples be collected as rows in X. Those will then pass through the neural network will give me Z and then Y. Y equals Q because the prediction of my network is just the output of my Q function. That is its best estimate of the expected return. And those reinforcements will be collected as rows of R. So now I just need these Q of S of T plus one, A sub T plus one. And you can think of these as this is just the Q function once I've taken my next action. So this is just rows of Y shifted by one and maybe slightly slightly altered. So what we'll do is we'll call this Qn for Q next. So now I can just rewrite the gradient descent updates, except we're using R, Qn and Q. And then X is the same. So I have X, R, Qn and Q. All of these except X have one column where X has however many dimensions I have in the input. So now T is equal to R plus Qn. So this is that approximation of best return. And then Z is tanh or some activation function of X times V as before. And now the update functions are pretty identical, except you'll notice that I replaced Y with Q. I placed Y with Q because the prediction of my neural network is the output of my Q function. So to make this even more obvious, we'll just replace Q with Y and you can see it here. So now T, I have a way of calculating that as my return plus the next row in Y. And everything else is pretty close to being identical to the standard neural network update. The only thing that's missing is we don't have this one over K that we had there. But because that's a constant, we can effectively just factor that out. Questions? So dealing with infinite sums. So for these tasks that have no final state, so let's say if I'm trying to balance a pendulum and my goal is just to keep it balanced, it doesn't end if it's perfectly upright or something. My goal is just to not let it fall over as long as possible. So for these, the desired sum of reinforcements is going to be over an infinite future. I might say it's going to time out after a certain time. My goal is actually to keep it balanced like 100 time steps or something like that. So this is going to grow to infinity unless I force it to be some sort of finite value somehow. So what we'll do here is basically if I'm just trying to keep something going as long as possible, I can't accumulate all of my experience from one to infinity because eventually I'm going to blow up. So instead, what I'm going to do is I'm going to discount reinforcements. I'll actually discount ones that occur farther in the future. So at some point, I'm going to say, well, I predict that this action is going to keep me in a good place for the next 200 time steps and I can't really predict much beyond that. So I'm not going to worry about that right now. In 100 time steps, I'll start worrying about that when my time horizon actually reaches that. So what we'll do is we'll add this factor gamma. And so this is going to be some discount factor between 0 and 1. So as the further I get into the future, I'm going to kind of scale down how much I take this reinforcement into account. So I need to introduce that into the objective function. So you saw that we put that inside the sum. And so when I rewrite my sum as the approximation of the next reinforcement and the sum of future reinforcements, I will also use that gamma term. So this is going to be some value of gamma sub 0. And then I'll use gamma sub k in the sum. And then this is just, of course, we can rewrite this sum as the sum of the t plus 1 reinforcement plus all futures. And so then finally, remember, this is just the same as the Q function for the next state, next time step. And so gamma will also be out in front of this. So what I'm just going to do, I'm just going to simply add the multiplication by gamma to the next state action Q value in the temporal difference error. So if I'm using a bunch of updates, these are batches. So what I'm doing, nonlinear regression, we had input and target samples in x and t. And so now one set of samples are collected for one set of weight values that is for one Q function. So after the updates, we're going to have a new Q function, which is going to produce different actions. So what I do then, I need to generate more samples. So I need to do these in smallish batches so that I don't train too many iterations for each time. Otherwise, if I do training convergence every time, then the neural network can forget everything that it might have learned from previous samples. So I want to say train for like 200 at a time. And it doesn't give me a perfect function, but I'm getting closer. Then I train for another 200 samples. It's like, OK, this is getting me closer. So I'm iteratively improving. I don't fully train to convergence with every batch. So any questions before I do the example? So we'll do an example. It's called a 1D maze. What's a 1D maze? It's a number line. Don't let it fool you. It's a number line. My goal is to land on some desirable place in the number line that I would just specify. This is a cartoonishly simple example, but it does illustrate. So let's take a chain of states numbered 1 through 10. And so I can be in any state. I can move left. I can stay put, or I can move right. And I can't move off the end. So if I'm in 1, I can only move right. If I'm in 10, I can only move left. And so I want to get to state 5, for example. So let's model this as a cost function. So for every move, it's going to be negative 1 or a negative reward. And then if I end up in state 5, I get a reinforcement of 0. So at this point, I'm just trying to maximize my reward, which is the same as minimizing my cost. So modeling the reward in this way will drive the agent to get to state 5 as quickly as possible, because once it gets to state 5 the first time, it'll see, oh, I didn't get dinged for this, so I want to do this more often. So the state is going to be an integer from 1 through 10. So let's approximate the Q function using a neural network. We'll have two inputs for the integer, so that is the state and the action, six hidden units, and one output. So these are the states and actions. It's just 1 through 10. Negative 1 for action left, 0 for action stay, 1 for action right. And then the state is going to be bounded between 1 and 10. It's just taken to be S sub t plus A sub t. So if I'm in state 4 and I move left, I should end up in state 3. So pretty straightforward. I'm basically just trying to find a desirable place in the number line. So here's my neural network class. This should look pretty familiar to all of you at this point. So I will store this. OK, actions available. Stay left, or step left, stay put, step right. So representatives changes in positions, we'll model them like this. So we'll just have an array of valid actions, negative 1, 0, and 1. Now, I need to force exploration by taking random actions. But because this is a neural network, as it learns, I want to decrease the amount of exploration in this case. We may not always want to do that. It depends on how well your neural network actually fits to the environment. This is very simple. So if I've got a really good policy for arriving at state 5, I don't want to risk disrupting that by taking a random action at that point. So we use epsilon to indicate the probability of taking a random action bounded at 0 and 1. So given a QNET, call it QNET, the current state and the set of valid actions in epsilon, we can define this function. This will return a random choice from valid actions or the best action determined by the QNET. So that is, if I have a 20% chance of taking a random action, then when it's presented with my action policy, I will either take the best thing predicted by that according to the state, or I will do that 80% of the time. The other 20% of the time, I will take the random action. As this is called the epsilon greedy policy. So I'll define a function epsilon greedy that does that. So basically, this is the important part. If I choose a random number that is less than epsilon, then I'll make a random choice out of valid actions. Otherwise, I will take the greedy move. So that is, I will run all my samples through my QNET and predict the best action for each sample. So now I need a function. What this will do, this is actually going to do the batching. So this make samples function will collect a bunch of samples of state action reinforcement, next state, next action. I can make this a generic function, my passing in functions actually to create the initial state, the next state, and the reinforcement value. So I'll define those functions first. So this is initial state, next state, and reinforcement. So basically, the initial state is just going to choose randomly from 1 to 10. So bounded at 11, because it's exclusive. And then new state is going to take in the current state and the action, and then add state to the action to give me the new state bounded at 1 and 10. And then the reinforcement will return negative 1 if I'm not in 5 and 0 if I am in 5. Very simple reinforcement policy. Gives me a syntax warning there, but it doesn't matter. So here's my next function, make samples. So I pass in the QNET, and then I pass in these functions for initial state, next state, and the reinforcement function, as well as my valid samples and the number of samples and my value of epsilon. So now I'm creating my X, R, and QN matrices. So these are just initialized with zeros. And then I will generate my first state. And then my action is going to be the epsilon greedy policy over the, as dictated by the QNET, given the initial state, S, and then the valid action of the epsilon. And then for every step in the samples, I'm going to sample the next state, compute the reinforcement at that state, and then choose the next action using the epsilon greedy policy, and then advanced one-time step at a time. And then this is plotting. So we'll just basically draw you a bunch of plots showing a bunch of ways of visualizing the output. So now for the main loop, we'll create 20 interactions. We'll call them steps for each trial, then update our QNET at the end of each trial. Then I'll run for 5,000 total trials. Now what I'm going to do is when updating the neural network Q function, I'm just going to run Adam for 20 epochs. So basically I'm going to perform a little bit of training to try and update my function. And then I will put in the next set of steps. So I will collect 20 interactions, update Adam for 20 epochs, collect 20 new interactions, update Adam again. So this allows me to basically perform these incremental iterated updates, not necessarily training to convergence every time. OK, so then I will create my neural network architecture here. We have 50 hidden units sitting above. I guess it's doing 50 now. I must have changed this. The description above it says six. Now gamma, this is my discount. So I'm going to scale every future reinforcement by 0.8. So the reinforcement at t plus 2 will account for 80% as much as the reinforcement at t plus 1. And then the epsilon decay, basically what this is is I have my epsilon value. I want to reach this final epsilon. When I'm done training, I want the probability of taking a random action to be 0.01%. So I need to start from 20% decay to 0.01% over the number of total trials. And so I'll calculate how much I decay every trial. So the epsilon decay in this case is 0.99. So now all this, here's my initialization of my neural network. I need to set the standardization parameters now so that the QNET can be called to get the first set of samples before it has been trained the first time. So I'll create this function. This is going to create my standardization now. I will set up my standardization with my inputs. So this is going to be 1 to 10. And then I will use the following means, 2.5 and 0.5, and then 0 and 1 for the mean standard deviations. And then the rest of this is just plotting. And now we call makeSamples. And I actually train my network. OK. This is just tracing the value of epsilon. This is actually performing the epsilon decay. And then the rest of the loop is for plots. So let's run this and watch it go for a little bit. So this will continually update. So basically what we can see here is I have the queue for each action on every set of trials. So you can see this updating. I'm getting a bunch of different randomly initialized samples. So for this one, this is going to be queue for each function. You can sort of start to see it start to take some kind of shape. So you can see that I'm getting some sort of peaks around 5 for the action stay, and maybe getting some peaks that are more correlated with moving left to right for the other states. So this is going to be the value of x for the last 200 samples. This is not easy to see. This is basically the sum of total returns because it keeps accumulating. So it becomes this big kind of block. Let me see if I can go through some of these other ones. It's not terribly easy. Don't want to. This one is the action. OK, so sorry for the jumps. It might make you some of you kind of seasick. Let's focus on the action for a little bit So you can see at the state, when the state is 5, the best action, this is plotted just as a line, but the best action between 5 is around 0, whereas the best action, now it's done. The best action for those states less than 0 is 1. The best action for the states greater than 0 is negative 1. This is the value of epsilon. You can see it decaying. This is the smoothed value of the total return. So you can see that it does train, and it does start to rise and kind of approach 0. And this is just plotting what each hidden unit is doing. So this is kind of hard to interpret. And here's the temporal difference error. So ultimately, this is actually what happened here. It basically plummeted for a bit, and then after more exploration, the TD error actually kind of starts to even out. OK, so now if I hit Define Run, I will not run this, because it says don't run live, because it takes about six minutes. So what this is doing, this is going through a grid search, trying to find what type of hyperparameters are actually best for this task. So we'll try a bunch of different numbers of trials, numbers of steps for trials, epochs, architectures, values of gamma, learning rate, et cetera. So let me skip to the bottom of this. I'll just plotted it in a Pandas data frame. So let me go through each of these, and then we'll talk about the results. So this is the number of trials. I've sorted them by our last two. I'll get that in a moment. So you can see for a number of trials, steps for trial, number of epochs, network architecture, value of gamma, et cetera. So the last two columns here are basically, this is the total return over the entire training process. And then this is the last two returns. So this is over those last two batches what the returns were. That's the value sorted by. If you want to take a look at it, do you have any sense of what network architectures look like they might perform well? Or network architecture in conjunction with other factors, perhaps? Yeah. So do you think the goal, the final goal, is the same or is it different for each trial? The goal is always the same. The goal is always to land at five. So if I change the goal, can it still land at both trials? Yeah, it could. Yeah. So if you can give an example of all fitting and registration, the ring function and the genome? Yeah. So if you can change the goal, can it still land at five? Yeah. So if you can change the goal, can it still land at five? The ring function and the genome? So this network is overfit to cases where the goal is five. So if I change the goal and retrain, I'll find it. This network has been trained to arrive at a goal that is five. If I change the goal to six or something, this trained network isn't going to work. But I can retrain the network. I might even be able to take this network and update it or something and tune those weights to update for goal six. Let's talk about generalization. Do you remember the block stacking example from last time? So what you can do is if I train it to stack two blocks and then I just change the environment to allow it to stack three blocks, because all it's doing is choosing an action on top of the topmost block, it probably would do an OK job of stacking multiple blocks. Because what it's predicting is basically, here's an action. It's relative to something very specified in the world. So it's very localized. And then I can basically update what is actually executed in the world, like what my agent actually physically does. And they can probably learn to stack three blocks. Or I could maybe take that train network and tune it slightly to be better at that task. But the state here is basically learning that if I'm in state four, the best action is to take one step to the right. If I'm in state six, the best action is to take one step to the left. If I'm in state five, the best action is to stay still. So if I change my goal to six, that is I change the environment, if it gets to state five, it's going to say the best action is to stay still and whatever reach the goal at six. So the environment, of course, is critical. So here, does anybody see what might be something of a discriminating factor in terms of success of this type of network? Is the number of hidden? We got 10, 10 here at the top. We also have 10, 10 and 10, 10 here at the bottom. The number of steps for trial looks like it probably does. So some of the worst performing ones all have 100 steps per trial, whereas the best performing ones usually have fewer. So one reason for that is that this is actually training less per trial. It's just pouring a very small iterative update, such as training for 10 epochs or something, updating the optimizer. Whereas here, it actually might be training closer to convergence. And then the next trial comes along and it has to forget everything that it learned in order to optimize again. OK, all right, questions about neural network reinforcement learning? I'm sure there are many. I can talk about this at some length if you want. So if you imagine a more complicated environment like solving a game level or something. Yeah, question. No, go ahead. I have a question about choosing activation functions. OK, yeah. Is there anything more that we need to consider now that we're doing reinforcement learning that I've never done before? Yeah, so choosing activation functions. So I think that's a good question. So I think that's a good question. That's really good. Yeah, so choosing activation functions. Let me think about that. So you have a couple of you do want to be kind of choosy about what activation functions you're using depending on the nature of the problem. So generally speaking, if you use a ReLU activation, it's possible you may lose some important information about, say, bad actions, for example. But also that would depend on how you formulate your problem. Because if your reinforcements are negative, maybe ReLU is a worse choice there because it's squishing out negative information. But if your reinforcements are really less positive, basically if the worst thing you can get is 0, ReLU might not be a bad choice. But if you're modeling it like we did here, maybe ReLU would be not as optimal choice. Other questions, comments, thoughts? So if you're trying to solve a more complicated environment where there were, say, multiple recurring similar circumstances that you're trying to solve some sort of game level or something like that, with a neural network what you get is you can increase the size of the network to accommodate different types of conditions. Yeah, question? On this Super-L, are there any activation functions like outside of any ReLU to consider? Yeah, yeah. So we didn't talk about that a whole lot, but there's a brief sidebar at the end, I think, of the ReLU notebook. There are a bunch. So the LU part of ReLU is linear unit. There are a bunch of other linear units. So exponential unit, Gaussian error linear unit, parameterized ReLU. So I'm talking about some of those, briefly, just to refresh your memory. So let's take leaky ReLU as one, for example. So ReLU, 0 until 0, and then y equals x. Well, maybe I want to squish out most of the negative information, but not all of it. So leaky ReLU kind of lets a little bit of it through. So it might be like 0.1 times x if x is negative, otherwise y equals x. Parameterized ReLU is similar to that. It lets through some of the negative information, but it's not a constant. It's usually a tuned value. So I can kind of decide, or I can try to learn the value of how much of the negative information I don't want to let through. Maybe I want to let through more of the less negative information closer to 0, and I really want to disregard most of the really negative information. Gaussian error linear unit is basically, imagine what ReLU would look like if it were a nicely differentiable function. So basically, we're trying to put the smooth curve in at 0. The Gaussian error linear unit is the one that is used in most transformers. So most of the chat bots, natural language applications, those are using Chibu units. Yeah. Other questions? Yes. Is that message for loop strategy that generally the best we can find? Oh, for grid search? I mean, if you're doing a grid search, yeah, this is like the way you would do it on if you're just doing your own. There are a number of libraries out there that you can use now. So Hyperopt, I'm not sure, doesn't do neural networks very well, but basically there's a lot of parameter search libraries out there that they will. One of the best ways to do it is basically, I have a bunch of parallel processors. I will put one instance on each processor and run them all at once. And so that way I can try a bunch of things at the same time, get all my returns back and figure out which one of these was best. But that, of course, it's going to be difficult to do just on your laptop. Yep. OK. Any other questions before I start on the project? OK. Let me open that up. Let me see. Where is? Where's the solutions? These are project proposal. And then here is an example. This is the report example, actually. OK. So the project proposal, so basically you can work in teams of up to four, as I've mentioned. The scope of your project needs to reflect the size of your team. So it needs to be more ambitious the more people you have. The project, as I've mentioned to some of you before, is really you think of this as an opportunity to maximize your own success. So that is, if there is a technique that you're particularly interested in or an application that you're particularly interested in or you have some outside area of expertise that you want to apply something to, basically what keeps you interested is what you probably would want to do for your project. If you have an existing research project and you want to put some sort of machine learning spin on it, that is fine. I do not object to that. So there are a couple of options. This is not necessarily exhaustive, but what I want you to do is be inventive and pursue some topic of your interest. But for example, if you need some ideas, take a neural algorithm that we've covered in class and apply it to some different data sets of interest and do some analysis and draw some conclusions. The analysis has to be more in-depth than what we've done in class, more than just like a bigger network work better or whatever. What I want to see is like, yeah, do the analysis of network strategies and different hyperparameters, but also I'm looking for error analysis. What are the things that your implementation fails on and why? Hypothesize. You can use other outside tools if you want, if it helps with the analysis. You can download code from the internet that implements an algorithm we didn't cover. So we talked a lot about neural networks. I will briefly talk about like KNNs and SVMs and stuff at the end, but by then you will be pretty deep into your projects. If there's some other machine learning algorithm, could be neural, could be non-neural that you want to explore. And maybe you compare that to something that we did do in class, that makes sense. If you have a bunch of coding that you're already doing, you're like, I can't bite off another coding project, you have the option to write a research report. Or if you just simply want to write a research report regardless of the amount of coding you're doing elsewhere, you can do that. What that means is you need to study at least five research articles of a topic of interest to you, again, a machine learning topic. You need to present a report that summarizes each article in detail, describing similarities and differences between the papers. And then you also need to provide a conclusion section that basically summarizes your takeaways on that topic. If you were to look at chat bots or something, you would study five papers on chat bots, and then you would write a report summarizing each approach and then talking about what are common approaches, what the current state of that field is. What you need to put in your proposal is basically just a confirmation that it is appropriately scoped. So remember, you start this now, you could probably get approval if you're really fast as early as like middle of next week, and you could potentially start then, but let's assume that the starting on fire is on April 6th, that gives you roughly a month and about a week to get everything done. So you need to, don't bite off too much, make sure this is something you can do within about a month to five weeks. In the project proposal, you need to show that they're appropriately scoped for time period and team size, and make sure you put effort into both the implementation and the analysis. Talk about what questions you're seeking to answer, and then what hypotheses you can make about the data that you'll be exploring using whatever methods you use. You need to explain why you wanna do this project, and the steps you will use to complete the project describes the methods you will use. So I need to see the sources, the data, are you gonna define new algorithms from implementations? Are you gonna use them from an online source? Where are you gonna get them from? So basically, do your due diligence, show that you have cited your sources and then go there and see what you're gonna be using. If you are working in a team, you also need to provide some definition of how the work is gonna be divided among the team members, so who's gonna be primarily responsible for what. This is not intended to be a hard, great wall between the team members. Of course, I expect you will be collaborating, you're gonna help each other out, but kinda need a sense that one person's not doing all the work and the rest of the people are just free riding. Possible results. So you can speculate on possible answers to the questions you provide in the introduction. This is gonna be a little bit different between the coding projects and the research projects. The results of the research projects are basically what you're gonna be looking for in your discussion. So what types of contrast do you think you might find between different approaches? What do you think this might tell you about the particular topic that you're researching, for example? Possible results for the coding project or of course, more along the lines of what do you think the likely outcomes are gonna be? You need to provide a timeline. So I wanna see like four entries with some dates and describe what every team member will accomplish by these dates approximately. Your grade will not depend on meeting these deadlines, but this is basically for your use so that you can come to me if you have problems, for example, and I can help maybe try to get you back on track. Okay, so this is pretty short, right? Doesn't need to be more than like two pages long. Submit this in Canvas and there's gonna be a drop box. So it's gonna be last name proposal or last name dash last name proposal and then don't email this because it's gonna get lost. So grading of this, grading is basically complete or incomplete and if these are not satisfied, what I will do is I will send it back for revisions. Once you revised it to satisfaction, you'll get the full credit. So this is basically 15% of the project grade and you will eventually get that 15% as long as you've heard it in and complete the revisions. So don't worry about like getting dinged on like if your proposal isn't clear or something, I will send it back to you for revision, needs to be made. So then also you make sure you're not like under committing or over committing. So like I said, this is 15 points. So do this and don't give these points up. So basically the grading on the project is gonna be 15% for this, 15% for the lightning presentation on the last two days of class. So what I will do is once I've gotten all the proposals in, I will count how many there are and then that will determine how long those presentations are. Usually ends up being about two and a half minutes for each one. So I will go into more detail of what you wanna do for that. But really for those last two days, every team is gonna submit a slide deck that has probably about three slides, maybe more, if you have a lot of teams, maybe longer presentations, but basically say, what's your problem? Not in an aggressive way, what is the problem you're trying to solve? I'm trying to solve what progress have you made towards it and what approaches you're using and then like what is left to do or what have you learned so far or something like that. Okay, any questions on the project deliverables? Yeah. We will, yeah. So I will do at least one lecture on transformers toward the end. Yeah, you could do, yeah. Oh yeah, I gotta do this first. You may not use chat GPT to write your proposal or your project. You may not use any chat, but like, can't use Bard, you can't use all the other ones that came out, you can't use like the one that the Chinese government put out the other day that didn't do so well apparently. So yeah, you cannot use a chat bot to write this. I want you to write your own words. So basically here's my attitude toward the chat bots right now. Right now I'm actually literally, you know, in and out of meetings with people that tilt trying to help come up with some sort of coherent positioning towards chat bots. So eventually I do feel we will come to some sort of coexistence and understanding of what is and is not an appropriate use of chat bots in the university. My attitude toward writing is if you're using a chat bot, you're not using your own words, okay? If you may possibly use like a chat bot to maybe help with like the mechanics or something, but ultimately it needs to be rewritten. You need to submit your own writing. And I will be checking against, you know, your written report and say your previous homeworks. You know, I'm gonna be looking for things to see like has the writing style drastically changed? Is were you making grammatical errors and suddenly it's like perfect, right? I'd rather see the grammatical error version, but I'm not gonna be grading for grammar unless it's like so pervasive that I can't understand. So if you spell things wrong occasionally, you make the occasional grammatical error, I don't really care that much because one, it's telling me that you're actually doing your own work and two, this is not an English class, okay? So I think at also the other thing, if you wanna study, if you wanna use like the outputs of chat bots in your project, that's perfectly okay. So using a chat bot as an object of study is a okay. I think that would actually be really cool to see. So just for some guidelines on like what my positioning is toward the use of these tools right now. Okay, other questions? But yes, we will be talking about transformers at least once. Okay, yeah. What? Yes, I will go about LSTART office hours at 3.30. Okay, bye, thank you. Okay. Can someone confirm you can see and hear me okay. I'm like, you can okay great. Yeah, sorry the ironically I've been San Francisco and the Wi Fi here is like, terrible. So hopefully this will hold out it seems to be okay up here in my room. Give me a few minutes to let everybody in. Let me get started. Okay. Yeah, good enough. I'll get started. So, I'm ready for logging in this should be pretty short. I was just doing the next lecture, and it is pretty brief and one after that is relatively long so what I'll do is I will just do. I'll do it on Thursday, and then we'll just take Thursday off, and then tomorrow or next week, we'll resume and we'll be back on the expected schedule. Should the Wi Fi crash for some reason and you lose me. What I will do is I will just record this and I'll post it of course, as normal. Yes, only just a couple of announcements today. So we are working on assignment three grading and we will hopefully get that back to you by the end of the week or so. And I know some of you had submitted some questions on assignment for I will do my best to respond to those in timely fashion. I'm going to be traveling tomorrow I'm going from San Francisco to the east. I will have like I mentioned before, like the layover in Denver so maybe I can spend some time responding to your messages. So, one announcement to just send out to, to all the CS undergraduate through here as you might be, as you may have heard, we are in the process of recruiting a new chair of the department and our first candidate, Bruce Draper is going to be doing his interview tomorrow and Thursday. And as part of the chair search we are. We want to basically get feedback from every constituent group on campus. So, are in the departments that includes, of course, faculty and staff and then people at the college and university level and graduate students but also undergraduates. So, if you are a computer science undergraduate I know there are a number of people from other departments and forgive me for just sending this out to the CS folks. If you are a computer science undergraduate and you have some time around lunchtime on Thursday, you will have an opportunity to meet with Bruce. And once you get free food, and you also get a chance to talk to him about his vision for the department how he sees it evolving in the next five years during an ostensible term as chair, and make your any of your concerns, or known or get any questions answered. So, at the very least this is an opportunity for you to get some free food. This would be in room 305 of the computer science building. So if you're interested in that, you have an opportunity to talk to our incoming chair candidate, and we'll have one more after that, probably early next week, where the same opportunity would be extended to you. So, if you're interested in that I would encourage you to take part in that we would like to see I would say, maybe a dozen or so folks at least attend this lunch. If you have any more questions about that feel free to reach out to me I can give you some more information if you're, if you want. I believe that's all I had to, to announce right now. Anybody got any questions about anything. Let me screen share up. Okay. All right. Sure screen. I'm gonna go ahead and turn the water real quick. Okay. All right, so this will just take as long as it takes and after that I'm going to let you go. Okay. So, basically what we're going to talk about today is going to be reinforcement learning in continuous spaces and this is going to be just a slight evolution of the quote 1D maze example that we use last time so if you remember, 1D maze I suppose So, we're just we have some determined goal position on the number line and our goal is to try and move our agent or our, our object whatever it is into that position. So that is if my goal position is five on the number line from one to 10. Then what I want is if I'm to the left of index five, and then I'm going to move to the right, I'm to the right of it then I'm going to move to the left and if I am exactly at five I want to stay still. So the way we model this is basically we have a discrete action space, which is negative 101. And then we will take these steps in those given directions. And I have basically a cost associated with taking a move. So that is, I get a reward of negative one for taking a move unless that move lands me in my goal state, in which case I get a reward of zero. And in this case we have a cost function that we are basically trying to to maximize the reward by virtue of taking the fewest steps possible or the steps that are most likely to land me in my goal because I get a non negative reward for that. So, that's kind of our deterministic version or discrete version. Let's think about how we can model this in a in a continuous space. So that is, let's, I'm going to say we have a marble. So now I have basically a flat flat plane. And I can move horizontally along this, and we're going to do that by say picking up and moving my marble but not going to do it by exerting forces on that marble. And so we can think about how this is actually going to work in say a real physical space and you can imagine that if I have a rolling object which is Sam can constrain the motion from into a single dimension, then my, the state can actually be modeled using the position of the object and its velocity. And then my actions are going to be forces that exert upon that marble so I can actually push it to the left or I can push it to the right or I can do nothing. So you imagine this is going to be a slightly different scenario that I wish I'd brought a marble with me and personally didn't think about that. But if you imagine if the marble is already moving in a direction, right, if I, if I may take no action is going to continue moving in that direction will assume basically I'm not going to have things like friction. So I'm going to say it's a frictionless space. You could model this as a as an environment that has friction if you had a more realistic simulator. In this case we're just going to be doing it kind of text wise and Python. But let's imagine that we have a marble, and I can push it in one direction or another, or I can do nothing. So of course if I'm already moving in a direction, I do nothing it's going to continue moving in that direction. So let's imagine that if it's got a high velocity say moving toward the right. If I push it toward the left if I exerted force and toward the left it might not slow it down, it may not move it start start moving toward the left, and it will slow it down. And it may in fact slow it down enough that it will. So, my goal now is that if I have a goal location. I'm going to have a slightly different policy, right so you imagine now that instead of taking these discrete steps left or right. I'm exerting forces, and those forces are going to have effects on the continuous motion of this object and so the policy is going to be quite So, unfortunately I can't really see any of your reactions I hope that was a clear explanation if it's not what I recommend you do in this remote scenario is basically just like raise your hand, or put a question in the chat, or even just like speak up and interrupt so honestly speaking up and interrupting is probably the best way to get my attention in this in this scenario. So our goal is now to get the marble in a particular position on the track. And so we're going to be doing this using a similar method. Basically I'm just going to be using my neural network as a regressor, and we'll see how this is different from the discrete case that we examined last time. So import my neural network with regression so I do my standard imports with NumPy and pandas and pipeline and also import my optimizers and my neural network class. So, now we're going to define our reinforcement function initial state function and next state function, but they're going to be defined to basically model this dynamic marble problem. So if you compare the functions as you specify them here to the ones in the notebook from last time you'll be able to see the differences. So, what are our variables. So we'll have first x sub t, this will be position, and what is called the unit meters for now, it's gonna be whatever it's a big marble right it's going to be moving in terms of meters, but this might as well be centimeters, and then prime is going to be the velocity in meters per second. Okay. So, if you look at what is my position at time step t plus k that is x sub t plus k. This should be my current position right at time step t, plus the integral over time current t to t plus k of the velocity. Right so that makes sense if I'm moving in a constant velocity over n seconds I need to calculate how far I moved over n seconds and then add that to my current position that's just going to be the interval of those values. So you know we all should be familiar with with how this is done. You know we've probably all done, I hope you've all done, integrals and your calculus classes. And so we know that velocity is the derivative of position acceleration is the derivative of velocity, and what's the derivative of acceleration it's actually we call it jerk. And then beyond that I don't think they have turned for that. So, we're trying to now calculate the integral the anti derivative of velocity with respect to time. So of course this is what this year. So I have going to be calculating with respect to t, I'm going to be taking the, the integral of x prime of sub t. So, that is the kth position for now is going to be the current position, plus the total of all velocities over k time steps. Of course this is a continuous continuous calculation. So for any time step t calculating change of position is basically going to be the change in t delta t is going to be the current position, plus the change in t however many time steps have elapsed, times the velocity for for those time steps. And then of course calculating velocity is going to be the same I'm just going to be adding the acceleration right so x double prime is going to be the acceleration at time t and this of course would be in meters per second squared or whatever unit of distances per second squared. Now, this is a continuous problem as mentioned, but of course we have this issue that I'm sure you have encountered before whenever you try to calculate integrals and calculus class we have computers being discrete machines. So, we know that the exact integral curve is unknown, but we know that its starting point is a particular value. So, if we have my starting point of integral curve as a sub zero here, we can use the Euler integration method to approximate it so that is I take a small time step, right, some discrete time step, and then I approximate the value of this curve over those time steps and of course here, right, we can see that because the size of this time step is large enough that my approximate time step is going to be the same. And so, this is going to be if you remember when if you studied calculus like I did, one of the first things you probably did was break the curve in the area under the curve into basically boxes, right, discrete time steps. So, you might draw something like a terrible drawing. Let's say I have a time step, you know, something like that and then the next time step I might approximate it like this much and then so on and so on. And then basically the integral is taken to be the sum of all these and then of course if I have some time step, you know, t, if I decrease this value of t, I'm going to get a better and better approximation, right. So, if I subdivide this suddenly I have these small boxes that are basically giving me a finer grained approximation of the area under the curve, right. So, that's the Euler integration method and then of course we can show that as the limit of this, that subdivision approaches, you know, infinity, then this is going to be an accurate approximation of the actual area under the curve. So, in this case, whatever delta t is where we define that as is going to be the Euler time step. So, here might be a fairly large value. So, maybe over a second, this is an imprecise approximation, but over say, tenths of a second, it's going to be better. So, how do we model this in actual code? So, first let's define our functions. So, our valid actions, right, remember these are forces now. So, I'm going to be exerting a force of negative one or one or zero on my moving object. So, now my functions have to be defined somewhat differently. So, for the reinforcement one, right, I take an s and s and of course this is my current state on my next state. So, my goal is state five, but of course I know that the likelihood of landing at exactly state five is pretty small. So, I want to basically optimize my policy to approach state five as close as possible and I want that the reward for that to be zero when I kind of get within an approximate integral of that value. So, what I'll do here is I'll define this as basically if my next state is within index of one, within one unit of my goal, then I will get that zero reward. Otherwise, it's going to be negative one for every action that I take. Okay, initial state is very similar, except I have I'm going to be sampling a continuous value instead of just some value between like zero and 10. So, I will take, you know, don't add some random noise to my to my starting position. So, the next state is kind of the continuous analog of the previous version that we had. So, element zero is the position element one is the velocity. So, now this has broken up my state down to two distinct values. So, not just the position, but now this is the position. And then my action is going to be one, either negative one, zero or one. So, what we do different here now is I'm going to define my order integration time step. So, here I'm going to say it's a tenth of a second. And so, now given my velocity that is s one, I'll be able to add that value. So, this is per seconds, we're going to multiply that by how many seconds you're in my time steps in this case point one, and I add that to my position. So, force is the action that I take. And so, then we need to do the mass of the object. So, here I'll just define the mass to be a particular number. And then we can, here we have say a little bit of friction here that we can, so I'm sorry, I guess I was incorrect in saying that we're going to model this completely frictionless environment. Sorry, maybe we're going to have a small amount of friction. And so, then given this, I'm going to take that delta and then multiply it by the force divided by the mass minus 0.05 to account for some friction and then times my velocity value. So, this will be my new velocity. So, now I'm going to have some balance on it, of course, so I don't want to roll off the edge. So, if I hit the boundary, then I'm going to set the velocity to zero, I'm going to stop moving. So, my initial state function above that's going to sample initial state, so it'll be a two element array. And the first element is the position of the marble. And second element is the starting velocity. So, the starting velocity is always going to start at zero. So, if I end up, if I start at seven points something, then it's going to be sitting there. And I want to figure out what action I have to take, and that might be exerting a force to the left or to the right, that's going to move me to a different place on my line. So, in this case, I sample my initial state, it gives me 6.13. So, we can say, okay, my goal is five, I started 6.13 and kind of to the right of this. So, given this, I'm going to randomly choose from our three valid actions. So, here it says, okay, I'm going to take an action of one that is exerting a force to the right. So, I'm going to do this, you know, a number of times. And so, then I'm going to sample these actions, and then calculate the reinforcements accordingly. So, here I'm going to do this for like 1000 time steps. What's going on here? Well, I'm going to make a random choice from my valid actions. I'm going to use that action to calculate what the next state is. I'm going to use the value of that state to calculate my reinforcements. And then I'm going to append that to a list and I'll plot it. So, run this and we can see what happens. So, after 1000 time steps, I started at, in this case, it's starting me at 1.1 and it is getting to 5.56. I'll try it again. So, here we start at 5.8 and we end up at 6.6. So, I'll try it again. I started at 9, I ended up just hitting the wall at 10. So, briefly, let's try and gloss this graph. It's a little bit hard to read, but we can see this is actually like a bad, let me try one that's a little more indicative. So, here, okay. So, we start at 8.8 and we can see that the, we can see how the position kind of moves to move toward the left and then it goes back toward the right. Then it takes kind of, it's just toward the left pretty consistently. And then it starts to go back toward the right and eventually it ends up pretty much not far from where it started. The other thing we can see here is that where we look at the reward here, it's the red graph, you can see that it's usually negative one, except for when my position is kind of within one unit of being at five. In that case, the reward is zero. So, we can see the modeling of the position and the velocity is sort of hard to read. It's the orange line there, but you can see how, you know, when it's exceeding two, for example, moving very, very swiftly toward the right and so on. So, you might have noticed something kind of weird about this and we'll get to that in a moment. Let me plot my last few values. So, if I look at the last 10 positions and the last 10 velocities, sorry, I read that again, I guess. So, in this sample, now this one starts at 3.5 and ends at 4.5. So, look at the last 10 positions, right, 4.46, kind of moving toward 4.5. Last 10 velocities is, you know, 0.02, 0.15, 0.02 again. And then the last actions you can see here to go from time step, let's see, t minus eight to t minus seven. You can order, let's say, maybe t minus seven to t minus six, for example. We can see that it's got a, no, sorry, I'm reading this backwards. My mistake. Sorry, I'm pretty tired. So, this compares zero and one. So, here we have an action that is zero. And then it was moving in a given direction. And so then the velocity is like 0.02. We have to take this action of one and the speed increases. We take an action of negative one, the speed decreases, take an action of zero, the speed decreases just a little bit due to friction. Take an action of one again, it starts to increase. So, did the marble ever get to the goal position? Well, not really. But we see some places where it approaches the goal position and that those are where we see this reward as being zero. But it never stays there and it doesn't ever seem to learn anything about how it's supposed to approach that position and stay there. So, what went wrong here? Well, there's a couple of things. One is basically we're just replacing the state with a state plus action and then just calculating the reinforcement actually just based on like two instances of the same state. Right. And so what we need is we actually need to have some way of approximating what the best action is according to the state action pair inputs. So that is we need to be using our network as the Q function. So we need to define our epsilon greedy function. Remember what the epsilon greedy function is. This is going to be there is some probability that I'm going to take a random action. Why do I take that random action? Well, it's because I might find some sort of strategy that does actually get me to the goal that I want, but it does it in a very suboptimal way. And so once I discover that strategy, I do not want to keep repeating that strategy in case it's very securitous or very cumbersome or something. I have to allow for the possibility that there is in fact a possibly better option there. And I might be able to find it by instead of exploiting my my best current strategy. I'm going to just take a random action. So the epsilon greedy function is going to specify some value of epsilon, say, point two. And so if I sample a random number that's less than that value, I'm going to take a random action. Other than that, I'm going to take the greedy move that is going to exploit my current best my best strategy according to my my Q function. So remember, the neural network is approximating as basically a function approximator that's allowing us to learn what the current best move is according to the state action pairs that I have sampled from the environment previously. So this is the epsilon greedy function that's basically the same as it was before. So I have the only difference here is that the state instead of being a single value, it actually be two values. Now it's going to be position and velocity instead of just position. But the function is written the same way. And all I need to do is just pass in that two element tuple or list or whatever into my state. So now I have my my Q net that will allow me to just like use that that function. So now I need to be able to train that function, of course. So here we can see just the epsilon greedy part is not actually training the function. This is just using it when it's already been optimized. So what I'm going to do is I'm going to generate some samples that represent my my experience from conducting various trials in this environment. So I've defined my environment is now having a model of a position and velocity representing my state and then a set of actions that I can take that are these are discrete actions, but they're going to have kind of continuous response within this space. So in terms of I'm not going to be picking up and moving my object, I'm going to be exerting a force on it, which means it's velocity will speed up or slow down in whichever direction. So make samples is written generally to accommodate for whatever whatever my representation of my state is. So here I define these function kind of placeholders where I can define what my initial state next state reinforcement functions are. So here, you know, I just I just call these functions. So next state F is going to take in my state and my action. Next state Rn is my next reinforcement. So that this should give me the resulting reinforcement from the previous state and the next state. And then epsilon greedy will just use the QNAT to choose the next action. So let's try this right. So if I create an S, this is going to be the leftover S from my my sample kind of randomized run here. So you can see that I've got four point five seven. That's my position zero point three nine. This is my my velocity. So basically at the end of this, that last kind of randomized thousand trials, I have my marble that's a position four point five seven. And it's moving relatively slowly toward the right. So this actually would eventually get to the goal, but it probably would keep going and overshooting it. So this state consists of position your velocity. So now this is a two element ray as shown. And so this two S is actually going to be three elements. So therefore, we're going to modify this make samples function just to allow our state variables to contain multiple values. So here's that. So what I will do is I'm going to update the state SN from S and A. And so then here, the rest of it is pretty much the same. So this is a very small, small adjustment. So if we can compare this version here, there is no next state. There's no call to the next state function here. So what this does is this is going to take my next state. I'm going to assume that my starting action is always zero. And so my next day when I take no action is going to be the same as the current state. OK, so now I'm ready to trade. So before we do this, we're going to make this plot status function to show how we're doing. And we can look at the various different things that we're going to plot. So like last time, we're going to show a bunch of different graphs and we can go through all of them. So what I want to plot is one, the probability of taking a random action. So that is my exploration over the number of trials. So if I'm doing epsilon decay, then I would expect that as my model gets better and trains more, then I'm going to have a lower probability of taking a random action because I'm fairly confident that my best strategy so far is actually relatively close to optimal, given the amount of data and training that I have. So the second thing, starting position of every sample. Remember, we're creating these samples. And each time I create new batch of samples, it's going to reinitialize my environment and start my marble in a new position. So I want to see how my training evolves, given that one time I might start my episode at 2.6, and then the next time I might start it at 7.3, and then one time I might start it at 10, the next time I'm going to start it at 4.9. And so as I train, I should have a better idea of how to get to the goal from my starting position. So here I'm going to mark in this graph, I'm going to mark the goal position at 5 so you can see how things kind of evolve. Next thing I'm going to plot is the latest policy shown as the action for each state. So we should have some graphs showing that given my state, this would be the best action, right, and this should be some sort of continuous manifold at this point. So this should be mean reinforcement versus the trial. So we'll actually smooth this and we'll show the mean reinforcement every 20 trials. And then five is going to be velocity versus position, right. So this should be in a well-trained policy relatively intuitive. So if I'm close to five, I should be moving kind of slowly in the right direction so they get as close to five as possible and don't overshoot it. So this fill between function is kind of cool. You can we'll see what that does in a moment. And then plots, I guess this should be six through 10. I don't know why I've numbered seven through 11. This would be the max Q value versus the state and then versus the actions versus the position of velocity. And then we're going to show some top down and some 3D plots so we can see kind of how this looks and what influences we can make from the 2D versus the 3D. So that's the top down and then the rest is just drawn. So let's write a function to test it. We'll plot the marble down at a number of positions and we'll try to control it and we'll plot the results. So for end trials and starting positions, I'm going to run a bunch of simulations for a certain number of trials and then plot that. Okay, so now we set up the standardization. This is pretty much the same as before, right, we have our, our means for my inputs and my targets. I'm going to use that to standardize my values when I'm using them to train the QNET. And so then I'll plot, you know, my, my standardization. So then here, but this is basically my training and my plotting. So I'm going to have, this is gamma is my discount factor, that is how much do I discount projected reinforcements like far in the future. I'm not very confident of those. So I maybe don't want them to have a huge bearing on the actual training of my model, because, you know, they're, they're, they're so far in the future, they probably don't matter all that much. And then number repetitions of the Q update loop, and then the number of steps between new random initial states, that's a learning rate, specify my final epsilon so I can calculate my decay rate. And then I create the QNET I'm going to use just use a fairly simple one that has 10 hidden units as a single layer. So here's my QNET structure. So three inputs. So for the position, the velocity and the action, and one output. This should just be the best predicted action. Where this ready best predicted the best predicted reward. So then I run setup standardization, and I'm going to specify some initial epsilon value. This is one, because when I start, I don't have any best strategy. Right. So I think this to one. I'm just gonna say, start by taking a random action and then I'll burn some that then they can decay my epsilon value. So then I collect all my samples, run train, and the rest is for plotting. So let's run this real quick. And there we go. So, we can see here for example. This is the random action probability starts at one. And you can see it sort of rapidly decaying towards zero because this should be learning something. Well what exactly is it learning, let's take a look at the mean reinforcement. So starts out, like, kind of point eight five. I'm not sure what the initial. The like the very first initial state that selected was might have been somewhat close to to five, but it pretty quickly drops towards negative one. And then so sort of just a bombs along here for about 300 time steps. And then it starts to actually learn something right so we can see here that it's at this point, right about 300 time steps, it sort of started to figure out what the best policy was. And so now here, if you look at the maximum of the, the Q values here and in 2d and 3d. And you can see that, you know, for example, what's the Q value, when the position is very close to 10 and then the velocity is negative right is negative point three, and then kind of the converse is true. And it's, when it's like close to five and the velocity of zero, the actions. This is pretty interesting so you can see that when my position is five. There's sort of the sharp boundary, given the, the actions that I have available to me. So if we look at the state trajectories for epsilon equals zero. So, this is my desired position. Right, this is five. And then you can see that if I have, say, a position of five the state trajectories for the velocity tend towards zero. Whereas if I have a lot of positions that are say closer to 10, then my state trajectories, you know tend toward the native numbers where if there's my positions are zero my state trajectories tend toward more positive numbers. So this is the policy for zero velocity, so you can see the kind of how well this maybe has trained. So, this is maybe not quite as optimal as I would like so I would expect that, you know, my best policy would basically be RRRR 0LLLL. For if I'm at zero velocities that is if I'm not moving I'm in one of these, one of these states, where should I be moving well what this has learned so far is that if I'm in states eight nine or 10 I should move left, which is good. But it's also learned that if I'm in states zero through seven I should move right, which is not quite right. So you can try to train this again to see what happens. So this is a good example of how this is actually taking up to like 15 seconds or so. You kind of see similar patterns starting to evolve right this this curve starts to look familiar. So does this one. And again I think this is, yeah. So, actually you got a little bit worse that time, right so now it's basically learning that if I'm at 10 I should move left. And if I'm at anything else I should move right. So try one more time, and then we'll move on. So you can kind of see things changing here over time right now it's basically like learning a strong policy just moving to the right. Okay, now things are starting to happen. We're getting some, some indications we should start moving left there. Okay, so this is done again. So, maybe this is not like a terribly successful policy and try one more time to see if I can get it to kind of demonstrate something nicer. One thing we're observing here is that reinforcement learning has a tendency to be unstable. And so often you start to learn something and maybe your X, your epsilon is decaying too fast and you kind of find some suboptimal strategy and stick with it. Or, I'm not liking what this is doing. But you can see that you know there's, it sort of finds some sort of degenerate strategy and it's kind of sticks there for a little bit. So sometimes you need more complicated strategies maybe we need a bigger neural network that would take longer to train, maybe we need more, more samples per, per episode that would allow us to, to kind of get, get more experience. So, let's, let's just sort of accept that this is sort of learn to kind of some kind of suboptimal policy and actually seems to be doing worse than the previous like five times that I tried it. So let's plot the or let's print out the last, last 10 rewards. So what this has learned here is that if I over time, the last 10 rewards, rewards that we got were negative one. So this was basically somewhat unsuccessful in training by, you know, a couple of different things like we can try maybe adding a adding more hidden, hidden units. Try this again. So you can see this taking longer to train this time. But now where are we at. Yeah. So, starting to learn a little bit more a little bit earlier. So, now, the policy for zero velocity in this case kind of seems to be just sort of bottoming out with always moving to the right. So what we're seeing here in this example is a strong tendency for the instability of reinforcement learning. So in this case for this, for these continuous problems, basically what we what we find is that say Q learning is not necessarily always the best choice. And so often we encounter these problems with stability when trying to use a sort of deep Q network. So we have other policies that I alluded to earlier in class, such as the actor critic methods. So you have say things like an EDPG or a soft actor critic or an A2C. And what that does, we have these two neural networks where one is the actor that is to choose the action. One is the critic that tries to predict how good or bad the action is going to be. Right. And so then the actor tries to get better at predicting good actions and the critic tries to get better at predicting the quality of the action. And then these two networks are updated in tandem. So that's one way of kind of solving some of these or at least addressing some of these instability problems that we encounter with continuous spaces. OK, anybody have any questions? This again, just for fun. Now we're seeing something desirable here. So now we have a policy that's kind of approximating what we'd expect. So this was now that we're done here, let me show you a case that we've been moderately successful. So if you look at my my policy for zero velocity, this is where my mouse is that state five. This is where we want to end up. And so what this policy has learned is that when I'm less than five, I want to move to the right. When I'm greater than six, I want to move to the left. And when I'm at either five or six, I stay still. So this is actually this is a moderately successful policy. And we actually managed to get one in that it's approximating when I get close to five, at least knows that I should probably try to slow my marble down. Whereas if I'm far off to the right, I need to be accelerating toward the left and far off to the left and you be celebrating toward the right. My action policy. So you can see here that where I have this last sample actually appear to be quite successful where my my marble position is within this red bar that has been one unit of five and my velocity is very close to zero. And so similarly, if you look at the actions here, it's learned that if my position is a ten and my velocity is like negative four, then the action, this is really probably don't want to do a whole lot here, for example, because I'm moving in the right direction. I don't want to accelerate too much. Whereas the opposite is true if I am say close to zero and my action and my velocity is four. Right. So I might want to accelerate a little bit. But I'm probably not too much. Whereas if I am at ten and my velocity or close to zero velocity is four, I'm moving fast in the wrong direction. And then I need to kind of accelerate or stop my motion and start moving back in the other direction. So now we look at the state trajectories and now this is the pattern that we would expect. As I am, if I am at zero and my action is my velocity is zero, then we can see that what I'm what I'm doing here is like as I'm moving in toward five, I kind of want to increase my velocity. But then I also want to start by crossover five. I need to be kind of moving back towards decreasing my velocity moving back towards the left. Okay. Questions. All right. So, in conclusion, I suppose, continuous spaces are challenging for reinforcement learning. And so you need to have sometimes more sophisticated techniques to try and solve these. We find these cases in, in scenarios like robotic manipulation, where you're trying to control some say some some movement of a joint and continuous space may to manipulate an object. So this is a problem that can be solved with a lot of continuous sampling and learning from experience, but it often takes some fairly sophisticated hardware and the ability to sample, you know, lots of lots of experience from my environment. Okay. So, next time so basically Thursday, just going to take off because I'm going to be traveling, and then we will reconvene next Tuesday. What we will do on Tuesday is going to be the reinforcement learning for two player games that is learning to play tic tac toe. That's going to be the subject of assignment six, and we see if I do have the schedule ups and schedules here. And then I will also assign assignment five, then. And so that's this is at present the day that assignment for is due so assignment four will be due. I will assign assignment five to have two weeks for assignment five, and then I'm going to assign assignment six before assignment five is due but you should have plenty of time for this. Also hope you're all working on your project proposals. That's going to be due next Thursday question in the chat. Very interesting. Okay. Thank you. Okay. So I guess there are no other questions or comments, then I have no problem with calling this a short day, and I will see when I get back home. Do I have all I will not have office hours today I have to go back to the conference. But if you have questions. Again, feel free to email me, and I will respond as quickly as I can. Thank you. Bye bye. Yeah. Okay everybody let's get started. Alright, welcome to animal and equine science 445 folding management. What you look like you're in the wrong room. Now, to CS445 introduction machine learning. 2023. My name is Nikhil Krishna Swami I am assistant professor. So today's syllabus day. So nothing. Nothing too intense basically I'll go through the syllabus and the class expectations. And I will then go through like the first couple of simple Jupyter notebooks to get us started wondering what the heck all this was about. I actually forgot the time and location of our class and so I went looking for them in the registrar, and then start I got a little distracted looking at all the other courses that are coded as 445 one of which is animal and equine science 445 folding management. So, if I seem very tired and unshaven and punchy it's because I am which we will get to the reasons for that minute. Okay, so what is this course why are we all here. So this course covers fundamental concepts and methods in data analysis classification regression prediction. Also data visualization. This is going to be a mostly neural network focused class. The assumption here is that folks have taken CS 345 or at least have a certain level of comfort in Python. So, if that doesn't describe you, you should talk. So, you will learn the principles of reading in data visualizing data examinings data using statistical analysis. I'll go through the syllabus and the course schedule in a moment to kind of familiarize you with the structure of the course. Basically we are going to be doing units on regression classification reinforcement learning, and then assorted other things and machine learning that we don't need to be fit in either of those. You will be doing a combination of experimentation and writing for your assignments. So again the assignment structure. We're also going to be using some of the latest features in Python, including Jupyter. Sorry Jupyter notebooks. So this entire class is based in Jupyter notebooks so I hope everybody knows how to use those. If you don't, Maybe you can set up a tutorial. I would like to take a moment to let our TA introduce herself if you want to say a few words. And that's going to be in 120 120 in the CS building. So I assume most of you are familiar with that if you're not, if you go into CS building that's one of those big labs just on the first floor. Okay. Just out of curiosity, I know this class typically gets a number of students from outside the CS department so how many can you raise your hand if you are a CS major. Okay, so most of you. How many of you are not CS majors? So, but a decent number of you. How many of you are grad students from not CS? So if you okay so this is tip, it's pretty typical. So we often get a number of people who are interested in applying computational methods to their field. What department are you in? I'm in the Department of Natural Engineering and some other people who raised your hands. Where are you from? Okay. And so what are you interested in working on? Okay yeah very very very data friendly and then you what do you work on? Okay, yeah, sounds like you sound similar to people I have had before in this class. So hopefully you've heard good things. I think it's going to be a fun class. I enjoy teaching and I hope you enjoy learning it. Okay, so about me. This is my third year at CSU. So I'm kind of midway into my, my tenure, tenure clock. So I got my PhD five years ago at Brandeis University that's in Boston. My primary research area is actually natural language processing. So in addition to 445 I also teach CS 542, which is the graduate natural language processing course. I do research in multimodalities that is a combination of language and vision. I also do a lot of work with intelligent interactive agents. So you know chat bots, but also more sophisticated things that actually learn to recognize gesture and interact with people in real time. I also do have done a lot of research recently into the geometric properties of machine learning representations. So if I start throwing the term embedding around a whole lot. That's basically what I'm referring to is the representation in the high dimensional space that is contained within a machine learning model. And we've applied some of this to low resource languages and natural language processing over languages that are not well represented that don't have a lot of data available that are maybe not not as widely spoken as some of the usual suspects like saying So I'm not doing CS. I spend a lot of time doing distance running I've run nine marathons most recently in Chicago I play music if you find me on Instagram you might find me banging out the occasional guitar solo. This is my wife. Here's my daughter here my three cats. My daughter is in daycare. What this means if any of you have children are familiar with young children, is that she's bringing back a fresh cocktail of diseases every week. So I recently just got over a stomach bug thankfully I recovered in time to teach in person. So, just fair warning that you know I will try my best not to get sick of course I'm definitely trying best not to get you sick. But there may be times when I'm not available. Worst case scenario I'll have to cancel class at some point, because I'm just like under the weather for whatever reason. Hope that's not the case. We also see the thing plus some on the way so just to note about that we are expecting our second kid at the end of May, yay. We are very excited. Although I'm also thinking about how tired I am going to be. I just want to note that the due date, my son's coming in late May, so this should not impact the schedule of this class. My daughter came a couple of weeks early, she came in June. So we seem to time our kids pretty well with the school schedule. But just in case you know if he comes early like it might land kind of during finals week but still wouldn't impact this class, but there is a chance that especially like as things go on you know I'm gonna have to take my wife to the doctor a whole lot so my availability may become less as the semester goes on. So just want to make you aware of that. And occasionally there may be instances where I have to cancel office hours because of you know I have to be able to go to an OB appointment or something like that. So just fair warning about that. Okay, so course principles. You're all adults. This is a college course. You are here to learn. I expect you to be able to demonstrate your learning, and I expect you to act like adults that is you know be respectful of, you know, show up, make an effort on the assignments. I'm just saying all this I've never had a problem with any of this before in any of my classes I don't expect there to be a problem here I'm just sort of stating it as as a de rigueur. So for the organization of the course they're going to be two primary hubs there's a canvas page that I hope you all have got access to. If you don't, for some reason, please let me know. The primary way to contact me is through canvas, because that allows me to keep all the course messages collected, and I don't miss things in my inbox. If you don't have access to the canvas for some reason please let me know immediately so I can fix that. So here's the course page, which I should show you briefly. So, here if you go to cs.colostate.edu slash Toyota CS 445. So take you to this semester's version of it on it you will see pretty much the same content as the slides. I just recommend that you check this regularly. So I will update this with my office hours and sire's. And then if you click on calendar, you will basically see the structure of the course. All of these, these accessible pages and working that when I had problems with this one works okay some reason I'll try and fix that but it's just a slide deck, but you click And then you get to the notebooks that we will use so you can run them yourself. You may expect to see minor updates in these. Those usually, you know, will be posted before class so but the, what you see here is pretty much current, the schedule might change. Sometimes we have snow days and stuff like that so in that event we may have to move things around a little bit. Last year we got through all of this content. There's a couple of things at the end that are not, you know, once we get through kind of dimensionality reduction. I do some stuff on you know non neural methods and also do a brief introduction transformers. These two can be sacrificed if necessary, although I would like not to do that because I particularly enjoy doing the intro to transformers. But in the event of say a snow day or something, we may have to move things around by a day or two. So just check the check the calendars you know it's coming up and so that you also know if any assignments are due. Or if the duty has changed. There's no textbook. There are some free resources that you can download here. So these books these resources are very good if you want to know more about the mathematics, you want to know other applications. The books are not required. So the contents of the class will be entirely self contained in the notebooks, but for further reading, you know you definitely can consult these. Okay, so course organization. Yes, on the course page, you just saw you'll see the calendar, you'll see the notebooks you'll see the assignments. Canvas is going to be for supplementary material and say if there's a reading I want you to do. Primarily it's going to be for discussions. So this is the place for mostly you to discuss amongst yourselves about the assignments. So, what does it mean to help each other with the assignments you may consult about the problem you may not share code. So that's basically the rule of thumb here. I will set it so that students can start Canvas discussion board so like if you do something in the assignment that you have a question about, start a discussion about it and anyone who knows the answer to that can can answer that. Just remember that like I can see everything you post there so don't go sharing code or do doing stupid things right this is not private. So there are six programming assignments. This is the majority of your grade 80% they're worth 100 points each. Then you also get a final project. This is worth 15% of your grade. And then the remaining 5% of class participation, class participation is kind of conditional upon the modality which you're taking this class I know we have some people who are online there's some people are also taking it asynchronously. So basically, class participation, if I know that you are engaged, you can assume you will get full participation points in a class like this what does that mean. Usually, if by the end of the semester if I can put a face to the name, except for those of you who are already. They usually means you're in a good place. Right. So, if this if in the class of like 8090 people if I learn your name at the end of the semester, you know, no offense it means you're, you're probably doing a good job of making yourself known if you aren't. Then, if I don't know your name, then it's probably so suggest that maybe you should be a little more engaged. So you know come to office hours ask questions speak up in class post on the discussion boards there are many different ways to get class participation. Okay, programming assignments, basically you're going to be given some data and some starter code. Everything will be done in Jupyter notebooks. So make sure you know how to use those. You can assume that we will be running probably Python 3.9. So, if you need if you need know which version of Python, I would assume 3.9 people not in this class and NLP submitted 310 assignments which caused a couple of issues, we had to debug. So, if you're a Python 3.9 I would just recommend you let us know that so that we can take that into consideration. If, say, something is not working. We will be using automated grading so when you're given the assignment you'll be given a greater script. This is the script that we run for the coding part of the assignment. So what you can do is then you can make sure that your code passes all the unit tests before you even submit. Right. And so as long as you run on an environment that is similar enough to ours and doesn't have any wild dependencies installed I mean like really wild out there we've tried this in multiple environments. Doesn't matter whether you run on Linux or Windows or Mac, you know, as long as, as long as you pass the auto greater it should work. So, your code needs to run given what is asked for in the submission so don't go installing some weird special package. Don't, for example, use the scikit learn implementation of linear regression to write the linear regression assignment that's not the point. If we see that you will be in major trouble. Also don't use any weird arcane packages that we may not have installed in our systems if we have to install that we will get unhappy and we will pass that unhappiness on to you. So you get partial credit for code and then partial credit for the experiments that you do in the explanation so generally, you have to fill out some functions to complete an algorithm, you have to run that algorithm on some data, and you have to discuss your results. Usually the split between code and discussion is roughly 5050. And so this ensures that people cannot say just skimp on the discussion and then pass the class with the 90% just writing the template code, right, you need to demonstrate your learning, and you do that by being able to discuss the experiments that you do and the results. So, leads me to the question of, are you learning versus are you chasing a grade. So, in this class ask yourself what's more important. Are you here to get an A, or are you here to learn something. Some people are here to fulfill a requirement. But that's not most of you. Okay, so I assume you were here to learn machine learning. So how are you going to demonstrate that. So, are you writing yourself ragged before a certain score are you staying up until three o'clock in the morning, getting no sleep and mainlining coffee into your veins to get 100%. Or are you doing it because you actually want to meet the challenge of the assignment. Right. And this should be because you're telling you the answer, because you want to meet the challenge of the assignment, if you are doing fine. And if you have concerns I will let you know if I think you're doing fine in the class you're worried about your grade, but I look at my gradebook. Yeah, you're getting like a 97 or something, I'll tell you that you're well positioned. So you have questions just ask to put in perspective, in my first course I got a C plus. I feel like things have worked out okay. Funnily enough, so I went to DePaul University in Chicago and I took AI programming, got a C plus. I recently learned that my professor there moved from DePaul to CU Boulder, and is now the department chair at the Department of Information Sciences, and he So I sent him an email like, hey, remember when I got a C plus in 2008 well now I'm a professor for machine learning at CSU. And he's like yeah great great to hear from you should come down and give a talk sometime. So, I guess my point here is don't hang up on the grade too much. Because in your future career be a graduate school or employment. They're not going to care the specific grade you got on machine learning they're going to know that you can apply the techniques. So like, if you get an A minus it's not the end of the road you can get a B plus, it's not the end of the world. Okay. What I just said. Okay, my grading style has been described as more than fair. If you speak to people who have had me as an instructor. In the same place that I am stingy I don't give A pluses. That is, the A plus is meaningless in college. For truly exceptional performance that is greater than 100% which is possible to do. I will give an A plus. But my rant is that A pluses are meaningless at the college level they do nothing for GPA is still four points, and they should be abolished because if I don't state this up front people start knocking down my door, ask me why they're 97.8 wasn't a plus. Here's why. Alright, the one commandment of this class. So I feel like we have to lay this out now. I shall not use chat GPT to help you with your writing in this class. Okay. How am I going to enforce this. Well, it's going to be difficult but I what I'm going to be doing them to be reading your writing very closely. Okay. So, GPT, as I've said before, writes like a high school or who's be essay 15 minutes before class. If you write like a high school or who's be essay 15 minutes before class you probably shouldn't be in college. Okay. And I assume that all of you can write better than this I expect you to demonstrate that writing does not have to be perfect does not have to be free of spelling or grammar errors has to be understandable has to help me, you know, walk through what you did. Don't use the AI agent to help write your code also, it wouldn't do very well at this I suspect because it doesn't have the context. It doesn't have the code that you wrote, it doesn't have the experiments that you ran. So it's not going to be able to discuss that. The exception to this. Some of you may be interested in doing a final project on chat GPT, and that's perfectly acceptable. That's actually great I would really be interested in seeing some projects like that. So one thing that you may want to do would be feeding example inputs into chat GPT and analyzing the output. Right. So, Jack GPT as the object of study is one thing versus trying to use chat GPT to actually do the assignment. So, that's clear. If you have any questions about what is or is not acceptable, please feel free to come ask extension policy on assignments. I am willing to entertain extension requests as long as they're justified need to make these more than 24 hours before the assignment deadline. And this is the assignment deadline for you. That is some people are granted accommodations that means their assignments are due at a different date from the one for the rest of the class so if you are one of those people, you can factor that that extension deadline. What you need to provide is your reason for requesting and a reasonable alternate date by which you can get the assignment done. Most people suggest usually you know a day maybe two, three, four is pushing it, but you know on with with justification I would consider that what I will then do is I will approve it, or I will deny it or I will present you with a counter offer say like you said Monday once you get in by Sunday instead. So, if you act together. You become much less easy to ignore, because if 20 of you say that you're having problems with the assignment. It probably generally means that I didn't allow enough time for the class at large to do the assignment. If one of you that says that my assumptions that you just started late but if you know, half the class says it, it's probably more problem with the assignment than the person. Okay. And the skill just in general, you know, collective action will serve you well in life. Okay, you may be asking what constitutes a reasonable excuse. So generally this lies at the intersection of things that are specific to you, and things that you have no control over. Okay. So, what that means, you know, plan travel. Right. This is specific to you, but you planned it before. Okay. You didn't allocate enough time to the assignment, obviously you know you could have started earlier. If you really want to go see the concert I haven't seen the concert in three years, I want to go see a concert. So, you know, I'm sorry. I know kidding. Hopefully we never face another situation like this but someone has did ask for an extension because Russia invaded Ukraine. You know, now specific to you, you have no control over it. If Russia invades Ukraine, but you're a first generation Ukrainian American and you have family in Ukraine right that's different. Okay, so you just need to let me know that and like that is something that would cause you know you specific distinct mental anguish that may impact your ability to do the assignment on the global financial crisis right, everybody is suffering. There may be things that have to be done with the class at large in such a situation let's say the, we rocket through the debt ceiling and the the stock market crashes through the ground and everything is just boiled beyond recognition. So, you know, we're in a wide pandemic, you know, I know some of you wearing masks I may on occasion wear a mask if I feel like it's warranted. So you know, we have collectively decided that this is not a reason to cancel class so you know we're going with it, but you know, your spouse asked us for a divorce you didn't see it coming which also no control over it right you can get an extension for that you know you have no you may you may know there's something you deal with but you don't have any control over it right so that is still specific to you so that's legitimate. You get hit by a bus right don't go do not actively go get hit by a bus to get an extension, but if it happens, you know, please let me know or have your friend let me know. And then you provide some documentation. If you get coded right and you're down, you know I've been there I've had coded, you're down for like three days and just really can't focus. Then I understand so just be communicative about this. Okay. Okay office hours. Currently, I have them set for 330 to 430 Tuesdays and Thursdays that is after class. On occasion I have moved them around sometimes it just turns out there's another time that works better I find I don't like that hour or something. But in the past having office hours in the same day as class is actually enabled me to keep one day mostly free for writing which has worked well. So I will post these on the website. Okay, so for office hours, please notify me that you intend to come. This prevents me from double booking. But also if no one shows up that sees me an hour to do something else which is great. Office hours are reserved for things to do with class. So if you come, if you come to office hours please make it about class, I'm sitting there, expecting people to come about class so I generally I don't allow you know my students to book during office hours, for example. So you want to see me outside of class, or you can't make office hours for some reason. So I have this booking tool so if you go to christmaswami.youcanbook.me basically this will go directly into my calendar and you can book a 20 minute slot. If you can't make office hours, or you want to talk to me about something else you know you want to talk about research or something that's not related to class. You can, you can set this. Also, but if you do book please do show up because then I make sure to be there and if someone doesn't show up having booked then I sat there wasting 20 minutes and I just get mad about it. So, and that will that will color my impression of you right as a flake. So don't do that. If you need to cancel let me know and I will remove my calendar. Alright assignment drop policy. Everybody gets one. So, there are six programming assignments, and you can drop one of them from your final grade without any justification you can just say, I will ask everybody, if you want to drop an assignment you can say I want to drop assignment five or something. So, this is a good point to point out when I'm doing grades the end of the semester, you're not required to drop any assignments some people do really well and want that reflected in their grade. But also it can be for any reason or no reason whatsoever. You can be unhappy with the 99% you got or you just couldn't get out of bed that day or that was a rough week or you know you just didn't get the assignment, but you must submit something for that assignment. So if you're missing anything in parentheses. Just make sure that box is full. Okay, it's empty. It's a zero. If there's something in there, I can drop it. Obviously there's no option to drop class participation or the final project. Okay, note on free rec so the assumption is that you've taken CS 345. This is now officially a prerequisite. So the last couple of years it's kind of in flux we had both of these machine learning courses that were run separately and in parallel. Now CS 345 is officially prerequisite. There are some people who I've granted to override for this course because you have, you know, demonstrated that you know Python. So the reason that I assume that people have taken 345 is basically I assume you know Python, I assume you know Python and NumPy. We're not going to spend time teaching Python. If you don't know Python, that is you didn't take 345 or 152 is the other one that does Python. You will find at the very least you will find this to be a steep learning curve. If you think you can learn Python effectively within the first week, then go ahead and stick it out and see if you know you can you can make it through the first assignment if you can then things will probably be okay. If that's not you, then I do suggest you take CS 445 next year instead after having acquired sufficient facility in Python. Okay, so attendance and policies. So, bring class attendance this means attending in class or if we're all remote for some reason on the zoom zoom is for the online students, or if you know you can request that you be allowed to attend by zoom for some extent you any circumstances So these lectures are for review for those people who have cleared it with me previously that they have some long standing conflict like a full time job. But in general, this is not intended to be a substitute for attending occasional absences are of course you know expected and allowed. I just asked for some notification. I grant exceptions on a case by case basis so I granted to you and only you so if I grant it to your friend or roommate that doesn't also apply to you. You have to request it separately. And you know because I will say yes that's fine. And I will keep all the emails so I have a record. Okay, COVID-19 this is the fourth semester they've taught where I've had this slide. COVID-19 is still with us even in the year 2023 university just sent out some updated guidance about COVID policy it was just basically like, please watch the announcements and we'll tell you if anything changes so it's just sort of shrugged and, you know, they'll let us know if if they're going to do anything. Basically, just watch out for updated guidance and we will follow the updated guidance in the university, you know, just take the necessary steps to protect yourself and others, you know as you see fit, and just be aware that things might change quickly. If you have coded right don't come to class. If I have coded class will be all remote, assuming that I'm on my feet, and such, and so on. All right, academic integrity policy, this really should be pretty straightforward. Don't play drives, like, not even once plagiarism is not a legal crime but in academia it is the highest crime you can commit. So the punishment for for plagiarism is basically automatic failure on the assignment, plus an additional deduction. That is the punishment for having found to plagiarize has to be simply worse than not just doing the assignment, it's not just a zero you basically get another X points deducted from your grade, which would be determined by myself. And then maximum penalty would be failure in the course or if you actually if you're a grad student possible dismissal from your program so don't do this. And I have not encountered many instances where people have played rise and I don't expect that to be the case I'm just throwing this up there as a warning so you know what would happen if you did. So that constitutes plagiarism being copied is equivalent to be to copy in this case, assuming that it is, it is intentional. So that is, if you intentionally copy off of someone you're found to do so that's plagiarism. If someone steals your homework and they, and you don't know about it, you're not liable but if you share it, you are. So hopefully that is clear so when you are consulting with each other, make sure that you're not showing each other code, but you can work through you know toy examples and the problem that I encourage you to work together to learn so I think that's that's a useful technique. Just make sure that you know where the line is and you don't don't get anywhere close to that line. So you know, if you're willing to sharing your solution you get the same penalties the person who copied off of us basically it. So don't use this as a strategy to win friends and influence people because I'm not going to be influenced as that, except for giving you a zero on the assignment and talking from your brain. All right. My least favorite slide. So, as your instructor I'm a mandatory reporter. What this means is that if I feel that there are this risk of threats of harm to yourself or others reports of discrimination or sexual harassment, I am required to report what this means is, you know, don't put this I want to die in your code comments, basically. So, you know, this is almost definitely a joke. Right, it's assignment for its March the weather is miserable, and the assignments not working. And like, I feel you here. But just because that's the case. I still have to file a whole report if you do this because in that you know 1% chance that you actually do want to die, there's a risk that you are going to harm yourself. If I don't report it, and then you come to harm. Then I could lose my job and the university can be in big trouble. Okay, so I'm just saying you know even ingest don't express your frustration this way there are other more productive outputs, and also don't threaten people right do not make earnest threats of harm, you know, do not behave inappropriately toward people any of these things I'm required to report again, not something that I've had a problem with before. Bottom line communication is key. It is always better to ask about something if you are unsure so if x is allowed, you know, ask me and I will tell you, do you want an extension you know give me reasons why and I will tell you if something is wrong right if you feel you know small things you feel like the class is going too fast or too slow or I speaking too fast sometimes I do that. Just let me know. But basically if you you know if you feel you're not being respected or something like that, you know, also let me know so if you feel something is not right. The first thing you should be doing is talking to me about it. Because if you say nothing I mean assume that nothing is wrong. Okay. And my promise to you is that I will listen to your concerns and take them seriously. Are there any questions about the class policies. Okay. Yes. So let's go to the, I'll, I'll nail it down later on let's just go back to the course schedule so you get a sense of kind of where things stand. So, this is, you know, as I said, roughly, roughly final. So you will notice that you get about you know a week week and a half sometimes up to two weeks for has to get closer to two weeks for each assignment project proposal is basically going to be rolled out mid March. So basically you do the proposal, the project comes in three parts, basically there's a proposal that you have just a little bit of time to basically write a short, very brief description of what you want to do, and I'll outline you know what the parameters are basically you can do a research report or a coding project. So if you propose which one to do, I might send it back because they have some questions, or I may approve it. Once it's approved and you basically have the remainder of the term to work on that project you may work in groups of, I think, up to three or four I will check. And then, in the last two days of class. Before the final final deliverable is due, we're going to have lightning presentations that is every group will come give usually comes out to about a minute and a half to two minute talk about your project. And these are very simple sort of, here's my problem. Here's what I've done so far and years, or here's what I've learned. But I'll, if you have questions about the project, let me know but we're going to nail it, the specifics down and you can, I believe, go to this and basically see what the requirements are ahead of time if you want to start thinking about what you want to do. So you're free to start brainstorming the kind of project you want to do ahead of time, we will be using pytorch in class but like your free use tensor flow and other libraries in your, your project proposal. And then the project requirements are like, a lot less restrictive than the assignment requirements you get a lot more freedom to kind of maximize your own success. Any other questions. Yeah. The project on I guess this is an addendum to research so the, the, the product, the project. If you have like an existing line of research, you can make the project kind of adjacent to that. It just has to be relevant to the class, but because the classes and machine learning. Right. There's a lot of ways that you can make whatever research you're doing relevant to machine learning you apply some machine learning technique to your area specialty So this should not be hard to do. My goal is not to add more work to your place I know a lot of you are very busy so if you have some existing research project that you want this to, to form a part of, you know, I'm perfectly happy to consider that. Other questions. Okay. So I suspect we will probably be able to end today, somewhat early. What I will do is we connect to the GPU machine. So GPU is not required for this class, although it does help. My password. There we go. But we will be running Jupyter notebooks on the GPU, and it will help for some assignments to just make it will make it go faster. But if you don't have access to GPU don't sweat it. Also remember that because you're in this class, even if you're not a CS student you should have access to remote access to department CS department machines, and we have a bunch of them have GPUs on them. So, Sarah, maybe you can tell people how to access them. We also have last year's TA also did a tutorial about running Jupyter notebooks remotely on the department machines that I will post. So, just so you're aware of all the the full spectrum of resources available to you. Alright, so the course of review. The beginning of this is basically the slide. Oh, I guess a late policy I didn't mention. So in the extension policy, the flip side of that is the late policy you get a 10 point deduction for every 24 hours that it's late grace period is roughly you know if you turn in really early in the morning. So I'm going to let that slide I tend to get up very early so if I see your assignment when I get up, I will assume it was submitted the night before so it's like, you'll get in by midnight be getting by 1240. I wouldn't sweat that too much. I'll probably let it slide. After that every 24 hours, it's 10 points. So not like 10% 10% 10% basically you know minus 10 minus 10 minus 10 off the percentage okay. So, if you get an extension, and then you're late past that extension that's in the late policy starts to apply. Yeah, this sounds confusing. Don't worry, I keep track of everything it's all done automatically in canvas. And if you have questions I will be happy to answer them. Basically, just talk to us about that. Don't ask your extension the night before make sure that you do it at least 24 hours. Yeah, use the canvas inbox for mail, that allows me to keep it separate and have it not get lost. So, examples of the kinds of problems that we're going to study. We're going to be doing you know, basically the usual suspects and machine learning that you may have been exposed to before digit classification, I will spend a lot of time in the MNIST data set if you know what that is, you know predicting miles per gallon from other factors about cars. Playing tic tac toe using reinforcement learning. And then, not sure if this gene expression one is still in the, the unsupervised learning notebook but I will check. But this may be a topic of project for some of you. Okay, first thing that you need to do, I recommend that you install the Anaconda distribution. We have a Python or Jupiter distribution installed already. And then the sci pi lecture notes is a good initial reading to kind of get started to try some examples to make sure that you know how how sci pi works. Basic, you know, Python numpy functionality and how it all runs in a Jupyter notebook. So, first thing we're going to do is just go through the execution of our first Jupyter notebook. So, first we can see I'm just going to do my standard imports import sys, this allows me to print the Python version. And so now we'll see that we're running on version 3.9.7. Every Python function file begins with a standard set of imports that you're going to be using, you know, and usually the first thing you're going to see is import numpy and p import matplotlib.pyplot as plt. So numpy is numerical Python this basically allows us to create vectors and easily manipulate vectors, and then pyplot is just our standard plotting library. And then import torch is going to import py torch which is the machine learning library that we're going to be spending most of the time. Yes. No, you can use that. Yeah. So you'll see we'll go through. We'll go through the notebooks and you'll see like some of the standard things that we use, and you'll get starter code that usually that will include all these these imports. And you can use, you can add other imports as long as they're like standard Python libraries or just like something that every user can expect to have installed in their machine. So you know, use pyplot not seaborn for example, I love seaborn it looks great. But don't assume that it is going to have it. Okay. So first thing we are doing here we're just going to check and see if this machine has a GPU. So you can do torch.cuda.isAvailable, and it will print either true or false. So here, this is my Mac doesn't have an NVIDIA GPU on it but the my lab machine does so I'm running this remotely on my lab machine. And so when I run torch.cuda is available, it prints out true. So what we'll do then is we will be able to compare the execution of some operations using Python on the GPU versus Python just on the CPU. So here what I'll do is I'll just initialize two uniform distributions 5000 by 5000. And so then if I initialize these, and then I'll just perform the module operator over these so these are created using np.random.uniform which means they're going to be created as a NumPy array. So, an N by N NumPy array since N is 5000 should create two 5000 by 5000 NumPy arrays that are filled with randomly sampled decimal numbers. Okay. So if I run this and takes a little bit. So, size of a is 5000 by 5000 so 25 million, size of B is also 25 million. And then if I just print C equals a module of B and then print that you'll see this is the operate the output of the module operator performed element wise over these two very large arrays. Okay, so the size of a mod B is going to also be 25 million. So, just to point of fact about tensors versus N dimensional arrays. How many of you have heard the term tensor before? Many of you. How many of you have not heard the term tensor before? A few of you, and that's perfectly okay. So, the specifics we're not going to get into exactly the differences between tensors and dimensional arrays in this class but both tensors and ND arrays are multi dimensional arrays. The two are not exactly the same. Okay. In that tensors are more generalized. A tensor and an array of the same dimensionality that is the rank will behave somewhat differently. One way to think about this is to kind of use this analogy. A linear transformation. So basically if I take some data and I stretch it or rotated or I move it, can be performed using a matrix. We'll see this in the next couple of notebooks. And then a tensor is also an operation in this sense. But a multi dimensional array is a data structure that's suitable for representing this operation. So, the NumPy, you know, NumPy uses N dimensional arrays and all packages use tensors and there are neat tools to just transfer back and forth between these if all you need are the numbers. So, for purposes of execution and operation in this class, you don't need to worry about this distinction. All you need to know is that in order to use things in PyTorch, things need to be of the tensor data structure. Whereas if you're just writing your neural network in NumPy, just using an N dimensional array will do. These can be transferred between each other, but you can't use a N dimensional array in an operation that's expecting a tensor. It's going to throw an error. Fortunately, it will throw an error that tells you that it's not a tensor and it needs to be a tensor. So, what we're going to do here now is that I'm going to take A and B, which were N dimensional arrays that I created earlier, and I'm just going to use the torch.fromNumPy command to turn them into the tensor versions of those same numbers. So A, T and B, T will now be the tensor versions of those randomly initialized matrices that we had before. Now I'm going to use this .to command, and you can pass values into this that are either CUDA or CPU. CUDA is the GPU acceleration library by Nvidia, and this is what allows ML computations on GPUs to proceed faster. So, one trick here is that in order to use these on the CUDA device, everything has to be actually be kind of relocated in memory onto that device. So, by running the .to command, I now have versions of A, T and B, T that are actually on the CUDA device. And so now, if I run A mod or A mod B over the tensor version, I can get the output. And you may have observed, if you're paying attention, that this happened almost instantaneously, and the element-wise version over the NumPy arrays took about a second, right? It took a little bit. And so, you can see that CUDA is allowing us to perform this operation quite a bit faster. How much faster, you may ask? Well, let's find out. So if we import time, this will allow us to keep track of the time. So, here's now, this is now just a numerical representation of the current time in continuous float form. And so now what I'll do is I'll see, I'll start the time and then I'll compute the time to complete the execution for both of these operations. And so you can see that NumPy took about 0.45 seconds and PyTorch on the GPU took less than one-tenth of that. And in fact, if you run it repeatedly, you may find that the difference gets even more true. So if I run this more, 0.44 versus 0.0003. So CUDA is a whole lot faster. So just a demonstration of the utility of GPUs. How did we end up here with GPUs? I think we will get to that in the next notebook that I'll do really real quickly. Let me see if I, I do not actually. I'll go through that at the end. So now let's try to visualize this. So we just saw how fast the comparison between NumPy and PyTorch and GPU is. Let's actually see what kind of speed up we get using different data sizes and where we actually start to get a real value add from CUDA. So same imports. This time going to import statistics. Same, just, you know, good practice to run this whenever you are using a new machine. Or even the same machine, just make sure the GPU hasn't something gone offline. So now we'll try a bunch of different trials. So you will see if you compare this code to the previous version, it's basically the same, right? I'm creating two n dimensional arrays, and then I'm moving them to tensors and moving them to the GPU. The only difference here is that now instead of doing an element wise mod operation, I'm actually going to use this at sign, which is matrix multiplication. We'll get into the specifics of matrix multiplication. How many of you are already familiar with matrix multiplication? I hope many of you. OK, good. So I don't need to go into that in any real depth. So basically, what, what is matrix multiplication? What do we do? Here's someone whispering. Yes. So if I take two 5000 by 5000 arrays and multiply them together, what is the shape of the output? Right. And why is that? So if I have, say, a three by two array multiplied by a two by four array, three by four, right? So basically take basically taking those the two inner elements must multiply together and the result is in the two outer elements. If I have a three by two array and try to multiply it by a five by two array, what happens? It won't work. We'll get an error. We'll see that. And that you'll see that in practice. And that's like almost the most common error that people have on their assignments is like the shapes of your arrays are not correct. So what I'll do here is I will try, you know, from sizes of 2000 to 10,000, I'm going to create arrays of these different sizes and then try to perform matrix multiplication over them. And then I will plot the time it takes in NumPy and the time it takes in PyTorch. And then we'll actually be able to visualize the difference. So this will run, you know, it will take a while because mostly because NumPy is kind of slow. So now we are at n equals 7,000 and equals 8,000. And you can already see, you know, CUDA is 10 times faster. CUDA is nine times faster. CUDA is eight times faster. And you can see the comparisons here. But this is not necessarily the most intuitive way to view this. We can get some average statistics. On average NumPy took .44 seconds. An average PyTorch is very slightly faster. But then PyTorch on the GPU is significantly faster. So now we actually plot these. Let's get the results for each type of operation. Just convert them to arrays to make sure they're plottable. I now have eight samples for each and then three types of computation that I tried. So now let's plot them. And here's the result. And so you can see the orange line is plain torch. The blue line is NumPy and the green line is PyTorch or is CUDA. I'm going to zoom out a little bit. And you can see what a difference that makes. Right. So big advantages to using CUDA. So you will note that the value add doesn't really start to appear until the data size increases. Right. There's a relatively negligible difference between the two when we're just using like 2000 samples. By the time I get 3000, 5000 up to 10,000. Wow. Right. So now you hopefully can just understand intuitively why GPU computations are a benefit. And so now we can just look at the seconds to multiply two matrices. And here is the result. So how do we get to this point with GPUs? Does anybody know the history of machine learning and why neural networks have suddenly taken off? Well, not suddenly. Over the last decade have taken off. So when did the first neural network come into existence? Yeah. Anybody, any other guess? Not a bad guess. Any other thoughts? Sorry, was that? Yeah. 60s. Okay. Any other any guess? Yeah. So the first neuron, I forget the exact date, the McCulloch-Pitts neuron. Now I'm forgetting. I don't think it was 1947. Might have been 1957. So, but then that neuron is basically just a single unit that takes an input, performs transformation, and gets an output. Neural networks have been around for quite a while. I think actually the first like networks, probably the 60s, I think. I forget the exact date. But they didn't do much for a very long time. In the 70s, 80s, there were some demonstrations at MIT of kind of doing some like, you know, digit recognition type things. Jan LeCun, who you may know of as the godfather of deep learning, was actually involved in that back then. And so it was kind of a cool party trick. Like, wow, I can get a computer to recognize a digit and I don't have to like hard code the shape of the digit into it. But it took, you know, a mainframe about the size of this room. And it could only do that. It was a one trick pony. And it, you know, really wasn't really impressive if you poked at it beyond the simple things that it could do. The real reason was that it really was due to a lack of data. But now we look at, say, within the last decade, a couple of interesting things have happened. One, with the internet, there's all sorts of data available. It's really easy to access. It's easy to crowdsource. So the amount of data available to create models has grown. The other thing that has happened is that GPU chips have become available. And so for this, we can actually thank the gaming industry, because GPUs were originally developed to accelerate rendering of better graphics on your gaming consoles and things like that. What do these two things have in common? Do you have any, did anybody know anything about the relation between graphics and machine learning? Yeah. Vector math, exactly. So how do I, if I have a vector that is pointing, say, a simple vector, one zero zero pointing off to the right, I want to transform it into a vector that points, you know, zero zero one to the front. Well, a simple way I can do that, you can actually create a matrix that represents that transformation. You can create a matrix that represents an arbitrary transformation between any two vectors. And so this happens three dimensions, and that's the use of GPUs in performing basically chips that are optimized for performing vector math, made graphics rendering much more, much more easy and basically creates a, you know, creates the visual quality of modern games. But if you can do it in three dimensions, you can do it in 500 dimensions. And so it just takes a little bit of extra compute. And so, no, we're not training chat GPT as fast as we're rendering a game, we're not doing it 60 frames a second. But just, I can take the same technology that performs a transformation over a three dimensional vector and perform it over a 1768 dimensional vector. It's just going to go slower, but no one's expecting to be able to train chat GPT at 60 frames a second. That's not the point. So GPU is kind of arrived right at this moment where we had enough data to feed into these things, and the acceleration to make training them not take eons. And so now the back propagation operations will go into when we talk about neural networks. Suddenly these very small updates can be done really really fast and at scale. And so, performing these really small updates over these coefficients that need to be optimized neural network can now be done in a reasonable time with reasonable resources and given definition of reasonable and boom, we have all of modern machine learning. So that's machine learning history in a nutshell, I guess. And I think, are there any questions about the intro material? Okay, so my last question for today. So when I was in high school, we had to buy those TI graphing calculators and like my parents were put out because they had to spend 100 bucks on it and it's like, why would I in school buy this for you. And then we saw how much teachers were paid and really go that's why. So, but you do you guys have to do that when you were in high school, you buy the graphing calculators. Do you remember in like algebra class, having to do linear regression with just like the Lin reg function, where your teacher was like okay you set up your data this way, and then you use the Lin reg function and does the rest for you. So Lin reg obviously stands for linear regression. And that's the topic of the lecture that we're going to start on Thursday, and the topic of the first unit, and you will learn how to do the Lin reg function on your own. So, you know, look forward to that. And I see no need to keep you any longer if there are no questions so I hope. Hopefully we'll be back in person on Thursday we'll see what happens with the snow. But if for some reason remote then we'll meet remotely otherwise I will see you on Thursday. They're actually a class and kind of, oh yeah, I think we had some of those like in a bucket somewhere. I think that's a good way to make forward that. Yeah, we might be able to get a little bit more of a sense of what we're doing. We need to get a little bit more of a sense of what we're doing. I think that's a good way to do that. And that's a very important thing to do. I think that's a very important thing to do. I thought you were going to say something. I think that's a good way to do that. Thank you. Okay, let's go ahead and get started. Sorry, we just found out that we have a paper to submit tomorrow and our co author is flying home from India and I thought he was the one who's going to submit. So, slight emergency. Okay. Just making coordinating with my student. Okay, so let's go ahead and get started. Let me share my screen. I want to, one point that I think I wanted to make real quick. Where is it. So, right, so I'm not going to, you know, remember the part about don't put you know Christ for help in your code comments. But I don't want to, I don't want this to be taken as like, don't ask for help. I'm just saying don't, you know, put something that could be taking this way in the code comments. If something is wrong, the best thing to do is to come tell me, because there are there are resources that you can be pointed towards. There are also options to manage the coursework to if you're having you know, issues with like course load or some things you know just don't, you know, put a don't put a cry for help in your own comments to be found you know when it's when it's too late. So just wanted to make that clear. But today's topic is me. Windows shade this here. So, which one Oh God I have so many things. Of course overview. Information theory. Okay, so today I'm going to do for you the first 30 minutes or so. Just talking about information theory and evaluation so basically how especially when we come to classification, you will be performing evaluation of your models and why we do it this way, because there are some very solid mathematical principles for performing evaluation in the ways that we do. And then following that I will begin the first real technical lecture on linear regression, which we will almost definitely not finish today because it's very long and so we'll finish that up on Tuesday. So, if there are no questions, I will go ahead and start. Yes. Yes, so they will be posted on the public facing page. So this one I think still says 2021 running from the contents of the slides are the same as long as throughout the updated version after class. Yeah, so just just as a reminder all the information should be available on the public facing page. Yes. First notebook. Oh yeah so I'll fix that. So the issue with that is that if you look at the link it says like envy or dot iPython dot org change the ipython to dot Jupiter, j y t e r it'll work. So that's just because I Python Jupyter notebooks, we call it ipython notebooks, and because it's courses existed for a while I think the URL is just inherited from a previous version so if that first one's not working, change ipython and the URL to Jupiter and it will work this afternoon I will fix the link so you don't have to do that. Okay. Anything else. All right, great. So, what is information theory and why is it important. So first let's just do an introduction to what it is. So information theory is the study of quantifying and communicating information, so information, we're kind of colloquially raised to think of that it's an amorphous quantity or quality so really, that is not something that is easily quantified or measured. So you know, you don't think of information as having, you know, mass or dimensional properties, when in fact it actually has all of those things. And we'll get into why that is now. So, as a field, information theory was first established in the 1920s by the two gentlemen on the left so Harry Nyquist and Ralph Hartley. And then it was codified in the 1940s really became a field, due to the person on the right, named Claude Shannon so Claude Shannon is kind of referred to as the quote father of information theory. And so, generally as a field this lies as the as the intersection of probability theory statistics and computer science, and really is just about what is one of the properties information that allow it to be measured numerically, and what does that entail. What does that allow us to do. So, the answer in information theory is the concept of entropy. And you've probably heard of entropy in, say, physics class. And so basically, the greater the uncertainty involved in the system, the greater the entropy. Right, so we clean up my well my wife cleans up my daughter's playpen in the evenings. And then my daughter goes in there when she's not a daycare and throws everything around the system trends toward disorder. And so the entropy if I take the playpen to be a closed system at the beginning of the day it has lower entropy the end of the day it is higher entropy one day, one way to think about this is My daughter has a whole bunch of toys that when they're put away properly or a little basket. Right. And so at the beginning of the day, they're in the basket. At the end of the day they're everywhere. And so, if I want to know where a particular toy is when the system has less entropy that is the toys are cleaned up, I least know that's more than likely to be in the basket. And I'm usually correct when the system has greater entropy when the toys are strewn everywhere if I want to find a particular toy, I don't know where it is. So there's greater uncertainty about a particular property of the system. And so it has greater entropy. So just in general terms that's a specific example. How much uncertainty is involved in the value of a random variable, or the outcome of a random process so the outcome of random the value of the random variables like position of the toy in within the playpen outcome of the random process is what's the way much which my daughter is throwing her toys everywhere. What's the process by which that happened. And so she doesn't throw her toys everywhere in a different place every day. In the same place every day is actually in a different place. So what's the uncertainty involved in that outcome. One another way to think of this is you have a fair coin flip. There are two possible outcomes. Whereas if you have a fair die there are six possible outcomes. So they're both fair, you know they're not they're not unbalanced in any way. But the coin flip has lower entropy because there are fewer possible outcomes. Okay. So, why is this important. So we begin with this quote by by Shannon, the fundamental problem of communication is that of reproducing at one point, exactly or approximately a message selected at another point. And he was thinking about this kind of in terms of telephone lines so remember this is the 1940s telephone technology was present but wasn't very sophisticated. And so you had basically static lines, or what he called the noisy channel. So that is, there's a person on one end who wants to say something, and there's a person on the other end is receiving the message, and there's some level of interference in the middle. And so, person B who's the recipient has to decode that message and they might hear the wrong word or something. Right, they might hear, you know, the, the, I don't know. And so they might hear a word that's like common, you know, hop, when the person meant to say cup. Right. And they might be able to disambiguate that when they hear the following words being of water. And they're like, well, a cup of water makes a lot more sense than a cup of water and so therefore I'm going to interpret this as being cup. Okay. And so this has a lot of implications for a number of different, a number of different concepts like the entropy and redundancy mutual information, the channel capacity of the Gaussian channel and the notion of the bit. The two in blue are the two that we're going to focus on in this lecture and for this class. So, if you've heard of entropy in physics class. There is actually a close correspondence between physical entropy and information entropy so in particular we talked about Boltzmann entropy gives entropy and Shannon entropy. Information entropy is Shannon entropy. It's given by the formula here through the denoted H. And so it's basically going to be the sum for all possible states for the times of the probability of that state times the base two log of the probability of that state. So, H is going to be the bits per symbol of an information source. So let's say, an alphabet of symbols, or a vocabulary and piece of eyes the probability of occurrence of the ice possible symbol so in our fair die example, the probability of every possible outcome is one in six. In an evenly distributed set of six classes, the probability should roughly approach, one out of six. Okay. So, if you have an unbalanced set then whichever set is whichever classes overrepresented is going to have a greater product base rate probability of occurring. So now, the other two types of entry boltzmann entropy is written with the following formula s equals case of the times natural log of w, or s is the entropy of an ideal gas so little bit Bolton was studying gases and the composition thereof case of B is the constant Boltzmann constant and w is for showing the kite which is German for probability. In this case the probability is the probability of a specific molecular configuration of a gas so I've got a vessel full of air. And I'm like, what are the, what's the probability of the molecules being arranged in a particular way. Well, you can't actually count the individual number of possible arrangements of molecules, but you can come up with a probability distribution that describes that right, what are the probability that they're all clustered over on the right side vanishingly small, what is the probability that they're relatively evenly distributed throughout the vessel, probably somewhere in the fat part of the bell curve. Gibbs entropy is a generalization of boltzmann entropy, and it uses a similar formula. So, negative case of the times the sum for all I have the probability of the state times the natural log of piece of I. So, the probability of the system case of B is the boltzmann constant and piece of I is the probability that micro state I occurs as a system fluctuates by gases in a vessel. The molecules are moving around and I micro state I is just like some, you know, choose some configuration was the probability that the gap the molecules are arranged exactly that way. So let's take a look at these formulas. So, let's take a look at the Gibbs entropy formula, Gibbs entropy formula and Schenn entropy formula. Do you see similarities? What do you see? Sorry, you mumbled. Probability, right? What else do you see? Logarithms. What else do you see? Constants, right? So let's take a look at how these things. What happened? I used to have like some. So, the probability is be a piece of I, what's W stand for? The German word for probability, right? So this could just as well be written P. We have natural logs, we also base two logs but it's pretty trivial to convert between bases and logarithms. That's not a big issue. And then we have a constant, right? In some cases, the constant has a negative but that's just a constant times negative one. So if it's a constant, I can factor it out. And then in this case, we have a sum and then in Boltzmann entropy, basically I'm taking the sum for all possible states, right? So if I sum this and W is just considered to be probability, then I'm taking all possible probabilities of microstates and summing them. So, effectively, these formulas are equivalent. So, how does this apply to machine learning? What are the information theoretic concepts that we will be learning in this class and why do you need to know them? If we take entropy that is H, so H sub X would be the entropy of a discrete random variable X. So that is, if I know the distribution of X, what is the measure of uncertainty in the value? So if I take this curve, which is going to be the entropy of a Bernoulli trial, which is binomial as a function of success. So if I have two possible outcomes, X equals zero and X equals one, then the entropy is maximized when the two outcomes are equally probable. So what is an example of a system that has two possible outcomes? A coin flip, right? And so let's assume that success is one possible outcome. Success is just arbitrarily defined as being landing heads up. So if the coin is heads up on the ground, then the probability of heads is what? 100 percent, right? If this is the event that I'm observing, the probability of that event must be 100 percent. So if I'm observing my penny being heads up, then the probability of tails is zero. If the coin is in mid flip, right? What's the probability of success? In this case, heads being up. 50-50, assuming that it is a fair coin. So the probability is maximized when the two outcomes are equal probable in the case of the coin that's in mid flip. So if I don't know anything about the outcome of the system, if my possible outcomes are evenly distributed, then every individual outcome is going to be equally probable and I will basically fall in this part of the n dimensional curve, where n equals the number of classes. Another concept is expected value. So this is going to be written with the bar E. This is just a generalization of the weighted average. So this can be the arithmetic mean of a large number of independent realizations of X. So if I have a fair die, possible outcomes one through six, and I throw it a bunch of times, right, the expected value of that is what? So if I have a fair die, right, and I throw it a bunch of times, so if it's a fair die, the weighted average is every possible outcome is one out of 16.66% likely. So let's say I throw it 100 times, and I get the average of all those possible outcomes, the value of that number should approach what? 3.5, right? So that's a little counterintuitive, right? The expected value of a fair die is 3.5, but we can objectively say that a fair die will never show 3.5, right, because it can't. It's not one of the possible outcomes. However, this is the expected value of all the possible outcomes. And so that's just because the formula is the generalization of the weighted average. So now I sub X, or sorry, I of X is called the self-information or, quote, surprise of X. And that is going to be the negative base two log of the probability of some possible outcome. So if you think of, say, Scrabble letters, right, every letter is assigned different point value. Q is 10 points, E is one point, and X and Z are also 10 points, and rare letters are more points. What this is, is those point values are effectively a realization, numerical realization of the surprise of each of those of each of those outcomes. So there are very few Qs in the bag of Scrabble letters. And so you're surprised and maybe a little bit excited when you take out a Q because you get a lot of points. So another way of thinking about this is that E is the most common letter in the English language. It's unsurprising and it contributes pretty pretty little information. So if I think about where E can fall in a bunch of words, right, if we count the number of E's in the header for the slide, Theoretic Concepts in Machine Learning, one, two, three, four, five, right, there's five occurrences of E. And it doesn't tell you a whole lot about what comes next to it. So if I think about where where do the E's fall? They're next to H, O, C, P, N, and then L and A. Whereas Q, for example, usually falls before U in English. And so if I'm looking at words like quit or quake or common English words or sequence, if I see a Q, there's a very high probability that there's going to be a U. Now, it's not always true, right. For example, you can have loanwords from like Arabic that have Q in them. It's not followed by a U, but those are comparatively rare. OK, and so that's why, among other things, Q is worth 10 points in Scrabble, because not only is it rare, also in order to make real English words, you usually have to have a U. OK. So now the entropy of X will equal the expected value of the self-information of X for X in the set of all symbols produced by the information source. So now the entropy of X can be given by this formula, which is going to be equivalent to the expected value of the formula on the previous slide. And so this can also be written as the negative sum for all X of the probability of every possible outcome times the log 2, the base 2 log of the probability of that outcome. So this is a powerful formula because you can combine it with when you have multiple variables. OK, so the joint entropy of two variables, X and Y, is going to be equal to the same formula, except I'm just going to plug in X and Y at the same time. So this is pretty straightforward in that if I want to take the joint entropy of two variables X and Y, I can then take the negative of the sum for all X and all Y of the joint probability of both of those times the base 2 log of the joint probability. So in other words, if X comma Y is the position of a chess piece on a board, then H of X, Y of the row and the column is the entropy of the position of the piece. Similarly, I can do almost the same thing with conditional entropy. The only difference is that I have to instead I can't just do the expected value of X bar Y, because that doesn't really make sense. I still have to use X comma Y. But effectively, it's going to be very similar to the formula on the previous slide, except now I'm going to take the conditional probability of X given Y. So the bar here means given, that is, if I know something about the thing on the right side of the bar, how much do I know about the thing on the left side of the bar? So in other words, if X is today's weather and Y is the season, then the entropy of X given Y is how well season Y predicts whether X. Right. So it is winter. And so there is probably a high probability that it's going to be snowy or Court Collins gets 237 sunny days per year actually when I interviewed for this job they said it was 300 and I looked it up and that was a lie. But 237 is still pretty good. But these those days are not evenly distributed, right. We're more likely to have sunny days in the summer. Yes. But this here. So if I take the if I sum for all values of X and all values of Y, then I take the joint probability of so let's say my I have four values of Y. So I have a spring summer fall and winter. And then I have, you know, let's just say sunny or cloudy for X. Right. So then if I take the joint probability that it is a cloudy day and it is summer, or it is a cloudy day and it is winter or it's a sunny day and it is fall all possible combinations. So those two there's going to be some joint probability associated with both of those things occurring at the same time. But that's not going to be the same as the probability of the weather conditioned on the season, because it's more likely to be cloudy and winter than it is in the summer. Okay. And so I'm going to compute the for all possible combinations, the joint probability of that combination of values. And then I'm going to compute the conditional probability for the same combination of values which is not necessarily I take the base two log of that and then they multiply them together. I compute all of this as a product. And then for all possible values of X and Y, I take the sum of that and then I take the multiply that by negative one. Now give me the conditional entropy. Other questions. So now the cross entropy, which is the key term here. So we're talking about cross entropy loss. For now cross entropy is basically if I take two distributions. So let's say I have a distribution P and a distribution Q and then X is a single event. But the value, the probability of that event is going to be different depending on which distribution I sample from. So then the negative, the negative of the sum for all X of the probability of X according to P times the base two log of the probability of X according to Q. That is called the cross entropy. So what this means intuitively is that this will be the average number of bits needed to identify if an event, identify an event, if the label set is optimized for Q rather than the true distribution P. So let's think about, let's say I'm trying to classify the season given a bunch of factors about the weather or something. And I have the true distribution of outcomes. So I say I take hours of sun. That's like almost a perfect predictor. Let's say I take like inches of precipitation and cloud cover and wind speed or something like that. And those are my three inputs. And I use that to try to optimize a model that will predict the season based on those three inputs. There's a true distribution that is like, this is the weather in Fort Collins for the year 2022 and that's going to be my true distribution. And then I try to optimize a model to predict that. It will become arbitrarily close to the true distribution, depending on how well trained my model is. We'll get into the training parameters later in the class. But it's probably not going to be exactly right. And so how off is it? Is it way off? Does it predict that the weather is winter a few times out of say 100 or 65 times out of 100? So that optimized distribution is Q, the true distribution is P. And so this is going to be how much more information do I need to identify that event if I'm sampling from Q rather than P. And so this is important in cross entropy loss because those labels that are incorrect, those they're drawn from say a poorly optimized Q will signal some difference in Q from the truth P. And the amount of that difference will tell you how much further you have to optimize your model to get close to the truth. Right. So far, everybody. Yes. So when you use that to like compare the different distribution. Yeah, you certainly can. Yeah, so, you know, one way, for example, in the process of optimizing a model and I'll go into model optimization. I'll go into model optimization like in the latter half. But let's say I have a model. I'm just trying to find a function that maps from an input to some output. And there's going to be it's going to be parameterized by some weights. My job is to solve for those weights and that's the optimization processes. If I'm let's say halfway through optimizing and I haven't quite optimized to convergence yet. My model may be kind of correct but not very correct. And so this is a measure of like how wrong am I, given the current state of the model, and this will tell me you know do I need to kind of shift my distribution further in one direction or another, and that's just going to be in multiple dimensions so but for now you can just think of it as like, do I move it left or move it right forward back, etc. Any other questions. Yeah, you first. What is what is a label. Okay, so in this case, you can think of it as basically being a distribution of clusters. So if I have let's take the example of the weather again. If I have three inputs of wind speed cloud cover and precipitation, and I plot them in three dimensions let's assume for a moment that these are good predictors of the season. And that means that you should have a bunch of points that are like winter weather, they're clustered close together, and a bunch of points that are summer weather that are cluster close together in spring weather and then autumn weather. And if these are good predictors, then those clusters will be closely defined. And so, in the optimization of a model the label is the output that I'm trying to use to map the inputs to. Yes. So, whether whether that's the output of this. So basically the cross the cross entropy of q amp a p amp q remember these are distributions. Yeah, yeah so the end of being a number. Right, and we'll see you know how we calculate the number and how we use it. So when I when I say how wrong am I, I mean I'm saying that you can, you can actually assign a number to that value which sounds counterintuitive, but it's always going to be relative to the information that the how you're representing the information that you're dealing with. Okay. So it's not like, you know, it's not always like between zero and one you can normalize it between zero and one, but sometimes how wrong am I is like 3.5. So that doesn't in and of itself mean anything, but it does relative to the data that I'm using to train my Alright, mutual information. So we talked about self information. That was I have x mutual information. I'm not entirely sure why the use the semicolon, but basically we have I semi colon y. And so this is, you know, how much, what do I know about why if I know x. Now, they can be calculated using the following formula. So again, for all values of x and y, I take the probability the joint probability, and then I divide that by the, by the base two log or multiply by the base two log of the joint probability divided by the marginal probabilities multiplied together. So, if I have two events that are conditionally independent. Who knows what that means if I say two events are conditionally independent What do I mean by that. Yes. They have no relation to each other so effectively it's saying if I don't if I have x, and I have why I don't know anything about x if I know why. Right. Now, another way to represent this is if I just take the overall marginal probability of probability of x p of x, and the overall marginal probability of y. Then, if I multiply them together if they are truly conditionally independent, the joint probability will be exactly the same, because there's no difference. There's there's no difference in the probability of these events occurring together, then there is of them occurring together separate. So make sense. Yes. If they if they were truly core if you had vector representations of these things yeah then then you would get a dot product or something something very close to one. That's assuming you're representing them as a vector which we'll get to in a moment. But for probability distributions. It's sort of like, yeah. I will have to check the math and that and get back to you to be sure but I think they think that intuitively that seems like a good way to think about it. They are generally, they're generally inversely correlated. So one way of thinking about this would be like, if it's dark at 7am. And it's, and what's the problem being dark at 7am and what's the problem being cloudy. Right. Those two things are probably related somehow, right, because if it's cloudy may raise the probability of it being dark 7am. Whereas, if I'm asking two completely unrelated questions like, what's the probability of there being a car crash on college, and what's the probability of there being a hurricane in Florida. Right one usually has no bearing on the other except maybe with an extreme interpretation of the butterfly effect. So if I have the probability of these two things they have individual probabilities and the joint probability of them occurring together is just the multiple of both of those. Okay, so this can also be represented as the expected value of the point wise mutual information PMI of the events x and y. So, point wise mutual information refers to single events. So again if I have car crash of college and hurricane in Florida. It's like how much information do I, does that car crash give me about the world versus how much information is that hurricane give me about the world. So, those single events, whereas mutual information in general refers to the average of all possible events hence the expected value. In other words, the, the mutual information of x and y is going to be the information entropy of x minus the information entropy of x given y. So, how do we get to the next bullet. So that is if what do I know about why, if I know x. So this is telling me what's the entropy of x given y so why has some conditional bearing on x. And so if I take the overall information entropy of x subtract that from it. This will tell me what do I know about why if I know x, because I'm assuming that x has some bearing on or why has some bearing on x. So in English, this would be like, if I see a queue, there's a higher probability that you comes next. And so these two things have a relatively high level of mutual information in English. Yes. So, I'm going to ask you again. Usually so the vertical bar means given. Okay. And so if I have x given y, then why falls in the right side of the bar. If it's why given x you just flip them around. Yeah. So the last of the, the conceptual definitions is KL divergence. So this is also known as information gain. And so it's written as D sub KL, and it's usually denoted with the double bars. So this can be realized as the sum for all x of the probability of x times the base two log of p of x divided by q of x. Once again, p and q are different distributions. Right. So if I have some distribution p, that's the truth, and some other distribution q, that is arbitrarily closer far from the truth. If I perform data compression, assuming q, while p is the truth, then D sub KL is the number of extra bits I actually I need to actually compress the information according to p. So in other words, if I have two people, Alice and Bob, and they're drawing colored balls out of a bowl. Alice knows the true distribution. So like the number of balls in the bowl that are black or red. And Bob has a different distribution. So let's just take very simple cases. Alice knows that all of the balls and bowl are black. Bob thinks they're all red. So Alice's distribution is p the truth, Bob's distribution is q, the erroneous assumption. And then when Bob takes a black ball out of the bowl, D sub KL will measure in bits how surprised he is to see that. Assuming his distribution, this is subject to like how many do we know how many balls are in the bowl total. And do we know that only black and red are the options. Right. So if we throw other possible options there, the number would change. But given other parameters that we know, then you can use KL divergence to measure in bits basically information and surprise. So another way of thinking this is information gain. So if Bob takes the black ball out, he has gained some information about the state of the world. Right. So what how much has he gained? Well, that's debatable. But we know a couple of things. If you thought they were all red, and he pulls a black one out, at least he knows that one of them is black. He doesn't necessarily know that all of them are black, but he knows now they're not all red. Right. So what bearing does this have on how we actually evaluate things in machine learning models. So we think about the probability densities. So now P and Q again, P is the density of the true labels as distribution over all the classes we have. What's the distribution of samples into those classes, and then Q is the density of the model label sets. We assume that the same number of classes right out of distribution is a different problem. It's a topic of open research. Let's assume that we specify the number of classes in our model. And then the only differences in just like the density of the distribution of classes in the ground truth versus the model. So if I draw predictions from Q, that's going to give me some prediction of the underlying data. And my goal effectively is to get Q as close to P as possible. And so I need some way to evaluate how incorrect the model is quantitatively. So classifiers make errors, but they make different kinds of errors. Right. So for example, if I have, well, I guess in the paper we're talking, I was just talking about with my student, we have this case where there's a severely unbalanced sample. We basically have comparatively very few positive samples that we're trying to extract in a whole bunch of negative samples. Right now. I could have a model that just says if I have like 99% negative samples and 1% positive samples, I'm going to say everything is no. Right. My accuracy be very high. But that's not a very good way of evaluating my model because I'm probably not going to get much higher than 99% accuracy, but I will miss all the relevant information that I'm trying to extract. So we need a better way of classifying errors than just plain old accuracy. Accuracy is great, but only useful in certain circumstances. So no model is perfect. So we think about spam filters, search engines, COVID tests. I'm trying to retrieve a certain set of relevant samples while minimizing the number of irrelevant samples that I retrieve. So if I have a COVID test that is quote 90, 95% accurate, and I gather 100 samples, how many tests are wrong? Well, how many tests are wrong? Five of the tests are wrong. But how are they wrong? Right. That could be important. So for example, if I test 100 people for COVID-19 and let's assume that 10 of them are actually infected, that's the true distribution. If I have two tests, A and B, and then A finds five out of those 10 positives, it gets 95 samples correct and five samples incorrect. 95% accurate. If I have test B that finds 15 out of 10 positives, it still also gets 95 samples correct and five samples incorrect. It also is 95% accurate. Are these two tests the same? No. So accuracy is how many do I get right divided by the total number of samples? So that is the true positives plus the true negatives divided by the total population. So if COVID test A finds five out of 10 positives, there are five false negatives. There are five instances, five cases of COVID that this test missed. If I have COVID test B that finds 15 out of 10 positives, then it's got all of the true positives and it also retrieved five false positives. Right. Now, which of these is better? Which of these is better? B. In this case, it depends on the use case though. In this case, the slides from 2021, so like written early pandemic, I guess now we just like don't care anymore. But you know, we didn't, we really didn't want to miss positive COVID infections. And so in this case, and you know, and many people still do not want to miss positive COVID infections. So in this case, COVID test B that runs the risk of retrieving some false positives, but has a far lower probability of leaving a negative on the table is the one that you want to use. So there are two measures that we use here. Precision, also known as the positive predictive value, which is the number of true positives divided by the number of true positives plus false positives. So in this case, the precision of A, we have five true positives divided by five true positives and no false positives. Five out of five is one that has a precision of one. Precision of B has 10 true positives divided by 10 true positives plus five false positives. That's 10 out of 15. And it has precision of point 666 continuing. Right. So this is basically precision is I have my dart board. And if I throw one dart and it hits the center, then I have 100% precision. Right. But if I throw 100 darts and one of them hits the center and a bunch of them go wide, that's going to lower my precision. Recall is kind of the inverse of that. It's also known as sensitivity. I'm not sure why I got cut off there at the bottom. This is 10 and that's 0. That's the number of true positives divided by true positives plus false negatives. So in this case, the recall of test A would be five false positives, five true positives divided by five true positives and five false positives. So a recall of point five. And then the recall at this point five again, this is those five positive cases that it didn't find. Right. The recall of B because it got all of the relevant samples, all the positive cases of COVID has no false positives. So it has a recall of 10 out of 10 or one. So recall is, you know, if I have my dart board and I throw just my success is defined as like having 10 darts at the board. If I throw 100 darts and 90 of them go wide, but 10 hit the board, then I have achieved my goal there. So the trick is if I test 100 people for COVID, you know, at least early pandemic and really still, we don't know how many people are actually infected until you do the test. And even then the test is not perfect. It's just kind of you have to take multiple tests. So we don't actually know this. So, we don't actually know how many people in a population actually have the disease. And so the consequence in this case, we're in 2021, of a false positive was a two week quarantine, which sucked. And you don't want to have to do it. But if that's it, and you know, you're not going to be locked down forever was a relatively low stakes consequence. So in this case, should we prioritize precision or recall? Recall. Right. We don't want to miss those positive tests, but it depends on your use case. So we will prioritize recall in this case. But there are other cases where you may want to prioritize precision. So there's a quote from this British jurist called William Blackstone. It is better that 10 guilty persons escape than that one innocent suffer. Right. So if, for example, as in 18th century Britain, the penalty for a lot of crimes was death. No going back for that. And so you don't want to accidentally execute an innocent person. And so it is perhaps better to let someone go. Maybe if they committed like a minor crime, then give them a very permanent, very severe punishment. So again, precision recall, what metric do you want to use? It depends on exactly what you're trying to measure and how you're going to use the results. But in most cases, we actually have to balance these somehow. So accuracy is one way of doing this in that you can take the number of true positives plus true negatives divided by the total population. But when you have more than two categories, this often is not meaningful, especially if they're not balanced, even if you have two categories and it's unbalanced. Also, accuracy may not be a meaningful metric. Let's take the example of an image classifier. And I have three classes, cat, dog and rabbit. And I'm trying to classify images as one of these three animals. So the precision of cats in this case would be the number of true positives, that is things that are cats that are classified as cats, divided by the number of true positives plus the number of things that are not cats that are classified as cats. So basically, in this case, true positive and false positive need to be read as members of the class that are correctly classified versus numbers not of the class that are classified as this class. And then true negatives would be number of things that are not of this class that are not classified as this class. False negatives are things that are of this class that are not classified of this class. So positive and negative just in this case are going to be always interpreted relative to the class. So in this case, if I write it out like this using what we call a confusion matrix, I can just calculate the precision by summing across all of the rows and then dividing, take that as the denominator and then dividing the number along the diagonal by that. So I take 13 divided by the total in this row, that's 25. I take 16 divided by the total in this row, that's 22. And I need 13 or 10 divided by the total in this row, which is 13. So has a relatively low precision of cats, 52, but higher precisions for class dog and class rabbit. And so what this tells me is what types of classes tend to be more problematic for this model. And so that can tell me where to optimize. Recall is the inverse. So I'm just in this case, I'm just going to sum down the rows, right? But it's just the number of true positives. So that number along the diagonal divided by the sum of all the numbers in the rows. When reading confusion matrix, I'm going to take the number of positive and negative. So it's not always as simple as just sum across the rows or sum down the columns. You need to pay attention to which axis is which. And in the assignments, you usually use like a preset function that does the same thing. So I'm going to take the number of positive and negative, and I'm going to take the number of positive and negative. So I'm going to take the number of positive and negative. So now what do we do? What's another way besides accuracy to balance between precision and recall? So we have another metric called F score. And it will the most usual case is that you can just use the function to calculate the accuracy of the So now what do we do? What's another way besides accuracy to balance between precision and recall? So we have another metric called F score. And it will the most usual case is F1 score. This is a harmonic mean between precision and recall. This is a harmonic mean between precision and recall. The formula is two times the quantity of precision times recall divided by the quantity of precision plus recall. So in this case, what I would do is I'm going to compute the precision for all the three classes. So here I take the three numbers for each of the classes, divide by three, and then do the same for recall. Multiply those two numbers together in the numerator, add them in the denominator, divide it, and then multiply by two. And here I get an overall F1 score of point six six. And so this is a typical measure that you often find of assessing the quality of a classification model. Now, this is macro average. That is, we are assuming our sample is relatively balanced. And so I'm not going to weight these numbers by the number of samples in the class. So you can do micro averaging, which does do that. So if I have an overbalanced sample, it's overbalanced in one direction, then I will weight that accordingly when computing the F1. Yes. When the weighting is micro average. Yeah, good question. So generally macro averaging will work for balanced samples or things that are close to balanced. It's kind of, it's a bit of a, kind of a qualitative game. So you'll notice that our samples here are not completely balanced, but they're close enough, right? So it's like, it's pretty close. It's like, I can probably get away with just doing macro averaging. If I did micro averaging, I wouldn't get like a hugely different number. On the other hand, if I had like a lot more rabbits than cats or dogs or something, you know, if I put all my animals in a barn and come back a year later, I'm going to have a lot more animals, a lot more rabbits, different barns, I guess, dogs will eat them. Then I will then I need to weight my sample accordingly. So this is F1 is a special case of the F measure, which is usually written F sub beta. This is just a tunable parameter. That's basically, how many more times do I value recall than precision. So in this case, like my COVID test that I still value precision, but I maybe value recall more, right? I don't want to throw precision out completely, but I want to value recall n times as more than I can set this value. And so instead of two, excuse me, one plus b squared, and then in the denominator, I'm going to have b squared times precision. And so of course, if beta is set to one, it comes after this formula. So this is just like a nice little schematic. It's actually from Wikipedia if you go to precision and recall. So let's say we have just some samples, and we have relevant samples here on the left side. So of those relevant samples, the things that are retrieved, those are true positives. And then on the on the right side, those things that are also retrieved are the false positives. Precision, how many items are relevant. Recall, how many relevant items. And so this is just like a little schematic you can use to remember that. Okay, I think that is all for this slide, Dexter, if any questions about these metrics. Right. So, now, let me proceed to the Jupyter Notebook that will get started. So, the first topic is linear regression with SGD SGD stochastic gradient descent. So this is our common optimization model or algorithm. So when I talk about optimizing a model, I want to bring the weights that parameterize my model closer to those weights that will predict the true distribution. So in this class, what we will be doing for pretty much all the units is we're going to do for whichever unit is going to be the linear version first, and then the nonlinear version. So, the first unit is on regression and regression, as you are probably aware is fitting lines to points. Right, so most machine learning. So, let's say all machine learning in some regard, is effectively glorified curve fitting. And it's just about what am I trying to do with that curve? Am I trying to fit, you know, point fit to the data? Am I trying to separate regions in the data? You know, am I trying to predict a trajectory? All of these things are done with lines. And so it's just a matter of how do I draw that line. So, if you're really enthusiastic about fitting occurs to points, you will love this class. Given a set of observations. So if I have n observations from one to n, and then a set of target values. What's the simplest model, we'll call it g of x that you can think of. Well, the simplest model is probably just g of x equals zero or a constant, but that's not going to be very useful. And that's all my data is laid out in a horizontal line. So, assuming that's not it, the next simplest model might be something like this. So if I have a model g, that is a function of inputs x parameterized by weights w, what are those values of w? So here you see bold, this can be assumed to represent a sequence of values. So bold x is going to be a bunch of different x's. Bold w will be a bunch of different w's. So these weights w maybe some weight zero plus the first weight times the first input plus the second weight times the second input and so on, until I go through all the inputs. In other words, I can rewrite it as the sum from for i from one to d of w sub i times x sub i. And so if x of zero equals one, then this can be written as just the sum from zero to d of w i times x sub i assuming that x sub zero will equal one and therefore factor out. So this can be written also alternatively as this vector of w's times the vector of x. What do you want to do with the font size increased? Yeah, okay, sure. So, we have this thing here, w, t, x, what is the t for? Anybody know? Transpose, right? So what does what does that mean? So let's just write it in code real quick. So let me define a vector w and x, and this will just be 0123 followed by 1230. So if I just do element wise multiplication, right, so if I print w, print x, and then I print w times x, this gives me the result 0260y, 0 times 1 is 0, 1 times 2 is 2, and so on. So this isn't what I want, right? I want g of x parameterized, but w to give me a single number. How can I get a single number out of this? Well, in the last lecture we had this matmo operator. So that sign is the matrix multiplication operator in NumPy. And so if I do w at x, I get 8. And so what if I just write out the matrix multiplication operation by hand using summations and multiplications, we can verify that it's correct. And so that is in fact correct. Okay, great, but you haven't really explained what t is. So let me try it again with two by four matrices, right? So if I have these two matrices, w and x, that are now two by four dimensions, I can print w dot shape and x dot shape. So this will show me exactly what the layout of my different matrices are. So I'll print w, I will print x, and then I will print w again element wise, times x, I get this. And then I'll print w matmo x. So what happened? So I've done this work so far until this point, but now it's throwing an error when I try to get it to print w times x. Yes. So if I look at two by four, remember how we do matrix multiplication, you basically take every element of a row and then multiply that by the first element of the column, right, that becomes the first value. And so what that means is that if I look at the shapes of my matrices, these two values here in those inner values have to be the same in order for it to multiply together. So I can't multiply these two by four matrices together using those shapes, right? And so you will see this error frequently, probably when you're doing your homework. And so you will find mismatch in the score dimension zero with GU func signature, blah, blah, blah. And people don't really know what this means. Often what it means is that the shapes of your matrices are wrong. Right. And it is either because you set up the data, you did, there's some bug in setting up the data or you need to reshape the matrix. So now let me transpose this so I can run NP transpose over w and then print these, print the shape. So now we have four by two and two by four. So my sense is this should probably multiply. So now if I transpose w and then multiply that by x, this actually works. And so if I wanted to get a two by two instead of a four by four, what would I do? I transpose x instead of transposing w. Right. And so now you know what, if you know what shape you want to get out, you should be able to identify how you need to reshape your matrices. So an n by k matrix times a k by m matrix will always equal an n by m matrix. And so if n equals one, then the transpose of w is actually equal to w internally in NumPy. Right. So if I just take the transpose of 0, 1, 2, 3 and print it, it's just going to give me 0, 1, 2, 3. So w transpose x is nice because it's linear in the parameters of w. And so we can do optimizations based on derivatives and we can solve this analytically. It's not so nice because it's also linear in the inputs and this greatly limits the complexity of the model. So one infamous example is that linear models can't solve the XOR problem, which we use to motivate neural networks. But a model that's linear in the inputs might be the best you can do. Let's say you have a sparsely sampled distribution, then maybe you only have a linear model that would actually be able to fit to this data. So fitting data samples to a linear model, let's assume the following situation. If I have a force F exerted on a spring, this is going to be proportional to the length of the spring. This is Hooke's law. The potential energy stored in a spring is proportional to the square of its length. So let's say that we want this rod here. So let's say I have some springs and there's a rod suspended between them. And I want that rod to settle at the point that minimizes the potential energy instead of springs. This is basically equilibrium for this system. So we can conduct a series of measurements of the potential energy with the rod in different positions and we'll store these lengths as a vector w. So given by this formula here, T sub n will represent the nth experimental measurement. So that is, in this case, potential energy. And then if G is an affine transformation, so that is its linear plus a constant. So if X are the lengths of the springs, and then G is some affine function over that, this can be written as we did above. So this is basically going to be the same thing that ultimately resolves to X transpose w, sorry w transpose X. And so this will have the parameters w0 through d. So when we have this function, G of X semicolon w, this w, those are those weights that parameterize the function. And so if I know the value of these weights, and then I can put in my inputs X, and then just perform this linear operation on it, this will give me the output for any new value of X according to those those weights. So now my goal is I need to find these the best value of these parameters for weights. So which ones give the best fit? That would be the one that you take the argmin of w for all possible for all the sum of all values for all our experiments, right? So T sub n is going to be the actual experimental measurement. And then G is going to be the output of the function. So this should just give me the least squares distance, right? So here what I can do is I can set the derivative, also known as the gradient. Yes. I'm sorry, say that again. The argument. Argument is the input to a function. So, argmin. Argmin is the argument, the input that gives the minimum value. So basically, I'm trying, I'm looking for the values of w, that will minimize the output of this function. And so if I, if my very simple version would be like, what's the, what's the argmin of X squared, right? I'm looking for that value of X that produces the lowest value of X squared. And this case it would be zero, right? Because that's the, because it's parabola. And so even if I go negative, the X squared value goes up. So what I can do here is I'll set the derivative, which is the gradient is the key term we use, but for now we'll just use derivative with respect to w to zero, and then we'll solve for w. So, you know, hopefully you're all, you're familiar with linear algebra and what a derivative is and what it means to set a derivative with respect to a parameter. Now we can do this with matrices. So, matrix formulas get a bit simpler. If we assume a couple of things. One, if we assume that the first weight w is zero is multiplied by the constant one, and then X i zero, that's the first component of the sample i is that constant one. So what we can do then is we can collect all of our observations into matrices and effectively allow us to stack up these matrices such that there are correlations between the inputs and the weights and the desired outputs and then solve for the value of the weights. So T is going to be the observations I want to fit to. So in this case, this would be my actual experiments where I'm trying to find a model that fits to my experimental data. So I'm going to collect these into a matrix T, given by this, and I'll collect the examples into matrix X, right, and is the number of samples. So this will be the Rose, and then D is the sample dimensionality that is the number of observations you made the number the number of things that you measure each time so in this case if I like four springs. I would have four measurements for each of these right. And now, W, these are the weights that I'm trying to find. So, in code. Let's set some dimensions. So n is three so that is will be the number of observations, this is a toy example. So, so let's say, and is the number of measurements. D is the number of variable values per observation so number of things that I measured here I will just initialize these values to random values of the desired dimensionality. And then I'll print them. So now you can see that I have one, one three dimensional array, or, let's say, one, one by three array, and then I have a three by four array or before matrix, and then a one by four. So, the collection of all the differences is going to be T, the targets, minus x times W. Right. And this should be an n by one matrix to form the square of all values and add them up I just do a dot product. And this only works if the value is a scalar which means that he has to be a column matrix. If you want to predict more than one value for each sample, T will have to have more than one column, which we'll get into later. So let's continue, assuming that he just has K columns, meaning you should want a linear model with K outputs. So let's compute this team minus x w. And this will give me according to these random values here. So this this output this is basically assuming these weights w. And given these inputs x. How wrong are these weights w and coming when it comes to predicting the output. So I can do the, I can use this to get a complete scalar value so this might be one way of measuring the distance from the predicted value given these in this case randomly initialized weights from the true value. So if I want a simple scalar value, I can just take that dot product. Also, just in point of fact, dot t is the same as NP transpose so I recommend using this just because it's shorter and requires less typing. So now to find the best value for w, get through this part. We'll take the derivative of the sum of the squared error objective, set it equal to zero and then solve for w. So, here's all the math what you will find is that in this class. We're probably not going to, I'm not going to dwell a whole lot on the details of the written math, because what we will find is that will end up doing the same thing in code. For those of you who are interested in how the mathematical operations work. The math will be presented for you. But effectively what we find is that I want to, you know, I want to take the, the derivative of the square error objective with respect to the weights w. And so this would be if I take the derivative I can bring the two down in front. And so now I can take the sum of the differences times DG with respect to the w. So this all works out in the end to negative two times what we see here. So, T sub n minus xtw times x. So basically look at, look at this, this is the target, minus the predictions, right, x times w times those inputs. So this gives me this, this again also uses all three of those values. So here's where we get the benefit of expressing inputs. So I'm going to use these two examples as matrices, because the song can just be performed at the dot product. Right, so these are all expressed as matrices, then I can just take that entire matrix x transpose it and then multiply it by matrix t time minus x times w which is itself would be matrix of the same size. So, if we just check the shapes and the size sizes of each matrix in the last equation above if I take x dot shape. It's three by four x transpose dot shape is four by three t dot shape is three w shape is four. And so now we can set this equal to zero and then just solve for w. So if I take the above equation set it equal to zero. Well, I can just divide by negative two is a constant. So now I can just divide by x transpose times x and that's going to give me w. In Python it looks like this. So, here if I take, I'll just use the NP dot linear inverse function, and then I can compute those values. There are a couple of different ways to do this in basic NumPy you can use the solve function. This will assume that the solution is going to be a function of the same size. So I'm going to use the same solution, and then I'm going to use the same solution to compute the same size. So I'm going to use the same solution to compute the same size. And then I can compute those values. There are a couple of different ways to do this in basic NumPy you can use the solve function. This will assume that x transpose times x is full rank that it has no linearly dependent columns. So that is you can't have a column that's can be represented in terms of other columns so I can't have a column that's like 123 and then another column is 246 right because 246 is just a constant times 123. So, solve function assumes that it is full rank. So basically this assumes there's no linear dependence so not a constant times a column not a constant plus a column, not two different columns added together. Assuming that is true. Then, I can, I can use this function, or better yet, use the least squares function this will not make that assumption. So, least squares actually produces a number of different things. So, what are those things. Well, one thing you can do notebooks is you can use the dot string so just put a question mark after the function, and it will pop up this little box or something. Lots of little box that gives you the doc string for that function so you know what the inputs are, and what it actually does. So this returns least squares solution to a linear matrix equation. It assumes the following parameters, and it will return the following things. So what I'm going to do here is this will return the least squares weights, the residuals, the rank, and value s which it said was singular values. But hold on a second, these solutions do not appear to be the same. So, if I look at the all close function. The easy all close function before. So, if I have two values that are equal to each other, like zero equals zero which return true right. So, what if I have zero equals point 000001. That's not going to return true. But remember in machine learning everything is estimation and so it is entirely possible to let's say write a neural network where you have an activation function that's a plus sign and inputs of two and three and it tells you that the output of that is 4.9999997. So, you have to get comfortable with approximation. So the all close function will basically say are these values close enough within a distance. So, it takes a very small epsilon value, and you can take any two arrays, it can be individual, just single numbers or huge matrices, and will tell you, are these things you know close enough to be equal to within a rounding error. Okay. So if I run this, it will actually say these two things are true. So, close enough. So all of these even though they may not on the surface appear to be identical, they are actually close enough to be the same. Now, there may be multiple solutions to a system of equations right many times, you know, a times, b equals a time C or a dot b equals a dot a dot c and b and c are not the same thing. Right, because I can actually multiply them by a different matrix a and get different values. So, the least squares and solve functions can be written with simpler arguments, because they're designed to find the value of w that minimizes the least squared error using this matrix product as an argument will simplify the regression implementation so linear regression. Right now we use the simpler version. So, let me just get w out of this. So now I can just use least squares that just takes in x and t and I'll pass our cond equals none. And then I'll get back w which is what I'm interested in. So, according to our random values that we initialized above here. This. These are the best weights to solve to basically map from those randomized inputs to those randomized outputs. So, what if I have thousands or millions of samples. In this case, x and t can be pretty large. So to avoid dealing with matrix operations on huge matrices and you just see. Okay, I get down to example of SGD in action. These can be quite large so to avoid dealing with matrix operations on these huge matrices. You want to derive a sequential algorithm that finds w. And what this does it uses the fact that the derivative of a sum is the sum of the derivatives. And so now you can express this derivative as a gradient, that is, a derivative and high dimension, so I can now represent my single derivatives in every dimension as just a vector of derivatives. So if you imagine, we're used to thinking in two dimensions where I have like some curve, and the derivative is the is the slope of a line at a given point right. So now imagine you have a three dimensional surface. So, what's the slope of the curve. Well, it depends which way you're looking right if I'm standing on the side of a hill. The slope is going to be different. If I'm looking at say east versus I'm looking south. Right, because it may have the surface is going to have you know different slopes in different dimensions and so if I am you know standing on a if I'm like leaving my, my house, and my driveway faces north. Then the northward slope is going to have like some negative values of slopes down, but then if I turn and face east it's a flat surface so like the eastward slope would be close to zero. Right. And so each of these dimensions is going to have its own derivative that can be represented as a vector, right can be negative and and zero or something like that. And that's the gradient. And so, gradient is just a high dimensional derivative, usually written with this upside down triangle symbol pronounced Dell, and it can be as big as you want. Okay. So here's the math for that so recall that what I'm trying to do here is I have my function of experiment rise by w is just a linear function. I now have going to, I'm going to take the, the error here so easy error with inputs xt and w. This is represented as just the sum of all targets minus the predicted value squared. So if I take the gradient of this, then what ends up going on here is now I have the gradient. I can now bring the square down in front. And so now I can take two times, T of n or T sub n minus g of x, w, times the gradient with respect to all w's of the right so now this is the error gradient. Eventually this simplifies to this thing at the bottom. So I can bring the negative two out in front. And so now this is going to be the sum of all the errors times the inputs. So now instead of summing over all of the samples. What if I just take a sample of the gradient. So if you update the associated weight for each sample, based on the gradient for that sample. So if I'm on my driveway and I'm facing north and it's, you know, sloped in a certain, a certain direction that might tell me that I might need to update that value more than the grading the other direction which is zero. So another way to think of this is, I guess in the last four minutes. If you are standing on the lip of the Grand Canyon, what's the fastest way to get to the bottom. I know someone wants to jump. No, you're going to die. What's the fastest way to get to the bottom without dying. Well what you might do is you might look in the direction of the steepest slope and move that way. Right. May not be the wisest choice is like a lot of cacti and stuff. But that would be one way to do it and so if I'm trying to get to the bottom, an inefficient strategy would be to walk along the lip or something like that right because it's going to maybe it's going to slope up and down, but it's not going to get me. It's not going to really get me toward the bottom very fast. And so I want to assess in which dimension. Am I going to have to fast, find the fastest slope, and I'm going to move in that direction. And so that's the gradient is this high dimensional slope. And what we're going to do is we're going to descend the gradient. So, and that's gradient descent. And I believe that's kind of all the time that I've got today, I will field questions in the last three minutes if there are any. So, we will continue with this on on Tuesday. All right, let's go ahead and start. It's 2pm. Okay, welcome back. Can you all hear me in zoom. All right. People still connecting to audio. Zoom folks is the audio. And I will assume it's good for the rest of you. All right. So, I think only announcement. Sarah you're doing a tutorial tomorrow. Okay. Can you just recap the tutorial on what and where. And what will you cover. Great. Yeah. So, if you are not familiar with how to do any of those things, I really encourage you to attend the first assignment is going to come out. When we're doing that. So first assignment is due to be rolled out on Thursday. So, make sure that you are familiar with everything that you're operationally going to need to know just to get the Jupyter notebook up and running. So, if you're not familiar with how to do any of those things, I really encourage you to attend the first assignment is due to be rolled out on Thursday. So, make sure that you are familiar with everything that you're operationally going to need to know just to get the Jupyter notebook up and running. We won't be using GPU for an assignment, I think, until a three. Nonetheless, this is you're going to need to know it. Or you need to be able to use it if you want to run any of the notebooks from like eight, nine, ten, etc. So, just make sure that you're at least comfortable with all those procedures. We're picked up last week. So let me share screen. All right, so. All right, so where we left off. Hide. All right, so we, we kind of motivated the problem of linear regression on this is probably something that you're all familiar with basically read this point we're still trying to fit a curve to a set of points. Right. And so if we have a set of observables, so we can take those to be the ground truth. And we have a set of inputs that correspond to those observables in the use case to the outline those basically, I have these, these springs. And I'm trying to effectively, you know, figure out what the spring length is that's going to allow the this this rod here to basically rest at equilibrium. Right. And so we want to minimize the potential energy and a set of springs and one way we can do that is basically if you have the spring lengths stored on as weights we're trying to solve for that, where the observables are the energy stored in any particular spring. And this can be applied to any linear problem. Effectively what I'm trying to do is I'm trying to find the rate of change, and then minimize the rate of change in those inputs. And so I'm trying to find the coefficients correspond to this input that's going to minimize the derivative or the gradient. Just a recap of terminology. What is a gradient slope, but in multiple dimensions right it's basically the high dimensional derivative. And so this is what we're trying to find if I have a bunch of inputs, I'm basically trying to minimize the gradient with respect to every dimension represented by those inputs. So, you know, we talked about you know just using kind of the least squares install functions on you know in NumPy that will I do that with some with relatively small inputs, but often will have a bunch of samples right and this can be very time intensive to solve. And so we don't want to have to do these huge matrix operations you want to minimize matrices are very useful of course, and can minimize the time complexity of these types of operations but still trying to do the operations over these huge matrices are still time complexity is still going to explode. So, so we can do is you can use this incremental form, where basically we're trying to find some sequential algorithm. You use the fact that the derivative of the song is the sum of derivative so now we can express this derivative as that gradient. That's just a matrix of derivative so just like everything else, where your inputs can be a matrix and outputs are usually going to be a column matrix. Those who see you can have multiple columns. The derivatives of course can also be represented as matrix. So this offset on triangle pronounce Adele represents the gradient so we have some function, g of experiment rise by w this is just the linear function remember this can be written is basically the transpose of x times T. And they're very if I'm doing this for every combination of weights and inputs is just be written as a song. So now we have this error, so this error function, he is expressed on as with the arguments of the inputs. So, effectively what I'm trying to minimize is the difference between T Taurus or X times W and T. Right so if T is my ground truth, X times W is my prediction, the correct model is going to be one that minimizes the difference between prediction the ground truth. And that's just expressed by this formula here. Right so inside the summation on every element of T the targets, minus the output of function g for input, X of n. This is just going to be the squared error and try to minimize the squared error. So now we're just back in familiarly square territory. So of course I take the error gradient. So now this is going to be the great with respect to W the weights of that error function. Of course I'm just going to apply that same gradient to the other side of the equation as well. And so then ultimately if I take the gradient. It simplifies to the thing at the bottom by very pretty straightforward calculus operations. I bring the exponent two down in front and multiply by the base. And then eventually we end up with the formula here. So now, instead of summing over all of the samples, if I just take the equivalent weight, then I can update it for the gradient for that sample. So that is five is one input sample. And then I have some weights that are just at this point arbitrator and then the input times those weights is going to be some distance from the ground truth. Right. And if it's the weights are wildly unoptimized, it's going to be very distant. If they're very closely optimized, it should be pretty close. And so I'm going to take that that error and then I can optimize the weights that correspond to that input sample for an optimize them with the error for that that sample. So the gradient for some sample and can be considered to be basically a noisy sample of the true gradient. So that is there's some true gradient that represents the gradient over all of the samples. And for a single input sample, this is going to be a sample of that entire gradient that is just subject to some perturbation or noise. And so I can take a small step in the direction of the negative gradient, try to bring the current guess for that way closer to the actual truth. So on the for some iteration K, I'm going to have this new value for some weight for some weight W and then on the next iteration, I should have a value for that weight that's closer to the ground truth. And this is called stochastic approximation. So in this case, we have W case K plus one. So this is going to be the value of W at iteration K plus one is going to be the previous value for that weight minus the gradient of the error function. Right. So this is written here. And then for this for this algorithm to converge, we have some constant here row. I want this to decrease the reiteration, not too fast and not too slow. So this is the least mean squares algorithm is derived by these folks with your own half. This is often referred to as SGD or stochastic gradient descent. So now here's an issue. If I have two output variables, let's say that I'm trying to predict two things about a car from other other parameters. Let's say I have a bunch of information, and I'm trying to predict what its miles per gallon and its horsepower is, then this value T sub n is no longer going to be scalar. So now into to predict two variables, I now need two linear models so I could do this by changing W, my weight matrix from a single column to two columns. And so now the first column will contain weights used to predict one value and the second column will contain weights used to predict a different value. So now effectively, what I'm looking at is if I have my inputs X, this is a matrix that for every sample contains the number of things I measure about that sample. So let's say I have a thousand cars. This would be a thousand rows in that matrix. And if I measure five things about them, there would be five values for each row. And you can assign some meaning to those values. And in the case of linear approximation, those values are pretty pretty interpretable. And then I have some weights that I'm trying to solve for that will multiply by those inputs that will predict a number of other things that I'm trying to predict. If I'm just trying to predict one thing about each sample, I'm just going to have one column of outputs. I'm trying to predict two things about each. I'm going to have two columns of outputs. And so then if I have, let's say, a thousand rows and then five inputs for each one, and then I have two things that I'm trying to predict, I'm trying to solve for weights that will take those five inputs and predict the two outputs. And so that means that it should have weights that are associated with every output that I'm trying to predict. So I'm trying to predict a thousand things. I should have a thousand different weights. So the linear model in this case would look the same, right, because I have weights that are just a matrix W. And as long as those matrices are the right shapes, they'll multiply together and I'll get the expected number of outputs. And so that's just one of the advantages of using matrix math is that if I just take that input vector and then they take the dot product of each of the two columns or n columns of W, then those resulting values will be the predictions of the items that I'm looking for. And now I just have to optimize those weights to predict both things at the same time. Now, you think this is going to take longer, shorter, the same time as predicting the weights to predict one value? Who thinks it's going to take the same time? Longer? It will take longer to converge to the same level of the same level of error. And that's because if we have two things like if MPG and horsepower are not very closely correlated in those, the two things I'm trying to predict, then I'm trying to optimize those weights at the same time. So it will take a little bit longer. But the formula is pretty much the same. So what do we do to the update formula in order to make this in order to make this happen? So we have to modify W to be the right shape. And then also for every sample, for every input, we have to specify the two target values. So this T sub n here, this is also no longer scalar. So now this is two values in a vector. So now I have to note this is bold T sub n. So now instead of a single value, I'll have some number of output samples I'm trying to predict. And that's just going to that will be an arbitrary number just based on whatever it is you're actually trying to predict. And so now to update the weights, you have to multiply each error by every input component. And this sounds like you would take a double loop. So in the last equation, we use matrix math and NumPy. And we did this, this operation can be done using broadcasting. We'll see the code shortly. If I use NumPy broadcasting, then that allows me to remove the loop over all the components in X and in X and W. So now I use broadcasting again, then I can remove a loop over the target output. Right. So now, yes. And here is referring to particular sample. So if we have usually what the number of samples would be denoted as big N. So if I say I have 1000 samples, that's rows in my matrix. And so I say big N equals one, and then sub n is referring to an arbitrary individual sample. Any other questions. Okay, so right now just operationally, if I'm trying to effectively scale up to an arbitrary number of inputs and outputs, this naturally means we're gonna have to scale up to number of weights. I want to avoid doing things like having for loops. There's a number of reasons for that we'll discuss in a moment. And so I can use the functionality provided by NumPy broadcasting to effectively remove those for loops. So if I use it to say remove the loop over the target components in T, as long as your matrices are the right shape, then the resulting matrix will be the correct shape for W. And so here, if you follow the convention that the vectors are column vectors, then the new weight update at iteration k plus one will be the previous weight at iteration k. And then plus some scalar value times those inputs, x of n. And then what is this last term here represent? This is the gradient for the error, right, how far is, how wrong am I so that you can think of it this way. Your model is always going to make a prediction. And you want to measure how wrong your prediction is. And the amount of wrongness will tell you how much I need to update my weights. Right. So if I'm really wrong, then making a very, very small update in my weights is maybe not going to get me very close to the actual value. So, but if I'm very, very close, I don't want to update the weights by too much, or I could sort of skip over that local minimum or the global minimum and just end up, you know, at some other, some optimal location. Right. Okay. So if row here is a scalar, then the input would be d plus one by one. So this is going to be d dimensions. That is how many things I'm measuring about my sample plus one because, does anyone remember from last time? What's the, what's the one here referred to? Well, it has that effect. That's not really why we do it. Yeah. Yes, that's, that's, it's the bias. Haven't really talked about the term yet. I'll talk about that in a moment. But remember, we have, we're basically taking weight times input plus weight times input plus weight times input. So we have w zero plus X of zero plus w one times X one plus w two times X two until we get to W and times X n. It makes things a lot easier. If I assume that there's going to be some place that I have to basically shift my shift, shift my curve. Right. So it may not necessarily intercept the y axis at zero. If we're talking about say just a two dimensional curve. And so it basically the math works out much better if I assume that that X of zero value is one. And so then I can multiply by some weight associated weight value. And that allows me to do the entire operation as a matrix because it's basically a sum over every weight times every input. So one is like weight zero. Yeah, well, one is like the constant. One is the constant. This will get multiplied. This is the feature that will get multiplied by weight zero. And so effectively, we'll talk about this in a minute but effectively this is a dummy feature that contributes no real information, but can be useful in optimizing the weights. So your inputs should be of size D plus one. So the number of dimensions per sample plus this constant one. And so then T sub n will be K by one for K being the number of things I want to predict the number of outputs. And so the transpose of that will be one by K. And so then if this X, T, W is also one by K, then I can, I can effectively subtract those and I'll get meaningful information. Right. And so if I string these together in the calculation, this will give me D plus one by one times one by K. So these two inner values, this should cancel out. And so this should give me a D plus one by K matrix. And that's the shape that we want that weight matrix to be. Questions? Concerns? So in Python, if you look at the implementation, you can see that effectively, this is the same as this part here. Right. So I assume everybody knows Python syntax. So I take some value for W, I'm going to update it plus equals row. That's our value here. And then times X one. So the X one is going to be my inputs with this column of ones that I append to every sample. And I'm going to take, you know, basically one of these samples, transpose it, and then multiply that by T for that that particular sample, and then the predicted value, whatever the output of my, my function should be. So the non-matrix way of doing this would look like this. So if I have the number of outputs would be my target shape. I'll take the second element of that element, number one, and then number of inputs will be X one. Take the shape of that. And then for every, every element of an outputs, then for every element of an inputs, I will then update the weight W sub ik for basically the weight that corresponds to for that input for that, for that output, plus the update function. So more lines of code generally lead to more potential for bugs. And that's something that you want to avoid. So we prefer to do this the matrix way. And so that way you're either doing everything right or doing everything wrong. If you're doing everything wrong, it will become obvious and you can fix your operation. So hopefully this has motivated, you know, a couple of things. One, why we use matrices for these types of operations. Two, the different components that go into computing an error update. And three, the intuition behind trying to minimize the rate of change in your gradient. So hopefully at least have some, some, some idea of, well, that all those things are important and why they matter. Any questions before I go on to the example? These notebooks will be here on. So this one, if you click there, get you to it. Other questions. Yes. So one that is one technique that you can use. So row here will correspond to something called a learning rate. And this can be a constant or it can be changeable. And so it depends on your use case basically you can start with a very high learning rate and decrease it as time goes on, or you can assume that I'm just going to specify a constant learning rate. So it, you can do that if the use case is appropriate, you don't necessarily require to. Okay. All right. So, an example of SGD in action. So let's actually see this actually executed over some data the data is not necessarily going to be meaningful, but we'll see exactly how the, the update works. We will also do a little animation so you can kind of see the line that we're trying to fit to this data actually be shifted and moved and rotated in real time. So this is basically an affine transformation. So effectively I take a line, and I can rotate it I can stretch it I can move it up and down. But the things that are collinear and the input should be collinear and the output. So there's going to be a limited number of things you can actually do with linear regression, but it's always a good first step to try. So, we'll just make some random data. In this case I'm going to have 100 samples of random values between zero and 10. And then I'll assign the target to be some function, the output of this function here where epsilon is just a bit of noise sample from normal distribution. So that can be done here so here my hundred samples, I will take my inputs and create random values from uniform distribution between zero and 10. And then I'll just take this into an end samples by one matrix. And then I will apply this function to create T. So basically, two minus point one x plus point five. The quantity x minus six squared plus epsilon denoted here by just this sample from a normal distribution. What is NP dot random not normal. This derives random samples from a normal Gaussian distribution. So this default mean, what is the default mean of zero default standard deviation of one in the above example we're using standard deviation of point one, because we don't need that much noise. So I do this. And this doesn't look very good. Right. Just a little little pie plot thing. Be careful exactly how you're plotting your data. So by default, it will connect everything with a line. So I want to make sure that the inputs are generated randomly between zero and 10 they're not generated in order. So it's going to basically connect these points in that random order so instead I will just plot them as points using the period. So here, here's my input data. Right, and it's just some random samples between zero and 10. And then I apply that function to generate the outputs and we get this this sort of curve that looks kind of like the Nike swoosh reversed. Do you think we can fit a linear model to this data. Who thinks yes. Who thinks no. I mean, it's kind of a trick question you can. It's not going to be a great model but you can do it. So let's let's let's actually go about that. So the first thing we're going to do is we're going to take that input matrix. So remember this was created up here as x. And I'm going to include this initial column of a constant one. So now I will take my ex insert one at the front, and we'll call it x one. Where do I put the that concept column of ones. Actually you will see that vary across implementations and in fact I do it differently in my two classes in this class I inserted at the front in LP, the code is written such that we inserted at the back. It doesn't really matter as long as you know where that what constant column of ones is, so that you're, you're updating your bias weights in the right place. For this class we will be fairly consistent, and we will be inserting that constant column of ones at the front. So, your data always consists of your input value will be the sample which is x or x one of the bias, and the output values which are the targets T. So let's make sure that we have the right number of inputs and targets. This is the correct 100 by two and 100 by one. That should be. So we have our targets that should be 100 outputs that have them arranged in a column. And then here I have my hundred inputs, which are just single numbers, except now I've added this constant column of ones. So now, 100 by two. Okay, so I'm learning rate, so that's that row that we saw earlier so I'm going to specify it to be some small number. So in this case I'll do point 001. And then I need to make sure that the, you know, I get the number of samples. So this is just going to be the number of rows and my input. And so I can just save that out as into a variable that I can reuse. So, I'll initialize the weights to zeros. And so here I will train for however many epochs in this case I'll do 1000 so basically an epoch where someone's pronounced an epic. I've observed this to vary between mostly UK and American English, I believe I'm actually saying it the UK way maybe. Anyway, that's a basic number of, in this case a number of passes through the data set. So, when you get to more complicated models we have things like batch size. And so there's a difference between like step and an epoch. But effectively, when we people talk about this just consider passes through the entire data set so when you have like a huge data set say a big language model that you're training on. You may hear like oh yeah we trained chat GPT for 40 epochs something like 40 epochs doesn't sound like a lot but remember how much data that's being trained over. So first of all it takes forever. And also, that sheer amount of data is contributing so much information, every pass through it that you only need for you box or whatever to converge. So, this is when we talked about talking about neural nets this is one of those hyper parameters things that you can vary that you have direct control over in the process of training your model. So, let's step through this code so if I train for 1000 epochs, I'm going to make a prediction that prediction is why, right. So why is now going to be one sample. So, let's say I'm doing n for range and sample so I'm going to get the end sample, multiply it by the weights, w. So, that should give me some predicted value for this input sample, according to whatever these weights currently are. So, now I have the target. And so this t should correspond to this input. So this should be the ground truth target of whatever these inputs should are intended to predict. I'm going to take that and subtract why the prediction that I made. That's the error. Right. How wrong am I, this is the difference between predicted value and the target value. And so then I'm going to update the weights by a fraction of the negative derivative of that square error with respect to the weights. So this is the learning rate that's the row. And so now here, this is going to be the input transpose times the error value. So, and then when I'm done I will print my weights. So, try that. And we end up with these two weights of 3.125 and negative point 264. So now this should give me effectively a weight that corresponds to the input feature. And the second weight corresponds to what? Sorry, other way around. This way corresponds to the input feature, and this first way here corresponds to what? The bias. Right. So, sorry, momentarily forgot where I put the column of ones. So now that bias is this is this basically this is the y intercept. Right. So the bias is what do I assume about my model if I have no other information. So basically if my input contributed no information. What's the best starting point that would minimize my distance from the actual data. That's the bias bias in a technical sense this will be how we're using it in the class, although at the end we will talk about bias kind of in the colloquial sense. So the way we can think about it is this bias is, you know, we think of it as a prejudice, it sort of is, if it's like what do you, what conclusion you're going to leap to about something if you have no other information. Right, it's you you prejudge something that's your prejudice. So I gave this example of if I, if my going something as I see someone wearing a backwards baseball hat I assume that he's a jerk, but I don't know anything about him. But I see someone walking down the street and I just looks like a real loser. And I gave this example in my first NLP class I looked in front of me and there was a guy wearing a backwards baseball cap. Sorry, I've since taken him on as a research student I have observed he hasn't worn a back to a baseball cap since not going to read too much into that. I got a great paper out so I'm But basically this is like, and I don't actually think that about people who are backwards baseball caps I'm just an example that I happened to notice there was someone in front of me. But basically that's you that's your, that would be like a prejudice or something that you would assume. And then once you get to know the person or the sample right you actually know like these, this is, these are the other ways that I need to be. I need to be calibrating when I'm when I'm deciding things about about this phenomenon that I have encountered in the most abstract sense. But at this point of bias is basically, if I know the information my model of all my other features are zeros, for example, what's my best starting point. And so that's the bias and that corresponds to eventually there's a dummy feature this one that we train that weight against. Yes. Great question. So there is, there's a whole lot of research into how we do this hyper parameter optimization for neural networks. Effectively, a lot of people will just be using trial and error. And so in this class that's mostly what you're going to be doing. There are some techniques you can use like say grid search you can try out a bunch of different values of different, you know, say different training lengths and see which one of these is giving me the least error. And then maybe I can try and narrow down exactly what that sweet spot is. But it's actually, especially when we come to deeper neural networks, there's like just a whole lot of research and how can I optimize all the different hyper parameters that I'm trying to deal with because it's not just epochs here. He will hear it is just epochs but it's not just epochs in broader use cases to have things like the batch size, the number of samples that you pass through at each time. The learning rate, you know, so for example, the, the trade off between training number of epochs and the learning rate right that can be directly proportional in fact or in some of the inversely proportional. So if I take a smaller learning rate, I might train for longer and get a better result. Or if I were to take a larger learning rate I could maybe get away with training for less. So, there's no like this definitive answer, but it's an area of experimentation. Yeah. You will need to loop them manually if to do grid search. At least there's one assignment where you have to experiment with hyper parameters. And so for that, you're basically given code that does it for you, you just have to fill in like what numbers you want to try. For like your project and stuff yeah you can import whatever libraries would help you. I'm not going to try and make you write all the code by hand for that. But for for certain assignments. Effectively you're given starter code and you cannot modify outside of certain places. So, that's it. All right, so we train this model. And this is effectively doing y equals mx plus B in two dimensions at this point. And so now I'm going to see how well this linear model fits the data, so I can just do this by plotting the models predictions on top of the actual data. And this is the, this is our prediction so the blue dots are data, the red dots are prediction. And so here you can see this x one, that mall w, that's that prediction value again. So, what do you think. Not. All great I guess. Let's actually see what happens as we're doing the optimization over those thousand epochs so I can actually write this code that will basically run an animation so you can see where that line is as it's processing every sample. So in this case what I will do is I'll initialize all the weights to zero. And then I'm going to collect the weights. After every update to plot them. So this isn't part of training, this is just part of visualization. And then I'll create a bunch of x values, and then for every set every pass through all the samples. I will update my weights and then we all plot the line for those input samples across the actual inputs, so you can see the prediction. So, let's go. And so now you can see the black dot that's the last sample that was just trained on the line is the current state of the model. And then you can see also how those values of W zero and W one are changing over time. Right. So, you know, you can, you're free to adapt this code to do animations, and we'll do we'll have some examples of these in other notebooks, but effectively we can just run that again. And we can see how the line just starts like kind of pretty much wildly opposite the general direction of the data, and then eventually we can see, you know, it's being that that wider set is moving up toward that three point something value. And then it, and then we can see that the, the slope of the line is also decreasing. And so you can see that those values are getting close to these two values here. Right. So the the approximate y intercept is just shy of three, and then the slope of the line is like slightly negative. Okay, that makes sense I hope. Okay. And so now we can see also those those values that I'm sort of, I'm going to qualitatively evaluating based on just looking at this line are being plotted here so again, W zero ends up just short of 3.0, and then W one is in this case it's just a little bit below 0.0 or negative point one. Okay, so all this is to say that matrix multiplication is basically just solving systems of equations. Right, so I have a bunch of inputs, and I have some output corresponding to them. And I have a bunch of these different samples and I'm trying to optimize for the values that solve that large system of equations. So to give a simple example we can solve a pretty simple system of equations here. So if I have 4x minus 3y equals 17 and then x plus 4y equals nine, I'm trying to solve for the value of both x and y and I assume that you have done this, probably in like high school algebra class. So one way we can do this, the way that you may have learned is just through substitution, so I can solve for x at first. And I'll say okay if I saw for x using the bottom equation, I ended up with x equals nine minus four y. Okay, and so now I can take this value that I solved for, I solved for y or x in terms of y, and then I can take that value and substitute it back in for the value of x in the top equation. So now I have four times the quantity nine minus four y minus 3y equals 17. That's the top equation. So now if I expand this out I end up with 36 minus 16y minus 3y equals 36 minus 19y which is equal to 17. So if 36 minus 19y equals 17, then I subtract 36 from both sides, I have 19y equals negative 19, y equals negative one. Now I can take this value for y, substitute it back into any one of my equations to solve for x, and it turns out that x equals five. So this way allows me to solve the system, and I end up with two values for x and y of five and one. But we can also do this with matrices. So, the way this works is effectively the coefficients here in front of the two values I can just plot as the inputs, right, and then the values on the right hand side of the equal side, those are the targets. So I basically have some sample four and some sample negative three, and then I have some sample one and some sample four, right, so these two constitute one input, these two constitute another input, and then the corresponding outputs of 17 and nine. And so, if I solve it this way, so let's say I have my inputs x, and I'm trying to solve for the values in these coefficients here x and y, I can do this by taking the transpose of x and multiplying it by t, and this should give me something close to the expected value. So let me make sure I've run everything. And if I do the inverse function here, I end up with five and one. So, pretty neat way of solving systems of equations using matrices, we and we can demonstrate that we're getting the right values, comparing to another method that we're already familiar with. We can also use least squares. So this we're trying to least square solution. Remember we, this function returns a few different values so W residuals the rank of the matrix and s. If we just look at W, those are the ways we're trying to solve for it also gives me five and one. So, a couple of questions, you know what our residuals. So, from the doc string, basically if the rank of a is less than n or m is less than or equal to and this is going to be empty erase that's what we get here. And so now if I print the shape, the rank of matrix x and then the shape of x and t, and then compare them, we can see that a is not less than n but m is less than or equal to n and so that's why the residuals are empty. What are residuals. Okay, so here is the definition in a nutshell of errors versus residuals. So the error is going to be the deviation of the observed value from some unobservable true value of a quantity. Whereas the residual is going to be the difference between the observed value and the estimated value of a quantity. So, in this simple linear example. These don't really apply on that as we get exact values and we know what our true values are, and they're also the same. But the prediction step is basically x transpose times t to solve for w. So if t contains an exact values, or the values that correspond to x don't linearly predict t, then the best you can do is some form of approximation. So we have residuals in larger neural networks and also working with more naturalistic data sometimes a residual is better than an error, because it allows you to optimize for the approximation. All right, so in the last part, we're going to play with some real data. And so basically, if you want a hint about how to do assignment one, you're going to want to refer back to this section, because we're going to be doing some very similar things just the different data. So, first, we're going to use this automobile data set from this UC Irvine machine learning database repository. So, there are two things, auto data or auto mpg.data and auto mpg.names. So let me download those. So let's look at the data first. So this is the first step that you're going to want to do before beginning any experiment is to actually understand how your data is set up. And so every, every data set you download is going to be represented slightly differently you need to understand that how the data is represented and how it's organized before you can really make any progress. So, let's take a look at the contents of auto dash mpg.names. So, you will find that there are 398 samples, and each of them has these eight numerical attributes and a string. So the names those attributes are going to be the mpg, it's going to be continuous value number of cylinders which should be a multi valued discrete displacement and horsepower and weight and acceleration are all continuous values. So, the number is going to be a multi valued discrete right just just years origin is going to be a number corresponding to some, in this case country, and then the car name is a string. So this just relies to look it up. So first thing you want to do is import it into Python and look at it so we're going to use the pandas package that you all are should be familiar with. So, generally, in this course, import numpy import high plot import pandas and in later assignments import torch should get you pretty much everything you need, with the exception of like you know sis and OS and some other system level Python packages. But generally those four should cover everything that you need. Where, where, where the difference you will be notified of that. So let's look at some of the lines and figure out how to read it in. So, just a note in this, in this notebook there are two cells. One for unique systems and one for Windows systems command is slightly different so depending on if you're running the notebook, depending on which system you use you're going to run one or other of these of yourselves. Since you're using a Mac will use the UNIX version. So here's our data. So we're looking at say the first 40 samples. Let's take a look. Let's remember what those values stand for. So, 18 miles per gallon eight cylinders displacement of 307 horsepower 130 acceleration of 3504 model year of 12 or. So, sorry, the weight. Okay, yeah, yeah, wait a 30 seems like a lot of acceleration, weight of 3504 it's a very fast car. Acceleration of 12 model year 70 in this case, 19 x x. So 1970. And then one in this case corresponds to I believe the options are us Germany or Japan. And then, so one here corresponds to the US, and this is a Chevy Malibu. So this is organized it's not necessarily the, the most friendly organization, but you can do some things to make it more readable. Also you'll see that there are some NA values here. Right. So, in this case, there are some cars in this data set that for example we don't have the MPG value for. Right, so we're gonna have to handle that, and there may be any values in elsewhere in other columns. So, for example, I think it's maybe, yeah so one is us to is at least Europe, and then three is Japan or Asia, more broadly. So, let's look into a data frame. And here's what we get and this is not friendly at all. Right. So, first of all, a couple of nasty things one it just read in the first column is the header, we don't want that. And then we also read everything in in under the in a single line. Right, so we have to format this a little bit better. So, the pandas out reads CSV docstring will basically tell you all you can do in pandas, or you can read through this pandas tutorial. So, we're gonna just go to format this is the easier to read. So, first I'm going to pass in header none so they don't get that first row as the header, and then I'm going to do them there. Set whitespace as the delimiter. So now I get a nice format. So now I can actually get a nice looking data table. And I have my values in the first seven columns and then the name of the car in the eight. And then I also have 406 rows nine columns because they have this one index column here. So let's take a look at just a subsection here. So if you look at rows 30 to 39. I see a couple of things. One, there's some any nans here. So this is going to mess things up so I need to get rid of them. So here, I can just do this by setting any values equals question mark. So I love this again. Let me look at, I can't see any change here but I did know that there were some nans in rows 30 to 39. So let me look at these again. So now let me do this drop in a, and so now I have 396 rows or 392 rows. So it got rid of about 10 rows or so that had unfriendly values. So now let me look at that same sample. And I still have that NA. Well, something didn't quite go right here. Any ideas. So the NA was removed in a specific place, but not in the data frame. Yes. It didn't happen in place. So I did that and then returned that but it didn't modify. Right, so I did, I said, I did df.dropna, but I didn't, I didn't actually resave this into df so I'm basically doing df.iloc and so it didn't do that in place so the drop in a function is not by default in place. And so now if I do df.isna.sum, this will tell me, you know, how many NA values do I have in each column. Right, so there are eight in column zero and six in column three. So now if I don't do it in place if I do df equals df.dropna and I look at that same subset. Okay, now it looks nice and clean. So now if I run df.isna.sum, these are all zeros. So this is what you want. Right. This is one way to make sure that you have no NANDs in your, in your data. Okay. So if you run into difficulties with pandas, I recommend just reading the tutorial. Some things to note if you're not familiar with pandas, but you are familiar with NumPy. So it's not that easy that rows and columns are handled in NumPy versus pandas are different. And so it can be kind of frustrating and I personally like whenever we do a paper and I'm working on stuff. If it involves panda they usually spend like an hour trying to figure out like what I'm doing wrong with pandas. So just fair point you know if you start working with this, do allow time to figure out what the errors are and make sure that you are processing your data correctly, especially if the data is a large set that you can individually examine every row of. Any questions. Yeah. NumPy, pandas, pyplot, and torch. So, and you think we should import those for everything or we should import one of those third step routes? You will always use NumPy for this class. In the in the code you're given usually the standard imports that you need. But just if you're, this is mostly for the people who would like to import like your favorite third party package that like no one else has ever heard of. I'm just telling you, those four will get you everything you need to do. There's no need to get fancy. And, and even torch like we're probably not going to we're not going to use that until a lot of assignment three I think. Yes. So you can just do this, if you look at the the read CSV doc strings will tell you like what, what everything does. So basically, Oh god, it's lost up in here. So, NA values. So additional strings to recognize as nan. So like if there's if there's a question mark in the data set. It might just be like someone mistyped something that they don't have the value. So basically I will just I'll turn that into a nan, and then I can remove it. Okay. Other questions. All right, so the first step to doing any kind of data processing is examining the data, the next step is going to be to visualize it. So one thing we can do is we can just plot the value of every attribute in a separate graph. And so I'll make an array of columns, column names to label the y axes. So instead of doing zero through eight. So now if I set the column names, I can actually see what every column corresponds to I don't have to keep referring back to that list above. So this makes things again, easier to present easier to interpret so don't neglect these pre processing steps because it will save you a lot of time, it takes more time up front, saves you a whole lot more time at the end. Okay, so now if I just do df.plot. Well, that's not terribly helpful is it was a couple of reasons why one, one big one. Yeah, right weight is in pounds so it's like it's between you know 2000 to 5000 pounds these are cars. So of course I'm following these raw values, but I'm looking at weight 3500 versus year 70. I'm not going to be able to tell which one was made in 1970 versus 1971 I can't even really tell you know what the displacement values are. So, I want to separate these out. So if I take a look at everything except the car names I can just look at, you know, the, basically see the different ranges of the different values, and already I can observe even though there's some missing values here because there are nine columns. I have things that range from 70 to 82 and then also things that range from like in the hundreds to the 300s. So, if I look at the type of my value they're all float 64. Okay, I can work with that. So now I can plot every data column separately, and use. So I'll just use this code here, so I can plot all of my different samples kind of nicely side by side using sub plots. And so I'll plot the sample number on the x axis, and whichever parameter value is on the y axis. So, take a look at these charts. And tell me if you notice anything interesting about these about these samples. What sticks out to you about this what relationships you see between the samples, and the different parameters. Yeah. They seem to be already ordered by years we have this like step function here. Anything else. Does all for young get better over time, I think so yeah because like it's ordered by year and we can see that as we can use this the sample numbers of proxy for time. You can see that MPG seems to be getting better over time. What else do we see. Yeah. Horsepower tends to be decreasing on the maybe a slight correlation. Yeah, so there's a couple of things you might be able to reserve. The first observation that you have to make is that the sample number is ordered by year. Otherwise, if you didn't have this this one in the bottom left, you don't know what order you're in. Right. And so, now that I can see this, I can draw some conclusions based on what I know about cars we know that auto technology has gotten better over time and so a car that was made in a later year is probably on average going to have a better miles per gallon than one that was made early. So, anything anything else that seemed interesting about these. Yeah. So, Well, we don't really know I mean we want to look at we need to look at the data kind of an aggregate so just from looking at the individual graphs. The first inputs we can make is that they're ordered by time and that can tell that can give us some other intuition about like what things are correlated with time but we don't see things like how does horsepower correlate with displacement or miles per gallon. Yes, in the corner. Yeah, displacement is decreasing over time so like some of these things seem to be correlated with time, but we don't know are they really correlated with each other. Right. Is it that cars are getting heavier or what's any correlation with weight. Is it that cars getting lighter over time and is that correlated with, is it really the weight of the car that makes the horsepower go down or the displacement go down or something. Right. As opposed to the year. So plotting displacement as a function of year may not be all that meaningful when really the dependent variable is something else. So let's try to predict one attribute, namely miles per gallon from the other attributes. So, first we're going to make a 392 by one column vectors that's going to be the target values containing all of those MPG values, and then a three 92 by seven matrix to hold all the other values. So I have these eight samples and trying to predict the value in this graph from the other ones. And so that this can tell me whether MPG is correlated with any of these other things we observed is correlated by year. So what we can do with this knowledge we can see whether our model is making some of the same intuitive judgments that we are. And if it is, then it stands to reason that it's other predictions can probably be believed. Okay. So, let me get the target values this is going to be stored in T. So I'll just print all of those. So these, this is the MPG data for every car in my data set. So I have the two variables. And so this is the, these are all those other seven values. Let me just check my shapes to make sure that they are what I expect so I expect 392 samples. I have seven inputs associated with every sample now and a single target value. Alright, so now let me make sure that I have the names associated with the, the right values so I want MPG to be my target name, and all the others to be my, my x names. So now let me see if a linear model makes some sense. I can do this by plotting the target value MPG versus each of the input values. Alright, so now take a look at this. So this is miles per gallon in terms of the other variables. What do you notice here. Sorry, say again. Right yeah so there's a correlation between weight and miles per gallon. So, right so the lighter cars here have higher MPG. What else do you see. Yeah. Yeah, so the cars with greatest placement have low miles per gallon, which might suggest there is some correlation between weight and displacement. Right. What else do you see. Same with horsepower so we see basically similarly shaped curves. When we look at the weight displacement and horsepower relationships with miles per gallon. So these are the, those are some of the continuous values. Any correlations between some of the other ones and miles per gallon. Yeah, so, well, there should be this. Yeah, here. Yeah, here. So is there, is there, is it an inverse relationship or a positive relationship. Yeah. Yeah, so I do not know. Some weird things happened in 1981 so I heard this guy called Ronald Reagan right. And I would have to look at my history to see if there were like any major deregulations that happened in 1981 the automotive industry. So, it is possible in fact that maybe miles beyond did get worse from 1980 and 1981. So yeah there's literally just recording me just now. So, yeah. And then also a couple a couple other things to note here. So, there's a discreet value we only have a three through eight. But there seems to be an inverse relationship between miles per gallon and cylinders. Right. And those of you who know anything about cars that makes sense. Also, yeah so origin also. So, the American cars, European cars and Japanese cars. And definitely in the time span that this data was gathered. Japanese cars had a reputation for being a lot more fuel efficient and American cars were like pretty lousy. So there is a correlation between the, the origin at least, according to this coding, and the miles per gallon but just keep in mind. So, there's a sort of a arbitrary number that was assigned to each these categories if it had been, if Europe was one America was two and Japan was three what we see would be the sort of V shape. Right, it would not be easy to fight to fit a linear relationship to that. So, in certain cases with these like multinomial classification labels, the way you organize your data because very important this one simple change could throw off the entire linear model. Let us, yes. Yeah, so you could I mean, the first of all, with multinomial inputs, a linear model is always not is like not good anyway. So, it's not always true that the way this data is set up there is a linear relationship that this could be a helpful feature. But in most cases this is not. So if you any sort of language work, right, if you, how do you represent word as a number of a one technique would be to have a big one hot vector for the size of my vocabulary. So, for example, if I have a one in this I have a one in that space. But what that means is that every word is orthogonal to every other word, and I have no way of telling if cat and dog are more similar to each other than cat and truck. Right. And so that's the way that you organize your your inputs, and the way that you represent your inputs using more sophisticated techniques can be very important. So, in the last 15 minutes, let me finish this example. So we've observed some linear relationships between some of these parameters, and these seem to make sense, given you know what we know about cars even if we don't know very much. So, now that this seems like this. So now that we've done these relationships that could be useful. Let's build a linear model so first let's tack on this column of constant ones to the left side of my inputs. So now, this will take care of that coefficient multiplication with W zero. And so you just do this using NP inserts and be entered to basically the array, where you want to put the the new value, the value that you want to put it on and then the number of inputs that you want to be computing over. So here I'm just going to transform x into x one, using that command. So now I have a 392 by eight array. If I look at say a couple of samples the first three samples. It's now represented like this. And so it's in exponent notation here but you can see that the first value is a one now for everything. So now we can add to the x names to the constant column. We call this the bias weight so we'll add bias to our names. So now, if I were to show this in the data frame, my first column should be bias and it should be all ones. That's not very useful, because we know that's the bias, but this will come in handy later when we're trying to interpret the weights in our train model. We could try to fit the model to all the data and see how accurately we predict the miles per gallon for each sample. But that means this model is going to do really well on this data but if we show it other data, there's no guarantee that it's at all going to fit to that data, it could be, you know, wildly off, or it could fit perfectly we just don't know. So a much better way to avoid this kind of overfitting as we call it is to basically hold out some samples from the training so I'm going to train on a subset of the data, and then demonstrate that the model is working by evaluating it on new data that hasn't seen. So, this will work here, because we are reasonably assured that any data we remove will more or less resemble the data it was trained on, but we want to be kind of careful in how we partition that test set from that train set. So, one way this could go wrong. What if I just held out all the samples from 1982. Right, or what if there was some weird anomaly in 1981 because Reagan deregulated something. What if I held on my 81 and 82 data and it trained on 7380 and suddenly it's not a good fit for that 81 and 82 data. So, we have to be kind of careful about how we select that so how do we partition these joints, these subsets. So a common practice is to randomly select some portion as the test set, and then keep the rest of the training set. So a typical partition would be 8020. But you know you can you see you'll see different distributions 7030 9010 even 5050 sometimes if the, you know, if the data is particularly sparse or particularly information rich. So we'll stick to 8020. So we'll partition our samples and do our training and testing sets will just deal with the row indices, and then we can randomize those and then take that same selected slice to get those corresponding rows and x and t. So let me calculate the number of samples in the training set. So I've got 392 total. If I take 80% of that and round it it's gonna give me 314, then the remainder 78. So if I use an 8020 train test split. I'll have 314 training samples and 78 testing samples and that will cover all of this, the data that I've got. So now I want to randomly select those 314 rows and take the remaining samples as the training as the testing set. So what I will do is I will just shuffle all the rows. So now I have my rows arranged randomly. And then I will take my end train which is computed previously as 314. And then I'll take that first 314 as the train and the remainder as the test. So now I have, we see we started 4D158. So I take those first 314. These rows are going to be my training data, and these rows are going to be my testing data. Check and see if they're disjoint. Do intersect 1D, this should be empty. Okay, so now I have successfully partitioned my training for my testing. Okay, so now I can take those train indices elements of X1 that will be my X train, similarly for t train, and then you know the same for X test and t test. Check the shapes to make sure that everything looks good. I have 314 training samples, 314 targets, 78 testing inputs, 78 targets. Okay, great. So now I can use my SGD loop previously shown to find good weights. So first thing we're going to do is we'll add a calculation to track the error. So I want to see how quickly the sum of squared errors over all samples decreases. So as I optimize my weights, that sum of squared errors over all samples should be going down. That means that I'm optimizing my models toward that global minimum. More meaningful is actually the square root of the mean of squared errors to RMSE. This will be a common metric for these regression examples. This allows us to have the units of the error in the same units of the target variables, rather than their square. So I'm trying to predict miles per gallon, I want to know how many miles per gallon I am off, not how many miles per gallon, miles squared per gallon I am, because that doesn't really make sense. So this allows me to have an intuitive metric that I can then report my results and say I'm literally this far off on average. So RMSE is going to be taking all the residuals I'm computing over the training data, or all the errors of the testing data. So here we come back to that residual versus errors. So I don't have any ground truth in my from my test data in my training data so I can use the residuals to compute an approximation. Then I compute their quadratic mean where R is an element of the set of residuals or E is going to be an element of all those individual errors. Okay, so now let's run this. So in particular, look at the tiny size learning rate, and we can try running it again. So, if I have a train for in this case, what 50 epochs, we can see that this root mean squared error value is in fact decreasing. And this is what I want to see. So, 9.77 down from 19.1. If I increase the learning rate or decrease or decrease the learning rate. What happens so we start with a higher value, but we end up, or we start with a lower value we end up not quite as optimized right. So we can play around with this learning rate, let me increase it. But if I make it too big, then we run into problems. So, effectively what is happening here if learning rate is too big is, I'm, there's some global minimum, I'm trying to approach, and it sort of keeps skipping back, back and forth across it. And so I will never reach that. So learning rate is going to be a very important hyper parameter. We can also train for longer by train for hundred epochs instead. So, we can see that now I'm getting a RMSE of about 8.2. So let's see what happens with my linear model, which is now just these weight matrices, and I'll use it to predict W for the first four samples, or predict miles beyond for the first four samples. So, this gives me some, some values. Remember these are randomized so let me predict, compare them to the actual MPG values. So I'll take a two column matrix, or I can use a for loop to print them. So now it's predicting a value of nine for a true value of 14, a value of 19 for true value of 17 and so on. So it seems to be off but it's hard to tell like if it's consistently high or consistently low. So, you can play around with the values of learning rate and number of epochs, to find the values that result in lowest RMSE. Let me print these four samples. So, this is just a pretty print version of this matrix above. Okay, so now let's try all the test data and plot the results we can see over the entire test set, how good my model is. So, there we go. Okay, so this is, you know, choose one of these, I would say this is not great. It is consistently low. But I also want to know how numerically how wrong I am so let me calculate the RMSE. Our predictions are about eight miles per gallon off on average. So, a couple of things we can do to fix this. One thing is to standardize the variables. So, we've discovered that the learning rate has to be really really small to prevent these updates from resulting in Nan. And this because this is because this larger learning rate when I multiply by these inputs x. This will basically send me back off the far side of that gradient curve. And so, is there a general way to deal with this problem. So let's take a look at the different ranges of our values. So I plot the min max of everything, we can see that I have, you know, ignore the bias but things from like three to eight but also things from 1600 to over 5000. So since each input values multiplied by its own weight the magnitude is those weights are going to be very dependent on those range of those inputs. And so, the step size and gradient descent is going to be very dependent on the range of input values as well. So to minimize this will standardize our variables this means that according to the min max of weights. I just, I want to know like how close am I to the minimum maximum values rather than what's the absolute value, and that means that the min max of weight can be rated from like negative one to one or zero to one, and so can the min max of cylinders or year or anything else. So we need to do this. So basically what we'll do is we will adjust the range of all those input variables we have a mean of zero and a standard deviation of one overall to training samples. And so those values that are most dispersed from means that the highest standard deviation of the set. We don't want to do this with the, with the bias, because they're all ones, you can't compute a standard deviation over a set of constant values. So, you want to make sure when you're doing when you're doing the homework like standardize your values before you append or prepend that column of ones. And so now also to do this correctly when you're partitioning data into training and test sets. You need to also only calculate those means and standard deviations using the train set, so that you're standardizing the test set to the same range. You might have something that is outside the min max of the train set and the test set, but at least that that value that you standardize it to be meaningful if you're using the same means standard deviations that you calculate using the train set. So then you store the means and standard deviation training data and then use them again. So we calculate the means calculate standard deviations. So I apply those to the train set. And then now I can see that I can, I now have standardized values. So, these should be a lot friendlier for my SGD operation means of the same shape. And then I can also perform standardization using those calculated means on the on the test set. So now, let me try this again and note the much larger learning rate here. So now I can see that I have standardized values my root mean squared error is now in 50 epochs down to about 3.3 as opposed to nine point something before. So now I can make my predictions over my test set. And these look like they're a lot closer to those true values. So let's try it again with the test data and plot the results. And this seems to be much better fit. So now we have standardized variables are much less sensitive to different values of the learning rate. Finally, what's most important what's the most important feature for predicting this with a linear model we can actually see this with a neural net is much more difficult, but with linear model we can actually see which weights are correlated or inversely correlated with the outputs. So which of these attributes we use are most important for predicting MPG, maybe you can remove some if they're not useful. So by looking at the magnitude of the weights. We can see which ones are positively or negatively correlated with the output. So, for example, the bias is the bias that has no real correlation. So if you look at the or this is the min max. So you look at the ranges, we can see no change in the bias. The, the standardized variables range from like negative 1.5 to 1.5. Let's look at the weights. So these are the weights that are associated with everything. And we can print them according to those different values they're associated with. So, I'll give them three minutes left I'll tell you what you observe about these values. So the bias is 23, which is probably pretty close to kind of the average miles per gallon of this data. And so if I know other information, 23 seems like a pretty good guess. So, for example, we observe that MPG increases with time and in fact, year is highly positively correlated with MPG. We also saw that it's a negatively correlated with weight. And so we see a negative weight associated with the weight input. Right. And if I had a ton of different variables might be easier to sort them by their magnitudes, so I can do that. And then I can plot them, according to which parameter is the most important. So, yes. I'm sorry, say again. No, this is because this is to, this is to sort them in just descending order of magnitude. And so then I replot them with the signs here. Right. And so now I can see if I that the weight is very important, but it's got a negative value so it's very important but it's the inverse correlation. So, year is also pretty important, but it's not quite as positively correlated with MPG as weight is negatively correlated. Okay. And so you can look at this data and see what you might expect to correlate with predicting MPG positive weights indicate that this positive correlated negative weights indicate that's inversely correlated. Okay, so I will do the last bit on Thursday before we get into all the newer features, because we have no time left. But thank you and I will see you in a couple of days. All right, let's get started guys please. Right, sorry about the delay I couldn't. My computer is not connecting to the Wi Fi for some reasons I can log into zoom. So, humans behind schedule. Just a reminder for attendance if I grant you a leave for like if you're if you're usually attending person. If I say it's okay to attend online for a specific class that is for that session only right if you want to attend, if you're, if you continue to be sick for example or whatever. Let me know that you're still sick so I do not expect you back. Right. But if I said it was okay on Tuesday. I need to know that you still plan to attend remotely on Thursday, and so on. All right, so today I will finish up the linear regression notebook. And then what I'm going to do is I'm actually going to assign the first assignment, and then go into notebook for because pretty much what you need to know in the assignments occurs in notebook three. Let me share my screen first. So, I guess, before I begin. Are there any questions anybody want to review anything from notebook three, the last part of this for the first 1015 minutes maybe of class. So many questions about the content so far. Okay. No. Oh, and also, so sorry I did the tutorial on Jupiter notebooks. So hopefully those of you who needed were there. It is recorded, and we will put the recording up on canvas in case you need to review any of the material. I also have a version from last year's TA that I think covered more or less the same thing but maybe presented slightly differently or may cover some different things so you'll basically get two takes on how to use the department machines. If you care to, if you care to view them. So, the expectation at this point now is that you are at least conversant and logging onto the department machines and you know testing your notebooks and stuff like that and making use of the resources there. So in particular, let's say you don't have a GPU, the work that requires that you can log on to, you know, some of the department machines. I will let me get there I will send around a list of GPU enabled department machines because not all of them have GPUs, but you should have access to be some of the do. Okay, so if there are no questions about the material so far just to recap we're just basically solving an optimization function with multiple inputs using matrix multiplications right and the way we do this is through stochastic gradient descent effectively So, you're trying to figure out with a set of weights that are arbitrarily poorly or well optimized at any point. For a sample you take that sample multiply it by the weights, and that gives you an output, and you measure the incorrectness of that output compared to some ground truth label, and then that error is used to update the weights in the correct direction. So, the descent of the gradient to the gradient here is basically this is a high dimensional derivative so just a slope in multiple dimensions. If it's easier to think about it this way you can just think of it as basically just a slope in three dimensions and you have basically two components of the derivative. And so I'm trying to minimize my position on that gradient, right so I'm trying to find as close to the global minimum of the gradient as I can. I don't know which way to go along the gradient but I do know where I just came from. So it's sort of like, instead of looking down a hill trying to see where the descent the great the direction of steepest descent is actually looking up the hill, seeing where the steepest direction I just came from was and walking backwards. Right and then I can see this should get me closer to the minimum how close did actually get me to the minimum and I am I going in the right direction, right am I moving along the right dimension. So previously, we did examples where we have an arbitrary number of inputs. So remember for every sample if you have an n by D matrix representing your inputs, you have n samples those the rows, you have D dimensions to each row that is the number of things you measure about that sample. Right. And for the moment, we'll just consider everything to just be tabular numerical data. So this is how we can solve for, you know, D inputs to predict one output. So if I want to predict the miles per gallon of a car from a bunch of different, a bunch of a bunch of different other parameters, I can do so, we can then also do things like figure out which of those other inputs are most important for predicting the that output and which of them are positively or inversely correlated with that just by looking at the weights in a linear model. So if you can turn something into a linear problem, it makes it much more explainable because then you can see exactly which input is correlated with the output. When we move to nonlinear problems neural networks this becomes difficult to impossible because of the number of operations that have to be should be performed at every step. But for the moment, this is a neat technique that you can use to basically see what's most predictive about my model. So the cool thing is now that if I have all of these input parameters, if all these inputs, I should really stop saying parameters because that means I have to wait. So I've all these inputs, I can use the inputs to try and predict any of the other values. Right. So for example, we were using these values to predict miles per gallon, but some of these values may also be correlated with each other. And some of them may be predictive of other things. So this allows us to have multiple target components. And so what I can do now is I can do things like use the values that I have for every sample to predict more than one thing about that sample. So, and what this means then is that we can basically use the exact same matrix operations and all you have to do is adjust for the size of your inputs and the size of your outputs. So if we regard T the targets as an n by one matrix of output values one per sample, all I need to do is add an additional column. So in this case, if I want to predict miles per gallon and horsepower, then every row will still represent the input values pertaining to each sample of car and the output values now be two columns, which are n by two where the two values are miles per gallon horsepower and n is the number of samples. So all we need to do is add those additional columns of target values. So now the T will become an n by k matrix where k is the number of things that you want to predict. So D is still the number of things that I measure about each sample, k is the number of things I want to predict. It doesn't matter if D or k is one or more. Right. It's you can still solve this using a multi input or multi output linear operation. So the operations are fundamentally the same in that you assemble the data for predicting miles per gallon horsepower. So let's just make sure that I didn't clear the kernel. So this is just going to pick up where we left off last time. So if you look at, if you recall these indices, so these numbers refer to indices in the data frame where the target data in this case zero and three, those are the columns that correspond to miles per gallon horsepower. So you arrange your data and pandas, you just look at your data and say, okay, these are the columns that I want to select for the input, the output, and you can just slice through the data set accordingly. We already established the names of each of these so I can print things nicely. So I will, as long as I have the order that I want my targets to be in, I'll just, you know, pluck out those those names from the names variable that I previously created and then do the same thing with the X names for those values I'm still leaving as inputs. So now that I have my entire data, I'm going to take columns one, two, four, five, six, and seven to be my inputs and columns zero and three to be my targets. So if I print these. Now, my shape, the shapes, my inputs and outputs are slightly different. So it used to be 392 by seven. Now it's 392 by six, of course, because if I take something out of the input and put in the output, you know, I can't stay in the input so I can't be predicting the horsepower from the horsepower. That would be too easy. So now my inputs are 392 by six and my outputs are 392 by two, and we can just Sandy check and make sure that we've got the right values by printing the names for each column so now using cylinders displacement weight acceleration year and origin, I'm going to try and predict miles per gallon horsepower. So, we talked about splitting into train and test so of course, just like you can have horsepower in both your input and your output. I don't want to be testing on the same samples that I trained on, because I'm going to overfit to that data, right and there's going to be no guarantee that the model that I train on this is going to perform well on other data that I haven't seen. So what I do is I approximate this by splitting this into a training test set with the assumption that the train the train set and the test are going to at least more or less resemble each other. So, what this means that I, if I use, if I train this, this model on cars on car data predicting miles per gallon horsepower. It's not going to do me any good, trying to predict whether right because even if I set up the matrices in the right shape. So, if I use the same number of inputs and same number of outputs, those inputs mean entirely different things as outputs mean entirely different things and unless there is this website called like spurious correlations that you can like check out, you know, number of like, I don't know, number of RSV infections compared to number of jaywalking fatalities or something and you find like that there's actually a relationship but it's completely arbitrary. So that aside, unless you find a case like that, you're not going to have a case where the inputs are predictive of the numbers, looking at the numerical values because they mean entirely different things. And so we'll see in the assignment will be working with weather data and you can see exactly you know how those values are being used there. But having done that, let's see, I'll just split my my train and test samples I'll use the same 8020 split that I used before so I have 314 training samples, 70 testing samples. So they are the are six and two elements for each sample respectively. So we talked about standardization. I want to standardize that I am not subject to the range of the values where I have one parameter one one input that has a really large range. Don't even remember why we don't want these really large ranges. Yes. So you saw that graph where like the the weight ranges in the thousands and other all the values are in like the hundreds or less. So first of all, it makes it really difficult to interpret. And, and if I'm trying to look at what the relationships are between different values that becomes difficult to do when I'm performing the gradient descent update doesn't even remember why a large values in your inputs are not desirable. Yeah. Right, so if I have, if I'm just trying to optimize you know linear function in one dimension I effectively have some sort of quadratic curve, where we know that there should be a global minimum somewhere. And I'm taking these little steps down the gradient approaching that curve. But remember the steps are calculated not only by the error, right, the distance between your output and your target but you also multiply that by the input that produce that output. And so if those values are really big. You're also going to be taking that step that error, scale it by the learning rate or some small value but then you're going to scale it up again by like 3000 or something. And so if you're close to that gradient you could basically just jump over to the other side. Right. And so you will never approach the minimum that will allow you to optimize the function. So, this does a number of things one it makes it difficult to converge, as we saw, when we're not standardizing if you remember the lowest our root mean squared error value got was about nine for this data, whereas after we standardized it got closer to three in the same amount or less training. And so one thing that we want to do is want to basically restrict to the ranges of our values to between, you know, zero and one or between some known, known constraints. So one thing that I will do then is I'll calculate the means and the standard deviations, and then I can standardize my value by by taking the raw values retracting the mean and then taking the quantity divided by the standard deviation. Key point, I only want to standardize using values means and standard deviations to calculate it from the training data. Why. Yeah. Right, I could have something in the testing data, let's say, let's take the weight of the car. And I have ranges between 2000 and 5000 pounds and I standardized all my weights and I get this nice distribution. And then in my testing data. I, one of my samples is like a semi truck or something and it's like okay this is now 30,000 pounds or something like that. You don't want to standardize the range of your testing data such that the upper bound of your, of your, your standardized ranges correspond to 30,000 in your testing data and only 5000 in your training data, because then for that for that value for that input. That 5000 pound sample is effectively can be treated similarly to that 30,000 pound sample but in fact, that much larger value that much larger rate value has some implication for the output value you're trying to predict. You standardized using your train your train date training data and use the same means of standard deviations to standardize your test data mean that any outliers in the test data will appear as outliers once standardized respective to the training data so if I were to combine all my data, it would look approximately the same. So we do that here, we insert the concept column of ones of course that's to insert that that coefficient that can be we can tune this bias weight to, and then similarly we add bias to our, to our names we can interpret it later. So now I have my 314 by seven and 314 by two training data. So now I can train this linear model to predict both miles per gallon horsepower. So, one thing I will do here is effectively this is the same function as, as we did before. All I need to do is change here is now I have my way to realization has two columns, because I need to, when I multiply by these weights, I need to get two columns out. So now this is going to be an inputs by two. And so then training is pretty much business as usual, I will take the predicted value and compute the error and then I'm going to update that using a friendship fraction the negative derivative of the square with respect to the weights. So now I can take the back of my squared error some so I can report it. And then I will print it out so let's run this. And you can see that we now can within about 50 training epochs, we end up with an RMSC of about 6.11. So, remember what the, when we were only predicting one value what the final RMSC was after 50 epochs, approximately nine, that was the first time before standardization after standardization it was about three something like that. So this is also standardized now the comparison we want to make is RMSC after 50 epochs of 6.1 when predicting two values RMSC of three point something when predicting one value that intuitively makes sense to you. Okay. It's, it's double I'm predicting two things when we expect the RMSC value necessarily to be to scale linearly. Not really. But it does make sense it's bigger just intuitively because I'm predicting two things I'm predicting two things using how many things. So, previously I was predicting one thing using seven things right so it may be that effectively, if I take horsepower out of my input. It might be correlated with miles per gallon so it's somehow a good predictor of miles per gallon in some way. I take that out of the input so it's less able to predict miles per gallon because it doesn't have this one key indicator right so I'm predicting more things or fewer things. So this is the reason it would take longer to converge so I might try training for longer I could conceivably approach a similar RMSC value, but also we may find that it sort of bottoms out somewhere, and we never quite get there so this is just pretty intuitive kind of data science if I have fewer inputs trying to predict more outputs it's going to be a noisier model. It might still be quite good but it's generally going to be noisier. So, now we can see that the, I put the weights here. And we can see that I've got two columns here. So right and I've got now it's a two by seven. And so we have the the bias weights up at the top and then the weights associated with every input below. And so we can, we can see a couple of things by just examining these weights so if we look at the the weights for the miles per gallon target that's going to be the first column. And then we can print the weight to the horsepower target that's going to be the second column. So, take a look at this and see what what you observe. So what things are positively correlated with miles per gallon. So, there is right we saw that you know there's a clear because the way the data is ordering is a clear correlation. And we have, you know, miles per gallon typically got better over over the year. Anything else origin, right, yeah there's there was this slight origin where because because we had American cars at index zero European cars and then Japanese cars, they happen to be ordered in a way that was kind of friendly to British miles per gallon. So, what things are positively correlated with horsepower weight. Right. So we need more horsepower typically to move a heavier car for example. What else. Yeah, so acceleration is pretty strongly negatively correlated right whereas displacement is positively correlated to about the same, about the same degree. And then the other side cylinders you know more cylinders more horsepower. So you know, just think about what we know about cars, some of these things make sense. Let's take a look at the biases right the bias is what your whispers speak up. It's the y intercept, it's this it's it's the, it's the some access intercept depending on how many, how many inputs you've got but it's the thing that I will default to if I have no other information right. So, we just look at these values. Let's pretend all the for we have some sample where every other value is zero. 18 seems like kind of a reasonable value for for miles per gallon it falls within that range of known miles per gallon values. Same for horsepower right 83 horsepower is like not very much but it's a reasonable value for horsepower. And so if I don't have any other information. I'm not as good as starting place as any 83 for horsepower is a better starting place than 18 for horsepower thinking about cars that were built in the 70s or the 80s. Right, so just when you're doing these linear problems. Your intuition kind of plays a lot of a big role in to serve, looking at your errors, and this becomes more and more difficult as we explore more, more complicated problems. And so the operation of optimization kind of remains the same. So, this foundation of linear regression is going to be the set of operations that we will basically keep keep modifying slightly each time as we get more and more complicated until we have arrived at things like neural networks and reinforcement learning so just kind of keep that in mind. And so the operation of inputs times weights equals outputs is going to be the core of pretty much everything that we do in this class. So now let's use our model to predict both of these values so I want to predict. Take for some my testing data predict miles per gallon horsepower. So my prediction shape should be 78 samples by two, and I will plot these actual versus predicted for both of these. So here are my two charts. This is predicted miles per gallon, and this is predicted horsepower. So, one thing we observed is that it didn't converge quite as well as the the single model predict the one value. So, if I remember to think back to I can scroll up the notebook I want to see it to what that plot looked like we were trying to predict just the miles per gallon. What do you remember about that compared to these how to how do these do compared to your memory of that, that plot standardized one after we standardized. So that line after we standardized that line is like, more or less dead on right it kind of cut nicely through those data points were here, we see both of them are a little bit on the low side right this also attests to the increased difficulty of optimizing this, this function, when I have only six inputs random predict two outputs, and because of the nature of horsepower versus miles per gallon. We were basically removing some information from each classification by taking that information out of the input. Yeah. Generally speaking, yes, it depends on the nature of the inputs, it could be that like it could be different use case where you might have three inputs that are just very information rich for some reason and really good at predicting those four things. There is like a strong correlation between those three inputs and those four outputs. And so in that case you wouldn't really see something like this, but generally speaking we have sort of a bunch of data that you've gathered for a bunch of different samples. Yeah. Yes, right. Yeah, so there could be like two columns in this data that are not really correlated right I could maybe, you know, acceleration is not maybe a strong predictor of miles per gallon or something, or it's at least a weak one. So maybe I could remove acceleration and predict miles per gallon acceleration at the same time it wouldn't see as much penalty, right, you can experiment with this and see. Yes. Are the non informative inputs hurting the results. Are the non informative in supporting the results. You, you could empirically verify this by removing those things that have low weights and see if you get a higher correlation with the other weights, or with the other inputs. Usually the non informative ones don't affect it very much that's why they're not informative so this these weights that are close to zero. Effectively what it's saying is that this thing given the data that I have it's not a strong predictor of this one way the other if I remove it I wouldn't see a whole lot of effect. But you might, right, you might see that actually this is just noisier. The reason that is the reason that it's poorly correlated is basically if I plot my out my desired output versus this input I just see things that are all over the place. And then I'm just adding noise in which case, removing that input might actually increase your performance. Okay, so what you'll find is that like machine learning is just like a really empirical field and a lot of times the answer to. Does this happen if I do this is do the experiment and find out and this may be some people doing really deep theory of machine learning they can have some like proof of this. But in most cases which you're going to find is we did an experiment we change the following conditions and the output was this and therefore we've evidence just that this is the case or not the case. And so likewise this is going to be a very experimentally driven course you're not going to be doing any mathematical proofs. So, if you're worried about that don't worry if you're really hoping to do that I am sorry. So let's see quantitatively how well we did in terms of the root mean squared error so in this case, what we get is for the training data, I will have for remember we get error values for both of those outputs. And so, for the miles per gallon I have a root mean squared error over the training data of six point six point odd. And for the horsepower it's 25.6 and then for the testing data. It is also quite similar right we see numbers that are really close. This tells me a couple of things one is that my training data is a good does resemble my testing data in a reasonable sense. And also, we can see that you know maybe in in raw in terms of raw value. It's harder to predict horsepower, then, then miles per gallon. So, we can just put this out in a more friendly form here they are again for review. What is this star doing here. Anyone know this particular intricacy of Python. Yeah. Right yeah so I've got you know to two arguments in this list, rather than having to just iterate through this list and print them out one by one. I can shove it all into a single line right and just split this out. So that these. This is basically being treated as an art as a list argument into the format statement. And so then it's being is generate the output, just nicely. So, this is just a little demonstration of this if I have this function foo, or I have three arguments So if I define a list that is 123. Okay, there's three things here this function needs three things. This should work. But it, of course, it doesn't because really this function is expecting three separate arguments, and this has a single argument that happens to be in three elements. But if I put the star here, then I can break this down into the three arguments and it'll that function will work. So, that was an aside, but it looks like we have a bigger error for horsepower, right, in our in our model. And so maybe you know one way of thinking about this is maybe this is due to the larger range of horsepower right so miles per gallon is pretty constrained between 13 and 25 or something horsepower is bigger, the values are bigger but also the range is larger. So, if I just put the min and max of the each target in the testing data, then I get something like this, right so for the, for the miles per gallon I have a min of 11 actually max of 48. Whereas for the horsepower I have a min of 44.3 and a max of 255. Yeah. So, this is going to be after. So, what this will do is this is going to be before normalization where I've unstandardized the outputs. So these are these are the actual these are the actual values and these are the raw, the raw data. Okay. Any questions about this content or the, basically the previous two days worth of content on linear regression. All right, so let me then go into the assignment. So, this was in the way. Okay, so the assignment, if you go to the first assignment, someone linear regression with SGD. You, first of all, it directs you to download the a one notebook from the public facing page so just as a note all the assignments are going to be posted here. If you click on this, you get this, you can download it using this. If you click on this often it's going to happen is you get a bunch of JSON, just save this as in the dot IP Y and V format and open it up in Jupiter notebooks and it'll let you actually get the interface. Remember, you need to basically install Jupiter and then start the Jupiter client through the, through the command line, and then you're getting going to get this window that I've got here. And this will turn this into this. So, those of you who have accommodations through STC these are already put in. So, for those of you who got those canvas handles this automatically so the late penalty. You remember 1010 points off every day that's late. This is factoring in any blanket accommodation that you may have, you know, it's the submission box closes 10 days after the assignment is due because if you submitted 10 days late doesn't matter if you did perfect it's still zero. So, I will just advise you that it is strongly to your advantage to get things in on time even if they're incomplete. So, for various reasons pertaining to how I calculate your grades. It's really better for you. If you get a 70% assignment in by the due date, rather than you know an 80% assignment in two days after with the late penalty. So that would just something to motivate you to manage your time. Of course things happen if you do need an extension reminder to let me know 24 hours before the due date. You're going to get a automated grader I'll talk about how to use that this downloaded here. Question. That was it okay. Yes it's a one grader zip, I'll show you how to how to use that in a minute. And then your regression of SGD. So, you're given some starter code that has the sort of fill in the blanks bits here. And so you're going to need to fill in the train use and RMSE functions and then apply them to some other data so basically the, the skeleton So this is given for you, we give you the inputs. We do part of this like the the printouts and things like this and the returns for you but what you need to do then is fill in each of these steps. Everything that you have to do here is contained in notebook three in some form which are going to need to do is look through, find that piece and maybe change the code slightly to accommodate either the way the data is formatted, or what we're asking you to do here. So you're going to need to have a list of all assignments in this class. And the steps are effectively written out for you in sufficient detail you just need to, you know, figure out how to implement these steps. So, the model is going to take in the inputs, the targets, learning rate, the number of epochs and then an optional argument verbose. And so that's just going to be, you know, whether or not you trade you reprint out during the training process. So you take your, basically at the end of train right you return this model used to take in that model and then for some input x predict what the value is according to your train model. So the shapes of the each component are given to you. And then RMS E is similar. You take in a matrix of predictions and a matrix of target values, and then you return the root mean squared error between those predictions and those targets. So, generally you the assignments are going to be split between coding and discussion. So, in this case you're going to get 60 points for writing the code. And then the remainder is going to be experimentation and discussion. So here's the train right calculated means standard deviations. We've done all this in some form, use function, and then RMS see right this I don't really need to spell that out in the code. So what you're going to be given then is basically some simple dummy data that you can verify whether or not you're at your limitations are correct. So here's a simple example. If we just have, you know, some, some function that generates x and t, then you can verify whether or not you're your implementation produces the same result. So, in a working train implementation or I'm not going to run this because train isn't complete break, but a working train implementation would give you a result like this. So, a working train implementation should look like this. So a strategy you might use is basically copy this so that you can refer back to it see whether your results look the same both qualitatively and then quantitatively by containing by looking at the numbers, and then the automated radar will run a set of unit tests for you. So, you know, here's just the plotting for for these tests. Then the real data that you're going to be working with is this weather data come coming from the CSU weather station. So this is the data file here it's just a text file right now so just save this and it looks pretty messy. Of course, so the first thing you're going to need to do is loaded up in pandas. So you get five points for reading it into pandas, and then check for missing values. Remove samples that contain missing guys so pretty standard data cleanup. Once you've done that then you need to create a linear model that will take the data here that you have like you know days, it's a year's worth of data, you have the days of the summums and things about like the wind the barometric pressure and stuff like that. Your goal then is to predict the next day's average temperature. If you look in this data set you will notice there is no column saying next day's average temperature. Right, so you have to manipulate the data set a little bit to extract the next day's average temperature. It is a year's worth of data so you will be able to do this for almost the full year. So this is going to be one, one sample that you will not be able to predict that for. So, you just focus on like some of these features, you, there are other features in there if you find them to be useful you can do that experimentation and discuss it. So if you focus on like these features, then what you're going to need to do is you need to modify the data file to add this next T av. And so you know here's a here's how you can, you know, hint is it is a hint on the naming convention you can use. So then you select these eight columns up here from the data frame assign them to an umpire. So you assign X to those columns for all but the last row, there are reasons for that. And then assign T to be the first column for all but the first sample. And so now the first sample is associated with the first row in X. And then that's the, the T, the T av for the following day. So use the train function to train a model over X and then and the T data and then try different combinations of learning rates and number of epochs. Use the use function to predict and make plots of the, of the, the target versus the predictions, and then do some discussion of your observations, and then you can use the value of RMSE as your metric here. So for a linear problem RMSE is a decent metric for other types of problems. It tends not to be all that useful. Then do some discussion about like which, which inputs are most predictive of the desired output. So we just did that kind of sort and print the weights. And then you can discuss what's positively correlated with inversely correlated. Okay, so this automated grader will allow you to effectively make sure that you get the 60 points for the code. By passing the unit test that will be enough, and then the remainder is just running what should be functional code and discussing your results. So, if you know how to run, you know, the, the Python script in Jupyter Notebook. Basically, if you download this, you'll extract a one grader dot py make sure it's in the same folder as the notebook and make sure what I recommend doing is really just putting them alone together in the same folder. So don't have any other documents in there. It's looking for a Jupyter Notebook that has the name formatted in a particular way so if you have like two copies in there it's going to be a little confused. So run this, run the cell, and it will basically print out you know the outputs of your unit tests and what the score is for each of those. So we're going to use a similar grading script. That's basically the same things which is different specific values right so that you can't get away with just hard coding the answer in or something. We will look at your code of course right so we're not going to just take the the auto grader on blind faith. So, you know, don't you, you look at the automated grader code to understand how your inputs and outputs are supposed to be formatted. Don't look at the auto grader code to see what the answer did unit test is and hard code that into the notebook right so you're provided this, you should look at it but you need to be using it in the appropriate way. Finally, you can get one point of extra credit on this assignment so this assignment is worth 100 points so if you get the extra point you get 101%. Problem is that if you think about weather data, the previous day's temperature is probably pretty close to the next day's temperature. There have been times like shortly after we moved here we had that one day where it was like 30 or 90 degrees one day with like a forest fire and then 32 degrees next day with snow. So that was an outlier in most cases right it's 30 something degrees today and it's going to be kind of similar to that tomorrow. In a time series the best solution may often be to just predict the previous solution. And that is the predicted value in this case will look a lot like the TF shifted to one time step. So to do better you can try to predict the change. So rather than the raw value predict change where your target is now the next day's temperature, minus the current day's temperature. So if you've set up your target data like this run the previous experiments no need to train to change the training function just retrain the model, then you can do other experiments with different hyper parameters, and then report on your results. Okay, any questions on this. Okay. So this is going to be due February 9, and this, you have an extension already. And so I recommend that you start early. Yes. Yeah, so you'll get, you'll get a number out of the, of the unit test that you pass you get like x out of 60 for the code. So basically what you'll get is you'll get a number for the code number for the discussion. Usually, these try to say you know great work or something if you get 100% basically means there are no, no glaring errors. If you notice something interesting you know sometimes I'll comment on it like you had a particularly good discussion. I will mention you know, for these points in the discussion. If you didn't do something and then how much you were deducted for that. So, we try to get things back within a week, although there are like 75 of you, and only one TA, so we asked for your patience, sorry particularly asked for your patience. Our goal is to get things back within a week after that for the deadline, give or take a day or two, but sometimes there are unforeseen circumstances. Okay, questions, comments. Yeah, you want to not change, not add any cells to the assignment. If you, if you do like a bunch of experiments you want to like show the results, you can just say you know, run train with this number of epochs in this learning rate or something, you can do that. You cannot change the skeleton code that you were given because the automated graders expect things to be formatted in a certain way you may break it. Yeah. Future assignments will kind of follow the same model of like you will look at the last week's lectures while we're doing it or will they kind of. They might separate a little bit depending on how things go but it's not it's, you will look at previous lectures within the last two weeks typically. So for example, if you look at the calendar assignment. So like assignment to mostly going to be contained within notebook five assignment. Is, and I'll just give you notice like for is the one that tends to tends to be the most difficult. That involves putting these together from a couple of different notebooks like 910 and 12. So, things get a bit more complicated as time goes on but generally yes. Okay. Yeah. Yes, I thought I put that in the notebook. So, oh yeah so you need to name your notebook has last name dash a one. So, in will download everything through canvas and rename everything is like, I don't care if two of you have the same last name that's not going to be an issue. So yeah, just do that. Okay. So let's move on to sort of starting with non linearity they're not in the way that we're necessarily going to do it for the rest of the course. So these models have been linear in the parameters and linear in the input features. So this is going to be not very useful for things where there's a nonlinear relationship between the input and the output. And of course, there are many things where this is the case. I think a linear model is like a really good first step for many things but it's often not the best model for most things. So, one thing you can do is basically screw around with the inputs by applying some function to them before they even go into your model to make the relationship nonlinear effectively what I'm trying to do is I'm trying to see if I have a linear model that's the only thing that I've got. But I suspect that there's a nonlinear relationship between some featuring the output. Can I do something to that feature to effectively make the relationship linear. Right. So, for example, if I can, if I observe the data is observing the data that does like a squared relationship between some input and the output. I can already tell that linear model is not going to do me very good but if I could just if I just squared the input, suddenly I would be able to do something with a linear model. So, with fixed nonlinear features you have a way of introducing nonlinearity as long as you know what kind of nonlinearity you're trying to introduce. So let's say you've got a single feature for each sample, and your data matrix will look just like this x zero through x n. You can try a bunch of other features like by by squaring it, or taking the cube or taking the fourth power. And now I have a bunch of other basically features that are that are derived from that first column that might be helpful. And this is simple to do in Python just using the H stack function. But we don't really know which of these powers of x is going to be useful. We can look at the magnitudes of the weights, right, we may find that maybe this fourth feature here is highly correlated with the output and that might tell us something. And as long as the input features are standardized we can do this but we can actually do some more things. So for example if you can actually build multiple models, we call bootstrap samples of the training data by trying different feature functions over these things. And then we can compute confidence integrals of the weight values. So, if a zero is not included in the range of weight values specified by the 90% lower and upper confidence limit, then we can say that you're 90% certain that the value of this weight is not zero. If the range does include zero then the corresponding feature is probably not one that's all that useful. So what is a bootstrap sample. A bootstrap sample is a smaller sample that's called bootstrap from a larger sample. This is a type of resampling where you take large numbers of smaller samples repeatedly from the single original sample with replacement. So here's a more in-depth explainer of that. Below is some code that will illustrate this process. So we will include this value lambda, which is going to be a penalty on weight magnitudes. We don't want things that are like overwhelmingly correlated or anti-correlated with this. We want to find things that are predictive, but not like hugely so. So, let me import NumPy and Pyplot, import random. So here's a train function. This uses the least squares function. So this is not the answer to assignment one. So if you try this, it's not going to work. And the use function is also not the answer directly to assignment one. Root mean squared error. This is very close to the answer, but I don't really feel that bad about giving away because it's like it's one line and it's not that difficult. Yeah. Little underrated. I was just reminded by the importance. In A101, are we able to just import pandas as PD? Yeah, you can, I think. I guess just. That's how I... Yes, I believe you can, because there should be no automated test on that. So it shouldn't fail that. And worst case scenario, I'm wrong and you try it. You run the autograder, right? And you'll find out if that was the mistake you made. That's easy to fix. Okay, so here we have some train use and RMSC functions. You will notice I'm using this NP dot I. So what is that? That is the identity matrix. So this is a 2D array with ones in the diagonal and zeros everywhere elsewhere. So NP dot I, I assume I equals I for the identity matrix. So let's make a simple function of X. So we'll just take this function here, negative one plus 0.1 X squared minus 0.02 X cubed plus 0.5 N. N is just going to be some noise drawn from a standard normal distribution. So I will take 40 training samples and 50% of them I'll use as training data. So now what I can, if I look at my X and T, what I'm doing here is I'm going to stack 40 samples from... Actually, so I'm taking 40 samples between 0 and 3 and then 40 samples between 6 and 10 for a total of 80 samples. And then I'm going to reshape them. So now if I plot my testing data versus my training data, I get something that looks like this. So if we do this just using dots, right, so we have this distribution. So now what we can do is we can add squared and cubed values of each feature and go up to the fifth power. So now I can have this XF, which is going to be my sort of my additional features. I'll divide my data into training and testing sets randomly. So if I've N rows, same as you did before, I will then just basically take all the row indices, shuffle them and then slice out those row indices from my training data and then make the remainder into my testing data. So what I will do is I'll take N rows times the training fraction around this, right, of course, because in case my training fraction doesn't divide neatly into my number of actual samples, in this case it's fine, I had 80. But if I had, you know, if I do like a 70% training test split and I had 315 samples that might not divide all that well. So it doesn't have to be exactly, you know, X and 1 minus X percent. It's just got to be pretty close. So if we do this, in this case we divide very cleanly we have 40 training 40 test samples. So now what I can do is I can color code my samples. I look at the train and test data so what you want to see if you have the ability to plot your data, you want to see something more or less like this where there's a neat overlap between the training data and the testing data. So there's a couple of things here so like there's no test sample that's really well represented in this area of the input. So it could be that maybe a model fit to this is not going to necessarily perform all that well and things that form the fall kind of within this narrow window. That's okay. This is, this is all generated randomly this is the sort of thing we expect. So now let's make models on bootstrap samples of training data, so models will just be a list of models one for each bootstrap sample. So, what I'll do is I'll create 100 models, and then I'll draw random samples with repetition from my, from my training data. So for example here I can specify that I want to draw these samples from a range of So here what I'll do is I'll just take n train minus one, and then draw that those those samples with repetition. So then I run the train function over this and now X train boot and t train boot will be bootstrapped training and and target samples. And so now I can basically create this list of models that are just weight matrices solved with the least squares function that should fit to this data. So, how many models do I have I have 100. Let's look at the first model. Right. And so now we see the outputs formatted as the weights. So these are the weights. And then these are the means and standard deviations of my inputs. So now apply all these models to the test data. This is pronounced why all I grew up in rural New Mexico where they think they're part of the south apparently so they said y'all a whole lot, which is a little bit weird. So I will take for every model, I'm going to use the use function, and I'll take the result of that and then I will append that to my why all which is going to be why is going to be our term for the for the predictions. And so why all will be all of my predictions. Of why all this is the list so what I will do I'll turn it into an umpire array. So we have 100 by 40 by one so we have 100 models. Right. Each of them is pretty making 40 predictions, and then one. Right, what's the one. So it's an extra dimension right so it is one column in the sense that like all of these things are being shoved in there and they have one prediction each but it's just, it's kind of just there I don't really need it all that much. So I can use the squeeze function, which will reshape this into and preserve the same content, and then effectively just get rid of this extra dimension that's that's ancillary. Okay, so I can turn my why all into the sheet that I need by applying the squeeze function then transposing it. And then I look at my why test. And I will just take this to be the, the mean of all of my predictions for every model. So now the RMSC test is going to be the mean if I take the mean of the error between the white test and the t test right the actual values, and the square it this is going to be my RMSC. So, my test RMSC is going to be 4.0 roughly. So, having bootstrapped all these models. I take the mean of all those predictions I can use that to compute a root mean squared error value. So now let's take a look at some methods are going to put to use next. There's two things here, NP dot linspace, this will return. And even these space numbers over an interval from A to B, and then NP dot reshape, which you may be familiar with, basically take some input and reshape it into new dimensionality as long as the shapes are compatible with preserving all the data. So, reminder that you can use the question mark to look at the doc string to figure out exactly what the input is to a function if you are countering errors. So, now what I'll do is I will create this even distribution that is choosing in this case 100 evenly spaced numbers between zero and 12.5. And so we end up with this. And so, same thing with reshape. So if I look at the shape of this, right, this even distribution should have 100 numbers in it because that's how many I created. I can reshape it into a column matrix, just by doing N plot and one, right, so 100 by one is still 100 so I preserved my 100 samples and now they're in a column like this. And if I look at the shape this is now 100 by one. And the only real trick is that the new shape has to be compatible with the old shape so I could reshape this into 50 by two and that works just fine. And even this would work right 25 by two by one by two. This will function, it will preserve the same data. It's just now I basically got a four dimensional array where things are maybe not very cleanly organized depending on what I want. But you will see things like this, like, if you're working with RGB images right every pixels represented by three values, but the convolutional net may require that everything be flattened into a single array or maybe three arrays for each of the channels. And so you have to reshape an N by N pixel image by three channels, right and then flatten out the individual pixels in each channel so you basically have an array for each one. So, the reshape gets more and more complicated as the, as the operations and the architectures get more and more complicated, but the real thing is I need to know how many numbers I'm working with. I need to make sure I'm not trying to cut numbers or add numbers. So, if I change if I do try to do a reshape over 101 by one over 100 samples that's not going to work right of course because there's no way that I can, I can turn 100 numbers into 101 numbers. Okay, so, um, what we can do then is I will generate some evenly distributed numbers all then each stack, basically the powers of those numbers together. And now I end up with this right of course zero the power of anything is zero. It's now we have all of my numbers from that original distribution up to the power of two powers of five. So now let me use the evaluate method. And what I can do that. So, I can use that here. This gives me 100 by 100 predictions. So remember this use method is being applied over every one of those hundred models in models. And so now I can plot those outputs. Right. So, for those models, each of them is a line. Right, we can see that they are all, you know, better or worse fits to this data. And so I can use that to measure the power of the numbers based on those, those power features that I've accumulated. And so you can see that for, for example, those odd powers are these ones here where it's probably dips here and then goes back up later where you have this quadratic or the quartic powers. You know from the positive value, and then hit our global minimum and then go back up. Right. And so you can see that this data can be modeled by a quadratic function or a cubic function or a quadratic function or a fifth power function. But once we get outside of distribution, this data is not fit to that. Right, so we have no idea whether the next sample is going to be down here or up here. Right. And depending on what other samples, you'll show up in this data set, we will find that either like the the cubic function is best, or the quartic function is best or so on. Okay, so now let's try some other data. So some real data this is going to be related to the design of holes on yachts so I guess like if any of you have a yacht. I'm sure there's a lot of other ones that normal people have. So let's take a look at this so we have this yacht hydrodynamics data maybe it's like this like the early sailing yacht, you know those are small not the megayachts. So, look at this data this doesn't really mean a whole lot to me certainly and it may not mean a whole lot to you if you don't know things about boats. So let's try and get some information about it so we'll just say that I happen to know what each of these columns means center of buoyancy prismatic coefficient length displacement ratio beam drop ratio length beam ratio and this thing called the fruit number. And then these are all my inputs and I'm trying to predict the resistance. Right. So I have these things I don't really know what they mean. So I'll go ahead and stick my header row on there with the names. Okay, so now I can see which of these values are associated with which of those parameters so we have you know a bunch of these similar values they're kind of all the same this fruit number is kind of slightly different. And then the resistance value changes. And the center of buoyancy, maybe the same for all of these boats in fact. So I'll just try to get the data to look at it another way, right, it can be messy to look at them. Look at the data, just in chart form. So, we look at center of buoyancy. And there's just, just kind of, there appears to be three clusters here but there's no clear relationship between this value and the resistance right. And the same kind of seems to be true for most of these things. Right, there's no monotonic relationship or near monotonic relationship between resistance and any of things, except for what this fruit number right I don't know what the fruit number is, but I can see that there is there's some sort of relationship on some kind of power curve with this right so I can at least say that in some universe the fruit number is predictive of resistance in a way that none of these other things really is. So this gives me something to look at. Another thing I can also do is just plot, all of the values, all the raw values. Right. And we also see that this curve appears repeatedly, if I just go through each of the samples so right, this appears to be the same thing as the fruit number, right, if I took all this pink curve and dispatch all those all those samples on top of each other I would get something that looks a bit like this. So what I can do is I'll then train a model to predict T from X. So I'll run my train function my use function again. And this gives me an RMSE of 8.8. Don't really know exactly what that means relative to other things until I tried different types of feature combinations, but I can plot the targets over the predictions, and we can see that they're not very well aligned. So this model is just a linear model, predicting this resistance from these from these inputs doesn't really seem to be fit very well to this data. So now, plot the predictions versus the actual values. And we get something like this okay clearly our predictions are the red line the actual values are the blue line, a linear model is not going to fit to this data very well, except for a couple of points here and there. So this last variable, the fruit number it looks like the square root or something might be linearly related to distance to resistance right if you look at this. Okay. If I take some root of this I can probably squish this down to a line in a way that I can't do for the other things. So let's give it a shot. If I plot the resistance versus the fruit number, I get this. So this is a nonlinear curve. So let me try and see if I can make this relationship linear. So the first thing I will try is taking the square. Right, this gives me this. It's a little bit better it's kind of hard to see the difference but this is something of a shallower curve at least so maybe I'm moving in the right direction. So, let me try the fourth power. Okay, this is getting better. Right, so this is clearly more linear than before even though it's not completely linear yet. Not quite there is try the eighth power. This looks like I'm, I'm almost there. It may not be perfect, but it seems like as I increase the power at least up until about eight. This is turning this highly nonlinear relationship into more linear one, one that I can predict using a linear model. So let me try it with the actual data. So I will take. Remember, we took x and we stacked on like a fourth power or something, or actually those the previous data my apologies, we just take x which is the raw numbers then we stack on the eighth power. So now I can train this. And now I get an RMSE that is much lower, right went from 8.8 to 1.5. And this will confirm that I'm doing much better job fitting to this data. Right, so I'm getting a lot more overlap seems like I'm on the right track. Other thing I can do is plot the predictions, first the line seems to be kind of a decent linear relationships there. So let's take a closer look let's zoom in on the first 50 samples. Let's see if we can do any better. Right, so this is doing pretty good but there's still some cases where maybe I could, I could fit better to this line. It seems like higher powers work better. We don't know, like, how much higher we need to go. And at what point I'm going to start getting a diminishing rate of return by increasing the exponent of x. But I can do that is I can visualize how the model does in terms of RMSE versus the exponent so what I can do is just run again run run a bunch of models, applying different powers to that input, and then look at the RMSE. So, here's that first one we saw as just the raw input has an RMSE of 8.8, 4.0, and now we start seeing these one point odd values. So, it seems like the RMSE if you look at this is bottoming out around about 13. And after that, it actually takes up slightly, but not enough to really be meaningful. So it seems like somewhere around 13 appears to be maybe my, my best value for the for the exponent. So, I can confirm this by just plotting the exponent of x versus the RMSE. And here we see this rapid decline, and then at about 13 or so you know it sort of bottoms out and receive this flat line there. Okay, so now finally what I can do is I can try the different exponents of x, 2, 4, 8, 13, and then 16 for good measure, and then use use that data, and then plot predictions. And this seems to be, you know, probably about as good as a predictor of this of this resistance value as I can get. So if I look at the predictions, these are the actual predictions, it doesn't mean a whole lot. But if I plot this against the line, I can see that now I've managed to turn this into a relatively nice linear fit right it's not perfect, as we, as the actual value increases the distance for the prediction also increases. But that is kind of to be expected. So, we've discovered something with this fruit number and the resistance value. Is this a reasonable supposition? Well, what is the fruit number in continuum mechanics the fruit number is denoted FR, and say dimensionless number defined as the ratio of flow inertia to the external field. It's a significant figure used to determine the resistance of a vessel through a liquid, so water or even air. In naval architecture it is explicitly used to determine the resistance of a partially submerged vessel. What's the relationship between fruit number and resistance? Well, I don't know. This paper does if you're interested to find out more, but we did empirically discover that there is a relationship using machine learning techniques. So, this is the point where you can, as a data scientist you can basically discover things without a lot of, you know, topic knowledge. You then want to talk to someone who knows something about that topic in order to figure out how to interpret your results. So, one thing that I will harp on is basically like where you have computer scientists kind of like rediscovering some field. So, like, since I worked in NLP and I actually originally a linguist in like 2013 there's a lot of papers saying like, oh yeah, we rediscovered linguistics using machine learning. It's like there's literally a whole field that's been doing this for like a thousand, thousands of years. So, you're not new. So, periodically, computer scientists will like discover something awesomely new using machine learning and it turns out there's like a whole field out there that has been doing that for a long time. So, it will serve you well to consult with experts. Nonetheless, you can discover relationships like this using machine learning techniques. This has been introduction non-linearity and on Tuesday we will throw all this away in order to introduce an arbitrary method of introducing non-linearity that is neural networks. So, good luck with the assignment and I hope you have a nice weekend and I'll see you on Tuesday. Okay, let's go ahead and get started. So hope you all made a start on the assignment. So it's going to be due a week from Thursday, by default. So hopefully that's going okay. I will have office hours this afternoon. And as I mentioned, I think I'm on the faculty committee and I may have to cancel some office hours depending on what we're doing with that. But currently this week it looks like I'm going to head off. If you are interested in our faculty hiring process, I encourage you to come visit the faculty candidates talks. There's usually going to be on usually a Monday or a Thursday at 11 a.m. We will send out an announcement to the CS list when those are scheduled. Other than that, anybody got any questions, concerns about the assignment? Yeah. You don't specify if they're just separated. Good question. You don't need to do that here. That is probably a good skill to practice. I had this conversation the other day. You don't need to do that for this assignment. We will have one assignment that is explicitly about that. And from that point on, you will be expected to do train test splits. Any other questions? All right. So who's ready to talk about neural networks? So I understand neural networks can be somewhat scary. So I'm going to introduce the neural networks in the easiest way possible. Everybody, please open your textbooks to page one. This is neural networks for babies. I'm going to read your neural networks for babies real quick. This is a ball. This is a neuron. It sends messages throughout your body. Give the neuron input and output, and it can help us learn. Give the ball input and output, and it acts like a neuron. The neuron can have one input and output or many. It starts to sound familiar. Is there a red animal in this picture? The neuron can tell us based on its input. When the neuron has an answer, it sends its own message. Does this animal have eight arms? The neuron could tell us based on its input. When the neuron has an answer, it sends its own message. Two plus two plus two plus two equals eight. Where do the messages go? Neurons talk to each other. They connect in a network. Input neurons look at parts of the picture. Output neurons have answers to the picture. Neurons in between don't see the pictures or give answers. They're hidden. How do the hidden neurons learn to decide? Training data can have correct labels on them. After training, the network has learned to label new pictures. A really big network can solve even harder problems with the help of computers. Now you know neural networks. Thank you. OK, class is dismissed. Now you know neural networks. Please go home and do assignment two. Now, as you can see, there's a whole lot probably missing from that. If you know anything about neural networks, the treatment of convolutions and back prop is not up to the standards of a major AI conference. So I think the part of the end where it says, now you know neural networks is a little bit weak. So for the remainder of this, we will go from linear regression to neural networks, not the baby version. We'll fill in some of the details. All right. So people don't believe me when I say this book is real. So I had to bring this up. Oh, I read my kid repeatedly. Now my daughter is at the point where she vastly prefers my wife to read things. I'm not offended or anything. Because she wants to read the same books over and over again. I don't have to do that, which is fantastic. But now she had a while where this was her favorite book from a week ago. So she was taking it to my wife to have her read it. And my wife is a historian. So she's very smart, but not in computer science. And when she gets to the part of now you know neural networks, no, I don't. All right, let me share my screen. And we'll go through neural networks for adults. All right, so if you remember, hi, this, that. Shoot, I put in the controls. OK, so if you remember from the linear regression lecture, I briefly mentioned that linear models can't solve the XOR problem. So I assume everybody knows what the XOR problem is or what the XOR operation is. Basically, if I have two inputs and then I'm only looking for those cases where one of those things is true. So if I have x is 0, y is 0, 0 is false. Neither of those things is true. So x, x or y is 0. If y is true but x is false, then x regular or y is true. And so is x, x or y is exclusive or. The same is true if the ones in 0s are flipped. But if both of them are true, x and y would be true, hence the one. x or y, one of these is true. But x, x or y is 0 because it's the exclusive or. Only one of the inputs can be true for the x or to be true. So we can also graph this. So in the version below, we have the blue x's where the x or is true. And then the red, the black circles where the x or is false. So if I have 0 and 0, it's false. If I have 1 and 1, it's also false. But in the cases where only one of them is 1 and the other one is 0, then it's true. So you can tell by looking at this already, if I'm graphing like this, this is clearly not a problem that a linear model can solve. There's no way to fit a line to these points when the points are in a square. So that should be pretty evident. So what I'm going to do now is I'm going to basically present the solution to the x or problem done with matrices to motivate how we use the introduction of nonlinearities to solve more complex problems. So basically working backwards, this is the solution to the x or problem. So if I have the following weights, 1, 1, 1, and 1, and then c equals 0 and 1, w equals 1 and negative 2, and then this bias is 0. So let's write these just in numpy form. So let me set those two variables. Now I'll set x to be some matrix that contains the inputs. So the first column can be taken to be x. The second column can be taken to be y. So 0, 0, 0, 1, 1, 0, 1, 1. So write this as a numpy array. We get the following. So now if I take those weights w and then multiply the input matrix by that first layer, I'm going to get the following. So if I do x at w, I get this, 0, 0, 1, 1, 1, 1, and 2, 2. So now I take this bias vector c, add that, and then I get the following there. Now this is going to be this nonlinear step. So so far I've just done a linear operation. The inputs times some weights and then I add some bias. That gets me to this point. This is not the solution to the XOR problem, as we can see. We have 0, negative 1, 1, 0, 1, 0, and 2, 1. It's not clear how that maps to the solution to the XOR problem. Now what I need is some function that's going to allow me to take the input converted to an output that begins at 0, rises to 1, and then drops to 0. So I'm going to apply this nonlinear transformation, where if z is less than 0, it's 0. Otherwise, it's z. In other words, I'm going to want to take the max of 0 and z. So I could write this function f of z that does that. And so then f of z will give me the following output. So now I have 0, 0, 1, 0, 1, 0, and 2, 1. So all that's done is basically taken this part here. And the only thing that's changed is this negative 1 into a 0. The result is that now I have these things mapped into a learnable space. So what's going on here? We have the input being 0 and 0. The output is 0. If the input is 2, the output is 1. And if the input is 1, the output is 0. And we have two cases here. That's why I only see one point there. So the output of these linear steps, effectively, among other things, turned this point here into basically 2, 1. This one here used to be before the nonlinear operation used to be down here. This is 0 and negative 1. So if you see where my cursor is, and then the blue x and then the point in the top right, you can see you fit a line to that. So that was the linearization of the input. But now applying this nonlinear step, this gets me this. So now it's in a continuous space that I can actually learn. So now I multiply that final weight vector, w. So we had input x times big W, which is one set of weights plus a bias. Now I'm taking that, applying some function to it to turn it into this space here. Then I'm applying another set of weights to it. And that gives me the output here, 0, 1, 1, 0, which, if you remember, corresponds to the XOR. So now all in one function, I can effectively take this neural network where I have the weights and biases pre-specified. That will give me the output that is the XOR for those inputs. So if my inputs are 0, 0, 0, 1, 1, 0, and 1, 1, I apply this function, XOR nn, and it gives me 1, 0, 0, 1. So I've done this all in one line. So it's kind of hard to tease apart exactly what the different components are. So I'll write it more legibly. I'll do a version of XOR nn. So we take the hidden weights. First of all, we take this input x. We have specified weights w and bias vector c. So now the hidden weights is going to be, I should probably call this hidden output, I guess, x at w plus c. Then active stands for activation function. We should go into a minute. This is some function f that we defined earlier over that value. And then output is going to be the output of this, so the activation, times another set of weights. And all that will give me the XOR. So by introducing this nonlinear function that allows me to take some inputs that are mapped to a line and then deform that and then multiply some other weights by that will allow me to solve this problem that a linear function was not able to do. Any questions? OK, so this example, of course, involved no training. I basically gave you the solution of what the right weights were and then showed you how once those weights are in place, we can use that to solve this problem. If you want to know how we actually train to solve this particular problem, there is an article here that you can peruse. I recommend not really doing that until we get to the end of the notebook because it's going to assume that you know neural network operations like backpropagation. So I'm going to go ahead and show you neural network operations like backpropagation, which you presumably don't yet because you haven't got there. But once you do, if you're interested in how to actually train a neural network to solve a problem like XOR is very logical, this can show you how to do that. So basically, the takeaway that I hope you have seen is that all neural networks have the same basic form where we have some function applied over some other operation where that operation is just a linear operation. So if we take the inside of this function, wx plus b, this should look exactly like what we were doing in the previous lectures. We just have some affine transformation weights w, shift it by some bias vector b, and that gives me the output. So I have a line, and I'm just trying to take inputs and map it to someplace on that line. So what's f? f is some nonlinear function. OK. These functions are usually called activation functions. And so last time we saw how we could do these fixed nonlinear inputs to introduce nonlinearity when it seems like there's not a linear solution to the problem. So this is somewhat labor intensive in that we first look at all of the data and see that there are some places where there actually appears to be some correlation between some parameter and the output, but that correlation is not linear. So if you remember in lecture four, we had resistance of the vessel on the output. We had resistance of the vessel on the y-axis and this thing called the Froude number on the x-axis. And there was some sort of quadratic looking curve. We're basically trying to figure out what actually is this curve. It looks quadratic. Is it actually quadratic? It turned out that the best answer is probably x to the power of 13 or something. But that took a lot of effort trying different feature functions over those inputs to see what exponent allowed me to convert that input into a linear function or to convert the input into a form where I could apply a linear function and get the output. And that's not going to scale very easily. What if we had some arbitrary way of introducing nonlinearities? So we don't know which nonlinear functions to use, but what we can do is we can pick a form of nonlinear function that has its own parameters or weights that will determine the grade of the nonlinearity it introduced. I know I'm going to deform the data in some nonlinear way. What kind of nonlinearity and how nonlinear am I going to be making that transformation? So we want the parameters of this to control the actual shape of the function. And there are a bunch of possibilities for such functions. I'm sure you can think of any number of nonlinear functions that just satisfy the property of being nonlinear. But we need some desirable properties that I'll illustrate here and then go into why we want those properties later. So first of all, I want to be computationally simple. And one major reason for this is that we're going to be doing this a lot. We don't need to have some wild polynomial being calculated for every data point over 1,000 training epochs or more. We also want initial small weight values. We want the value of that function to be close to linear. And then as the weights, the magnitude of the weights increases, the function becomes increasingly nonlinear. So now you can think of, let's say, on the x-axis, we want it to be closer to linear. And then as the values grow to extremes, we want it to be more nonlinear. We also want the derivative of the function to be computationally simple for similar reasons as we want the function itself to be computationally simple. We also want the magnitude of the derivative to decrease as the weight magnitudes grow. And perhaps we want this to be asymptotically. This is not always true, but it's generally a desirable property. We also want the maximum value of the magnitude of derivative to be limited. So we want the derivative itself to basically have some known maximum value. So let's go about deriving some properties or from these properties what a desirable activation function might look like. So first, let's start with just a linear weighted sum using the familiar formula, xtw. So for some input sample x, we want s, the output of this function, to be small if the magnitudes of the weights in w are near 0. As those magnitudes increases, we want the magnitude of s to also increase. So for example, if we have some data here, just weights that are arbitrarily chosen numbers and then inputs. And then if we change these values here, let's just say let's make a bunch of them much bigger. We can also see the value of s also increases. That should be pretty intuitive because we're still dealing in the world of linear functions right now. So now, if we want this function s, let's try to construct a function where s is the shape of the derivative that we want. So basically, s is going to be the derivative of some function. I'm going to derive the function f, where s is its derivative by first constructing the derivative and then seeing what the interworldly antiderivative of that function is. So first of all, we know a couple of things. If I were to take the negative of s and use that as an exponent, this means that I can take whatever function that is and make it asymptotically decrease to 0. So we want that according to point number 4. So we can use base e. Base e has a lot of nice properties, one of which is that e's derivative is e to the x. And the antiderivative of e to the x is e to the x. So e allows us to deal in natural logarithms, has some very nice properties with regard to differentiation and integration. So first what I'll do here is I will just plot some inputs that are evenly spaced. And then I will plot the e to the negative s. So of course, as we all know, we can see that as the value of s increases, e to the negative s is going to decrease and approach 0. So now remember we're constructing a derivative. We want the maximum value of that to be the limit the maximum value of that derivative. So here what I can do is I can then take, say, divided by 1 plus e to the negative s. And so now unlike this one where the maximum value is basically going to be infinity as I get more and more negative, and the minimum value has a limit, by doing this, I now have basically limits on the minimum and maximum value. And so the maximum value is the important one that we want to control here. And by virtue of that also, we would also want the negative of this to also be limited. Right now we're in a window of an interval of 0 to 1. So we don't really have that problem. But we'll see what happens with that in future. OK, so this does what we want as s grows more and more negative. But we also, as s becomes more positive, we want to bring this function down to 0. So one way I can do that is just by taking 1 minus 1 over. So now this is going to start at close to 1 and eventually decrease to 0. So I want some combination of these. So I basically want something that starts like this and then rises and then falls again. So I want the left half of this function at the top combined with the right half of this function at the bottom. So what if I just multiply them? OK, so that looks pretty good. This is doing what I want. It adds asymptotes to 0 at both extremes. And then the minimum value is limited to 0.25. So the last desirability that we want is we want this to be computationally simple. We take a look at this function. And we can see that there are some common terms. So for example, what I can do here is I'll just create a bunch more points. And then I will recompute this function. So we see we have basically e to the negative s. It's in terms of 1 plus 1 over. So all numbers we know how to deal with. 1 and the e, effectively, with an exponent. OK, great. So now what I can do is I've got this. And I've got this function. Now let me see if it has property, if its antiderivative has properties that are also desirable. One library you can use is this thing called SymPy, symbolic Python. This is the only time we're going to use it in class. But it is kind of fun to play with. Basically, what this allows you to do is define operations and then actually perform differentiation or integration on them. And it will give you the resulting formula. So effectively, you can use this symbolic Python to sort of solve those pre-calculus problems that you've probably done before. So first, need to init printing. Allow me to use Unicode. So I'll define a symbol, s. And so this s symbol will just be the symbol s. So now I can run this .diff function. So what this does, this is just going to differentiate f with respect to the symbols. So in this case, I have some function f of s that I want to differentiate with respect to s. So what I do here now is I define my function, so s to the fourth. And I want to differentiate with respect to which symbol s. And now we can see, if you remember your calculus, we should expect the derivative of s to the fourth to be 4s to the third. And so this will actually put that out for us. OK, so we see the SymPy.integrate function should be pretty evident what that does. This is going to integrate the function with respect to the symbol. So what I'll do now is I will define the function that I defined above, 1 divided by 1 plus e to the negative s. So I'm just writing this in terms, in the SymPy formula, in terms of the symbol I just defined, s. So now I want to integrate the function y times 1 minus y with respect to s. So this is going to give me this, 1 over 1 plus e to the negative s. So if f of s equals this, then the derivative of f of s is f of s times 1 minus f of s. So we are now just derived at the common sigmoid function using neural networks. So remembering that s above was defined as just a linear function xtw, then we have some function x, some function of x parameterized by w, equals 1 over 1 plus e to the negative xtw. And so remember what this is. This is just going to be the output of that linear operation where we have our weights and then make some sort of prediction by multiplying the input by those. Questions about this so far? So let me define some helper functions. So I'll just define f of s. And then I will define its derivative, so df. And then I'll plot the function versus the value of s and the derivative of the function versus the value of s. And so we get this. So this function s in blue, we can see that it rises asymptotically to 1, whereas its derivative is that function that we saw before, and it caps out at 2.5 and has asymptotes at 0 at both extremes. So this is called the sigmoid function because it looks a bit like an s. And it is bounded between 0 and 1. And so now we can use this function because it has these nice properties that we have identified in neural network operations. So first thing we're going to do before we get to the neural network of hidden layers is to just apply SGD to fit the sigmoid function to some data. So we're still working more or less in the world of linear regression. It's just we have this final step of applying the sigmoid function so that we have some sort of nonlinear output. All right, so what we're going to do is we're going to find weight values that satisfies the sum of squared errors in the output of this function. So what I will do here is I'll just define, again, some points. I'll take this function here and just add some noise. So basically, this is I'll take t times or t equals x times 0.1 plus some randomly sampled noise from a uniform distribution. And it's going to give me some data that looks like this. This looks like I could reasonably fit a line to this data. It might not be the best fit in the world, I could do it. But it might be nice if that line had a bit of a wiggle in it. Maybe that would fit to the data a little bit better than just a straight line. So if you think about what this data represents, the values in x are, at this point, those are just inputs. They don't actually represent any real data points. The values in t, though, are our targeted values. And these are derived, in this case, by some known function applied to x where we've applied some random noise to it. So those targets are not going to be neatly identifiable. They're not deterministically identifiable from x. But you should be able to still fit a curve to it. So here's a training function where I will put in my inputs, my targets, my learning rate, and some epochs and train for a certain number of steps. So what I'll do here is I'll just train this for 100 epochs and then plot the results. So here we go. After training, it ends up with these weights, in this case, negative 1.7, 0.4. And then if we apply f here, which is our sigmoid function, this is going to give me this output for the sample according to the inputs and then apply times the weights and then apply some nonlinear function. So we get this. And we can see that it's probably a decent fit. You can see that it's nonlinear. And it does fit to this data decently well. I'm not sure that it's necessarily a better fit than the linear operation. It's hard to say. But you can see that also the nonlinearity being applied is quite slight. This is not hugely different from a line. So let me create some other data. So different distribution, different function. So 1 plus negative x times 0.1. So now we get this. Probably mostly linear, but again, could be fit to with a nonlinear function. Let me try this. And that fits pretty well as well. So seems like I got a decent way of taking my sigmoid function, computing an output according to a linear operation, applying the sigmoid function to that. And I can use that to fit to what might be some somewhat nonlinear data. So the question now is, if I have this data, how could I fit this function to that data? This is nonlinear. I would want my sigmoid function to be able to capture the nonlinearity that is obviously present in this data. What I've got is the ability to train weights. And once those weights are trained, take those inputs times the weights, apply my sigmoid function, and then fit to some nonlinear data. So if I try it with this data in this curve here, what I end up with is here are the weights that it's trained. And the result is that it gives me this. That didn't seem to work very well. Let me try it again. Compute some more data. So it fits decently well on one part of the curve, but not the rest. So we can see here, when x is positive, it seems to be kind of fitting to that OK. But when x is negative, it's just more or less a line. So there seems to be some sort of inflection point here around x equals 0. And we want to find out what kinds of weights will make the sigmoid function go down from negative infinity to 0 and then rise again. This is what will be needed to actually fit to this data here. Whereas here, we're only kind of getting one side of the equation, if at all. So we don't know what those weights are. And they're not really easy to find, because the only weights that I've got here are two weights that effectively, the first weight here, that's a bias, as before. So how much am I shifting up and down the y-axis? And then the second weight here is, what do I apply to the linear operation, or what weight do I apply in the linear operation before I apply the nonlinearity? So I'm not really doing anything that affects the slope of this nonlinearity in any real way. All I'm doing is I've defined the sigmoid function, and I apply that over the output of the weights times the input. So we could try maybe using two of these functions and adding them together. So what if there were one set of weights that caused f of s to decrease until around 0 and then remain roughly flat here, and then another set of weights, we'll call those v, such that if I take x times v and apply f over that, it would be roughly flat and then start to increase. So we'd expect there now to be two sets of weights. We'll call them w and v. So now we're talking about the world of multilayered neural networks. One layer is going to have, in this case, two units that output f of x times w, each with their own w. And the second layer is going to have a single linear unit with its own w. So does everybody get the motivation for having these two different sets of weights? OK. All right, so now let's talk about linear models, as we're familiar with, as neural networks. So what I'll do here, and I cannot guarantee that we're going to get through the entirety of this notebook today, because it is fairly long and there's quite a bit of math. But what I'll do, like usual, is I'll present the mathematics first, go through, effectively, how we are deriving the different operations that we're going to be using in the construction of the neural network. And then at the end, I'll have the Python version that translates the mathematics into code. And demonstrates how you would actually write these operations. So remember how we do just linear modeling. We have inputs x, targets t. And then we have, for every sample k, we want to find the weights k that minimizes the squared error in the k-th output. So we'll say for x sub k, I want to find the weight vector w sub k that minimizes the output between t sub k or between the prediction y sub k and the actual output t sub k. Then we'll use that to make predictions. So what we'll do to make this go faster is we'll take all of these weight vectors w sub k. We'll collect them as columns in a big weight matrix w. Some, the x with the total above it, I'll use to denote x with the constant 1 column. So remember, we always add this bias column so that we have values against which we can train the bias weights. The target value for the k-th output for the n-th sample is going to be t sub nk. So remember how we set this up in a linear model. If I have n samples, number of things that I've got, each with d dimensions, number of things I've measured about that sample. And then I have k things that I want to predict about that. So this could be just a single value. Could be multiple values. So if I have the first sample, and I'm trying to predict the second thing about that in the output, this would be, assuming that we skipped the first one, actually, the second one would be indexed at 0. This would be t sub 1 comma 2 or something like that. So we're using this to calculate the error. So we have e of w. And so what I'm going to do is I'm going to take for all samples, for all outputs, so sum of all n over the sum of all k. I'm going to take that output for that sample minus the prediction. And then I'm going to square it. I need to sum this for all combinations. So now I'm looking for, effectively, the value of w that will allow me to minimize this function. Now w can also be calculated as, remember, if we rewrite these as matrix operations, as x sub t, let me increase the font size a little bit, x total tx. Then I transpose this, or take the inverse of this. And then x total transpose times t. So we can compare this to solving for the value of w in notebook 3. So what's the contents of all these matrices? Remember, w is going to be w associated with every output and every input. So there are k things that I'm trying to predict, and then d dimensions to every input. That's going to have a weight value that's correlated with each of them. In a linear model, what I can do is I can actually look at my weights and decide what was most important for predicting the output in a neural network that becomes less easy because of these hidden layers. So if w looks like this, then my prediction y is going to be biased x times w. What are the shapes of these things? So I have n samples, d dimensions, add 1 for the bias. So x tilde is n by d plus 1. w has to be d plus 1 by k in order to multiply with this, of course. And so if I multiply these two things together, then y must be an n by k matrix. And so this should make sense because, the things that are represented in y, those are going to be the k predictions for each of the n samples. So if I have 100 samples and I'm predicting two things about, I should have 200 individual numbers represented as 100 rows in two columns. So for every element of that output matrix y, it is going to be equal to the nth sample with the bias times the kth weight vector. And this can be drawn kind of like this. So what's going on here? I have my different inputs, x0 through xd. And I'm going to multiply each of those by the associated weight, and then sum those. And that's going to give me the associated output. So now, if I want to add nonlinear combinations of inputs, what I'm trying to do is I'm trying to transform x into some function. We'll call it phi of x. So for example, if phi of x is phi over this big weight matrix, and if I'm trying to add nonlinear combinations, what I might do is I might raise this to a power or something. So this is the same thing as introducing nonlinear features in the inputs. So now what you can think of that is that instead of raising it to a power and stacking a bunch of those things together, I'm going to have some arbitrary function that is applied to this. And then I'm just going to replace the output x by phi. And so now I'll use phi to represent phi of x. And so therefore, phi of phi sub n is going to be this function phi whatever it is over x sub n. So I'm searching for a function. But we have learned now that functions can also just be represented as a linear operation with a set of weights. So if I have f of x parameterized by w, my goal is try to solve for w using some algorithm like SGD that allows me to minimize the error between my predictions and my outputs. So what I'm doing here, this is the part that's going to fit into the interior of the neural network. And what I'm going to do is I'm going to try and predict the weights that give me the output and then minimize the prediction error when there are multiple transformations being performed at every step. So when we talk about, I should have pasted this again down there. So when I say all neural networks have the same basic shape, which I did way up here, I think. So I say all neural networks have the same basic form. What we can now do is we can expect this function to kind of be stacked on top of each other. So I'll take this output. I'll perform another operation over it and maybe another operation over that. And every layer in this neural network represents one more function being applied. So all of these things can now just be nested. Different functions are the same? Different functions, yeah. So remember, a function here I'm just talking about f of some x parameterized by w, which means that the w could be different each time. So when we talk about neural networks, I knew this was going to come in useful at some point. So you've seen this diagram that you've seen neural networks written like this. So every layer here that is every column of red dots is a function. And so each one of these is parameterized by a different set of weights, meaning each one of these is a different function. And our goal is now we're just trying to solve for those weights that parameterize each of these individual functions at the same time. It's a function of a function of a function of however many layers you have. Yeah. So are we implementing these functions to the basic function that we have and then we're getting the output out of that? Effectively, yes. So I'll come to the mathematics in a moment. But you can kind of think of it like this. So if I have just a linear method, a linear model, it's got one input and or a known input and just like a known set of outputs, it's a linear transformation between them. I can compute the error and then use SGD to optimize the weights that are going to minimize. Let me insert a hidden layer in the middle of that. If we just look at those first two layers, and let's just disregard the activation function for a moment, it would be just like a linear operation in that if I knew what the output of that second layer was, I could use that to minimize the error between the predictions of the outputs. But the problem is the targets that I have, I have a two-layer neural network actually correspond to what come out of that second layer, which I haven't even touched yet. So the input is going to be transformed by some function into the hidden layer. But what that number is not clearly correlated with the actual output is because there's another function that must be executed over this intermediate number to get the output. And I don't know what that function is just yet. So the output we get is not the actual one. Right. The output you get can just be considered some sort of scalar that is not interpretable. That's why I talk about these as hidden layers. So if I get the output, I know what the units of the output would be, what it's supposed to represent. Is it a class? Is it a miles per hour or something like that? But that hidden layer in there is going to give me some scalar outputs. It doesn't really have units or anything like that because it's kind of combined multiple channels that input data on its way to getting an output. But I haven't got the output yet. So I can't assign any meaning to it. Would you say it would be a bit more optimized for the original data that we had instead of the way of getting it to be? Sure. It can be. So the idea is, and we are getting a little ahead of ourselves, but this is interesting. When you've correctly optimized a neural network for a task, you can actually get very useful representations out of the interior. So for example, I think I mentioned in the first class, a lot of my research involves these things called embeddings, which are basically continuous representations of classification labels. And so you can actually take these embeddings that are hidden layer representations and use them. They're just numerical values or numerical vectors. By themselves, they're not interpretable, but you can use them for other tasks. And they actually preserve a lot of information. So basically, these hidden layer representations preserve the information that is necessary when the network is well-trained. But a human would have a hell of a time trying to figure out what the actual meaning is. You can do things like cosine similarity to figure out where are the clusters in this thing, is it similar to, but it's not something you can say, like, OK, this number represents a bird, this number represents a feather, this number represents a microphone. Right. OK, so we have some function. Sorry, any other questions? So we can easily spend two days on this notebook, so that would be fine. I think that's how much time I actually built into this. OK, so now let's assume we've got some function phi that is going to be an operation over x. So now what I want to do is if phi times w is going to give me my output, now I want to do the derivation that's going to minimize this error. So whenever I multiply phi times these weights and then square the sum for all my samples, that's what I want to minimize. So I want to find, so you can see now that I end up with w is going to be equal to phi times phi raised to negative 1 times phi times t. So now I can use it in the same formula that I had before, where I have y equals phi tilde times w. So now I'm focusing on if w is this output layer, this thing is actually going to produce the prediction, I want things that are going to be useful going into w, I want things that go into w to be useful to predict that output. But the trick is there's kind of the separation between the inputs and the outputs. So the x is the input. It goes into some weights, we'll call them v. That produces phi. I want phi to be useful when multiplied by w for predicting the output. So now I have these two functions that I'm trying to optimize at the same time. But there's no generic way of arbitrarily separating those. It allows me to do this over a large number of samples at scale. So if I know that this is a nonlinear function, I want to introduce some arbitrary way of having nonlinearities and nonlinear operations performed over my data so I have activation functions. So get to that, details on that in a moment. So if we take a look at this, this is what I just discussed. I have the inputs. Something happens to them. We'll call that thing phi. And then whatever happens to them, I can multiply by weights w to get my outputs. And so this yellow box is the black box of a neural network because I don't know the nature of this transformation. So this is passed through some nonlinear function, multiply by w to get the output y. And now I'm just trying to figure out what the hell goes in this yellow box. So focus on serving its purpose. Can we use training data to find out? Training data can have correct labels on it. So if I know what the correct output is, maybe I can use the training data to actually optimize the weights. So the textbook, so to speak, talks about vision. We'll talk about convolutional networks in lecture 13 or something. Right now, it is doing regression. So it's all still numbers. But the same principle applies. I can use the training data to figure out what needs to go in phi. So we've now just entered the world of neural networks where 5x is going to be the output of some layer of adaptive units. So 5x, we'll call it h. H is typically what we use for the activation function. So I've used a bunch of different terminology here. F of s, phi of x, h of x, all referring to the same thing. So because the neural network is a universal function approximator, it's not really useful to talk about the function in abstract. You want to know what function I'm talking about. So we'll use h to represent the activation function. In specific, the activation function is this nonlinear function that's applied over the output of a hidden layer. So this sort of looks like this. So now, before, we had something that looked like this side of the equation with the x's going straight into these blue nodes here and producing outputs. So the only thing that's different here is that instead of x going directly into the blue nodes, it's going into these yellow nodes, where they have weights v. These x's are multiplied by the v's. And then before the output is produced instead of a sum, you have this activation function h. So let's say x goes into the first hidden layer. It's multiplied by weights v, which at start is just going to be arbitrarily initialized. It gives you some number. You apply the function h over that number. It gives you a different number. We'll call that z. And then you have a bunch of different z's. And z's go into the next layer, which have weights w in them, that also maybe randomly initialize different numbers. The z's are multiplied by these w's. And then each z, you take the linear sum of z times all the w's, that gives you your output. So now, I'm still in the world of trying to optimize weights. I just now have two separate weights, two sets of weights, v and w to try and optimize. So the dimensionality of each step will be as follows. So n will be the number of samples. d is the number of things you measure about n. So x tilde will be x times x by d plus 1. v is going to be d plus 1, because it's multiplied by this, times some other dimensionality m. And I'll just specify how many m's I want to get out of that first layer. So now, z tilde. If I have x times v is equal to z, this means that it should be of dimensionality n by m, because we have the d plus 1's, those should cancel out. But this also has to go into another matrix operation. So I need to append a bias vector onto it. So now, this will be z tilde is going to be z with the bias, which is going to be a dimensionality n by m plus 1. So w naturally would have to be how many things are going to come in, m plus 1. How many outputs do I want? k. So this should be m plus 1 by k. And so z tilde times w, the m plus 1's should cancel out. And so we end up with an output that is of size n by k. So the final operation looks something like this. So if z tilde is h applied over x tilde v, so here's h. This is x tilde with the bias times v. That gives me z. Then I append my bias again to z. This gives me z tilde. And so then z tilde times w is equal to y, which means that I can write this all as a single function. So here we have y is equal to h of x tilde v. I apply the bias again to that, multiply that by w. So the two layers in this case are called the hidden layer and the output layer. In larger neural networks, the last layer, of course, is always the output layer. And then anything besides the input layer are going to be hidden layers. So we talk about the last hidden layer or the first hidden layer. And we can talk about the things that are represented at different points in the neural network. h is the activation functions for units in the hidden layer. If you have multiple hidden layers, you may have different activation functions. And we'll talk about some of the different activation functions. And so we'll be doing gradient descent in the squared error. So we want an h that has some of those nice properties we outlined before. We want its derivative to not grow out of control as v grows and want that derivative to be easy to calculate. So let's try a couple of functions. What about a polynomial? We can plot a polynomial and its derivative to see if it satisfies the properties that we want. We have h given by this, and then its derivative given by this. We can plot the derivative with the dashed line and h with the solid line. So does this look like, take a look at this, do you think this is a well-behaved derivative? No. It's kind of the opposite of well-behaved derivative, right? Well, what we want is we want things that as the magnitudes grow, the value doesn't grow out of control. This is doing exactly that. It's a polynomial function. So we'd expect that. So we don't want this because remember, the gradient descent procedure is going to take steps that are of a size proportional to the derivative. So this derivative gets huge. And so it's a high positive as a increases, and it's a high negative as a decreases. And so the gradient descent, it could be very unstable. So what we want, we don't want to be skipping back and forth across that global minimum again. And so if the gradient grows out of control, we risk that. We also have things like the exploding gradient problem. This can be solved. The exploding gradient problem can be solved through relatively simple techniques. Bigger issues, this thing called the vanishing gradient problem that we'll get to. But for the moment, we don't want these derivatives to have properties like this. So two common choices for functions with well-behaved derivatives are the sigmoid function, as we saw before, right? 1 over 1 plus e to the negative a. And then this thing called the tanh function. So the tanh function is given by this. Differences are the, well, do anybody know the difference just off the top of your head between the sigmoid and the tanh function? What are the bounds on the sigmoid function? It's bounded at 0 and 1. What are the bounds of the tanh function? Negative 1 and 1. So the sigmoid is an asymmetric function that is bounded at 0 and 1. And the tanh is an asymmetric function for its bounds. So let's work out their derivatives. So we'll work out the derivatives. And then I'll give them to you and we'll plot them. So these are the two functions. So h1 is sigmoid, h2 is tanh. And then we have the derivatives of both of them. So now let's take a look at these. So the blue lines, that was the sigmoid function that we plotted earlier. We see the value top out at 2.5. We also see those nice bounds at 0 and negative 1. Then there's the tanh function in red. So similar. It looks like a very similar function it is. It has the same overall shape, except it's bounded at negative 1 instead of 0. And therefore, we have a steeper derivative here around x equals 0. And so we actually have a maximum value of this at 1. But both of these functions will still satisfy those nice properties of derivatives that we want earlier. So it's got a maximum bound. It doesn't grow out of control as the magnitude increases. So both of these are friendly functions. So these derivatives are computationally simple. So are there anti-derivatives. They decrease in magnitude as the weight magnitude grows. In this case, both of them do so asymptotically. And they have limited maximum values. You satisfy a lot of nice properties. So anyone know what the sigmoid function can be used for? Say I have some arbitrary scalar number. The sigmoid function will squish this into a range between what are the bounds of the sigmoid function again? 0 and 1. So if I take a 10, the number 10, take the sigmoid of 10, it's going to give me a value that's pretty close to 1. So sigmoids are nice for turning things into binary probabilities, for example. And so you can use sigmoids for, say, binary classification tasks. Tanh function is useful, particularly in hidden layers, because what it does is it will actually preserve some negative values. These may be useful, because there may be things that are inversely correlated with some input to that layer, be it a hidden layer or an output layer. And you may actually want to preserve that. So generally, you will see tanh functions being used in the interior of neural networks. It's not the only activation function, of course. There are plenty more that we'll go into later. Sigmoid functions are useful for things like binary classification tasks and more often will be seen in output layers. All right, questions? All right, so now this is like the gnarliest part of this, I think. Training by gradient descent. So remember the intuition behind gradient descent. If I assume that there is a high dimensional derivative, I'm trying to descend that and trying to find where it is closest to 0. So the gradient is going to be defined by the error and try to minimize the error. So I want to get to a point where I am so close to the true solution that when I move along the gradient, the error update is being so small that I can be said to have arrived at something arbitrarily close to the solution. So remember that we use the mean squared error in this case. So between each target value t sub nk and the output predicted value y sub nk, I'm just going to take the difference. I'm just going to square that. And because every target and every output is going to be defined for a given sample and a given measurable output that I'm interested in, I'm going to sum this over all n, all k, and then average it for the sizes of n and k. So now e is no longer a linear function in the weights. So this means that we can't set the derivative equal to 0 and solve for the parameters like we did before. So instead, what we can still do is we can do gradient descent in e by making these small changes to those individual weights in v and w in the negative gradient direction. So it's not I can't use linear algebra to solve for the inverse function anymore because e is no longer a linear function. But the intuition behind gradient descent trying to find some global minimum or as close to it in this high dimensional derivative as I can still holds. So I'm sort of doing this a little bit blindly in that I don't know where I'm going, but I know where I've been. And so I'm just going to look where I've been and walk backwards, go down the slope. So the update is more or less the same is that if I want to update this value for v sub jm, I'm going to take whatever previous value it was minus some learning rate rho times the derivative. And this is the same for v or w. So often for this, I'm going to have these two learning rates, rho h and rho o be different. But often they're presented as the same. There are cases where you can actually have different learning rates in different layers. And this can help convergence. Most of the major packages don't allow you to do that by default. You have to do some kind of pie-shorter or TensorFlow hacking to get that to work. In most cases, a constant learning rate or at least a single learning rate across all layers is what the package will give you by default. But there are cases where it may be desirable to have different learning rates. So we want to use this to find the global optimum. That is the values of v and w that minimize the mean squared error. So for this take a more simplified view. I'm not sure that this looks very simple to you with all the errors, but this is the simplified view. So we have this full picture. We want to focus on modifying a single way. Let's say v1,1, this one here. This is going to be based on a single error between the target t1 and the output y1. So for the moment, to make things a little bit cleaner, let's drop the subscripts. Let's just focus on the single hidden unit and the output unit that is relevant to this computation. So this input x, whatever it is, goes into v. And then this gets multiplied by all those elements in the matrix v. Those get summed. We then have some value that then gets turned into putting the activation function, which then deforms that by some non-linearity. This gives us z. z is then multiplied by w. And I take the sum. This gives me the target, the output. And I just want to measure the difference between the prediction y and the target t. So the forward calculation, this is simplified. So I'm going to ignore the bias and all of the terms. Right now, we're just looking at single terms being multiplied. So no need to worry about the matrix multiplication. So if y equals w times h of v times x, in other words, y equals w times z, z equals h of a. And a equals v times x. So since e is equal to t minus y squared, dE dv should be d of t minus y squared dv. So chain rule to the rescue here. So basically, I'm trying to represent t minus y in terms of things that I've already calculated here. So the error is going to be d of t minus y squared with respect to dy times dy dz times dz da times dadv. Because each of these terms, like y is represented in terms of z, z is represented in terms of a, and a is represented in terms of b. So again, if this looks intimidating, no fear. This is presented for your interest if you are interested in how the mathematics works. When it comes to the code, all this will be done as matrices. Basically, I'm going to be doing this for the individual elements, show how it's done as matrices, and then show the code, which most likely won't happen until Thursday. But we'll get there. All right, so now if I take the derivatives of all of these, I end up with something like this. So 2 times t minus y times negative w times dha da times x. So what this term here, of course, is this will depend on what h is, which function I'm using. So for the moment, we can assume that if that h is tan h, the derivative of tan h happens to be 1 minus tan h squared. So this page here at Stack Exchange will explain why, if you care to go into that. So now we have a formula that I can actually plug in for this. So remember that z equals h of a, so I can rewrite this as z. So 1 minus z squared can be written as 1 minus the square tan h of the input, because the tan h is h. And so now the entire thing ddv can be reduced to negative 2 t minus y times w times 1 minus z squared times x. So let's break each of these terms down. This is the derivative of the error, this first thing here. So t minus y, that is the error between the individual sample. w, that's the weight that's being updated. 1 minus z squared, that's the derivative of the activation function. And x is the input. So with the exception of the activation function here, this is the same as we were doing linear regression. The three components I need are the error, the weight, and the input. The only thing here that I'm adding that is new is this activation function, because there is this nonlinear function that I've performed over the input. All right, questions about that? What was 1 minus z squared? We go back to this. So we broke down ddv in terms of all of these things above. So these are the individual elements of the weight multiplication. We can use the chain rule to break it down into each of those, basically the multiple of these derivatives. We can easily take the derivatives of all terms except for dz, because we don't really know. We know that z equals h of a, but if we don't know what h is, we can't actually turn this into a formula. But there are a limited number of things we can use for h. It's got to be an activation function that has one and has a set of nice properties. The only one we've talked about so far in any real depth is tan h. So we will assume that h is the tan h function. The derivative of the tan h function is 1 minus the tan h squared. It is. It's a fact. And because if h of a equals z, then z is effectively tan h of, let's say, x, the input. So this 1 minus z squared can also be thought of as 1 minus tan h squared of x. Everybody cool with the rest of? So far so good. This is more or less just a linear operation with an added nonlinear function and its derivative. That seems intuitive. OK, let's add another output. OK, now we look like this. So same thing. Same things are happening. I've got a single value being multiplied by some weights v. Apply my function h. It gives me z. OK, now z is actually going two places. So there's a weight w1, and there's another weight w2. So just like in linear functions, I want to predict two things about the output. I have two columns in my final weight matrix. So that's pretty straightforward. So this is going to be that first value. This is going to be the second value, and they're going to give me different output values depending on what those weights are. These have some meaning that I can use to compare to the prediction. And so now I'll get a different error for each one. So now things get a little bit hairier. So chain rule again. So what's new here now is that instead of just having t minus y squared, I've got two things. And I've got a sum. So I need to take the sum of a derivative is equal to the derivative of a sum. And so what I'm going to do is I'm now going to sum these errors, these squared errors. And now this has to be taken with respect to v. So this works out like before. The only thing that's different here is now the numerator of this equation. But now we can distribute the, or sorry, I'm going to try. I now need to compute this with respect to the different y's. d y1 and d y2. So if I compute this with respect to d y1, I can then put this other term out here, d y1 with respect to d z. This is the equivalent to what's going on here in the single output version. But now I have two y's because I have two outputs. And so now I need to compute with respect to both of them. OK, so now I can have d y1 with respect to d z and d y2 with respect to d z inside the parentheses. I can compute these derivatives similarly. So this works out OK. d a d v is just x. d z d a can be again be written as d h a d a. And so now I'll put in the derivative of the tan h function here. So now we can think of the errors calculated in those output units as being sent backwards to the units in the previous layer. So if we'll call these delta values, then the derivative expressions will refer to those as delta rules. And those delta values are back propagated, so sent backwards into the previous layer. So this is that back propagation you might have heard of when you were discussing neural networks and is notably not addressed in neural networks for babies. So that's basically the small error value that is being used to update the weights. So just like in linear regression with SGD, we use that error value to update the weights. The thing here is that in those weights w, the value that is being used to optimize w is dependent on the value of v. So if v is very, very wrong, w might also be very, very wrong. And so the output could be very, very wrong. So if I get an output, if my target is here, let's pretend an arbitrary space, and my output is here, and they're way, way different, I don't really know, is this because the output weights were wrong or it's because the hidden weights were wrong. I have to allow for both of them. And so I have to assume that there might be something that's wrong about v that's making the prediction when z is multiplied by w also wrong. So this error backpropagation should be used not just to optimize the weights of the output layer, but also to optimize the weights in the hidden layers so that when an input flows through the hidden layer into the output layer, it's going to get me a better result. So intuition behind backpropagation, at least everyone clear on that, you can think of like, if you're a play like one of those pachinko machines, you put like a coin into the top and it bounces down to the bottom, you can think of like those, the bottom slots, those are your targets, right? You know where things want, you want things to go. And let's say you want a coin of a certain size to end up in a certain position. The weights then would be something like the sizes of the pegs in the pachinko machine. So you want to increase, decrease the size so that your input of a certain value is going to go the right way through the pachinko machine. The non-linearity could be something like you replace the peg with a spinner or something like that. So it's effectively just sort of this big machine where you want to, you know the input, you know where you want it to go. You want to just mess around with the interior of this machine until all your inputs get where you want them to go as closely as possible. It's just instead of wooden rods, it's numbers. All right, let me see how much, what we got left. OK, OK, that's going to be like bad. OK, maybe I'll get as far as the full version of that problem. I'm a little behind. OK, so remember these derivatives. If you don't remember these derivatives, you can forget these derivatives and just come back to them before class on Thursday. So we're basically differentiating with respect to two things, W1, which is going to be an element of this output layer, and V, which is going to be an element of the hidden layer, currently the only element of the hidden layer in this simplified example. So DEDV is the function that we see up here. It's got these components. It's the summed derivative of the squared error times the derivative of the activation function times the input. And then DEDW is much simpler. There's only one output. So there's one error. And then the input to this layer is Z. What happened? Yeah. So that's much more straightforward. So the hidden layer, with respect to V, that's much more complicated because you have multiple outputs flowing backwards into this single unit. And you have errors for both of them. Yes? Is it not required to derivative with respect to W2? Well, it's the same thing here. So the formula is the same. The only thing is that the value in this, the specific numerical value in this would be different. So this could just be cloned for W2. So now, if you go back and look at those update rules that we had earlier up here, now let's just plug those back in down here and see what falls out. So the update rules for the delta. So new value of W is W minus DEDW. Same for W2. And then so that's going to be W plus the learning rate times the error times the input. So this delta is going to be rewritten like this. So now V is going to be previous value of V minus DEDV. What is DEDV? We gave it up here. So now this is going to be the learning rate times the different errors times the individual weights times the derivative of the activation function times the input. One question you may be asking, where did this negative 2 go? At this point, this is just a constant. So if I multiply this by some constant learning rate, we can just assume that this is going to be factored into that. So if you're worried about this, don't. So now delta H is going to be delta O delta 1 O times W1 plus delta 2 O times W2 times 1 minus z squared. So this here, how do we do this? The reason we have this here is because of the update rule for W, we're just taking this error to be one of these delta values. And so now if there are two outputs, there are two delta values. However, this function here for the V update accounts for an arbitrary number of delta values. All I need to know is which one is which and just slot it in the right place and you add all of them. So we have two minutes left, so I'm going to stop here. If there are any questions, let me know. I will have office hours starting when I get back to my office. Thank you. I will see the rest of you on Thursday. Okay, let's get started. Sorry I'm running late. I was late bringing your faculty candidates back from lunch. All right, so today I'm going to finish up the introduction to neural networks notebook and then time permitting I'll start the next one which I think is atom optimizer if I remember correctly. So, what I'll do, I'll just recap kind of gradient descent where we left off last time, and then continue to the end with demonstration, and the code. So, assignment one is due a week from today for most of you. So I hope you all have made progress I had some people come to office hours. So clearly, some of you are working on it. So I just recommend that, you know, everybody take advantage of office hours if you need help, because things are only going to get more complicated from here. So if there are any other any questions before I get started, let me share the screen to zoom. All right, no questions, but the material or general class procedures. All right, so let's get going then. Here it is. Alright, so just recall how we train my gradient descent is pretty much the same neural networks as training in linear operations, in that we are taking the error between some prediction y and some target T, and then you square it. And then you sum that squared error for all of our samples. So the only trick here is that we may have some n number of samples for each sample we want to predict some k number of outputs. Right. So now you're having a many to many correspondence. And so for every sample for every output, you need to compute the equivalent error between your prediction of that output for that sample to the target value. So just to clarify a point, the targets, this is just a question that came up in office hours. The targets are the things that's known, right, known data that you're trying to optimize the model to predict. So target T, y is the output of a function. So if we assume that there is some function f of x parameterized by w, x is your input, that function outputs y, your task is then to solve for w. The trick with neural networks is that there may be a nonlinear function. And your goal is to optimize the weights for a nonlinear function. And we do that by basically performing linear operations and then taking some nonlinear function and taking the output linear operation as the input to the nonlinear function. The nonlinear function is an activation function. The one we're working with currently is tan h. There are others that I will go to go into in within a few lectures. But for the moment, we can assume that the tan h function is the nonlinear function of choice. And it has some nice properties such as being linear near zero and less linear as the inputs go to more extreme values. So the trick, because the error is not a linear function of the parameters, we have to set the derivative, we cannot set the derivative equal to zero and solve for them. But you can do gradient descent the same way, right? So the only thing is you just have to update the equivalent weights for every input to the said weight matrix, be that v or w, for everything that that weight matrix is intended to output. So as we saw the update, the delta rule for v, the hidden layer weights is pretty straightforward. So if we break down what happens here with the DEDV, we end up with something that's fairly familiar. So for example, this is the two outputs, if you look at one output here, the DEDV is simply the partial derivative of the squared error, right, with respect to v. We then can use the chain rule to break down what everything in this term actually means in terms of other values in our equation. So just recall that z is at this point, the output of the hidden layer, there's a scalar value that's not intended to be inherently meaningful. But when the network is optimized, the output of this hidden layer should be useful information for the computation in the subsequent layer, which is in this case for a two layer neural network, the output layer. What is z is actually the is actually h of a, where a is the scalar value that comes out of the linear operation. We perform a nonlinear operation, in this case, tan h over it. So therefore, we're going to use the partial derivative of tan h or the derivative of tan h to compute the partial derivative, which is going to be 1 minus tan h squared. So similarly, we have a constant, we have the weights, we have the partial derivative of the activation function, we have the input. When we add another output, the only real difference is that we now have to sum the squared errors for each output. So this is just a recap of what we did last time. So what we end up with is that the delta rule for the hidden layer, where there are two outputs of this hidden layer, we need to sum that error for each output. And in this case, if we were only trying to optimize the weights for a single output node, so the function remains the same. So we have this error term, z, which is the input to that layer, and then a constant. So now the delta rules involving the delta rules involving the deltas look like this, we have the learning rate for the output layer, the error term, the input. The error term can also be written delta sub 1, 0. And so then for the hidden layer, the error term is going to be the sum of the errors for each of the individual outputs times those respective weights. Okay, so that was just a recap of what we talked about Tuesday. Any questions after that refresher? All right. Now the full version of backpropagation is going to look something like this. So take just look at a single weight here. Let's take v sub d1, which I think this is actually d0 here. This one here looks like maybe a typo. So if we're looking at just how we update this single weight given some set of outputs that depend on the output of this node, right, this should look familiar compared to the one above here. The only difference is we're now dealing with an arbitrary number of outputs. So what we do here is this error term is going to be output y1 minus the equivalent target, and then output y2 minus t2, and output yk minus tk. And this should be k's, they're not 2's. I didn't make this figure. I'll try and see if I can Photoshop it to be correct. So what we do then is these things will need to be summed. And then we will take, so if you look at the function here, effectively what we're doing here is now this generalizes to whatever number of outputs we actually have. So this is t1 minus y1 times w1 plus t2 minus y2 times w2, and so on and so on until we get to tk minus yk times wk, right? So we only have to use the derivative of the activation function once because we're simply looking at how to update one weight in one node. So here this h is going to be the activation function produces z. And so all of these then will need to be summed, multiplied by 1 minus the activation function, or the activation, derivative of the activation function squared times the input to this node. So now the only real thing that I need to do here that is different with multiple nodes is I'm going to be doing this at scale for every single one of these. So as you can imagine, this starts to grow somewhat out of control if I were to try to do it element-wise using these update rules. So what we're going to do then is we're simply going to apply matrix multiplication and matrix operations to do this. So here we have the update rule for an individual weight, v sub jm. I'm going to take the current value of that minus the learning rate times the error derivative with respect to that weight. Same thing for the output layer weights, and we saw how these terms here will break down into the full formula depending on whether I'm looking at a hidden layer weight whose update rule depends on what happens in the output layer, or if I'm simply looking at the output layer weight where the only real thing that I have to calculate is the error in that output term. So the reason that neural networks get more complicated is really entirely because of these hidden layers. So what this means then is that you can represent a purely linear function as a neural network that doesn't have any hidden layers. So if I have, if instead of this, if these yellow nodes weren't here and are going straight from input to output, it would be the same as a linear function, and I'm simply just trying to optimize the weights here because the assumption is that there's no non-linearity, this is a regressor, there's no non-linearity being applied here, so there's no non-linear function whose derivative I need to take in order to... So what this means then is that one neat trick you can do is that you can actually create neural networks that you can use to just do like rich regression, and you can, if you set up the code correctly, you can use it the exact same way and basically just say, I want to have no hidden layers in this, and it'll perform the same as a linear model, which means that a linear model is a good baseline, and also when we get to certain assignments, you can set up your neural network in that way to compare the neural network to effectively a linear model with no significant changes to the code except for specifying some of those hyperparameters about the hidden layers. So we took these original update rules, we saw the operations that lead us to these expressions for doing gradient descent, so right here is the element-wise error term times the relevant weight. One thing here now is for simplicity's sake, before we ignored the addition of the bias column just to show how the math worked, but now we can add it back in order to make the matrix operations work correctly. So why are we doing this? If we're just looking at how to update a single weight, it doesn't matter whether that weight is sort of a meaningful weight or a bias weight, the update is the same. If I want to make these into matrix operations, remember the math works out much more cleanly if I have this bias column so that I can assume that W0 times X0 is simply W0 times 1, because that first column X0 for every sample is 1, and so we can turn this into basically your standard and dimensional linear function with some sort of Y intercept. So hence being the Z tilde and the X tilde, if you remember from last time, we're using the tilde notation to signify some weight matrix that comes with the addition of a bias column. So if we look at these, let's work on driving this result using the output layer weights. So here this is kind of our standard formula for computing the total squared error for all sample for the output. So here what I will do then is I will simply take the derivative of both sides. This allows me to bring this negative 2 down in front. So now I'm simply summing over for all n for all k the error terms and then multiplying that by the derivative of the weight, the output with respect to the weight. So what is Y sub nk prime? Well, this is equal to for all m, which remember that's the output of our hidden layer, for this other step, that's the weights that are in our layer. So for all m, I'm going to take weight m prime k prime times Z n m prime. And so now this works out eventually, I'm kind of skipping through the math here, but eventually if I take the derivative of both sides here, I can then take the derivative of the thing that we just saw here. So we're driving this. This is how it can be rewritten. So I can put that into the partial derivative function there. And then eventually this will kind of cancel out to this part here. So now we have basically a constant times 1 over n times 1 over k times the sum for all n, remember that's all samples, times the sum for all n of the target n sub k or target t sub nk minus Y sub nk. So that is the target value for that sample for that output minus the predicted value for that sample for that output times the relevant inputs to the output layer. So I apologize for alternating in terms of input and output. Z is going to be that input to the output layer for sample n for output of the hidden layer m. So m here is just one of those columns in that hidden layer weight matrix. This is not something that is again a meaningful like scalar value that has any real interpretation. It's simply some sort of feature that will inform what t sub nk would be. So we can think of this as if I had a single layer, just some linear function where I have kind of random outputs or stochastic outputs or arbitrarily derived outputs, this part of the neural network, this hidden layer that output Z will allow me to approximate those values. Now the intuition there is that those values are somehow useful numerical features for predicting the true output that we're actually interested in. So questions about that intuition? Okay, so that was just for the output layer. So now the harder one of course is going to be optimizing those hidden layer weights because we are dependent upon how wrong those outputs of the hidden layer weights were when predicting the final output. So again you can think of it as a function over X produces Z and a function over Z produces Z. So we can think of that as a function of the final output. So again you can think of it as a function over X produces Z and a function over Z produces Y. I'm trying to update the appropriate, I'm trying to get the appropriate values of Z to map from X to Y, right, but you'll notice that Z does not directly involve Y even though it does directly involve X. So I have to use some error in Y to correctly update Z for the appropriate weights for processing X. So therefore I need to know how many outputs, how wrong are all the outputs that derive from a particular term in Z. So we'll begin with the same formula. So the error formula is the same and the derivative is going to be the same as above. Now knowing that Y sub nk is equal to the formula given previously, so then I can substitute this in for Z. So remember Z is going to be the output of the hidden layer over some function of X, right. So if I take V, my hidden layer weights times X, my inputs, and then I apply some function H to it, this is going to be equivalent to Z. So now we can rewrite this and so now if I'm looking at basically what is, I'm trying to calculate what dz dv is, I can take all this whole monster here and stick it in where I'm trying to calculate Z. Okay, so now I'll just, because this is really gnarly and hard to read, I will just let H of VX equal, be equal to A. Yes. The D, this one here, this is going to be the number of things I measure about X. So we'll talk about X as being an n by d matrix, meaning that every X is like a number, is some sample number n and then there's some thing about that end that I'm measuring d and I have d things. Okay. All right, so let A equal H of big V times B big X and so then I can just all use A in place of this this gnarly equation there. So now I can rewrite this as below. So then after some simplification, I end up with the following equation. So negative two times one over n times one over k times the sum for all n times the sum for all k of, again, the error term here, times the sum for all m, that is all outputs of the hidden layer, of W sub m prime k prime times effectively dz because we wrote z as this dz da where A is the input to Z times X. Okay, so let's now work backwards. Remember what all these things refer to. X is the inputs, A is going to be sum weights times X, right, so this occurs both in the numerator and the denominator of this derivative. The trick is that in the numerator, applying some activation function, nonlinear function H to this, the tilde is there because that output has to have this column of ones appended to it so that it can be an argument to another linear operation in the next layer. That next layer is parameterized by weights W, right, and so every W is going to take in n, like we can call them inputs, right, this is just inputs of the hidden layer, not the same as the raw inputs to the network, but there are m things that go into W and for each of those m things we want to produce k things, okay, so m by k matrix. So that's this part and then this part here is effectively what you should be familiar with now, error term, average overall samples, and all outputs. So to summarize, error function that you should be familiar with is a squared error, and then the derivative of that error with respect to each weight is going to differ depending on which layer that weight is in, okay, so we can have arbitrarily large networks where all the hidden layers have update functions that are some form of this, this one here at the bottom, here I'm kind of mousing over, and then the output function is going to just be a version of this for this to update the output weights, all right. So that was backpropagation, that is how do we update those individual weights based on prediction error, but how do we get that prediction in the first place? This is going to be the forward pass, so when you're writing a neural network you basically have the forward function which is I'm going to use my network in its current state, and if I'm in training I'm going to assume that current state is not optimized and that's going to give me some error that I can use to better optimize those weights using the backward pass. So the forward pass, this is effectively, right, we remember what this is, this is going to be big matrix V times big matrix X, this is just broken down in terms of the individual elements, then I apply H some activation function over it, this gives me Z, right, so each individual element of Z, Z for sample n, and then quote hidden output m will be defined by this, and then finally I'm going to take all of that, add a column of, add a one on front if necessary, and then for each element in that updated matrix I'm going to multiply that by the equivalent weight, and then take the sum over that for all m, that is all things that go into the output, the output, and the backwards pass is given by the equations that we derived previously, okay. So that was all the mathematics which took us probably in total an hour to get through between Tuesday and today, so you can go through this and this is I believe the most math heavy notebook in terms of like individual equations that I have written out, but if you are interested in how all this comes together you can review this, but you don't have to actually worry about this so much for doing the assignments, which I think is going to be a relief to most people, so let's actually go through the process of turning the mathematical equations into functional Python code. So the first thing we're going to need to do is, right, we've been looking at these individual scalar operations, I'm operating over some individual input sample for some individual measurement about that sample, right, that's going to be x sub nd. I also have let's say z sub nm, which is going to be some function over some individual x times some weight v, right, this is going to produce some value nm, which is going to be for every input sample of all n, I want to produce m measurable scalars about it, right, so these are just unit list numbers effectively, and then finally I'm going to take that whole thing for each one of those, apply some activation function to it, the tanh function, and then for each of those outputs I'm going to pass it into another set of weights that's going to give me the k things I actually care about, and then I use the error to optimize both weights in v and w. Of course, what I said in lecture three is basically don't use for loops because that they're slow and it just introduces a lot more opportunity for error, so we're going to take advantage of matrix multiplication. So the first thing we're going to do is convert these individual scalar operations into matrix expressions, so if you have the version of how we define z sub nm as this, I basically want to try and get rid of this sum operation here by combining things into rows and vectors or rows and columns, so we have h for v, some arbitrary column or arbitrary row, times x for some arbitrary column, okay, so now as long as these things match up, then I can start to rewrite my equation, so z sub nm will basically be, I'm going to take, I don't care about the row, I'm just going to take all of the m in whichever row it is, times all of the n rows in whichever column it is, right, this allows me to actually perform a single matrix operation to get one term of the output matrix. I'm just going to use the commutative property, swap these two things around here, and so now for every row and every column, I can just collapse this into one big matrix, so x tilde is going to be my input with the bias, and then v tilde is just going to be my hidden layer weights. As long as these things are the right shape, they're going to multiply, because all I have to do is make sure that these two inner terms are the same, right, so the star here is going to be some variable, and as long as these two have the same number of items, I can perform the whole thing as a single matrix operation, so now I do that, if I just perform this nonlinear function over there, that's just going to operate element-wise, right, so I'll take the tanh of now every element of this resulting matrix, and that will give me z, and then the same thing basically applies there, so I just want to get rid of the sum sign, and so all I need to do is make sure that I can rewrite my operation so that I have some input that has the same number of columns as the other thing has the same number of rows, and so if I can manage that, then I can collapse both z and w into big matrices too, and so now I have z tilde times w, and that's going to give me y, okay, so this part is should be pretty straightforward, I'm just looking to make sure that for every individual element, I have a matrix of the appropriate shape that can be multiplied to something else, the backwards path for v looks, is well, is more complicated, so we just have to do some more manipulations of the individual numbers in order to get the matrices to be in the right shape, so here's the individual operation, I'm trying to get rid of these sum signs, I can get rid of the sum over k by basically taking the column k to be some arbitrary number, so this will allow me to do that, so we write those as the stars here, the only thing I need to do to make sure, sorry what was that, it wasn't me, okay, that's like, I've never heard that sound like Peter before, so I was a little worried, oh press here to power off the projector now, press here to keep the projector on, I guess the projector must have been on time, on for a long time, okay, we will hope that it doesn't die before class is over, if it does we have to spend another day on neural networks, okay, so backwards pass, so what I'm trying to do is, I'm trying to, first I'm trying to get rid of the sum signs, I can get rid of the sum over k by turning this into just some arbitrary column, the only thing I need to do there is I need to make sure to transpose the weight matrix here, so because I don't have any commutative property that I can just use to swap these two things to make sure that the dimensions align, instead because I'm going to take this to be a column matrix minus a column matrix times a column matrix, the result of this is going to be a column matrix, and so then I will, or sorry, my bad, result, this is a row matrix minus a row matrix times a row matrix, so the result of this will be a row matrix, so in order to turn this into a column matrix, I'm just going to have to transpose it, all right, in NumPy you do not actually have to do this for individual rows, remember, because if I have a one-dimensional array, I can just multiply them together, and then using the matmul operator it will give me a single number, so NumPy will basically coerce the shape for you, but you're almost never going to have single row or column matrices in dealing with neural networks, so you can assume that this is always going to be the case. All right, so the next thing I want to get rid of now is the sum over n, so what do I do there? So this remains the same as before, and so now I need to do something with the n's to turn them into arbitrary rows, right, so if I rewrite these as stars, I just have to make sure that I'm doing this basically for all n for each individual n in that row. Similarly, because I again don't really have a nice, what I want to do is I want to make sure that the inner dimensions of this thing aligns with this thing, right, this is not really easy, I have to compute all of this in order to figure out whether the dimensions are right, but I can already tell that I'm going to have, I've got n items that have m individual corresponding columns, and then I have multiply that by something that also has n rows for j columns, so in order to get basically the two n size dimensions on the inner, all I'm going to do is just transpose this whole thing, right, so the shapes of this line up such that I end up with n by m, and so if I transpose it then I'm going to have m by n, which is going to multiply nicely. Okay, so I'm almost there except I have this problem in that the right side has subscripts written in terms of mj or m and some other elements, whereas the left side is jm, so I have to make sure to line these up because I can't take v sub jm and then add something that is in terms of something sub mj, right, because then I will want the dimensions are going to line up and I'll be adding or subtracting like the wrong element from the in the update matrix from the wrong element in the thing to be updated. Okay, so then what I will do here is, so I have this is the update function according to what I had computed previously, so what I will do is first I'm going to transpose it, okay, now this allows me to swap the individual dimensions, and then what I can do is I can treat for v for every row j, every column m, then I will be able to take the learning rate times 1 over n times 1 over k, so for all inputs and all outputs, times the transpose input, times the whole error function, times the transpose weights element-wise multiplied by the derivative of the activation function. So the end result now is now I've got things all lined up, I've got I can treat these arbitrary members elements of the of the matrix just as individuals, so if I can handle them all the same, now I can collapse everything into a big matrix, so given this I can take, so v is going to be some new hidden layer weights, and for that I'll take the previous hidden layer weights plus the learning rate times 1 over n times 1 over k, times the transpose input, times the error, times the output, times 1 over or 1 minus z, in this case the derivative of tanh. So this w with the caret is going to be w without the constant input row, so this is just going to be sort of raw w or w tilde with the bias, and so now the backwards pass for w works out rather similarly. So the first thing I'm going to get rid of is the sum. This is pretty easy because I only have one sum to get rid of, so we can basically do it the same way. Again all I need to do is transpose the resulting array that results from taking the error term for every row, and then multiply that by z tilde. Again my subscripts are a little bit inverted, so the right side has mk, or so the left side is mk, but the right side has k and m, so then what I will do is I'll take these two terms and just switch them using the commutative property, and so now this allows me to turn these into arbitrary rows or arbitrary columns, and therefore I can collapse now everything in this into a big matrix. So w is going to be old w plus the learning rate times 1 over n times 1 over k times the transpose input to the hidden layer, so this is now playing the same role as x tilde did in the above, so these two are kind of playing equivalent roles here, and then this is just the raw error. All my t's minus all my y. Okay, and so now the shapes of these should work out correctly, and so now all together in math right what is z? z is some function h applied over bias x times v, and then I apply I append a prependum bias to my my z multiply that by w I get I get y, and then the update for v is going to have to incorporate a couple of other things right not only the inputs and the error, but also the weights things that are to be updated in the subsequent layer, and then the derivative of the activation function the non-linear functions being applied, and then w is kind of the same just simplified basically the only thing different this is this can be considered the w update can be considered a special case of the v update where the activation function is just a linear pass through so x equals x what's the derivative of y equals x it's one so this is just the same as multiplying by the derivative of y equals x which is one, and there are no weights there are no subsequent weights to be updated so I don't need to add I don't need to multiply by anything here. Okay, so that was the mathematics now how do we do this in python so your code is going to look something like this right this is not necessarily going to be exactly what you're going to do in assignment two, but this is generally the shape of the code so first of all I'm going to take an input x and a target t what I want to do is I want to do this forward pass right that's these two things or the first two things up here is a z and y so what I'm going to do for that is I will just add ones to x so this is this add ones function oops can be assumed to be some function that's just going to put a constant column of ones on the front of your input matrix okay so now I'll just call that x1 this allows me to keep things straight so then I will multiply x1 and v this is going to give me what I was previously referring to as a I take the 10h my nonlinear function this is going to give me z here right so z equals h of xv is that's what's happening in these first two lines I have to add a column of ones on the front of z so I'll apply that same add ones function and then I take the z1 multiply it by w this gives me my output y so now for the back prop step I'm going to do gradient descent on the derivative of the squared error with respect to each weight to update v and w so first thing I'm going to reuse t minus y to much places so I'll just calculate that as the error and so now if you look at these two columns here or these two rows here you should be able to see that they're basically just reproducing in Python what we did here so what is v it is the previous value of v plus some learning rate times x1 x1 transpose times the error times w transpose and then element wise multiplied by the derivative of the tan h function and then w is going to be the same thing so why am I not really using one over n or one over k well I know the sizes of n or k so this is just a constant so I don't really need to worry about that all right questions all right so the above equation is going to this is going to be like a single gradient descent step so we're going to be using all of our training data in x and t to calculate the the gradient of of e with respect to the weights so we don't want to use the for loop so we're going to actually use instead of writing the for loop we're going to be using the full gradient so here's an example this is just dummy data so this data doesn't signify anything what I'm going to do is I want to create a non-linear function that I can fit a neural network to like this so if you recall from like assignment three if this loads so we had like we did some exercises where we had this function that was at best only vague at best vaguely linear this god it's a long notebook like this one here right so we created some function that this is really a non-linear function right because I created a some some dummy data and then applied a polynomial over it so we know that that's likely to be a non-linear function and so now my goal is instead of just fitting a line to that which is a pretty lousy fit I want to try and fit a non-linear function to it using a neural network so there so for example here is a function that has clearly some non-linearities in it with the use of sign and I'm going to try and fit a neural network to this using a using non-linear functions so I will apply some noise to make things a little bit more interesting so this epsilon is going to be a random variable drawn from the standard normal distribution between with a range of negative 10 and 10 okay so let's see what it looks like so let me do my imports so first I'll make some training data and I'll create some helper functions so this is I'm just going to create some training data that is where the inputs are evenly spaced across some some domain and then I'll take the this function and apply that to that to that data to create this non-linear function and I'll do the same thing with some testing data where I have just different inputs that are random that are sampled differently so these are generated using the linspace function the x test is basically I take the train data and add a little bit of noise to it not so much that the testing data is going to be completely out of distribution but enough that it like it's close but it hasn't been seen before so that's just the idea here and so that is even though this is dealing with random random inputs here this is what you should expect for pretty much any type of machine learning problem if you want it to behave well that is the training the training data and the testing data should be more or less resemblance right if I'm trying to use my network fitted to weather to classify miles per gallon it's not going to work very well and the same is true for for neural networks as well okay so now I'm going to add this helper function uh which I'm running a resell so I'll add this helper function add ones and so this is just doing what we did previously in in notebook three just in a convenient wrapper function so I'll standardize standardization in this case is the same as we did for for linear regression so I'll compute means and standard deviations over the training data and then I will standardize both the training data and the testing data using those computed means and standards take this now I apply the add ones function to both my training and my testing data I'll do this right now so that when I get to the evaluation phase my testing data is the shape that I might never expect so generally good practice is just to make sure that you have your data all set up in advance before you before you're going to use it and then just make sure that you're not accidentally like inputting the testing data into your training function or something okay so previously we did our weight initialization in in assignment one and in lecture three using just initializing them all to zeros so here with the neural network we're going to actually initialize the weights to random values so if all the weights are initialized to zero all those hidden units will earn identical weight updates and that would be as if we have only like one hidden units okay and so basically you cannot reliably get your network to fit when you are initializing your rates to zeros with most network architectures you could also you could do assignment three using random initialization of the weights it's just that well a couple things happen one your code may not pass the automated greater because we have no expected values assuming that you initialize using zero but also even if you even if that weren't a factor different initializations will take longer or different times to converge because you don't know exactly what the initialized weights are and so you don't know how many gradient update steps you need to get to the same level of root mean squared error so with neural networks this is a pitfall in that if you have different weight initializations you may get different results usually if the data is such that the network and the network architecture is such that it can converge you don't end up with a radical difference in in many cases but generally it's good practice for reproducibility to do things like set a constant random seed to make sure that you're initializing effectively two deterministic values all right so now let's set the parameters of our of our neural network the first thing i want to specify is how big is this hidden layer right so i have n samples and i have d things that i'm i want to measure about this and i have k things that i want to predict but somewhere in there there's a transformation by some dimensionality m right i basically have to decide the size of them so i'll use 20 the the the number of layers that you choose has certain implications for the the results of your training based on the nature of the data and normally people arrive at the appropriate number of hidden layers or hidden units by some sort of empirical evaluation or trial and error or grid search there are more sophisticated techniques that you can use that are somewhat outside the scope of this class but there are some toolkits that will do things like hyperparameter search for you all right for the moment we'll just say okay we'll use 20 hidden hidden units i'm going to specify two learning rates one for the one for the hidden layer and one for the output layer so you know you can initialize these to be constant you can use some techniques based on the the sample size if you want so now i'm going to initialize v and w randomly right so i will initialize in this case from the same distribution all i need to do is make sure that they're the right shape so i have x train dot shape one that's the number that's d that's the number of things measured about each sample plus one for the bias column and then the output of this has to be the same number of hidden units right so the output shape of this should be hidden units and then i add one to that and then the output of the output size is the this outer dimension of w so set the number of epochs in this case i'll do 100 000 i'll collect a couple of things for plotting training and testing error so now here's the user of the real operation so i will perform the forward pass and the training data remember i've already applied add ones to x train so that's done for me multiply that by v take the tan edge of that apply the add ones function to the output that gives me z1 and then y equals z1 times w and so that is basically what we are doing up here okay next thing is the i'll complete the error right so this is going to be t train s minus y so these are the training targets and then the prediction so now the backwards pass these are just the two functions that i created before so the only thing that's really different here is i'm using x train s instead of just like x1 so you have to make sure that you're using the the right the right data usually if you're putting in if you've assigned multiple variables to different splits of the data and you use the wrong one it'll probably just throw some shape error that should be pretty easy to spot all right so now after this this allows me to do the forward pass in a single line so if we break this down for the sure to predict the the test i can just take the add ones function apply that over the output of the linear operation here so if you work outwards this is x times v plus with the ones take the tanh add ones again multiply that by by w and now this allows me to to compute the the error relative to the the testing data all right so now then what i can do is i can collect collect the error traces for plotting and then all this this is just a matplot lib functions all right so let me run this and we can actually see this network working in real time so here we can see the root mean squared error on the y-axis plotted versus the training epochs on the x-axis and this will run for about a hundred thousand training epochs and so you can see that the the training error is much lower but we also see some significant progress in the testing error nonetheless the testing error kind of appears to bottom out at around maybe 0.25 whereas the the training error is much lower so this this network should be able to fit to this data decently well we can look and see what the predictions are right so here we have training samples so this is the i'll see plot the actual and predicted given the the training data the testing data and the model for every x right so if i run this again we can actually watch this this second one how maybe not i guess this does that i thought this was going to like animate in real time i guess it didn't say to do that so what we see here is now this is going to be the the blue line is going to be the training okay i guess it is moving a little bit probably faster than that oh well so what we see here now i guess the point i'm trying to make here is blue line is the is the the training data the green line is the fit to the training data this is the model so we can see that the blue line and the green line are pretty close right so the green is the predicted values of the blue line is the actual values and the orange line is the testing data which overall is still pretty good like it's a decent fit but it's not quite as good so we can see how this this higher testing error is being reflected in the in the output scene okay final thing is what are the different what are the different units actually predicting so they will just look at the output units or sorry the hidden units and so for x there's like 20 lines here you can only really see a few of them because most of them are basically these lines clustered near y equals zero but you can see how like for some of these units for different values of x they're outputting different values so these hidden units if we look and just see sort of how many of these are not just clustered here at y equals zero you know one two three four five six seven eight maybe there's maybe uh eight units that are probably doing most of the work whereas the bulk of the units are either always outputting is a very small value and then the sum like this one here whatever it's optimized to is basically a right around zero it's changing its output from very positive to very negative right this is going to be after the application of the tanh function because that is going to bound us between negative one and one all right so that's the end of intro to neural networks questions i'll go quickly through i'll start adam at least before we we before we dismiss all right any questions about neural operations yeah i was wondering how do we decide the right kind of blocks to how do you decide the right plot for yeah i mean it depends on what you're trying to visualize right so for example if you're plotting training and testing loss or error you know this is something that you'll see commonly so you'll see training time and then the the error or the loss function on the y-axis this allows you to show that your network actually is converging and where and how how well it's able to be optimized so it really you know there are so many things that you could measure about a neural network and its output so it's really dependent upon what it is trying to show i'll have some examples of say plotting hidden unit outputs in multiple dimensions later i think in notebook maybe seven so i guess there's no there's no real like rules for this in that it's dependent upon the data and it's also dependent on what you're trying to show with the data right because depending on how you visualize the data it may kind of tell a different story that makes your point or a different point better or worse so i think the most i'll say right now is like during this class you will have multiple opportunities to see different types of plots and you can see which ones maybe are intuitive to you and which ones are not and then in like when in your final project you'll have the opportunity to kind of experiment with how you want to visualize the data if it's a classification problem versus a regression problem versus something else all right any other questions yeah yes yeah um so i think i mean you'll definitely know if you're wrong if you see it not converging there is there are some things weird things can happen especially as your networks grow larger in that and your input sizes grow larger and that you may see your network doesn't converge for like a long time and then actually like i think i have a maybe i have an example let me see if i okay maybe maybe we'll discuss this and i'll do adam tomorrow or or uh tuesday so like here's here's some research that i'm doing um and basically we're like trying to predict you know some uh some outputs from uh an experiment from data that we gathered in the lab and um with the loss plot one thing that we see we obviously see things like this so for example if i'm looking at i'm plotting four things here right so we have um a training set a validation set and for each one i'm plotting accuracy and the loss so this is a classification problem we'll talk about loss functions as opposed to error functions later but just take this to be you know classification error and then accuracy right so the accuracy is actually sample accuracy which means that even though i have this nice smooth curve as the loss goes down which is what i want to see there's a long time where that that is not actually reflected in the classification accuracy because it's basically it's a not a regression problem it's basically is this sample being predicted correctly or not and so because of that we see these kind of jagged leaps where it goes up a lot and then in fact the out the validation accuracy goes down again and then it sort of converges better and better in these in these stepwise um increments right so it's like it's getting i don't know 70 percent or what is it like uh 85 percent of the samples here and it's really not getting any more than 85 percent of the samples and then suddenly it starts getting 88 percent of the samples and then eventually it gets up to like 97 or something like that so you can plot you know different metrics and the different packages will allow you to do that pretty easily so you can plot like accuracy loss even you know other metrics like f1 precision that we talked about earlier so again you know depending on the nature of your task if it's a regression function you probably want to see a lot of these nice curves and you should see them both in you know in like well i guess probably only going to be using error for that so you should see kind of the error curve start to to decline and it should be pretty much consistent with a couple of exceptions that we'll go into later where if it's classification you're going to want to look at like measures like accuracy or some of these more information theoretic precision recall type measures so i guess one thing you can do um will use pytorch and you may be might use tensorflow on your projects but there's like a lot of um built-in metrics that you can use and so most of these packages basically allow you to create a validation metric in your uh in your training just with like the change of a string and then you can plot all these things and see like which ones are actually useful information to present okay all right other questions yeah how to decide how many hidden layers are included in the network great question uh short answer is no one really knows um long answer is there's a number of techniques you can use um so uh one of the network one of the like notebooks we're going to be doing is like how to find good parameters um so you can do things like grid search basically specify a bunch of different options you want to try and try all of them and see like according to say your error metric your accuracy metric which one is best there are some rules or some i would say rules of thumb i guess that you can consider um based on the sizes of your input so if you have n by d inputs the more d's you have more dimensions generally the larger network you're going to need um so effectively what's going on here is i look at the the architecture of this network that i created is basically four hidden layers that are 128 units 64 units 64 units and 32 units okay um i ultimately i arrived at these empirically and then i tried different things and saw what worked better but why am i why did i choose 128 for example as opposed to 64 in that input layer well it's because the raw dimensionality of the input was something like 780 so if i take 780 dimensions and project it down to say 64 dimensions i'm losing a lot of information so effectively what we're doing it every hidden layer the network size is the the layer size of the dimensionality of the output which means that you're projecting whatever comes into that layer into that dimensionality so if i have 20 hidden units i'm going to be projecting whatever comes into that into 20 dimensions so what if your input is less than 20 dimensions well it'll project it with the addition of noise because there's no other way to do it right it's gonna not it's basically going to not optimize some weights because it has no information available to optimize them what happens if i put in way more than 20 things it's going to have to project that down to 20 dimensions and so i could lose information that could be useful so in this case um i'm trying to effectively classify three classes using 780 dimensional inputs right so big information bottleneck but i don't want to go too fast so projecting in this case down from 780 to 128 sort of turned out to be the happy medium between you know 64 and 256 people usually use nice multiples of two for computational properties sometimes you might just use decimal numbers so you know it's not unheard of to see you know 20 hidden units 10 hidden units 100 hidden units whatever um and sometimes people will find that some weird number just works really great like 347 which is that a prime number that might be a prime number and like it just seemed to work better than 346 or 348 um case in point i guess if you're you're heard of the burt model you may have it's like it's a major nlp model so basically you've heard of chat gbt i assume okay so gbt is a sort of one family of big nlp models and they use what's called the decoder layer of the transformer meaning they're basically good for generation burt is sort of the is a bidirectional encoding of representations from transformers and the logo was always burt from sesame street and that's that only uses the encoder layer of the transformers basically meaning that it's taking raw input turning it into a demand an n dimensional representation that uh what that does is um the burt model is basically taking you know words turning them into 768 dimensions but they use it you they do it using stacked encoders and so that is you have one feed forward network another feed forward network another feed forward network and so on how many encoders do they use how do you decide how many encoders do you as well burt uses 12 why 12 well they tried a bunch of other stuff and 12 work best so there there are you know there's probably good reasons why 12 works better than say 11 or 13 but it's not entirely clear why 12 works better than say 16 it's probably a happy medium between robustness of the representation and compute time so all that is a long way of saying everybody has their favorite techniques there's some really smart people who have probably found some evidence-based ways to explore this space sometimes it requires an enormous amount of resources to do this exploration so some folks just do trial and error or grid search or you know what have you yeah all righty um what else all right let me um let me start the atom notebook i guess um i doubt we're gonna have time to go through that in uh so of course it might take me 15 minutes to find my uh find my folder um all right where does this work there is okay uh so all right gradient descent with adam so um i guess just in brief uh in the in the last 15 minutes um so who is adam uh adam is about a person adam is as a function um so there are many ways to descend a gradient right what we've talked about so far is basically looking trying to find the direction of steepest descent and taking little steps in that direction well this seems pretty inefficient right maybe i could take a bigger step but as you've observed one way to take a bigger step is to have a large learning rate right the learning rate is eventually the step down the gradient uh but if your step is too big you or maybe you may find yourself stepping back and forth over that optimum you don't want that but let's think about um if i'm standing on the edge of a canyon i'm trying to find my way to the bottom if i know that i'm at the very lip right i could probably take a big step and then like i'm not going to overshoot the the minimum so that's fine so maybe i can be smarter about how big a step i'm taking or basically scale the step that i'm taking using uh using some metric so um there are other ways of finding error gradients that will lead to fewer steps before we get to optimal weights so um this thing one of them is called adam and there are various adam variants that are in use um so this uh uh this one is sort of the original one from uh 2014 i think um so adam stands for adaptive movement estimation so you can kind of see where we're going with this if i'm really far from the gradient from the minimum i can justify moving faster if i should be getting closer maybe i should be slowing down right uh so there are a couple of um you know sort of more in-depth gentle introductions you can read is the additional paper here by king man bha and then jason brownlee has this this nice kind of gentle introduction to atom optimization but basically uh for general purpose gradient descent algorithms you want to collect all the weights into a neural network into one vector so this way you can actually use arbitrary gradient descent algorithms for an optimization problem as long as i know what what hyperparameters i need to apply where so uh let's say we assume these weight matrices v and w so to collect all of these we can run this pack function so now what i'll do is i'll take v and w flatten them both and then just stick them together so it just leaves the h stack function so let's just test this before going any further so what i can do is i'll create some numbers these are just arbitrary numbers just to represent hidden layer weights and then some to represent output layer weights so if i print them these are obviously not good weights for any real reason they're just illustrative so if i have v which is you know a five by two and then w which is a three by six if i flatten all of them and again you'll observe that like these these matrices would not uh would not multiply you have to make sure that the outputs are the right shape so i need to flatten them this is going to give me an array of zero through nine and then an array of 23 or 37 and if i run the pack function we can see that the output is what i expect i basically just have a single array that contains all those numbers okay so now imagine that we sent this weight vector off to some gradient descent optimizer and then it returns some new weight values i need to unpack those into the right shape for v and w so i'll create an unpack function that is good to do more complicated than the pack function because it reshapes w back into the constituent matrices so of course for that i need to specify what shape those are and then this will create you know create that into the number of rows number of columns for each one and then reshape basically split the full the flattened the flattened packed vector at where according to those shapes the the two different weight matrices should be and then reshape them appropriately so if i define that and then i pack v and w and then i run unpack this will get me back my my original one all right so um how is v defined right that is when we run a cell that invokes v what happens so first thing what i can do is set what i do is here is i'll set w zero is equal to two thousand and then if i look at v you will see that that first element is now set to two thousand so what happened i actually didn't recalculate w so why did the value change so if i do if i create this v2 which is basically a a an instance of v then if i run if i show v2 and w i still see this two thousand here okay great because i just effectively cloned v now let me set w zero equal to one thousand so wait a minute v2 didn't change right why not because the unpack method is going to create these output matrices that preserve the same places in memory as the vector that was used to create them so if i change w you change v so this can be useful for efficient usage of memory to make sure you're not overwriting something you don't need to overwrite so if i print w right this is that packed vector i can see one thousand there so v2 didn't change because it was effectively making a deep copy and but if i change w and then i print the packed vector we see the same contents here now so i need to update w in place in order to do this so if i just do you know in place multiplication that that'll work but if i do w equals w times 0.01 that will not right so now w has a 100 in the first position and similarly now so does v right so this is basically accessing the same point in memory another way to update arrays update values without creating new memory is to basically just do a slice over the entire over the entire array so what i'm doing here is now just saying for every element in w take w times 100 and so now if i print w this will give me 10,000 in the first position and you know well 100 times everything else and then v and w you will see that now the v and w have all been multiplied by 100 okay so all of that that's just how you create your format your weight matrices so that it is appropriate for doing this kind of general optimization using using gradient descent so all right in the last eight minutes real briefly intro to adam so adam is short for adaptive movement estimation it's just spelt adam like the name it's not an acronym it is pretty straightforward to implement it is computationally efficient little memory requirements provide that you are creating your matrices in the appropriate way or so it's not just creating deep copies everywhere it's invariant to diagonal rescale of the gradients and it's well suited to problems that are large in either in the data or in the parameters and so it's it's generally quite efficient as we'll see presumably on tuesday we can get adam-based optimization to converge a whole lot faster than sgd okay it is also appropriate for non-stationary objectives and for problems with very noisy or very sparse gradients and the hyper parameters have an intuitive interpretation and typically don't require a lot of tuning so sgd maintains the single learning rate for all weight updates and this doesn't change during training so remember the learning rate is the step adam combines the benefits of these two of two extensions to sgd one is a to grad for adaptive gradient which this will maintain a learning rate for every parameter and this improves on performance with sparse gradients and then rms proper root mean square propagation which also maintains per parameter learning rates and these are based on the average of recent magnitudes so that is how quickly a weight is changing and that means that the algorithm does work like these online or non-stationary problems so adam provides the benefits of both using these things called the first moment and the second moment so if any of you have taken physics this may be familiar to you first moment is the average of recent magnitudes of of the gradients for the weight in question so that's just the mean of recent very recent values and then the second moment is just the square of that so this is just the uncentered variance so formulas that say if i have i'm going to take like the past four would look something like this so this is again related to the concept of the moment in physics so these are expressions involving the product of a distance and a physical quantity so basically this accounts for how the the mass of the of the object is actually arranged so the first moment of mass is the center of mass and the second moment of mass is the rotational inertia so just think of this in terms of if my weights were quantities in space right if those were masses then the center the first moment would reasonably be the center of mass right they're all distributed kind of unevenly and like a potato it wouldn't necessarily be the center of of material but you know rather center of mass and then the second moment is going to be its its variance so you think of the first moment is probability distribution as the mean and the second moment has its variance so you can have raw variance or centered variance and for adam the second moment is wrong so what adam does in the last few minutes is it basically calculates an exponential moving average of the gradient and the square gradient controlled by these two parameters beta one and beta two and these are decay rates so that is if you think about if i'm on the lip of the grand canyon i might run down the trail at first right because it's really steep and then i'll slow down as i get to the bottom so how how much do i slow down as i approach the bottom or as i think i'm approaching the bottom would be controlled by these two parameters okay and so these are just the decay rates of these moving averages and so exponential decay will decry it will describe the amount of reducing an amount by a percentage over time and so the time here is training epochs so the first moment calculation will look something like this so m which is some some value for the moment equals beta one times m plus one minus beta one times the error so here what i'm going to do is i'm just going to look and see there's this controlling parameter how much do i need to reduce my momentum or my movement so all right i will end there for today and then on tuesday we'll pick up with the adam implementation all right thanks everybody i'll see you next week Okay, let's get started guys. Your screen share. Alright, so I'm gonna tell you a stupid joke that I just heard. So everybody here speak French. How do you say the letters GPT in French. Right. What does GPT mean in French. Means like farted. Okay, so basically if you do cat I farted in French, you get shot GPT. Thanks to Professor Louie de Lepouchet for that one. So anyway, those of you who have been to office hours, you see the meme on the cork board, right? So it's like the Drake meme and it's like chat GPT, no, cat GPT. So I didn't realize this. Another did my wife actually, even though she doesn't speak French, and has seen that meme. But then Louie de Lepouchet caught me on that. So that was my highlight of the morning. Alright, anyway, so now I'm going to tell you about talking about the dude named Adam. So let's go do that. So just a reminder that assignment one due on Thursday. So if you do need an extension, I will need your reason for an extension by the end of the day tomorrow. And then I can get back to you. You can send it to me earlier of course. But then I can get back to you with either the assent or a counter offer or denial if your reason is just bad. But so make sure that if you have that, you make sure you get that to me soon. Again, extending circumstances if you do get hit by a bus before class on Thursday. Right, so that obviously I can adjust accordingly. Again, once a reminder not to throw yourself in front of a bus to get an extension on a one I believe these the subsequent assignments are a good deal harder. So don't throw yourself in front of a bus for those either though. Alrighty. Any questions about hope you all have made some progress in the assignment. Yes. I have a question about this assignment in part 15 points. Yeah, I have like a certain amount of sentences. I have mine to be chunked out like two sentences. Okay, it's fine. So, what the reason I have things like that there is just because what I'm most concerned with is basically are you able to do meaningful analysis I don't really care whether it's long or short some people are really good at doing like very dense but dense analyses and like they have you know, five sentences but they really capture the gist of what they've done in those sentences. But some people if I just say that some people tend to flail there are certain students who really do need a kind of And so that's why that's there is like you know if you if you need a benchmark of where enough where you've done enough you know here's a reasonable benchmark, if you've got this many sentences, it suggests that you've at least been able to put reasonable thought into it, I will of course read it to make sure you didn't just feed it to charge a bet to give me your answer. And because it is also good as generating you know fluffy sentences on end. So make sure that you're of course writing your own sentences but that's what I'm really looking for is just strength of the analysis and whatever it takes to get there that's important. Okay. Other questions. All right, let's continue talking about Adam. So you remember Adam from last week, Adam, he is a gradient descent optimization algorithm. So remember that Adam is not not actually an acronym it's short for adaptive moment estimation. Yes. Oh yeah sure. Okay. Yeah, so Adam is short for adaptive moment estimation, and it has the benefits of being straightforward efficient with little memory requirements. And it's very friendly to this method that we've done, basically stacking all of your your weights together, so that you can perform optimization on all of them at once. Right, this helps in our in the efficiency of the computation. It's also appropriate for things like noisy or sparse gradients, and also the hyper parameters have an intuitive interpretation. What is that interpretation so moment in this case is moment as in momentum. So if you think about moving down the slope of your gradient if you're very far from the optimum, and you're trying to reach the bottom, it may be wiser to move fast. So if I can figure out that I probably have a long way to go I'm just going to move quickly, because I don't expect to reach the optimum any point soon and so I'm not likely to start skipping back and forth over it so what I'm trying to get to with SGD, I'm just taking these tiny tiny steps with the goal of reaching the optimum and not not going, you're not going back up past the optimum. So this allows you to be a little faster, so you can basically take larger steps. When you're far away from the optimum, and then slow down when you're closer to the optimum and do it in a smart way and not just having say a decay in learning rate where you start fast and are deterministically moving slowly. Remember that the two key variables in Adam are the first moment and the second moment so this is basically the average of the recent magnitude so that is, if I've taken big steps recently, then it may be more reasonable for me to take a relatively large step now. That second moment is also like what's my rate of change over these recent samples so if I've been if I've not been changing, probably not good and means I'm probably not approaching the optimum. But if I've been changing relatively little, it means that I can still take big steps and I don't need to really need to worry about slowing down anytime soon. If your first moment values are still large. Yeah. They don't, they don't, they can get stuck so this is why I'm so the question for those of you zoom how do how do these algorithms determine distinguish local from global minima, and the answer is they do not because this is a global minima. Because this is where the data set balance is really important so one thing that happens commonly, I'd assume asked me about this this morning is that if your data sample is way overbalanced. And you have like if you're a classification problem you have three classes but you have 10 times as many members of Class A compared to Class B or C. And then your criterion is just like, if you're in training is just accuracy or something. It's really easy to basically say well I can't get a better answer than just classifying everything as Class A is going to get me 96%. And I can't really do much better than 96% for a class for three way classification tasks so why not just classify everything is Class A. If I have 1000 samples of Class A and 20 symbols samples each of Class B and C. Okay, so this can happen, and gradient descent or Adam or any optimization algorithm will happily let you do that. Adam may have the benefit, as we'll see when you look at the loss charts of sometimes allowing you to basically skip out of a local minimum, and then climb that hill and then find basically a steeper path to descend down to a global minimum. And so there is some benefit to basically allowing it to sort of overshoot what might be a local optimum and SGD tends to, especially if your learning rate is very small, it'll find a local minimum to stay there because it can't go anywhere fast enough that lets you get out of that local minimum. So this leads me to sort of how do we, how do we determine the quote speed, at which I can move through the gradient at every optimization step so if I've been moving very very fast. I can probably continue moving very very fast that is I can take bigger steps. If I've been taking big steps over say the past four training iterations, but the steps have been slowing down. So my rate of change is now going, you know, my, like, the derivative of my, of my steps length basically across the gradient is declining. It may serve me to still take a relatively large step this time but maybe make it a little bit smaller. Right. So, what this suggests is that the gradient I'm moving along the gradient away this suggests that I'm still moving along the steep slope of the slope is getting slightly less steep. So maybe I am getting a little bit closer to my minimum and so then I should probably slow down a little bit so that I'm less likely to skip out of that, that bowl, that minimum. Okay. So naturally this is closely related to the concept of moment, along momentum and physics so basically we're looking at the distribution of values around, around a mean or a variance right so the probability distribution of these values that you're sampling is going to be the same as the rate of change that you're taking at each step. Okay, so what we have are these tunable parameters, these beta values. And so this is going to be a decay rate for the first moment estimates, and then beta two is going to be an exponential decay rate for the second moment estimates. And then we also have a constant here to make sure that we are not dividing, remaining division by zero. So this alpha or row, this is the learning rate that you specify that's a constant. So I'll just say, I'm going to do Adam with a learning rate of point 001 or something like that, and that value is not going to change. I'm like I can say some reinforcement learning algorithms. I think in a few months. But here I have these other values that will say, I can perform some calculations over my first moment and second moment estimates that is sampled from the previous end training iterations to determine how I need to adjust my step size so my step size here is not a rigid constant rather something that I can tune a little bit based on these values. Okay, so any any questions on the Adam recap I'm going to go into the implementation now. Everyone's cool with that. All right. So to show this. Let's do what we have done in all this kind of work on a dummy example. Okay, so I will make some training data this is going to use very much the same, the same formula that we use in notebook five, this is basically the same function so I create some, some evenly spaced values on the x axis apply some function this case the same function from notebook five. It's just a nonlinear function with some noise added. Right, so I have introduced nonlinearities here by trying to have it model some variant of a sine function, and then making it difficult by adding some noise. So I'm going to get my optimization to approximate this nonlinear function, I'll make some testing data, the important things that they resemble the training inputs. So now I'll create my add ones column, or my add ones function that this will do is just be a generic way of adding that constant column of ones. So I'm going to simplify my hype some of my hyper parameters for my neural network in this case, the thing that I'm looking at is just what's the number of hidden units in my aware so I'm just going to assume I have a single hidden layer, and I'm going to create a number of units in that that will be that will allow me to introduce nonlinearities. So now if I create 10 hidden. So I'm going to look at memory of two sets of weights, V and W right via the these hidden layer weights so I take my inputs, multiplied by those things, and this gives me some scalar values that have been applied on the new function to those now nonlinear eyes scalar values are then multiplied by the weights and W, the output layer weights to actually give me the output values, still a regression problems I'm just trying to fit your scalar values to scale the values. So now if I look at, you know, take the, the shape, basically just do prod of the V shape this will tell me you're to buy in this case 10. So, 20. What is NP prod. So this is the product of an array of elements over a given axis so in this case the array of elements was two and 10. It's just the shape. And of course there's only one axis. So now what I can do is just confirm this is correct by just checking these two are equal right one plus one times and hidden. Why is it one plus one. Well we have one input. Right, this is my input value and x, and then I have the constant column of ones. Right, so there's going to be two inputs one of which is going to be a bias, that's going to have a bias weight trained against that, and then there's the actual feature guy that's going to have some weight training against that. So I'm going to set the other parameters of my neural network so and hidden, I'll set my, my learning rate in this case to point one, then I can also just scale the learning rate in this case by the number of samples times the number of outputs. So you can do this to kind of optimize the, the learning rate before you start training to what you think might be at least a value in the appropriate range for the amount of data that you got. So now then I'll initialize the weights to uniformly distributed values between, you know, these are normally distributed between negative one and point one or negative point one and point one. So now let me print the current state of my neural network. Right, so I've got a learning rate in this case, point 005 seems like a normal learning rates kind of in that range like, you know, one one hundredths down to 10 to negative five or so. So that's typically typical learning rate value. And then here are my values currently of V and W right these was randomly initialized. So they don't really mean anything right now. And they're also certainly not optimized to this to this function. So if you remember the pack function from earlier in the notebook on Thursday what that does is it will take my different weights from the different layers and pack them all into a single vector, so that I can apply optimization operations over all of them at the same time. And then we have the inverse of that which is the unpack function, I just have to supply the appropriate shapes that I want to unpack those arrays into, and it will give me the actual arrays back. So now I'm going to set my number of epochs or train in this case for 100,000. So I'll take these that this many steepest descent steps in the mean squared error function. And then I'll do some sort of collection for plotting. So finally this is the meat of the operation here so this should look familiar to you already so we have our inputs x1 multiply that by V apply a 10 h function to that that gives me Z I need to append and put a column one to the front of that again. So I'll do that by W that's going to give me my final output, and then I calculate the error, right just my ground truth target minus my actual predicted value, and then use these in the backwards pass so just a reminder of the pieces of this operation. So Z is 10 h over the input, right, this is going to be the derivative of 10 h over the input. And so then the error. This is actually going to be the error term for this for the sample, I just have to transpose it to make sure that my weights are in the right in the right shape. So this is going to be the gradient in W is much simpler so this is just going to be the input Z input to that layer times the error. I can pack my gradient values into a single vector, using this, this function, and then W which are my packed weights, I can just optimize all of them at the same time by subtracting row times the gradient so what's happened here is that I've computed the layers separately. Right, so I've done a gradient of V ingredient of W. So, then what I do is I pack them into a shape that's going to be the same shape as all the ways, right, basically have a gradient for each weight, so the gradient arrays are going to be the same shape as the actual weight arrays when I pack them they end up in the same shape as well. And so now, now I have a single array I can do a single operation over them. So this allows me to be much more computationally efficient. It'll create error traces for plotting and the rest is just pipeline. So, take a look at this. And now we can see it start to converge. Right, so we can see the train and test RMS E. So the train has converged down to a pretty low value and test is slightly above that. And after 10,000 samples, basically you can see that we optimize entirely you know pretty early on. Right, and then there's not a whole lot of improvement going on there. You'll notice a couple of things that's pretty subtle in this but you can sort of see this little hook here at the bottom of this graph right. This is one of those cases where due to the atom. For a three year that I'm here. So, this is going to be. We didn't actually use Adam here. Sorry, my bad, we're going to get to that later. So now we look at ignore everything I said like past 30 seconds. Come back to that in a moment. This is actually just a weird, this is not nothing to do with the gradient is like just a weirdness in the plot looks like. So what's, what's the next thing here. So this is just going to be my actual predicted values and then according to the model. This is what it's predicted so it's maybe not very good. And then these are the hidden outputs for each unit right so each one of these represents the output of this unit. Depending on what the input is. Yes. This is just one layer. Yeah, this is currently was using a single layer with 10 hidden units. Alright, so now we're going to repeat this training loop using Adam this time. So, previously this is just sort of vanilla SGD as we've learned it already. And then the, the version with Adam so differences here. So same neural network right still 10 hidden layers are sort of 10 hidden 10 hidden units one layer. I'm not going to specify these other values so small epsilon just to prevent dividing by zero right tiny number. And then I have these two beta values. So let's just go back up refer to what beta one beta two are again. So beta two is the exponential decay rate for the first moment estimates and beta two is the exponential decay rate for the second moment estimates. So in this case, these values are set are set to be point nine and point 999. So we can see that effectively I'm going to have a kind of a 10% decay for the first moment estimates, and then a really really small decay for the second moment estimates. And then row here I'll just set 2.001. So, same thing as before, I'm just going to initialize my weights randomly. Now I'm going to specify these two other things, empty and BT and then beta one T and beta two T. What are those we'll see those in a moment. So these are going to be bias corrected moment estimates that I'll use to basically update the those beta one and beta two values. So, up until this point, look at the highlighted code. This is the same. Right so standard operations input times weight supply 10h add a column of ones, multiply that by output weights that gives me the value, then take that value subtract it from the target that gives me error, use those error values to make the backward pass. All right, so now I need to approximate the first and second moments right these are going to be estimates about how fast I've been going down the gradient, so far, so let's think of it as like, you know, momentum and acceleration or something like that. So what I'll do is I'm going to look at the gradient on this is going to look at the gradient, the error gradient with respect to W, because I want to look at how fast I'm kind of getting down. Sorry, this is all of them together. So maybe when we look at how fast I'm moving down my gradient defined by all of my weights. And so what I'm going to do is I'm going to take beta one times empty so the kind of existing movement estimate moment estimates, and then incorporate the decay value. So remember when the first moment the is kind of the, the mass, quote unquote, store the center of mass the center of the product distribution, and then V is going to be the second moment of the variance. So I'm just going to take the square of my gradient. And then I will use the beta two value to optimize that BT. So I'm going to use these beta one t and beta two t's to basically correct for bias. So this is going to be so beta one t is just going to multiply that value by the currently the pre-selected or the set value beta one, do the same for beta two, and then m hat and V hat are going to be these bias corrected estimates. So in this case I'm going to take empty divided by one minus beta two t, and then now obviously for for BT. So now how do I actually perform weight update so the formula is slightly different here. So you still have the same components, except instead of the error, I'm going to be updating based on these moment estimates. So these moment estimates are kind of not averages but derived from the previous end set of updates. So again if I've been moving really really fast my error is really really large, I can justify taking a bigger step. So if my error is pretty small, if it seems to be moving in that direction, I should take a smaller step or I should take a lesser less big step. And so what I'll do here is I'll take m hat divided by the square root of V hat, add an epsilon just in case V hat is zero for some reason. And then I'll multiply that by row, and that's going to be the amount by which I update all my weights. So, store in the error trace, and then I plot, I create some testing, test data and evaluate it using the forward pass just in a single line. So here, remember, so X test one, that's our inputs testing inputs multiplied by V that gives me Z, or let's just say A, apply 10H to that to get Z, add ones to Z so this is going to be the same as Z one above, multiply that by W that gives me Y. And then finally, just plotting as before. So, let's go. So what do you notice comparing this with Adam compared to SGD, by the loss curve? Yeah. Yeah. Yeah, I didn't really, they sort of, I mean, the individually converge, right, and they sort of, they kind of plateaued a value, but it's maybe not quite as neatly aligned, right, we see here, the test RMSC is actually lowered first in the train, and then around 10,000 epochs they tend to switch. What else do you notice? Yeah, the train here. So, I mean, we may, we might be working with the training data a little bit. But what, what did you just notice about the shape of the curves? The Adam one is much more, give me a word, abstract, or just I was going to say just like bouncy or something. It jumps around a whole lot. In particular, we see that it's not usually with SGD, you see a nice curve from a high value to a low value and it just sort of reaches some minimum value and tends to stay there. Right. With Adam, huge drop, slight drop, bigger drop. Oh, now we're going back up again. Go down again. Okay, now we start climbing. It sort of looks like, you know, when I'm running a marathon, I like to look at like the elevation map beforehand and this sort of right here looks like that looks like that part that was put it like mile 29 or mile 19. That's just like hell because I'm two hours into a run and maybe climb a hill. So basically what's going on here is that this is skipping out of the gradient. So there's some minimum that it's encountered here and it might be a it might be the global minimum, it might be a local minimum. And its moment estimations are such that you're multiplying that by the step that you're taking across the gradient. And we say move down the gradient because the goal is to reach this minimum, but the gradient is just some surface in multiple dimensions. And I just sort of keep moving in the steepest direction. If I'm moving the steepest direction from where I am now, if there is sort of, if I'm moving in this direction, there's suddenly a big hill, or maybe even just a little hill. So let's say my slope here is very steep. So I'm moving down it very fast. And then there's a slight upward hill in this direction. So my step size is sort of from the tips of my fingers, then, and I'm starting here, where my left hand is, then I'm going to be taking a big enough step that basically lands me over here. Even though the minimum is somewhere here around where my belly button is. So effectively by taking a step that big, I have kind of skipped over that minimum. What Adam is hoping to do effectively is that it's hoping that maybe this is a local minimum. And so by moving past it, I can get on a trajectory where somewhere further along, I will find another minimum that maybe will lead to the global minimum, if one exists. So here what seems to be happening with this data, and if I were to run this again, even with different numbers, this might well change, is that there's a minimum in the gradient of the training data that we are able to find. We get practically down to zero here. There's some fuzz here at the end that suggests like maybe we're taking a really small steps back and forth across some global minimum. For the testing data, remember that every the gradient defined by every data set is going to be different, even if they resemble each other. And so the minimum for the training data that we've calculated is maybe not the best one for the testing data. So perhaps the minimum, the best minimum for the testing data is back here somewhere and we actually kind of found it, but in the training data, there was still somewhere else to go. So perhaps it was not actually able to find that. So Adam has some advantages and disadvantages. But in this case, if you look at where our error curve is relative to the training data, it's doing really, really well. So questions. Yes. So if you wanted to hit, assuming that that dip is the global minimum, to kind of change this to better hit that dip and not bounce out of it, could you be changing the betas then? Yeah, so you can you can change the betas. You can change that exponential decay rate. You can change a number of things, right? You can change the betas here. You could change the learning rate itself, right? You can change the number of things, right? You can change the betas here. You could change the learning rate itself, right? How much am I scaling the whole thing by? Maybe I just, this data says that I should be taking smaller steps overall. I can even potentially just change the number of training epochs. So like, I can only see this after the fact, but it sort of seems like maybe I hit that minimum at 10,000. Maybe I should have stopped training there. You can use some techniques like early stopping or patience, where I can see if I use, say, my validation accuracy as my criterion, I can sort of say, if I don't see an improvement in my validation error in this case, if I don't see an improvement in my validation error for like 10 epochs or something, I'm just going to say I'll stop now because I'm probably not going to get any better than this. So the number of techniques you can use. The problem with this is that this dip here is in the gradient for the test, and I'm just plotting this for comparison. When I'm training, I do not have any notion of this, right? So I'm kind of trying blindly. I'm hoping that training against this data, I'm going to be encountering test data that resembles this closely enough that this will be a good model. And here, even though we kind of generate it from similar formulas, it seems to be a decent fit. It's not like the RMSI values are huge or anything, but maybe not as good as it could be. Other questions? Yeah. Yeah. Yeah. So basically here, we're looking at this. This is in case the hat ends up being zero. So that is your movement. This is the variance of your your second moment estimates. And so when would this be zero? This would be zero if the past and moment estimates that I'm sampling are all the same. Right. So this would this would indicate a couple of problems. One, you're not converging. You're sort of you're you've got you've gotten the exact same error value for the past and epochs. But that might be the case, you may sort of plateau for a bit and then and then sort of find your way out. So in the case that like you're going to be plus epsilon, you're going to be dividing by this. But let's assume this is zero. Right. So this was zero would mean that I am kind of getting I've been getting the same error for the past and epochs. So what I'll do is I'll add some epsilon and then divide by that to kind of hope that the first moment estimates can just be enough to sort of shake it out of this this run. This can allow this can cause us to kind of bounce out of that gradient. So, for example, if this value is too small, you might end up sort of taking a big leap. It's like I'm not moving. I'm just going to take a leap of faith and I'm going to jump way out there and hope I land on a favorable part of the gradient that allows me to do optimizing. So you don't want your epsilon here to be too, too small in this case, because you might have situations like that. You don't want to be too, too large, of course, because then you one don't want to just you cut your your move and estimate down to nothing because that also would stop training. But then you also don't want your step to be too small to be effective. So again, this is sort of one of those hyper parameters in the Adam paper. They found kind of what they found to be best values for the betas. And I don't recall if they found like a best value for the epsilon if I if they did, I should have written it down to get to the part of the notebook. Other questions. Yeah. So, finally, Let's take a look at this. If you remember the model for SGD was sort of just a straight line. Right. It's the green line is just kind of straight. So it sort of optimized. It didn't really make use of the nonlinearities available to it, to be quite honest, just sort of optimize the straight line through the data that approximated the correct slope. Here we can see that the blue line is the is the training data. The green line is the model that follows it very closely, which is also witnessed by this very low RMSE value at the end of training. So it's doing a really good job of fitting to the training data. And it's, it is getting the general shape of the curve of the testing data but not as well. Right. So you mentioned it we're fitting in here, this might be a case where we are fitting very, very closely to the training data and not fitting as well testing it's It's not like it's a bad model. This is a pretty simple case. It's still getting the overall shape pretty well. But if I'm looking at training versus testing, then it's definitely doing a much better job fitting to the training data than this to the testing data. And then finally here are the, the actual hidden outputs for this. For this sample so. Okay, questions on on Adam. Yeah. Um, yeah, so you as you want it. Well, I guess the obvious one is like it's simpler to to implement so because getting started. I would suggest mastering SGD before moving to Adam. Also, it does have SGD is a little bit more deterministic. So when you're in assignment to, for example, I believe you were asked to compare and contrast SGD and Adam and you'll see like the Adam loss curves you get these kind of more stochastic things where they bounce around a bit more, meaning that you may have these kind of spikes, where you are popping out of some minimum, and there is a small chance that you sort of get off on a wrong track, and your training actually you know will sort of collapse after that point. So SGD is like somewhat more reliable, assuming all other conditions with regard to the quality of your data are true. A bit more deterministic, easier to implement has fewer moving parts. So, you know, one of these things. These are things that you can all try, like when you have when you have a different, different types of data you can try you know different types of optimizers Adam and its variants are extraordinarily popular. And so basically, the everything is built on a backbone, but SGD is kind of, it's a bit pedestrian right everything goes really slow, you're just taking very slow deliberate movements along this gradient. So it's not good for really big tasks. Adam will allow you to converge faster at the cost of maybe a little less predictability and some risk of kind of going off into the woods and you're training failing once in a while. Yeah, other questions. So, you know, Adam variants are used everywhere. So most of the large language models are trained, usually using something called Adam W says Adam with weight decay so you actually specify a value where if a weight has been updated in the long time it's it's kind of assumed not to be important. And so then it's valuable kind of attenuate. So I guess the corollary of that is basically the larger model you have for a given task, the higher probability there is that some subset of those weights are just irrelevant. So if I have a trillion parameters in my modern model to do, you know, diffusion or language generation or something like that. Probably don't need all those billion parameters some 10 million of them might be incorrectly optimized or could just as easily be zero and really wouldn't change the performance. So this allows us to that property sort of allows us to do things like fine tuning where I can assume that for a different task, maybe I can use those weights that aren't really being used and better optimize them for task performance. So, these different optimization techniques allow you to kind of leverage different properties to neural networks. So, it's now 237. So I will start the next notebook which is going to be on finding good parameters. So, controls, go to number seven. All right. Let's start this. So, optimizers data partitioning and finding good parameters. This is a notebook on a couple of different topics, all of which are going to be important for doing assignment to which I'm currently planning on assigning Thursday. But basically optimizers are these fun, these operations like SGD and Adam, right, how am I actually managing my movement along with gradient in order to try and better fit to my data. So when I talk about the optimization function this is going to be, you know, I've used SGD or I use Adam or I used our mess proc or use Adam W etc etc. So, I guess when discussing Adam, we just discussed how to create the single vectors of weight values. And so we can also view parts of that vector and find the weight matrices for every layer of a neural network so I can say, I got my entire weight matrix and I just have my, my update vector, and then just by performing a single operation I'll be able to update all my ways at once, that's been much more computationally efficient. So, here is a function that will do that will create the views on a weight matrix automatically so let me just create a random sample. So I'm going to create it and I'm going to create this, this just this object. So in actually view the values I have to slice it. So, now I can see, instead of a three by three sample I just have nine values. So now I'll define this make weights and views function. So what this does is it creates the shapes of the different weight matrices let's say I've got you know weights V and weights W and maybe other hidden layer weights of pre specified shapes. And so then I will take, I'll take all of those and then stack them into a single vector. Right, so this gives me something like this array. So let's create this all weights vector. And then I will, in this case, I'm just initializing it with some uniformly distributed values. And then I'm going to build this list of views. Remember a view in Python is just a shallow copy. Right, so if I change the value in the view I actually change the value in the original, as opposed to a deep copy which creates an actual separate place in memory. Here what I'm going to do is by creating the views I can then change the value in the sort of you shallow copy, and that's actually going to change the original weights. So what I'll do is I'll reshape the corresponding elements from the vector of the all weights into the correct shape for each layer. Okay, so for every shape in this list that I passed in, I'll create basically a view onto the always vector in that appropriate shape. So now this allows me to treat V and W as separate objects but if I modify my V and W objects I'm actually modifying my all weight vector. So what this does that is that when I actually get to my training step, I can have my update vector, and then just apply everything every operation to every element of the weight vector at one time. So now this allows me to keep things organized in that I can just see like what the values of say V or W are, if that's all I'm interested in, while still maintaining the computational efficiency of a single operation at training time. And you know let's take a look what the shapes of the weight matrices would be if I had a neural network with two inputs, two hidden layers with 10 and 20 and 10 units respectively and a single output. So I'll build that. So what I'll do is I'll specify the number of inputs and in number of hidden units per layer, the number of outputs, and then I will initialize an empty list that will store this so for every, for every hidden layer. What I'll do is I'll append one plus the number of inputs, right, the one because of that bias column. And so it's gonna be one plus one plus number of inputs by the number of units in this layer. Right, so if I have two inputs, this is going to be one plus two so three by the number of units in layer. First one was 20 so we get three by 20. So then, the next thing I'm going to do is I'll. So then for each one right if I have three by 20, and then one plus 20 so 20 is what comes out of that first layer. One plus that is 21. And then we just have a single output. And so then what I will do is I will append you to one, and then plus one by an H, and then I'll set an into NH because then H is the number of inputs to the next layer. Right. So I can just do this. All right, so then I'll make weights and views. And so now these are like all of the, the weights right so you can see that this is, these are what's how much is this 20 by three by 20. And then 21 by 10 maybe, and then 11 by one. Right. So let's make some data with two inputs per sample and a single target output. So what we'll do now is we'll have our target values, the x and y coordinates, and then we'll make basically a kind of a train map, so we'll have. I just have like a square area. So I'll just make a coordinate and then I want to create some hills by specifying the z coordinate is the output. So, what I'm going to do is I'm just going to create like the surface, where my inputs will be say latitude and longitude, and my output would be like altitude or something like that. So, what I'll do here is, I will specify some centers. So this is like in my 2d plane this is where I want the centers of my hills to be, and then the heights for each hill. So, Judy coordinates so we can we can look at this as like my input would be two by two, and my output will be five. Right, so this is these are coordinates and then this is the height, whereas for an input of five and four, my output will be four. So you can see here just by looking at these, you can see that this is like a highly nonlinear in that I have the same output for or five for two different entirely different values like two and two and eight and two have the same output, and then five and four and three and seven also have the same output. Right, so very nonlinear function. So this should be something that I, you know, wouldn't require a neural net to to actually predict. So I'll define this calc heights. So why am I doing this, because I don't want to just have like is really sharp and don't have a completely flat plane and then like one point in the air, but I want to actually have a surface. So, what I'll do then is like for every point, I want to be able to take in a value that's not one of these inputs and calculate the appropriate height based on will assume you know a circular hill with an even drop off from the from the peak. So I'm going to plot these using this mesh grid function. So what this will do is it's going to take, you know, take coordinate vectors and return coordinate matrices is going to create on nd coordinate arrays for vectorized valuations of nd scalar vector fields over nd grids. So that is basically taking these numbers here and turning them into a nice even surface. So what I'll do is now I'll create my surface. So I'm going to have two evenly spaced lines. So I'll just have an x axis and my other x axis in this case just by two horizontal surfaces, and then I'll make a grid out of these two arrays. And then I will allow now printing x will allow me using the mesh grid function, allow me to show the coordinates of every point in the 2D grid. So if we take my 20 points, I have a point at 00 and also have a point to like point 50 and so on and so on until I get to 10 right so I've got you know zero evenly distributed numbers until 10, and then like the next row of zero through 10 next row zero through 10, and then a corresponding column at each of those evenly spaced values. So now for each of these and I have like 400 points for each of these I now want to apply the calc heights function that's going to give me, according to the hills and centers that I specified previously, what the what the values for each of these points would be. So this allows me to take now not just like five points, but 400 points and create a relatively smooth surface. Any questions about what I, what I'm going to be doing here. So now running the calc heights function, given the centers, right, this is going to be the height of each point in the grid. So, if I have this should be 400 by one. So remember this is H, right H is 400 by one, X was the 400 by two array here. So if I had a 10 H stack, if I stacked X and H together, what would that give me. First of all, how many columns do we have. If I stacked X and H so X has. So, so, this is this is a trade here. Right, this is X. So if I stack H and X side by side, how many columns do I have? Three. Right. So if I now have three columns, what do you think say a row of this stacked array represents? XYZ, right? Yeah, so this should basically be the 2D coordinates on the ground, and then the height of the hill at that point. Okay. So now you kind of get a sense of what probably this is going to look like that's actually visualize this. So I'll use, you know, just some visualization toolkits, just using axes 3D, I'll be able to plot these in three dimensions. And that gives me something that looks like this. Right, so I should have 400 points. Basically, these are my two horizontal axes, and then these these peaks, these are those five hills that I specified, and it's kind of smoothly interpolated the surface between those. So now you can see the highly nonlinear nature of this function. So, I know we're just going to play with lighting for a little bit so we can make this look a little bit cooler using light source. So this is a library that basically creates a light source from a specified point and it will render it, render the surface in sort of a really nice way. So now if I put a if I just kind of take a point light like over here and shine it on my on my surface, it looks like that. So it starts to look more like a landscape. All right, so now we can make some data. Right, so the whole point of this. You can use these cool visualization tools and it'll make your your projects and your assignments look really nice. But the whole point of this was to actually try to fit to this to this surface right. So let's make some data so the axis is going to be these points on our base plane, these are going to be the inputs, and then the target values for T is going to be the height of those points. So for every point in this on the surface, I should be able to calculate a height for that, because I've already created a smooth surface I've got this calc heights function that should be able to take in a number arbitrary number and give me the height of that point on the surface. So, what I'll then do is I've already calculated z right use those are the heights for each of these. So just make those into my target values. Now this is the thing I'm trying to predict. So, if you look at the shape of this data I now have 1600 points in my inputs so these are going to be 1600 by two right so this is 1600 x y coordinates, you can call them, and then 1600 associated z coordinates so now if I just have these stacked together y z coordinates if I slice off that last column and make this the thing I want to predict, I can set this up really nicely as input to a neural network that has two inputs, some a miracle occurs in the middle and then there's an output. Right, that's what we're after. So, we observed in the previous notebook that there may be a tendency towards some overfitting. So, who can define overfitting for me we've kind of alluded to this term and I'm sure many of you know what is overfitting. Yeah. Yeah, so we have overfitting is where the model is really good at optimizing into whatever patterns it finds in the training data, and it becomes so good at that that it's not good at anything else. Right, it's sort of like you you train for years for use the running metaphor again right you you saying bolt and you like train your whole life to run the hundred meter dash, but you fall apart when it comes to marathon, because although there's like a superficial it's really not at all like the thing that you've been training for so you say in both overfit to the marathon, and Elliot Kipchoge the guy who will like won the has the world record marathon overfits to marathon right and neither of them is going to be good at the other sport. There might be better than the average person at either, but they're not going to be particularly good at that other sport. So it's also like, you pick your metaphor it's like taking a football player and hope and assuming he's going to be like an Olympic League swimmer or something like that. So, how do we avoid this yes. So I said it again. You know, that was designed to like to detect one thing, but it's better detecting everything. So, just to repeat the question if you have a neural network that's like, was that you designed to do one thing but ends up being better at something else in that question. That would be a very weird case. Yeah, so you could argue that that is maybe a kind of overfitting, but it's like, it's sort of something must have gone longer in training in that point. So I guess over overfitting is not necessarily. You try to use a model of a design for one thing to do something completely different. Like, yeah, sure it overfit to the data compared to this other thing that doesn't resemble the training data at all. But you can't assume. So, that would be like trying to use a hammer in place of a saw just because they're both tools. Right. It's like, hey, you have different tools in your toolkit. I do not expect to be able to chop down a tree with a hammer. It's going to be very, very difficult. So, those are design choices that you can make and like if you assume if you design your network assuming that you're going to do one thing. It's not fair to apply it to a different thing. So you see like, you know, that kind of bad faith critiques of some some some papers like, well, your network doesn't do this. Well, it was never designed to do that. So, this is not the problem I was trying to solve. You're really not making a fair criticism of this right there. Probably other things you criticize it for, but that's not it. But that being said, generally the goal for neural networks if neural networks are universal function approximators that is if I'm assuming that there's a function that maps for my input to my output, and my job is just to find that function. The neural network is a universal function approximator meaning that with the right combinations of nonlinearities and layers you can approximate any function in principle, it just might be a really gnarly function that takes forever to approach. Nonetheless, you can do it. So the goal with neural networks is that always some level of generalizability. So what we do, we don't want to have an you don't spend all this blood sweat and tears in training this neural network that does just one thing on one data set once I want to be able to reuse this, at least somewhat. So, one thing that I can do is make sure that the data that I'm going to evaluate on it's never been exposed to before. Right, so this was something that we kind of have slipped under the radar with assignment one and that you don't have to do the train val test split. Because the data is friendly enough it's actually cyclical if you look at its temperature over a year so like the last date is very similar to the first date. So it works nicely there. But generally speaking you cannot assume that's going to be true. So what do I do there. I want to create these train validation and test sets. So what's the role of each one of these obviously we know what the training data is for this is what you actually fit your model to. And the test data is some unseen data that I've never, the model is not been exposed to. And this is what I want to actually perform well on this is where I'm going to basically prove that my model is a good enough approximator to this thing that it's not seen before that's what makes the argument that I've actually approximated that function that I'm searching for. So, these things involve a significant amount of computational power the examples that we're going to be using are not really all that big but we're all familiar, you know, with, you know, the large language models large vision models. Those take forever, and neural network training has taken forever for a long time one of the reasons that it didn't take off initially was that it took a mainframe the size of this room to do simple digit classification. And so I'm like okay this is a great party trick. I don't see what it's actually useful for. Well now we have the technology, the technology and the tools to speed it up, such that we can actually use them for real things, but it still often takes a while so again I don't want to sink all this effort into training this neural network and then run it on my test data and find out that it completely collapses. I want to have some reasonable, some reasonable assurance that I'm going to perform well on my test data. So I'm going to do for that. This is where the validation set comes in validation set is an extra set carved off of your training data that during training you continually test against right you're not training on this data. So you should not overfit to this data, but you can check your model against that to see on this other unseen data set that is not the test set. So this is what I can use to tune the model. Am I performing reasonably well. And so I can use this to find things like good hyper parameters I can kind of see like is my learning rate too high am I skipping over my my optimum the gradient is, is my neural network the right size. Do I have the right number and appropriate number of nonlinearities in it so these are the sorts of things I can, I want to be able to do. So I have a nice property of the training validation and test set to resemble each other. Right. So if I'm training on this hill date I need to get like a roughly uniform sample of points on this mesh. Right, I don't want to validate only on this corner, because this is not going to give me an accurate picture of whether or not my model is fitting to the rest of the data. So, effectively what I'm going to do is I'm going to train on some points on this mesh, I'm going to validate and other points on this mesh, I'm going to test on further points on this. So that's what I'm that's what I'm looking for. So what I'm going to do then is going to shovel the samples into a random order. Right, we did this before. I'm going to partition the data into endfolds and maybe say, I want to create and some partitions of this data. So I'm going to put the data in the first fold to the validation set. The second fold to the testing set and then the remaining folds into the training set. So, generally we want to have both your data in the training data, which will have a substantial enough sample in the validation and test that you can be reasonably assured that your model is going to perform okay on that data. So what I'm going to do here, this is just going to do the, the, the proceeding in code. I will shuffle the row indices we did this already in one of the previous notebooks, or to specify the number of folds here I'm going to do five. And then I will divide my number of samples by the number of folds round down to make sure that I always have an integer. And then I will accumulate those different folds into the different samples so like I'm going to in this case, just like I mentioned I'll take that first forward make it about set second hold make the test set, and then the remainder, make it the training set. So now let me print out the number of folds five, the number of the number of the first fold, and then how many samples in each one right so 320 by two, two inputs and 320 by one that's that output. So now what I can do is I can specify which one of these I want to use for which fold so the. So, the x x validate and t validate is going to be fold zero right there are two elements here. The first one of which is going to be the training the training and the second of which is going to be the targets. So, then I'll do the same thing for the first four this is going to be the test or the second for the test set, and then I'll just stack the rest together into the, the training data. So, now if I look at the shapes of each of these. The training data is 960 samples, each of which has two inputs and then the targets, and then I have 320 samples each in the validation the test. So, this is a pretty generously sized validation and test set in that the validation and test set or like one third the size of the training data. In this case that's okay, because there's like not that much noise in my in my data that I'm trying to predict. But the size of your validation and test set is another one of these things that you want to be judicious about choosing when you're when you're performing training. Okay, so now I'm going to basically run a solution to assignment to that you won't see because I've saved it off previously. But we'll see how we can actually use a neural network to fit to that that hill data. So, what you can do, you're not required to do for assignment to but once you complete the neural network class definition you can save it off into a file called neural network.py. So, assuming that you perform well in assignment to you now have a implementation of a neural network that you can then just reuse indefinitely. And then later, when we are through with assignment to I will actually just give you a neural network implementation to use and other things, just in case you know you're not so you're not relying on a potentially buggy implementation of a two. Okay, so I've got this neural network up you Wi Fi all import that. Now let me look at just the size of my dimensionality of inputs right two samples, and then a single output for each sample. So now what I'll define is the actual hyper parameters of my neural network the actual architecture, so the inputs the input layer is always going to be the dimensionality of the thing that you're measuring right so if I've got two samples or two inputs there should be two things right two nodes to inputs to the network. This list here this all to specify the hidden layers, so 10 units and five units. This could be as long as you want so basically the way that this, this neural network class is written is I can just add numbers to this list and will automatically create a new layer of that size. So finally I need to specify the outputs and of course the output is going to be how many things am I trying to measure right so in this case I've just got one thing that I'm trying to measure the height of the land at that point. So it's going to be single output so and then the default here, the is the 10 h activation function, because we have not talked about others just yet. Okay, so now if I just print the neural network. This is sort of in my my non pi neural network implementation. This is my architecture, so I've got two inputs, one layer of 10 units one layer five units one output of use the 10 inch activation function. When you're using libraries like tensorflow or pytorch, they do come with a handy print function where you can actually print out the number of architecture and see you know the sizes of the different layers, and all, all the fancy things that you can do there like different activation functions and residuals and whatnot. So, I find the train function that is much like what we have done previously. So again, a generic one that takes in the inputs the targets number of epochs I'm going to train for learning rate and then method is just the optimizer so here I've been using SGD. So, let me run it. So you can see it's working. It's pretty fast. This is not a very simple, not, there's a pretty simple problem. So my error after 10,000 20 epochs ends at 1.19. So I can experiment with a couple of other things right so I can try and see what happens if I use Adam instead. Right. So in my implementation I, I've got a way to basically just pass in which type of optimizer I want to use it will use that method. So let's explore using Adam instead. And we can see, you know, first of all here we can see firsthand one of the benefits of Adam so in both cases I've trained for 10,000 epochs. But I've even in the first thousand epochs of training with Adam I got lower error than the whole training for 10,000 epochs of SGD. Right. So this is one of kind of a tangible demonstration of the benefits of Adam training takes about as long as case it takes about 2.61 And these little spikes here, right, that's the sort of skipping across the bottom of the optimum so it's, it's getting there and maybe trying to find its way out finding that that's not a good way out going back down and eventually to sort of settles there and maybe is a little bit of movement And then we'll see when you you do assignment to the Adam optimization is a little more approximate. Basically there's a sort of a toy test function you can use just approximate the minimum of a parabola and SGD will get you there exactly. So we know that that that is the minimum of that parabola and Adam will get you close, but not quite close enough to be useful, but not exactly. But I would I guess close enough to be useful but not exact is like a pretty good motto for most of machine learning. Okay, last thing we'll define the RMSE function, you all are probably familiar with this already. And then I will have defined my use function and I'll just apply that over my training data, and then just print the shape so basically here this is giving me the actual output so this is being stored, and then I'd put the shape, and that's 960 by one. So over the x train, right 960 samples each of two inputs through the neural network, and then it gives me an output for each of those. So let me print some, some values. So what I'll do here is I will put the RMSE for the, for the, the training data the validation data and the testing data so basically what this is doing is, I'm just computing the RMSE for my chart my train targets, and then my prediction so and then dot us of X training that's going to give me all my predictions. And then you can see that I've done a pretty good job at splitting my data such that I'm training in a way that is allowing me to get reasonably close testing or validation and testing error. Yeah. So you you you can do that. This is this is something I guess I, I guess I admitted that here so this is something that you can set as a as a criterion in kind of TensorFlow or pytorch. Here we're not really doing this because this is sort of homebrew implementation. You can set it up so that you're. So you can set it up to do that and print out you know every and epochs, what's my validation accuracy or error or whatever metric I'm using based on the state of my network right now. So you can think of what we're doing here at the end is, I've just doing a check on the validation data before testing, so what I can do here is, if I am not sure, right, so you're not supposed to touch the testing data until you're ready to apply. So what what often happens is, you know, evaluators will keep like a hidden testing set, and it's just like you don't get to see this, you have to write your own network the best you can. And then you send it to me and then I will apply and I'll tell you how you did. Here we're not doing that but you can imagine that we did. So testing data is often this. I want to know how I'm doing, so I will check against the validation data and if this value is lousy. So if I just print these two, and it's like okay instead of point 1138 it's like 10. So testing data is often this. I want to know how I'm doing, so I will check against the validation data and if this value is lousy. So if I just print these two, and it's like okay instead of point 1138 it's like 10. Like okay, this neural network was wildly bad. Something's wrong. I need to like add more hidden layers I need to add use a different activation function, whatever it is. So then I can try and change the hyper parameters of my neural networks so I get a lower value. Okay. Yeah. So, it varies in the so if I remember correctly in assignment to you're given it's like a similar when you're given template code, and you have to fill out like train and use function and something in, you have to change the optimizer to change from SGD to Adam. And then in assignment three you have to do some sort of grid search. Okay. All right, let me, let me get through this in the remainder of the time I think we're almost there. Okay, so we trained right it seems like we're doing a pretty good job. Let's actually try to visualize this so now what I want is I want to be able to take my, my surface. Show all my training points show my validation points and show my testing points. So let's take a look at that. And this is what we get. So, the blue points are trained the yellow points of our Val and then the green points are test. And so we can see that we seem to be doing a decent job. Right, so the training points. It's kind of it's a little bit difficult I don't think I have this no it's not set up to rotate. But you can see the training points are like very closely fit to the surface, and the validation points and testing points are mostly to there's a few like here's a testing point that's kind of maybe not so close, but it seems to be doing a pretty good job of being able to predict the height of the surface from the, the x y coordinates. All right, and then finally, let's see if we can visualize what the hidden units that actually learned. So previously, we had these lines saying okay for this x value, my kid and you and is outputting this value. Let's see if we can do something for for this data. One of the things to notice like as we get close to the edge here you'll see how we kind of see these these dips here. So, this suggests that basically it's not very good at optimizing for this local neighborhood probably because of a lack of data off the off the surface right we have no points here. And so it's kind of continuing transit may have observed from this direction, leading it to be a little bit lower than the actual value. Right. This might be a reasonable prediction of like what might happen if we extend to the terrain in this direction, but we don't know. So this is, I'm not going to go through this code in any real, real depth this is just for visualizing the outputs of the hidden layers three dimensions. So, this is what we get. And so you can see now how this is not necessarily very interpretable per se. But we will tell you for each input what the unit and every hidden layer is putting out. And this might be useful for, you know, if you want to trace the path of like a single point, right, we want to figure out like why is this testing point down here, then you could actually calculate what each hidden layer is outputting with a network of this size So this is like reasonably tractable like you could probably do this math if you were motivated enough. The black box nature of neural networks just comes from having it happen at scale. Right. If I have 10 or more, you know, dozens of hidden layers each with thousands of units. It becomes really hard to trace you know what's going on with a single input. All right, last thing we need to examine the effects of various hyper parameters so we can very different things right we can try different lengths of training different hidden layer structures. Different, you know, different optimizers etc so you can try all these things. And each of these are called hyper parameters right the parameters are the weights. My neural network is a function or combination of functions parameterized by weights. So what I'm trying to solve for are those parameters. So those are the coefficients, the hyper parameters are the things I actually have direct control over. I don't go in my neural network and tune all 1 billion of my weights. What I want is a function is going to let me do that automatically. And in order to achieve best performance, I'm going to be looking at those things that actually have control over such as the learning rate training time model architecture optimizer etc etc. So the hyper parameters are just the property of the architecture that you actually have direct control over. So what we can do now is grid search, but I can say that for different combinations of learning rates training durations layer sizes and optimizers and whatever else you chair, you care to examine. I'm going to instantiate a version of the neural network with those hyper parameters, train it, see how it's doing against my validation or my test data. And then from that I can decide which one I want to use. So, the role of the validation data is really, is really key and I'm trying to find the right hyper parameter combination so if you assume the test data is not to be seen until testing time. I don't want to tune my model on my test data that's cheating. So instead I'm going to tune on the validation data that I'm just assuming is reasonably resemblant of the test data. So in this case, this is similar to what you're going to be doing I think in assignment three, except that's classification, I think, maybe. So what you're going to do is you're going to try a bunch of different hyper parameter combinations and then try to observe trends like what happens as the learning rate decreases or what happens as I train for longer, or what is SGD doing worse than Adam and by how much. So due to time I've already run this I'm not going to go through this again. But what I want to look at are which parameter values are best. So, I can plot them. Right, I can just plot the RMSE, but that's not really helpful but I want to look at is every plot with respect to hyper parameter values. So, oh, what happened there. Well, a bug happened. But if we go to the one on the calendar should give us a version that we can look at. Come on. Two minutes. All right. So, all right, so now we can check out the different hyper parameter combination so for example, if I want to look at what happens that to the training validation and testing when I use the SGD optimizer, we can clearly see that this is a larger architecture train for longer, and that seems to work best. Right. Whereas for Adam. I can see that I still get that with the architecture size seems to matter more. Right. So here I have 1000 epochs and 5000 epochs with the same architecture, and I don't get that much improvement when training for 5000 epochs so this suggests that if like I'm trying to optimize for compute time maybe Adam is going to give me the best bang for my buck which we observed already in that previous notebook. So what I can look at is just put it on a pandas data frame, and then we can actually compare each one so I can see you know, where do I see the low RMSE number as well. I see them with Adam more than SGD, and I see them with larger architecture so it seems like the things that are most important are going to be the architecture size and the choice of optimizer. Once I've got that, I don't get much benefit from training for much longer. Right, so I get a minimal benefit from training for 5000 epochs versus 1000 epochs. Overall, yes, the best thing to do is just to train the hell out of it. 5000 epochs with Adam on this big architecture. Yes. This is true right yeah and so that is true what's the reason for that so if you look what's on the y axis RMSE. Right. So we also see that these two, like the the best SGD performance is not at all comparable to the best Adam performance. The best Adam performance is like way way better than the best SGD performance so if you were to plot both of these on top of each other we basically see like SGD kind of up here. And then Adam is really kind of showing how strong it is. So, this is sort of a method of deciding like what are the best type of parameters for me to use for this data. That is all for today. So, assignment one, two, on Thursday if you need an extension, remember reminder to get that in by your request in by tomorrow for my consideration. All right, we'll see you Thursday. Yeah, it's them. Yeah. Okay, let's get going. Very, very good chances. Class is going to be short today because if you look at the schedule today is the ninth and we actually finished this notebook on Tuesday. So we're actually like a day ahead and Raylu is a pretty short notebook. So I'm going to do that and then I will assign the second assignment and I didn't bring my GPU machine so I can't run half of the next notebook. So I'll just call it a day after assignment two and then I will set up my GPU laptop over the weekend. All right. So assignment one due tonight. If you don't have an extension, of course, some of you requested one, some of you have arranged one. So but for the rest of you, you if you hear me say this and you're like, oh shit, I did I forgot to request an extension and I really need one. I think the best thing for you to do is to get something turned in today. Okay. So for various reasons about how I run this class, it is much, much more to your advantage to turn things in on time, even if incomplete, than to try to spend a couple of days getting things wrapped up and then submitting it late because you will get a 10% late penalty, well, 10 points, not percent of the remaining grade, basically like minus 10 for every day that it's late. It is definitely in your best interest to get something turned in today, even if you feel it's not complete. Just trust me on that one. Okay. Are there any questions about? Yes. I lost our special one. Yeah. So yeah, I will have office hours at the usual time at starting at 330. I will try to give you feedback. So you have a rubric about different points that I will send you. Excuse me. What we try to do is basically say, count it off X points for not doing Y. If you do perfectly, we'll say like great job or something. So if you get 100, don't expect a lot of feedback. If you do something like super cool in your notebook that I can't resist commenting on, don't do it. But if you just checked all the boxes, you know, just take your 100%. If you do get counted off, we'll try to indicate what that is for. Other questions? All right, cool. So what we are going to talk about today is ReLU. Who knows what ReLU is? A few of you. So we talked about activation functions. Activation functions are these nonlinear functions that we apply to the output of a hidden layer. And meaning that when we back propagate the error through the network, the weights in that hidden layer are effectively optimized with the assumption that a certain nonlinear function will be applied to that output. So that is if I'm applying the tanh function to something and I want a specific target result and then there's some transformation that needs to be applied before applying the tanh, the particularities of that transformation are going to be different assuming the tanh is there versus when it's not. Right. So that's the activation function. The activation function is constant. It's usually not parameterized very much, except in maybe some very specific ways. But basically it's what you typically do is you apply an activation function to an entire layer. Right. So you've heard of this thing called ReLU, which is this weird term that people use all the time and no one really knows what it means. So my goal here is to basically demystify machine learning in this class. And so one of the goals is to tell you what all these weird incantations actually mean. So if we're going to address nonlinear problems, and assignment two is going to be nonlinear regression using neural networks, we need some way to include nonlinearity. We do that with activation functions and any sort of nonlinear function can in principle be used to introduce nonlinearity. Now, if I'm assuming that my activation function after my transformation of my inputs by my weights is the tanh function, then my optimization is going to be performed in a certain way so that my weights end up at certain values. If I change that function, of course, when I perform the same optimization operation, the weights are going to end up different. The reason for that is because when we are performing back propagation into hidden layers, one of the things we have to incorporate is the derivative of whatever nonlinearity that we apply at the output of that layer. So if we had something that you can do, and it's not really clear why you would do this, is if you have a neural network where you have no activation function on any layer, and it's just a linear pass through, you still end up with just a convoluted way of solving a linear. And so what that means is that when you're applying these nonlinearities, when you're applying the computed, if you had no nonlinearity, if you remember how we performed back propagation the output layer, it's just the error term times the input to that layer, then if I had no nonlinearity on my hidden layer, when I'm trying to apply back propagation to that hidden layer, and the derivative of a linear function is just 1, x equals x, then I'm not applying any extra term to my back propagation operation. And so if that were the case, then the backdrop for the hidden layer would be the same as the backdrop for the output layer. So what that means is that when I change the activation function, I just change that one term, the derivative of whatever that activation function is. Given that, we of course need our activation functions to have some desirable properties so that when you take the derivative, values don't vanish, they don't go out of control, they allow you to keep optimizing and taking the appropriate size step down the gradient toward that global minimum. So as you've probably read, because I was kind of rambling, we have the desirable properties include computationally simple, because we want these to be fast, we're going to be applying them at a high number at a large scale, you don't want to be computing some arbitrarily complex function for multiple units and multiple layers. We also want for initial small weight values, that function should be close to linear, because as the weight magnitudes grow, we want the function to be increasingly nonlinear, such that when you reach magnitudes of extremes, you're not having those values greatly affect the input much more than input values that are maybe closer to zero. Again, the derivative of the function should be computationally simple for the same reasons as item number one, I'm going to be taking the derivative during backprop again at exactly the same number of activation operations that I formed during the forward pass. So that must also be computationally simple. And then the magnitude of the derivative should decrease as the weight magnitudes grow, and this should perhaps approach some sort of asymptote, so that if I have an extremely high value, it's not disproportionately affecting the adjustment in weights during backpropagation. And then finally, the maximum value of the magnitude of the derivative is limited. And as we will see, some of these properties are more desirable than others, and in some of them we can actually let slide a bit when certain other conditions are met. So given these properties that we identified previously, we also identified two functions and their derivatives that have these properties. One is the sigmoid function, given by this, and this derivative is sigmoid of x times one minus sigmoid of x, and the tanh function given by this, where its derivative is one minus tanh squared. Both of these obey these properties. Sigmoid, of course, is this S-shaped curve bounded between which value and which value? Zero and one, right? And the tanh function is bounded between negative one and one. So these two functions effectively have the same shape, just with different bounds. And so then the derivatives also have kind of, they have basically a hump in the center. So it's just that the maximum value of that hump is different for the two functions. All right. What do these functions actually do? Let's take a look at them. So we'll plot them with code that you've seen before. So I will define sigmoid function, the tanh function, and their respective derivatives and plot them using different types of lines. So here is the plot. This is what we see. So the blue lines, that's the sigmoid and its derivative. Red line, that's the tanh and its derivative. And so we can see how they obey these nice properties where, when I'm close to zero, this is more or less a linear function, both of these. The maximum values as the inputs go to extremes are bounded. Same is true for the derivative. It has a limited maximum value for sigmoid. It's 0.25. For tanh, it's one. And then also that the derivative decreases towards zero as the values go to extremes. OK. So we see the derivative of the sigmoid and the tanh function decreases as x gets further away from zero. Now remember the three things that we need to actually compute a weight update over some predicted output. So one, the learning rate. Generally speaking, what's the size of the step that I'm taking along the gradient? The error between the output and some target. How wrong am I? If I'm very wrong, the total step that I want to take would be bigger, scaled by learning rate. And then the gradient at z. How steep is the slope? If I'm trying to approach the bottom, I'm trying to approach a place where the slope is effectively zero. And if I'm on a steeper portion of that gradient, then that's going to affect how far I move down. So absent fancy things like atom, which also further adjusts the total size of that step based on things like previous weighted averages and variances. These are the things that you need. So you take some combination of all of these, and this determines how far and in which direction I need to be moving along my gradient. So what this means is that if the gradient is small, then the weight update is likely to be small as well. Generally speaking, this is what we want. We don't want weight updates to be too huge and shoot us off into the abyss or actually trying to approach the abyss, the bottom of the gradient. That's going to shoot us off into space up the curve of the gradient. So we like these derivative functions that don't explode as the magnitude of x gets larger. And we don't want that weight up to be too large such that we miss this global here. I should change that to global. We don't miss the global optimum entirely. OK. But if the activation functions are too small, there are some downsides as well. So if you have some function h, that's our activation function. And if the output of that function is too close to 0, then the outputs in that layer with that function will be close to 0. And sigmoid actually has this problem, which is one of the reasons why you prefer tanh. Sigmoid, we have really negative values, which may often be the case. You have values close to 0. And so as opposed to tanh, where the set of values close to 0 is pretty small, tanh of 0 is 0. And as I move away from 0, it linearly also moves away from 0 until I start to reach more extreme values. Then also, if the derivative of that function is too close to 0, then you can't communicate useful information to back propagate the weights. Because I'm going to be multiplying the change in weights by, among other things, the derivative of the activation function. And if that's really close to 0 or is equal to 0, there will be no change. This last point in multiple applications is generally known as the vanishing gradient problem. And it shows up in a number of different circumstances, most particularly in RNNs. We'll talk about that briefly later in the course. And among other things, what this can do is it can cause the training of the neural network to just slow to a crawl or stop. Because if you end up with a place where all of your gradient derivatives are 0 or a bunch of them are, suddenly individual neurons that might be useful for the problem, those weights are no longer updated. So I could end up with my weights at a place where they're arbitrarily far away from the best solution given that neuron in that layer. But it has no useful information about which way to go. So it just stops. And we don't want that, of course. And so both the sigmoid function and the tanh function can suffer from this problem. Everybody clear on the motivation so far? OK, so we use this thing called ReLU. ReLU stands for rectified RE linear unit. So the unit here refers to the neuron. So we have a neuron that this ReLU function is applied to it, and we'll call it a ReLU unit. And so you can also have ReLU layers where the entire layer uses ReLU activation. And we also talk about ReLU networks where all the layers, all the neurons use the ReLU activation. And it turns out that these ReLU networks actually have some very interesting properties that we'll discuss briefly at the end. So in the example so far, we've been using the tanh function for everything. It's like this is our default non-linearity. This is the only one we've explored so far. We're just going to apply the tanh function to everything. We don't really talk about tanh networks so much, but you could. That would just be a neural network where you use nothing but the tanh function. ReLU networks, because they have some interesting properties, tend to be considered a single group because those properties apply or appear to apply to all ReLU networks. The function itself looks like this. So already you can see that unlike tanh, this is a what kind of function? Piecewise function. So it's zero if the value of the input is greater than or equal to zero. Otherwise it's x if x is greater than zero. In other words, this is basically taking the max of zero and x. So there are multiple ways to write this function. And we will use the for the for our implementation, we'll just use np.max. So if I have some input a and I take np.max of a, I get this. So all it's doing is just taking the positive part of x. So if I am zero or less, the output is zero. If I am greater than zero, the output is x. There are some small debates about whether you need to bound it at zero. So it's like less than zero, it's going to be zero. Or if it's greater than or equal to zero, it's going to be x. We end up mathematically end up in the same place. Although I think people who study like real deeply the mathematics of machine learning may actually have some strong opinions on this, because based on where you make that boundary, sometimes the properties of the network may actually change. But for our purposes, I don't think you really need to worry about that. So here's our function. And it simply takes the positive portion of x. So what's Rayleigh's derivative? Right. So we'll describe it here and you can tell me what it thinks, what you think it looks like. So if x is greater than or if x is less than or equal to zero, then Rayleigh of x is a constant zero. So the derivative of any constant is also zero. So for this portion of the function where it's less than or equal to zero, the derivative of that should also be zero. So that should make sense. Now, if Rayleigh of x equals x, the derivative of y equals x is one. So we should all know that. And so therefore, if x is greater than zero, then d Rayleigh dx is equal to one. So the derivative of Rayleigh, also known as the rectifier function, is called the heavy side step function. So what do you think the derivative of this function looks like when I show it next? So I see a line. Yes. A line and then another line. What do we think? So we define d Rayleigh as the heavy step function. We get this. And this is what we said. We, of course, are assuming it's we're drawing it like the continuous function. So it does draw does its best to draw a vertical line here. And really, it's drawing it between negative point zero or between zero and point zero, zero, five or something, because we're sampling 100 points. But yeah, what we're seeing is it's zero from negative infinity to zero. And then suddenly, it's one. OK. So let's see what actually happens when we apply this to some weights. So first of all, I'll define this term s being a weight sum. And then I will just take create some random numbers. So let's assume that these are inputs to a Rayleigh function. So first of all, let's see what happens if I take the tanh function to this. So I get this. So if these are my original inputs, this is the tanh of that. So tanh of zero is zero. The tanh of the negative numbers are increasingly negative as the negative number, the magnitude of the negative number increases, and then also increasingly positive bounded at one as the magnitude increases. So basically, what we have is I have a bunch of a bunch of weights and sort of squishing them into a distribution between zero and one. Rayleigh, I can also basically take a function like this for some input s. For those elements of s that are less than zero, I will set them equal to zero. And then that will give me the output. So Rayleigh of s looks like this. So this is tanh of s where everything is squished into that s-shaped curve. And this is Rayleigh of s where all the negative values are simply set to zero. So if I take there's s again. So now, what about the gradient of s? So, well, I guess point here, just this is one of those peculiarities of memory. So here I put in s and then I modified s in the function. So I return s and I can take Rayleigh of s and it's going to give me this. And then if I print s again, because I modified s there, it's going to give me s. So just to be careful in this implementation, what we'll do is we'll make a copy of s. So I actually have a deep copy of s. I'll store that as y and then y will be the Rayleigh version of s. So I can return y and then if I print y. Oh, what's going to happen? s and y. There we go. All right. Now, what about the gradient of s? So I will just define again, d Rayleigh. I'm sorry. The gradient of s. s and well, so s, this s is modified in line, right? So this is modified in sort of this version, this non-memory safe version of the function that I wrote. So really, s, what we want to be doing is returning s to the original value here. So if I were to do this again. So. All right, so let's start from the top here. All right, so I'm going to define s. OK, now, s is this. I'll take its tanh. OK, whoo, whoo. Let me define Rayleigh using s inside the function. I'm going to take the Rayleigh of s. OK, now I get s with all the values, negative values to zero. Oops, I changed s in line in the function. So I actually changed the value. Of course, if I apply Rayleigh to this, I will get the same thing because all it's doing is taking values that are zero or less and setting them to zero. And so the only thing that gets, quote, changed here are things that are already zero. So nothing actually changes. So a better version of Rayleigh will make a deep copy of s. Of course, this takes more memory. So let me define that function. Let me redefine s to what it was previously. So now I have the original s, y equals Rayleigh of s, y. And then s again. So there's the original s. So now I have these. All right, so now what about the gradient of s? So let me define my derayleigh function. So s will be just n samples by n units. First, I'll just create a deep copy. And so now I will just define the heavy sidestep function in line. So here we have dy, where those elements of it are less than zero. Those become zero. Where they're greater than zero, they become one. Now I still have to deal with zero. So here, because every value in the function has to be given an output value, I don't want to be, I don't want to define, I don't want to end the function here. Because then if I put in a zero, it'll give me some error, probably. So I will just decide that, effectively, the same thing is true, where if I just set this equal to zero, I'm doing the same thing as if I did that. So now derayleigh of s gives me this. So all the negative values have become zeros. All the positive values become one. Then there is one zero in there. That remains. OK, so there's that again. And there's just multiple ways of doing this. So I can take the, I can just copy Rayleigh. So there's another alternative way of doing this would be because Rayleigh is already setting those negative values to zero. I just copy the Rayleigh output, which where the negative values were already set to zero and the derivative of those would also be zero. And then all I have to mess with is now zero or the positive values. So that's one way. Gives me the same thing. And then another way would just be to make a copy of s. And then I'm just going to return basically a Boolean where if dy is greater than zero, this would be true, which if I do as type int, that turns it into a one. So this is basically just saying all of those all those elements of this input that are greater than zero will become true or one and everything else will become false or zero. So these are more or less efficient ways of performing the same operation. OK, so now the weird part, right? Why does this actually work? Because it doesn't really seem very intuitive that this piecewise linear function would be able to introduce nonlinearity into a neural network. Right. Why would this work? It's not totally linear in that it's got these two pieces, but it lacks the clean curves of the sigmoid of the 10h and just sort of looks like I lopped off the bottom half of my linear function for no reason. So why does this work? So let's remember that the sort of modeling curves like the following adapted from notebook five, just I'm using the 10h function. So again, what I'm going to do here is I'm going to create some some inputs and then apply a known nonlinear function to them. So in this case is just, you know, this is this is like basically linear function with some random noise. Then we're going to train a model to fit that. This is the same. This is just doing linear regression, except I include a 10h function here. So I'm sort of trying to optimize my weights with the 10h in the mix to model on a linear operation. So this gives me this. These are my actual weight values, weights and bias. And here's the model. Here's what the model predicts. The orange line. Right. So you can see that it gives me a very slightly curved line. This is effectively trying to do its best to straighten the 10h function. So it's applying weights that are really small, such that when I apply the 10h function to it, I get something that's like really close to the original input. OK, so there's a bit of a curve here. If I were to train for longer, probably this curve would flatten out a little bit. So what these functions do is they're going to bend the otherwise linear regression to better fit the data. So you may observe that by adding this noise, there is actually because of the random sampling, there actually is a bit of a curve to the data. Right. So if I were to model this purely with a linear function, maybe it wouldn't be quite as good a fit as I did it with the 10h function. So ReLU does effectively the same thing, except instead of having a smooth differentiable curve all the way through, it's picking one point in the line and making one bend. OK. So this is a lot more computationally efficient, obviously, because it's just it's really just a linear function with a bit of a twist, but it's also less precise. So let's say that I have this data below and they have these blue points and these orange points. And let's say that I wanted to somehow draw a line to separate them as cleanly as possible. Right. So what I'm trying to do is I'm trying to draw a line that will be about here and then here, you know, going to try and fit it as closely as possible may not quite get it because these points really closely overlapped and then go down again. OK. So we could probably do this with a 10h layer or two. So if you remember from Notebook 5, we had two distributions to these random noise samples where one kind of curved up and then one kind of curved down. And we can just apply a 10h function to a linear function and kind of fit that if there's a single curve. Then we had one that sort of more like a parabola. And so now I want a function that's going to go down and then go back up again. And you can do that with a single 10h, but I could do that with, say, two 10h functions. One that would put the curve for the negative values and then become flat. And then one that starts out flat and then at zero starts to climb. And so then if my weights, if I have two hidden units, each of which has a 10h function on it and the weights in one are effectively taking the negative values and making them close to zero, that's going to give me that curve that starts flat and rises. Then I have another one that takes the very positive values and makes them close to zero. That's going to give me the one that starts to fall and then ends up flattening. And then I add those two together and I'll get that nice curve. So that's how a 10h layer would do it. So this would basically draw this like smooth envelope around the orange points. Regularly with a single bend would have a much harder time, of course. Right. So it might, you might be able to infer that like, okay, I get one line and one bend. So I'm going to put my bend here where my cursor is, and I'm going to have straight lines on their side. Okay. Probably not too bad of a fit, although you do risk having certain points like maybe this one that fall on the wrong side of the line. So this is going to be less precise. But what if you have a bunch of different railway units, and each of them is going to be optimized to put that bend in the right place. This could approximate the same curve. So it's sort of like, instead of drawing a circle, you draw like a 36 sided polygon. It's going to look a lot like a circle. Okay. Or 100 sided polygon or your pick your number, the more you the more you get, the better it's going to look like a smooth curve. So by these railway units working together, this would allow us to approximate the same curve. So outside of this nonlinear function, all the neural network operation is doing for every unit is y equals wx. So that's just moving the line around, and then, and then stretching it, you know, rotating it, basically choosing the slope. And so then you moved around by some bias B to figure out where to place it. And so the right bias vector, it will allow you to choose smart places to place each of those bends and then the remaining ways will sort of tell you how I'm supposed to, how I'm supposed to rotate the entire curve. And so then each of these, let's say what I could do is I could optimize for like a small piece of the curve is kind of a shallow bend like that. That's going to handle some part of maybe from all the values like up to negative four or something like that, or up to negative two. And then I find another one that is optimized to place the bend in a slightly different region, and then so on and so on. And then I add all those together, and it's going to do a pretty good job of creating something that looks like that curve that I want to separate these regions. So what that means that ReLU's real strength is in numbers. And so we can have a really large number of these rectifier units that actually approximate the nonlinear behavior of functions like tanh. And they do it, even though you're using them in greater numbers because it's so much faster to calculate the ReLU function, they actually do it with greater speed and greater efficiency. So if you want more on like how does this work, there's this explainer here that you can check out. So these are some pretty nice examples of trying to separate, you know, sort of curved regions in data. All right, questions. Okay. All right, so now back prop is relatively straightforward, because as I've kind of foreshadowed already, the only thing that you really need to change is the derivative of that activation function. So it's not as if there were no derivative back prop in the hidden layer is the same as back prop in the outer layer. So all the only thing that I'm adding is the derivative of the activation function. So we have, remember, for our neural network operation, if V is some hidden layer, I'm going to multiply X by that, apply some activation function H, gives me some scalar value Z. I add a bias to those, and then multiply those by output layer weights W, and that gives me my prediction. So here is back prop. Let's just look at the W line first. This is back prop in the output layer. So this is going to be T minus Y. This is my error term, how wrong I am. I'm going to multiply that by Z, which is the input to that layer. I'm going to do this for all of my samples for the entire dimensionality of the output. Right. And then there's some learning rate in there. So the same thing, all I'm doing is, all I'm changing is basically this part of the equation, when I go from the output layer to the hidden layer. So now instead of just the error term, taking the error term times the output weights times the derivative of Z, in this case, the input to that layer. So what is Z? Well, Z is actually going to be a function applied, a function H applied over X, V. So that has to be incorporated into this operation. And so I need to know what the derivative of an activation function is. If H is tan H, then the back prop operation will look like this. So I'm going to have one minus Z squared because the derivative of tan H is one minus the tan H squared. If H is ReLU, then the back prop through V is going to look like this. So I'm going to have everything up until this point, the activation function is the same. And there it is multiplied by the derivative of the activation function, which because if the activation function is ReLU is this step function. So now it's going to be dependent upon what the precise values of each element in Z are. So I'm going to take in some weight matrix Z and those elements of it that are negative or zero become zeros and those elements of it that are positive get left alone. So let us view how those weights actually change. So what I'll do is I will create some data, I'll represent sample inputs and targets like you've been doing, and then I'll create some randomly initialized weights. And then I'm just going to simulate one iteration of training with the two different activation functions, tan H and ReLU. So I'll have basically versions of Z and the output as assuming that the activation function is tan H and then one assuming that it's ReLU. And then I'll store the error using each one, perform back prop, and then calculate the change and you do that. So these are the weights of each weight. So these will be the delta between the previous value of those weights and the current values. Alright, so we do that. Now let's view how the weights change depending on the layer and which activation function. So I'll just view this in a pandas data frame and just reshape the data so we can we can view it. And here we go. So if these are the initial weight values, just for say the first nine here. This is what happens after I apply the tan H function. Okay, so we can see that for example, for those values that are very close to zero, they remain very close to zero for those values and all of them are pretty close to zero. So you don't this effect is not really very pronounced, but those things that are a little further away from zero, you know, get a little closer to negative one or one. And then we can look at the amount of change, right, how much each each one has changed. Now, ReLU, we can see that after the ReLU function, there are certain values that have some change applied to them and certain others that are completely unchanged. And this is all dependent on the whether they were positive or negative after you take that and then multiply them by the inputs. Okay. So now we can look to the same thing for the output layer, right, and we see similar phenomena. The only real difference that here we don't have that many, we don't see any values where the value has not changed at all. And this is mostly because we have this additional computation in the way that hidden layer that's providing other information. So finally, ReLU is not the only linear unit type activation function you can use. There are plenty of others. ReLU is the simplest, but you can use JLU, this Gaussian Aerolinear Unit. This is the one that has been used in most state of the art language models. So I don't exactly know what GPT uses for certain, but I'm pretty sure that it's some sort of JLU function. And then there's the sigmoid linear unit, which is basically just x times sigmoid of x. This ends up being another smoothed version of ReLU. So it's sort of, it looks typically like the ReLU function, except it doesn't have, it doesn't have like a sharp bend at zero. It's got more of a smooth curve. So all of these, well, not necessarily all of these, many of them, they try to smooth out that bend in ReLU because it makes for a nicely differentiable function all the way through. Whereas, yeah. So let me do, let me do leaky ReLU first. So leaky ReLU is basically, instead of being, instead of clamping everything at zero before, for all negative values, and then just letting x pass all the way through, what we do is we let a little bit of that negative value through. So there's some constant value, usually like 0.1 or 0.01. And what it'll do is it'll take, for negative values, it'll do, you know, k, whatever that is, times x. So it's going to, a little bit of the negative value through, but not that much. Right. Maybe take one, one percent of that negative value and then the, and then the positive values just get passed through. So parametric ReLU is the same thing in that we're going to try and learn, or we're going to try and let through some of those negative values. But we don't know how much, right? So I could just leaky ReLU everything, but maybe that constant 0.01 is like not the best value. Right. Maybe, but I don't know what it is. I actually want to learn that from the data. So we basically have a tunable parameter that allows us to figure out what the best value of this quote negative leakage is. And so this can actually be learned during training as well. And exponential unit, what this will do is in order to keep mean activations close to zero, it'll actually exponentiate the negative values. So all of these are effectively methods of leaving the positive values alone and trying to do things, the negative values that are meaningful, usually in combination and also don't cause your training to collapse. And all of them are usually more computationally efficient than 10H. Some of them like parametric ReLU, you have to, you actually have to tune a parameter in there. But they all have different uses and they can be very useful for different types of problems. You will find that convolutional neural nets usually use combinations of ReLU layers. Okay. Questions about ReLU? Yeah. Oh, the place where you. No, I don't. Okay. So I guess there's two answers to this. Yes, you can. As in it's definitely possible to write a function that does that. Does it satisfy the nice properties that we want with neural networks? That's less clear. Because pretty much everything gets standardized. And so you're assuming that everything is going to get clustered around some mean, which then gets standardized to zero. And so zero tends to be the best place to start making differentiations about your data. So yeah, you probably could write a function that's like, I'm going to maybe learn where the best place to start letting positive values through is. But there's a good chance it's probably going to be like really overfit to a particular data set, because there might be some data set, like where you standardize everything. And for some reason, there's like a weird secondary node. And it might learn, okay, this is like a really good place to start my linear part of my ReLU unit for this data set. But once you train on that data set and you try something else, even like different tests, it's going to be like, okay, I'm going to try this. So it's just, you know, back of the envelope guess really sort of seems like this would not be something that would be very productive. And so you're going to have to train on that data set. And so you're going to have to train on that data set. But once you train on that data set and you try something else, even like different testing data, it's no guarantee you're going to have that same property in the testing data. So it's just, you know, back of the envelope guess really sort of seems like this would not be something that would be very productive, although maybe people have tried. Other questions. Okay, so let me go to assignment two, and go through that real quick. Okay, so general shape of this assignment is very similar to assignment one in that we give you some starter code, you to fill out parts of that starter code, apply, apply that to some dummy data to make sure that your functions are apparently written correctly. So you actually have to perform some experiments on it and discuss your experiments. So you will give you in this case a neural network implementation. If you do this assignment correctly, you will at least have a complete neural network implementation that you can use yourself. If you don't want to do that well, that's questionable because it's all written in NumPy, it doesn't have GPU acceleration. So you can use it. And if you're a talented hacker, you can probably translate it into PyTorch or something. But we'll also use a PyTorch neural network later so you don't need to do that. But again, if you succeed at the assignment, you at least have in principle a neural network implementation that you can use and experiment with and does give you a lower level of control than the PyTorch or TensorFlow implementation. So we do have that advantage. So you're going to need to do that to complete the neural network, define a function that partitions the data into training, validation, and testing sets like we showed in the previous lecture, apply to a data set, and then define a function that's going to run experiments with different hyperparameter values. And then you're going to describe your observations. So first thing you're going to need to do is you need to complete the optimizer class. So we give you a mostly complete optimizer class that has complete implementation of SGD. What you need to do is refer to notebook six and turn that SGD implementation into an atom implementation. So this cell will implement SGD, and then there will be some implementation of atom that you need to complete. So we give you, for example, we already give you like, you know, MT, VT, beta one, beta two values. What you need to do is you need to finish the atom implementation by updating these values and the values of self.all weights. Okay. So then you can test that. If you do, if you're correctly, you should get the same results as shown below. What do we see here? So I alluded to this property last time. So the test here is basically finding the minimum of parabola. SGD finds it exactly. We know the minimum of the parabola is at five. SGD finds it at five. So the atom gets very close, but doesn't quite find exactly five. But this is supposed to, this is the expected value, right? You're supposed to get 5.03900403. Okay. We can also see here that, for example, both of these are training for 100 epochs and SGD starts with an error that's very close. And in this case, because it's a very simple problem, arrives at zero pretty quickly. So, atom starts with an error that's like way off 16, but it also gets, it like closes that gap within the 100 epochs. So if you look at the change in the level of convergence from the start to 100 epochs, atom is basically eliminating this error of about 16, where SGD starts very close and only has a little way to go. Okay. So you can also see kind of some of the properties of the two functions there. Now you are going to implement the neural network class. This is going to call the optimizers functions. So we already give you that. The call to optimizers is done. So first you need to complete the use function. So you can make use of the forward pass function in your use function. I'm not going to give you much more information than that. You have to figure out exactly how you use the forward pass function in the use function. This should not be too difficult, I think, if you've been paying attention to what the forward pass is and what back prop is and what it means when you cease training. Okay, so do that. The rest of the class is pretty much given to you. So complete the use function. You can assume that X is standardized, but you have to return the unstandardized prediction from the use function. So then you test out this test neural network function. Your results should be the same as these below. So you can test that there. And so we give you these graphs and basically if your output matches these graphs, done it correctly. Again, as always, I recommend download this notebook, create a copy, just in case something gets messed up, you can always refer to this one and you also compare your results visually to this one and numerically. Okay, now what you're going to do is you are going to cut and paste, basically create a copy of your neural network class cell here, and then modify it to allow the use of the Rayleigh activation function. Why do I want you to cut and paste? Well, mostly because I don't want you to risk screwing something up that would throw off these results that you're trying to test. Okay. So yes, it does seem kind of extraneous to copy and paste this large chunk of code, but I think it will help organize your stream of thought and also prevent you from maybe creating undesired results. Okay, so what you're going to need to do is you're going to modify the neural network to allow the use of the activation function. So right now it assumes that the activation function is tanh. So if we go to in the train function, for example, you will see somewhere. Forward pass, radianf. Find this tanh. There it is. There's the tanh. Yeah, and the forward pass. Here's the tanh function, right? So what you're going to need to do is you're going to need to modify this to take an argument activation function that will specify either the string tanh or the string Rayleigh and depending on that value, apply the right function. There is no Rayleigh function in NumPy, so you have to define your own. So this should accept a matrix of weighted sums and return the Rayleigh value. The implementation of how to do that is basically given to you in the notebook, which you need to do is modify that for use with the neural network class and make sure that your inputs are all correct. You also need to define rad Rayleigh. Then you need to add some if statements to the forward pass and the gradientf function that depending on the value of this activation function argument will apply the tanh function or the Rayleigh function correctly. So this should be pretty straightforward if you just have a new class variable that has one of the self dot variables in the neural network constructor that will then accept the value of the activation function. Finally, the experiments. So you're going to use the auto.mpg data that we've used in some lectures in lecture two. So let's apply these neural networks to predict miles per gallon using different neural network architectures. So kind of doing the same thing that we did in lecture three, only this time we're doing it nonlinearly. So the data processing is going to be more or less similar to that. You should end up with 392 samples after you remove all the samples that contain missing values. Then you're going to need to partition the data into five folds as we showed in the lecture. So this should return train validation and test partitions for the inputs and the targets. And then you need to write this function run experiment that will have three nested for loops that basically do a grid search for you. You're going to try different parameters of the number of epochs, number of hidden units per layer, and the activation function. So these two can be whichever you want. Activation function is just tanh or Rayleigh. Now that allows you to compare the training time network architecture and activation function and see which combinations work best. You should try zero for one of the hidden units per layers because that will be a linear model. So if I have no hidden units in my neural network, it's just a linear model. The way we've constructed this, it just passes zero in and get that. So then for each set of parameter values, create and train a network using Atom, which you should have implemented, and then evaluate that neural network. So you can collect all the parameter values, RMSE, and then when you're done, construct a data frame that looks something like this. So then before you start this nested for loop, you need to call your partition function to form the train validation and test sets inside one experiments. Use the same partitions for all of your experiments. So a sample call to the functions would look something like this. So what you need to do is then change which values of these should all be constant, right, X, T, and F, since day five, and then you need to specify which ones you want to use there. So this is just a list of numbers. This is just a list of lists, and this is just tanh and Rayleigh. Then find the lowest validation error and the lowest test error and report on the parameters that produce this, and then discuss how good your prediction is. So just you need to describe these three different observations that you make. Plot the RMSE train valent test sets versus the combined parameter values. We did a version of this at the end of the previous lecture, so you can model it off of that. You can work with a partner on this. If you choose, you're not required to. This is the only assignment that you're allowed to work in pairs until the final project. So you're given a grader like before. It should be uploaded on Canvas. Same procedure as before. And so I recommend keeping the notebook in a folder with just a grader. So then you will get 70% for the code and 30% for the discussion and the experiments. You can also get some extra credits by look up the the swish activation function or start this article, and then you can implement that as a third choice of activation function. If you do that successfully, you'll get five points extra credit and you can just do my experiments. Just say comparing just comparing the three activation functions, say, take your best performing set of other hyper parameters and then try the three activation functions and discuss the results. Okay. Any questions. All right, great. I'll let you go. Okay. Okay. Okay. Okay. All right, let's go ahead and start. So I am not as on today as I usually am probably feeling pretty tired. Huh. I feel I mean I like feel physically five is like have not been sleeping a lot recently so I'm like, uh, with water bottles that Every time it comes in is like a different water bottle right there. Like, someone keeps leaving a different person keeps leaving their water bottle there. Okay. If you forget stuff in this room, like, come back and get it because I'm not picking up after you. Okay, so I'm alright so hope you all saw the announcement. I unfortunately am not going to be able to have office hours at least not very long if you come like right after class I can probably talk to you for a little bit, but I will have to leave. Before the the 430 got a pet family emergency. You have to attend to so, but I think you know people have started PA or assignment to it's not due for a while so you know, hopefully you have time to kind of figure out what's going wrong. I hope things will be back to normal by Thursday. And so I guess that was only announcement for today. Other than that, any anybody have questions. They might be kind of short, we're just going to do auto grad basically. All right, no questions. Okay, so let me get my material up. Sure screen. And then we make sure they can still connect to the. Okay. I brought my GPU laptop and then I forgot to charger at home and it was down like 13% battery life so have to back off to putting things on the lab machine. No, we're going to answer the wrong password. No, we're doing. Of course. All right, what if I disconnect. There we go. Okay. But again, Okay, so before we start this I'm just going to do a quick tutorial that I realized actually missing from this notebook on computation graphs just so you're aware of some of the terminology that we're that we're using so basically we all kind of have this notion of what what the schema of a neural network looks like right we have a basically a bunch of you know circles, and they're in rows, and you have edges between all the circles in consecutive rows. Right so those rows are of course those the layers and the circles those the nodes in the in. So, every so neural network is basically it can be represented as graph. Right. So, as we all know graphs have like the neural networks dramatic representation nodes and edges the trick is in a neural network. Each of those has basically representation that is realized mathematically either as a value or a function. So, we can say that if for an expression x, we can actually just graph it like the following. Right, so you can actually have a graphical representation of any expression. And so the question is just what do I represent as nodes and edges and how do I connect them together. So, if I have some expression x is just going to be represented as a node with an x in it. The edges represent function arguments. So for example, if I have some function where that's f of you, where you is the card network that the functions like the cardinality of you. Then what I can do is I can any argument that is connected via an edge to to this other node will basically represent the inputs to that function. Right. So, for example, if I have another function f of UV, that is you transpose times V, then I can take some other values So if this if this node represents the function, then the edges that go from these nodes representing the expressions into this other node representing the operation over them will then take whichever values I specify here and use them as arguments to this function. Okay. So, a node with an incoming edge is basically just a function of the parent node. So what does that mean for a neural network. What are the parents, what are the children. So in this case you can just have the children be the, the nodes and subsequent layers. Right, so now if we think of that lattice style representation, the children are going to be connected by edges to their parents in the preceding layer, which is in a fully connected network would be all of the nodes in the preceding layer. And then there's going to be some function that is applied to them. Right. We've already seen what those functions are right is basically an activation function over a weighted song. That's the operation that that occurs in our network. So now, what this means that we can then represent these things is these neat computational structures that allow us to kind of get away from the from scratch building that you're doing in NumPy into something that has more efficiency that we'll see. So, we do this by allowing a node to compute things. So of course this makes sense because the node is now a unit in the hidden layer. Right, there is an operation that's going to be that's going to go on in that in that layer in that node. So, a node can compute two things right can compute its own values that would be the song the weighted song that comes out of that node using whatever input is coming into that node. And then it can also compute its derivative with respect to each input. So, why is it important that we can compute both of these things given what you now know about neural networks, why is it important that we can compute that a node can compute its own value using its inputs. So new here. So new here. All right, why. Yes. So it can be it can be used in subsequent computation right so if I have a, if I have a node that is connected to some other nodes in a subsequent layer. Right. I don't need to know what the arguments were in the previous layer. All I need to do is take the output of this node, and then use it as an argument to subsequent subsequent layers. Right, so this means that I can eventually just do kind of dynamic programming style operation right where I can take the output of some computation and just use it in a subsequent computation. And then of course, why is it important that we able to be able to calculate the partial derivative of these values with respect to each input. What we use the partial derivative for. Yeah. Calculate weights I heard something back propagation. Right. So the weight update that is the back propagation. We need to know the derivative of things that have happened in subsequent layers right so be being able to allow these nodes to compute both of these things now allows me to do both the forward pass and the backward pass. So, for example, in these examples the nodes they will compute you know cardinal X and then X transpose times y. And then in the left graph. This, this will compute dfd you may increase the font size. So dfd you and then the right graph will know how to compute dfd you and dfd right because that those are the arguments. Okay. So now the graphs, the entirety of the graph represent functions. And these functions could be expressed. You know, these are these are could be, say, nullary functions they have no arguments over example, this thing. Right, this is an expression, also known as a nullary function. There are no inputs to this it just is its own value. And then unary right if there's one incoming edge this would be a unary function. This would be a binary function up to basically just an arbitrary value of an end so you know the, a single node in a neural network So this represents an n-ary function for the size of the previous layer. Right. If I've got n nodes in the previous layer there are going to be n inputs to every node in a subsequent layer assuming a fully connected network with none. We're not talking about like residuals or convolutions or anything just yet. But just assuming a standard feed forward fully connected network. This is what you can expect. So basically just all the way up to some value of n. So now we can express arbitrary functions as graphs. So let's say I have some expression, x transpose times a, where might we encounter like this for an arbitrary value of a, what a represent in this case. Yeah. An image or just or just generically any input, but generically let's say in the interior of a neural network what might it represent. Let's say I've got a three layer neural network and I'm looking at like some function of a in that second layer, right second hidden layer. So this is going to be the output of the previous layer. Right. So this can be a would be, you know, h of x times v, as came out of the previous layer. Right. So this really can be anything. So what we can do here is that if I have some input, x, it has some function applied over it. In this case the function is just the transpose. And then that output can be used to as inputs to another function. So here we see for example I have an input x. I have some input, then I have some input a that might be another raw input or it could be a feature map or something from from a previous layer computation, and then I can perform operations over them. Now here, interestingly, what I can do is I can represent sort of arbitrary graphs that might have loops, they don't, these don't necessarily need to be directed or a cyclic. So I can have an expression that is x transpose times a times x right. I don't need to define x twice I simply need to define the edge that tells me where x needs to go what it what x is an input to. So for example I could take the previous one, right, we've already established that x transpose times a can be represented by this. So anytime I see that I can just take that same graph, right, clone it, pop it in there. So now I have another multiplication by x I just got to figure out where I put x, right, I already have x I can reference the same x, there simply just needs to be a function that's going to take the output of x transpose times a and multiply it by x that can be represented like this graph. And so, another way you can do this is you can also represent the same expression right x transpose times a times x with a simpler graph. Right, so I could have a function of you and M. So that is actually the whole of this expression all I need to do is pop in those values that represent you and M. Right, so as long as I define the function that is being executed in the node, right, there are many different ways that I could represent the same expression. So that means that is to say the competition graphs are not necessarily unique for a given function. So, if we take a more complicated one so xt times ax plus bt times x plus c, it may look something like this right so the first part is already given here. Right. We've already seen that. And then there's going to be another node is basically the sum of these three, three expressions, and all I need to do is provide the inputs there. Right, so I've already got the output of xt times ax, I need to provide some some graph that's going to give me a BTX, and then something that's see right so I've already got x I can reference that same x again a third time here when computing be transpose times x and see is just an expression. And so now all of these get fed into this node that is just a summation function, and it will give me that output. Right, so that is to say, what we can do with these things is now allows us to build complex operations from simple building parts. This is interesting for neural network purposes because we can write neural networks as computation graphs. Right. So the nodes are the quote neurons and the edges are the quote synapses. We don't really talk about synapses in computational neural network and just, I don't think I've had this rant yet in this class so here we go. So the term neural network, like, whoever came up with that is just an absolute genius at marketing, because, like, if I if this class were called you know introduction to nonlinear optimization or something, you know, two thirds you probably wouldn't be here. But machine learning and neural networks are just like incredible terms of art that just draw people to the field because it's like seems like something mystical is going on. So, the neurons are sort of loosely inspired by cognitive architectures in that there is an input, you know, in the brain electrical signal, it's transformed and then the transformed input goes elsewhere and becomes an input to something else so these British doing functions right the transformation is some sort of non linearity times over some some way that some, and we're just simply trying to figure out like what those right values are. So, in the sense that it's neural neural like machine learning doesn't really bear any known resemblance to actual learning, except perhaps coincidentally there are cases in the brain where we see processes that seem to be like supervised learning like reinforcement learning or like unsupervised learning. There's a whole lot of things that the brain does that we don't have neat machine learning metaphor. There's, there's, there's some way to go. But the the term neural network seems to be here to stay. All right, given that we can write neural networks as computation graphs. We can also write loss functions as computation graphs. So that is you can write loss functions within the innermost SGD operation. And also you may have observed their plug and play. Right. So if I have this expression, right, I've already kind of determined some computation graph for say the first term of the song. I can just clone that in whatever form and then just drop it in. Right. And then all I have to do is, if I need to reference a term that is referenced in this in this sub graph, I can't, it's really no big deal. And then finally, you know, we can, so we can basically construct this graph and just like use it in someone else's program, which is basically if you have used TensorFlow and PyTorch, that's what you're doing. You'll specify what you want your layers to look like. It's going to create a computation graph for them that allows you to do all these operations. And there's a whole bunch of like C++ or whatever that happens under the hood that you never have to touch. Right. And therefore this allows us to make efficient gradient computations because we're going to write the entire thing in say Python, the underlying C++ or C or, you know, assembly code or whatever that is so much faster is doing all of that for you. And so you get a significant speed up with these packages. All right. So finally, if we have some operation, right, H is going to be some nonlinear function we'll call the tanh over Wx plus b or my weighted sum. And then why my output would be some other weights v times h plus a, a would be like another that other bias. This can be represented, our two-layered node can be represented something like this. So I've got my weights and I've got my input. And then I've got a bias. This is explicitly representing the bias as its own term, but as shown, we can just consider this to be another matrix. And so now we add these, we take the tanh function. This gives me my output that goes in as the missing input that it's looking for for this other other function. And here are some other weights v and then I multiply them and then I add some bias and there might be some other nonlinear function here that I want to apply. So plug and play neural networks, courtesy of computation graphs, very simple, allow us to do, to do certain things. And, you know, like I mentioned, artificial neurons kind of loosely mimic some functions of biological neurons, that they have the advantage of being able to find patterns in this is from my NLP class. So I say language data, but generally just like data in the large, we don't have to do manual feature selection. Like we don't have to try and figure out which power I need to raise some term to in order to fit my linear function. But they can take quite a bit of time to set up correctly as you may be observing. And also the way that you represent the input data is very important. Everything's still got to be numeric. And what we want is we want to capture something about this is kind of, again, NLP specific but we just want to find a way to capture important features without trying to extract them manually, to at least to the extent that we are, we do not have to. Okay, so why all that about computation graphs, because this goes into the topic of the main lecture, and we're actually still a day ahead, which is good. And I anticipate this will be able to finish this with no trouble today. So today I'm going to talk about pytorch particularly the auto grad capability, and then also the says and module I'm actually not sure if we cover that here in the next one. But either way, we'll talk about it auto grad and hopefully you'll see kind of how the construction of the computation graph allows us to use these features of pytorch, you know, with sort of a single line of code rather than having to write say a dozen as you may have had to do for for a second or two. Okay, so, first of all, who here has used pytorch before. Alright, so we will get you will get some practice in this in this class. So starting here and then in homework three. And then, subsequently, you'll get the ability to kind of play around with pytorch see how it works differently from say the NumPy version, or similarly and then also see how we can get significant speed benefit so why use pytorch well obviously you probably aware of some of the benefits of pytorch is that it's faster, as I alluded to, it's more efficient as I also alluded to provide support for GPUs right which speeds up everything as you saw on like that first lecture, significant amount and see that again. So, the code is usually easier to write right so if you see how I'm calculating the output of my prediction, what I'm going to do is I'm going to take, you know, why is, if I'm using the form from the code in the assignment. So, I'll take like the last output that I got, which would be the output from my previous layer times the weights, whatever it is, add the bias, and then apply my 10 h or my nonlinear function to and that gives me why. So, if you're having a little bit hard to keep straight in pytorch, you basically just define the hidden layer, and that becomes the function right as we saw in the computation graph, and then I just specify what the input to that function is, and we'll do the computation for me. So, the code is more opaque. And if you're like trying to figure out if you're trying to like infer what machine learning operations are from reading pytorch code is not going to be very easy, but having done some already you know what's going on in those hidden layers right now and you see why equals hidden layer of why you should at least have some intuitive understanding of kind of what that's doing. Okay. So, the, the core of pytorch and also other packages like tensor flow is this thing called a tensor. As we alluded to in lecture one, you know, tensors and NumPy arrays are both n dimensional arrays but they are not equivalent. Right. I'm on the lab machine is scroll is all fast. So tensors are a much more generalized form so remember that a tensor technically is basically a transformation, and the numerical array is sort of a form suitable to representing that transformation. So, the tensor being a transformation is a function. And so that we have the tensor as say, a weight array living in a node. That's the function that gets applied over some input. So, the torch package contains these classes and functions that are mostly very similar to NumPy. So, if you do like V stack and H stack and all the array manipulation you can do in NumPy, you can do a lot of the same stuff in pytorch. It also provides convenient functions to convert back and forth, right, you want to minimize the use of these functions because it does take time to convert the NumPy data structure to the torch data structure so basically set up our data and NumPy converted to tensors, then you can run pytorch, when you want to do data analysis convert it back to NumPy arrays and you can do things like, you know, make plots and all that stuff. Okay, so let's just view this in action right so if I just create some data this is now just a list, right, this is just a list of lists. As we know I can take a list and turn it into an array using the NumPy dot array function, this is going to give me this. So now I have a two dimensional three by three array, representing this data. If I look at the type, this is going to tell me okay it's made up of 64 bit floats, and it's a NumPy and D array. So if I create torch dot tensor using the data, it's going to give me something very similar. So I can use the same function or the sort of the same function equivalent in torch, right so instead of creating an array and creating a tensor, but I can I can do that operation over a list. I can also do that operation over a right give me the same thing. So now is this represented now this is a torch tensor, and it is this one, and it is composed of 32 bit floats. So, a little bit different there at the NumPy version is 64 bit floats the torch version 32 bit floats by default. Of course you can specify with the D type, which kind you actually want to use. So what I can do is I do torch from NumPy of a, this is going to give me the same numbers, but now it explicitly says that the D type is torch dot float 64 because it was created from 64 bit floats from NumPy. Right. And then running the, the type and the D type will confirm that. I can also do torch as tensor. Right. And this is going to give me pretty much the same thing. So again, same numbers, right mostly I'm taking this from a, and this is going to create 64 bit floats. So, and so now basically C and D contain the same information and contain the same data type. Okay, so the next one. So now what I can do is I can take D, right this thing that is created and turn it back into a NumPy array. So now this is going to be an array. And if I take the type of this it's going to say okay now this is an ND array again, and it is a float 64 data type. So, what's the distinction, right, distinction is torch dot tensor makes a copy of the data, whereas torch dot from NumPy and torch dot as tensor do not copy the data. So, remember what happened when we were doing in the first part of the Adam notebook. What happens if you do a deep copy versus a shallow copy so if I do, if I make a shallow copy. What happens if I change something about the original copy if I make a shallow copy and change something about the copy. It does change the original right if I make a deep copy. The two are completely separate there you copy the data to a different place in memory, let me do whatever I want with this new copy and nothing about the original will change. So, I ran into this. Like yesterday when I was doing some manipulation for doing some some data and like I changed the class labels for something and all of a sudden all my plots like the colors are all wrong. It's like, I got to rerun the model. So, be careful. Okay, so here's a again. So now what I can do is, if I, oh god. Okay, so if I create this tensor from using the from NumPy function right so this is B as a tensor. And now I set that first element to 42.42 Okay, here's a. And here's B. So this is that shallow copy and by changing a I've also changed B. So now I'm having basically torture is is referencing the same place in memory for the actual data. So now I've got sort of a data structure that is of the right type to to feed it into into pytorch. If I tried to put a into a pytorch neural network it would give me some errors saying you know was expecting tensor but got NumPy array, but if I put B and it would be fine, although the numerical data is the same. So now I could do torch dot tensor, right, which copy is the data, as noted. And then, oh god, get this wrong. The scroll set to a different direction. Okay, so here's B, right, and then B 00 is 42.42. Now I said a 00 to 12,345 okay there is, and B is still the same. So, torch dot tensor will create a deep copy. Now we can also use the at sign from matrix multiplication, as we do in NumPy. So here let me create some data. Right. And for a and B and I could do C equals a at B, and the shape of C is 10 by 20. Why is the shape of C 10 by 20 given what it was calculated from 10 by 5 and 5 by 20 those two inner terms are the same and so when you multiply you end up with the two outer terms as your shape. Okay, so don't need to do that about randomness I guess. So now if I do tensor versions. Right, so if I do the dot ran function just kind of the same as the NumPy random uniform. So it's going to give me the same thing right, but now when I put a CT dot shape it's going to be just torch dot size, same dimensions. So again I can pull the same values out of this is just represented slightly differently. So, now where we, we get with what we get with AutoGrad is basically the ability to calculate gradients automatically so I'll begin with these two quotes to deal with hyper planes and a 14 dimensional space visualize a 3D space and say 14 to yourself very loudly everyone does it. This is by Jeff Hinton, the father of deep learning, and I do, I think this works I think my technique is slightly different. So when I think of a high dimensional array literally just imagine a really really long three dimensional array. So our tiny like human meat brains are not good at conceptualizing things in more than three maybe four dimensions if you think of time as a fourth dimension. Beyond that, they're just you see these long strings of numbers and like this, this looks too dimensional to me or one dimensional or something. So I think what you're saying is that there's the value in every dimension that is orthogonal to all the previous ones. Once you get beyond three. This is not something that you can actually visualize very easily. What you need to I guess what you need to know is that vectors, even in high dimensions preserve those similarities across those high dimensions so things that are similar to each other, numerically or semantically will point in a similar direction and high dimensional And that operation is equally true in three dimensions as it is in 1000 dimensions. So, as Abraham Lincoln said this is pretty cool. He did actually say this. So in the Cooper Union speech in the section addressed to the southern people. He actually says this that is cool. I believe he meant it in the sense of like that is kind of cold or cold hearted. Nonetheless, if anyone tells that Abraham Lincoln didn't say this is cool, you can, you can show them the reference. Now I can't find my notebook anymore. Okay. So anyway, people are like pretty bad at calculus, just in general. And in more than three dimensions, it gets even worse. Right. There's so many numbers, you have to calculate the derivative for like every, every element of a high dimensional array with respect to all the by holding all the other values constant. And so really part of the reason that machine learning sometimes feels like magical is just as like, oh, so much math going on. That, yes, you could sit down and actually calculate like what's going on inside of a neural network in principle, if you had all the time in the world and the super powerful computer. But doing those operations you know by hand or like line by line would take greater than the lifetime of the universe to actually do. So of course no one actually does it. So we basically take the workout by by doing by using matrices. And then, as we mentioned, we have GPUs that are optimized for vector math and matrix multiplication, so we can get this big speed speed up with the GPU hardware, but there's still explainable math that's happening at each step. If you were if you had the ability to zoom in on a single iteration, a single gradient update, we will see the operations that we saw happening in lecture five or like updating a single weight is just remember all that's happening at a massive scale. And we think about modern applications, it is just truly mind boggling. So that makes it impractical to calculate these gradients for these giant gigantic composite functions, especially in the high dimensional space. So for example, you know, what's the derivative of sine of x, cosine of x right so we learned that in in trigonometry. And so we can see like how we can actually calculate this. Right, so they create on 100 evenly spaced points from negative two pi to two pi, and then plot the sign. Right. So, why equals sine of x and then the derivative would be cosine of x, I can plot these right this is what we would expect. So, now what I can do is I can, I can do this using pipe torch and tensors. So I can use the value of sine of x, right those numbers that I created, I'll create a tensor from that array. Here's the tensor. And so then, currently I look at this, this argument is a parameter of this tensor requires grad. I'll get to what requires grad in a minute. So it says false. So right now requires it does not require grad or required gradient. So I set that to true, right, just by, you'll see the function version has an extra underscore there. So I set requires granted true. Now I can see that it will give me this extra little bit of information at the end so we'll set that set requires grad equal true and will tell me that whatever I print out the tensor. So now when I'm writing requires grad equal true, I'm forming this computation graph, this backwards graph that is the history of every operation. So, when we think about how a node can calculate itself its own values from its input, and then also its derivative, and those values are going to be dependent upon the children, or the parents, the values of the children subsequent nodes are going to be dependent upon the values of the parents. So now I'm going to create this graph and so this backwards graph is going to be this history, right, so if you think about the actual operations being performed. If you start from the input you perform an operation over you perform a few executed function over that operation, you then take that value another operations performed over and so on and so on and so on. And so, each of those operations can also be part of the graph. And so setting requires grant through will allow me to basically automatically calculate every component of that sub graph every every component of the sub graph within that graph, as I'm performing operations. Alright, so this will aid in calculating these composite gradients. So I'll just look at this grad FN it currently says none will start assigning values to it soon. So now I can define the sine function, right, so of course I'm going to define why being sine of x. And so now here's why if T. And so here's the sign you can see that we're basically starting from a value really close to zero, and we're ending up we're getting closer and closer to one and then eventually we end up with close to zero again. So, no doc stream for that one. So the shape of this is 100. So, just a tensor that has 100 100 elements in it. Okay, so now if I look at the backwards, so why t dot backward. It's confused the gradient of the current tensor, with respect to the graph leaves graph is differentiating using the chain rule we've seen that before. So it accumulates gradients in the leaves. So you may need to zero grad attributes to them to none before calling it we'll see what happens if you don't zero out your gradient. So, backward will basically compute the derivative for every x in y t that has required requires grad set to true. So, why t is basically constructed by in this case performing the sine function over some, some other opera some other value in that case, So this will basically compute the gradient for every value in that in that array or in that tensor. Okay. So, so now here, if I just do dot backwards over an array of one 100 ones. So this argument will represent the gradient of y sub t, with respect to itself, and it's going to be all ones. So now if I look at xt dot grad. God, there it is. Okay, so now if I look at xt dot grad right I did, I did y t dot backwards with the gradient with respect to itself, not to look at xt dot grad, right we started one. We go down to zero. We end up back at one. And this is now actually the cosine of x. Right, so I started with just some, some values, I computed the sign for those values. I took the gradient of the sign. And now that original input that x is now set to cosine. How did that happen. So let's look at how we compute, why t right why t is generated by taking the sine function over xt. And since why t is created by performing this operation over xt. If I run why t dot backwards, this will keep a record of updated gradients for xt because xt was one of those arguments in the computation graph that was used to create why t. So this only does it if xt requires grad is set to true, which we did earlier. So, therefore, you just need to be alert to your in place calculations, right so if I'm doing operations over something that uses something else as an argument. If that other, if that argument has requires grad set true to it I may end up calculating the gradient of why with respect to that in the original input. So you got to be careful, right, you may have it you may not actually want to do that. So you have to you have to make sure that requires grad is set to true, only when you need it to be. So let's visualize some of these operations. So first you can see what's going to happen. I just want to plot xt versus yt, and then it's going to get mad at me. So why did it get mad at me, why do you think it got mad at me. Sorry. So dimensions. Let's see what happens. What it wanted so it didn't say anything about the dimensions. So xt and yt should both have 100 elements. The error is, can't call numpy and tensor that requires grad use tensor dot detach dot numpy instead so remembering all those operations to convert between numpy arrays and tensors. Those only will work, assuming that basically the tensor is not attached to the computation graph. And so, if I tried to do that it's basically saying hey I'm not done with my computations over this, don't try to take it away from me. But of course, pi plot being pi plot. So it needs to be a numpy array, I can't turn it back into a numpy array until I detach it from the computation graph. So there's this neat function dot detach that you can use to basically say, I've got this tensor, I don't want to do something with it like visualize it or use it as you know an input feature to another operation or something. I need to detach it from the current computation graphs remember when that docstring said that dot backwards accumulates gradients in the leaves. Right. This is one of those leaves. So I need to detach the leaf from the tree that is the graph in order to use it anywhere else. So we will use the detach dot numpy function and again when you're doing assignment, I want to say three. You may run into this issue. So tensor dot detach will return a new tensor, this will be detached from the current graph. And this result will not require grad. So if I have something that requires grad and I detach it from the computation graph, there's not much point in having requires grad or what I'm going to use requires grad for if I'm detaching it from the graph, there's no graph to compute the gradient. So I can do whatever I want with it and not worry about changing that due to other operations having been performed on the graph. And then the dot numpy will convert the tensor to a numpy array. So that's going to go back in familiar territory. So now here we go. So here is xt and yt right that is the blue line. So we have xt being the original input and then yt being the design function and then xt dot detach right that will give me the same x values and then xt dot grad is going to give me the derivative of y. So that's sort of that computation graph magic that we just discussed. And so now, instead of plotting, you know, two separate functions, sine of x and cosine of x, I'm really just using properties of the same two, the same two variables xt and yt to get the values that I need. So, questions. Be a little bit mind bending. So where are we at now? Like halfway through actually have to move faster. Okay. So, now, let me do yt equals the sine of xt, and then I'll run yt dot gradient again, or yt dot backwards. And so now xt dot grad gives me something like this. Hang on, we started to go down close to zero, and then the back at two. But I do know that the derivative of sine of x is cosine of x right that hasn't changed. So what happened. Let me plot it. Now we can see this. Right, so now it's telling me that the blue line is still sine of x, and the orange line computed by xt dot grad now is like cosine of x except it's bounded at two and negative two. So what gives I didn't actually change anything. So the magnitude of the derivative is twice what it should be. And this is because dot backwards is going to add the gradient values to the previous values. So I'd already calculated one gradient that was effectively cosine of x. Then I did it again, but cosine of x is still there. So it's like, okay, I'm just going to take whatever this is and I'm just going to add it to this. So now, my gradient is sitting there at two times the cosine of x. So we must explicitly zero out the gradient. If I do this, then the same addition is true, right, whenever I compute the gradient is going to add it to whatever whatever is in the gradient. But if the gradient is zeroed out, that's zero. And so what I get is the actual gradient that I want. Okay, so use this dot grad zero function, and this will print out the, this will zero out the gradient. So now what I can do here is if I do, you know, four, 10 times, I'll just create, I'll take the gradient of the sine function, and I'll print out that first element. Right. So the first the first term should be one for cosine. Now if I do this, you'll see that I get, you know, one, and then two, and then three, and then four, and then five, all the way to 10. Because it's every time I run this, it's adding that gradient. So I get that accumulation of values. So we add one to the existing value. If I put the grad zero inside of the for loop. Now it's 11111111 so I keep getting the right value here. Okay, so always know you need to be zeroing out your gradients. All right, so now, having done that, what we're going to do, we'll run through some examples of training some simple models using SGD and PyTorch. I'll compare the NumPy version and then the PyTorch version. So here's the NumPy version. This should look pretty familiar to you. So we see our X's and then my T is just not some nonlinear function, a square of X. I'm just going to use my SGD implementation that's similar to what we did in assignment one. And that gives me this. Right. So here is my T and this function Y. This is like the best linear function that I can fit to this. Right. So I'm still doing just linear operations. Now we do it in Torch. So what's different here, right, I'm creating these tensors from the NumPy arrays, and then I create my weights as Torch.zeros. I'm not using autograd yet. I'm still just doing the calculation manually. So gives me an error, expected scalar type long, but found float. So if I look at X, it says it's at 64 bit int. So this error is actually a little bit misleading. In fact, it suggests that some float should be an int, but X is already made of ints. But W is now made of floats. So rather than make W into ints, because integers do not make good weights, typically will make X into floats. Okay. So let me run a version of this again. All I've done is I've just added dot float here. And now this works just fine. Very similar result. Okay. So why are we actually using Torch if it looks identical to our NumPy code? What's the point of this exercise? And just to get you thinking about computation graphs, apparently. So where this really shines, let's take advantage of automated gradients. So if I've got this computation graph, the reason that I do it is that I can automate the calculation of gradients. So why am I dealing around here with this for loop when I can do everything much more cleanly? Okay. So here we go. So same up until this point here. Now instead of the update, I'm causing just calling MSE.backwards. Right. So MSE in this case, this is my loss function. And so to update the gradient, I just call whatever instance of my loss function I've created and just call dot backwards on that. Okay. Because the dot backwards calculates the gradient. So when I was doing like YT dot backwards and YT was the sine function, it's going to calculate the derivative of the sine function. But I'm always calculating the derivative of the loss function. And so I just define what that is and then call dot backwards on that. And then finally, I also do with Torch, no grads. So I can kind of not update the gradient. I can sort of do a temporary. I don't really detach anything from the computation graph. I just sort of pause the reading calculations on that so that I can use that to update the actual weights here. So you can compare that to what we did before in the commented code. And this gives me, again, pretty much the same result. Other things you can do is you can use predefined optimizers. So here, you know, I'm defining, you know, what define optimization function, you know, manually. So here I can do this using some predefined function. So where previously I had defined my SGD operation, I can actually just pull that out of the library. So this is going to look like this. So now I can basically instantiate my optimizer as an instance of Torch.optim.sgd. I can specify what weights I'm going to be optimizing and what the learning rate will be. And then after calling loss dot backwards, I can just call optimizer dot step. And this is basically this thing to perform one step of weight optimization. And so now for every training epoch in 100, I'm going to do one per step. And each time I'm going to zero out the gradient. All right. And this also gives me the same result. So, you know, rather than define SGD or Atom or what have you by ourselves, we can use this predefined optimizer class. And the storage of Optim is this package that implements a bunch of different optimization algorithms. All of them have like, you have to define different parameters for them, right? You have to find like the, of course, the ways you're going to optimize, but also for Atom, you need to define, say, those beta values and things like that. And at this link, you can see a list of all the implemented optimizers that you may want to use. Questions on that so far? The other thing we can do is we can use predefined loss functions. So what was my loss function to this point? What was I using? I'm just doing a regression problem. Mean squared error, right? So I'm trying to minimize the error. So here we go. This part, this is the definition of loss function right there. Now I can optimize that. But maybe I don't feel like doing that. Maybe I forgot the function for mean squared error. Maybe I'm doing a more complicated function like, you know, categorical cross entropy loss. Something that's like, I don't remember how to write this precisely and I don't want to mess it up. So PyTorch will give you the loss function like before. So here I can define this MSE func as torch.nn.msc loss. And so now I just calculate the MSE as this function, whatever it is, over my targets and my predictions. And then I just call it backwards and I can optimize my weights. So if I run this, I do get a problem. So what is this issue? Right. Found D type long but expected expected float. This is similar to the one above. So we see this where we had X made of ints and it wanted floats. This is cropping off the backwards pass. Inputs are T and Y. So we know Y is made of floats as it should be. So therefore the problem must be in T. And so now I can add dot float to T when I define it along with X. So the loss from it is the same. Boom, boom, boom. Here we go. Okay, I actually will talk about torch.nn module. Okay. NN stands, if you haven't guessed already, stands for neural net. So basically this allows me to create a lot of the infrastructure that I'm going to need for neural net and then define my specifics like how many layers and what types of optimizers and loss functions. This is not going to look simpler for our linear model, but if you try to apply it to multi-layered models it will. So all I have to do is define the specifics of my layer with a single line of code each. Okay. So number of inputs, number of outputs looks similar. And then everything is going to be torch.nn.sequential and then I define the layers here. So basically this is saying the only quote layer is just a linear function mapping from input to output. But if I had multiple fully connected layers, I would define each of those in turn. So now if I just print the model, you'll get this nice print out of what the model looks like. And so you can see what all the layers are. This allows you, like if you forget, say, what the input size to a particular layer should be, you can print out the entire model and say, okay, hey, layer four is expecting that things are going to be 256 elements. And so that's where my error is because my input somehow is not 256 elements. So if I print model.parameters, I get this basically this list of the parameters that I've solved for. So in this case, they're just they're random. Right. So I've just got some random weight and some random bias weight. So now I can actually use this to try and solve function. So this looks similar, except all I've done is I've added n inputs. I've added n samples and inputs. And so now I've also added y equals model x. So this is just saying my model is a function. I define it. However, I'm going to run that entire model over this input. So this is sort of the higher level version of running like hidden layer of y. There's all the hidden layers are all the layers in there. I'm going to run all of them single forward pass. And if I do that, then this gives me. Once again, I've solved the problem. You'll notice if you're paying attention, you'll notice that like these are not exactly the same solutions. In fact, some of them have like the orange line is like a little bit higher. So the slope is slightly less than the bias is probably a little bit higher. So remember, these are all estimations. You initialize weights randomly and it optimizes based on your error as best as it can. And so you may get slightly different outputs each time. It is, in fact, perfectly possible to write a neural network that has two inputs that are the number numbers two and three with a single unit. His activation function is a plus sign, and it's going to tell you the answer is four point nine nine nine nine nine. This is the thing that happens sometimes. So just be aware that this is all estimation. So by adding a hidden layer or two, then what I can do is I can describe the structure of the network like this. So we see here torsion and sequential. So one linear layer that maps from size of inputs to that first layer size. So in this case, both hidden layers are 10, 10, each function, right? Define my activation function to what function gets applied after I perform this operation. That output then goes into another linear layer that's going to map it to whatever size and hidden one is. And then another 10 H function that's going to map it to the linear, the output, whatever size that is, in this case, just one. And then everything else is pretty much the same. Run that. So now I'm actually get now instead of a linear function, right, it's able to optimize kind of the curve of the line. It's not like it understands that there's a squared function here or anything, but it is able to match those values pretty closely. We do get a bit of a weird zigzag here at the end. So maybe Adam will do better. So what I can do here is I can, for my optimizer, I will just define torsion.atom.adam instead of .sgd. And then I can change my learning rate appropriately. And then I run that. And there we go. Exactly where it needs to be. So basically optimize to this almost perfectly. OK, so now let's actually make use of the GPU. So it's trivial to move data and operations down to the GPU with PyTorch. So all we need to do, well, first I'll run it without the GPU. So here I'm just going to perform matrix multiplication for 1000 times over some random numbers and run this and we'll see that it takes about a second. So pretty fast. This is running on my lab machine. The CPU itself is quite powerful. But we do have this thing on this machine called CUDA. Torch.cuda.isAvailable will tell you whether your machine has CUDA or not. Generally good practice to write your code to accommodate either in case you end up on a machine that has no GPU or you can't find CUDA. At least you can still run even though it will probably be really slow. So now what I can do is I can do .to CUDA. This is going to say take this tensor and move it to the device. This is not done in line. You have to reassign the variable. So at.to CUDA is not going to change at. It's not actually going to move it. But I do at.to CUDA. Then every time I reference at, it will get the version that's on the GPU. So now we're still running on the CPU. So here we go. CT equals CT.to CPU. Using CUDA on the CPU is not much faster. It's actually slightly slower. Because CUDA is a GPU acceleration library. So why am I using it on the CPU? So what I can do here is I can define this function. Use GPU. And in this case, I have this code written so I can use the Linux machine or my other laptop, which I would have brought except you can't connect to the internet anymore. So now let's compare the speech of the torch.NN model on more data using the GPU and comparing it to CPU version. So first I'll just set use GPU equals false. And then I'll run this nonlinear model. Move everything to the GPU. And this is similar as before. Moving data to GPU. Moving data to GPU. Training took about a second again with GPU. And took 3.15 seconds without it. Okay. So now the torch.NN.module forward function. We just saw how to implement this using this combination of sequential and then linear and 10H layers. And the forward calculation for the neural network is defined this way. So now we can just define a class that extends torch.NN.module and define the forward function explicitly. So here I'll define an N net class that extends this class. And I can define the forward function that this is going to be very similar to the forward pass function in assignment two. So I start with, you know, I just set my y equal to my input. And now this allows me for each hidden layer, I just recompute y as the output or as the operation over the output of the last layer. And then this will give me the final output. So now I can set, you know, larger network, 100 nodes each, a larger learning rate. And then I can move the data to the GPU and run. And we can see that it takes about one second. So basically, the GPU magic makes it such that the larger model you have, the greater speed up you're going to get. The differences we see here with the simple model is like a factor of about three. But then as your model is growing, grow larger, it becomes a factor of 300, 3000 and so on. And so you can define your neural networks, pretty simple syntax, and all the other code kind of remains the same. And this allows you to very easily experiment with like different layer sizes and different hyper parameters and things like that. All right. I think we'll be it for today. So sit down. We're almost done. So, yeah, so we are I think we're done for today. So on Thursday, I will start introduction to classification. Good luck with the homework. And I have like a few minutes if you want to talk, but I'm going to have to leave early today.